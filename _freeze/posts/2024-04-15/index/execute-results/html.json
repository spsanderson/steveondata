{
  "hash": "2cab692d1d80e45706b0065c433f97e1",
  "result": {
    "markdown": "---\ntitle: \"Estimating Chi-Square Distribution Parameters Using R\"\nauthor: \"Steven P. Sanderson II, MPH\"\ndate: \"2024-04-15\"\ncategories: [code, rtip, operations, glue, unglue]\n---\n\n\n# Introduction\n\nIn the world of statistics and data analysis, understanding and accurately estimating the parameters of probability distributions is crucial. One such distribution is the chi-square distribution, often encountered in various statistical analyses. In this blog post, we'll dive into how we can estimate the degrees of freedom (\"df\") and the non-centrality parameter (\"ncp\") of a chi-square distribution using R programming language.\n\n# The Chi-Square Distribution\n\nThe chi-square distribution is a continuous probability distribution that arises in the context of hypothesis testing and confidence interval estimation. It is commonly used in goodness-of-fit tests, tests of independence, and tests of homogeneity.\n\nThe distribution has two main parameters:\n- __Degrees of Freedom (df)__: This parameter determines the shape of the chi-square distribution. It represents the number of independent variables in a statistical test.\n- __Non-Centrality Parameter (ncp)__: This parameter determines the deviation of the distribution from a null hypothesis. It's particularly relevant in non-central chi-square distributions.\n\n# The Goal: Estimating Parameters\n\nOur goal is to create a function within the TidyDensity package that can estimate the df and ncp parameters of a chi-square distribution based on a vector of observed data. Let's walk through the steps involved in achieving this.\n\n# Working Example\n\n## Setting the Stage: Libraries and Data\n\nFirst, we load the necessary libraries: `tidyverse` for data manipulation and `bbmle` for maximum likelihood estimation. We then generate a grid of parameters (degrees of freedom and non-centrality parameter) and sample sizes to create a diverse set of chi-square distributed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(bbmle)\n\n# Data ----\n# Make parameters and grid\ndf <- 1:10\nncp <- 1:10\nn <- runif(10, 250, 500) |> trunc()\nparam_grid <- expand_grid(n = n, df = df, ncp = ncp)\n\nhead(param_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n      n    df   ncp\n  <dbl> <int> <int>\n1   284     1     1\n2   284     1     2\n3   284     1     3\n4   284     1     4\n5   284     1     5\n6   284     1     6\n```\n:::\n:::\n\n## Function Exploration: Unveiling the Estimation Process\n\nThe core of our exploration lies in several functions designed to estimate the chi-square parameters:\n\n`dof`/`k` Functions: These functions focus on estimating the degrees of freedom (df) using different approaches:\n\n*   `mean_x`: Calculates the mean of the data.\n*   `mean_minus_1`: Subtracts 1 from the mean.\n*   `var_div_2`: Divides the variance of the data by 2.\n*   `length_minus_1`: Subtracts 1 from the length of the data.\n\n`ncp` Functions: These functions aim to estimate the non-centrality parameter (ncp) using various methods:\n\n*   `mean_minus_mean_minus_1`: A seemingly trivial calculation that serves as a baseline.\n*   `ie_mean_minus_var_div_2`: Subtracts half the variance from the mean, ensuring the result is non-negative.\n*   `ie_optim`: Utilizes optimization techniques to find the ncp that maximizes the likelihood of observing the data.\n*   `estimate_chisq_params`: This is the main function that employs maximum likelihood estimation (MLE) via the bbmle package to estimate both df and ncp simultaneously. It defines a negative log-likelihood function based on the chi-square distribution and uses mle2 to find the parameter values that minimize this function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Functions ----\n# functions to estimate the parameters of a chisq distribution\n# dof\nmean_x <- function(x) mean(x)\nmean_minus_1 <- function(x) mean(x) - 1\nvar_div_2 <- function(x) var(x) / 2\nlength_minus_1 <- function(x) length(x) - 1\n# ncp\nmean_minus_mean_minus_1 <- function(x) mean(x) - (mean(x) - 1)\nie_mean_minus_var_div_2 <- function(x) ifelse((mean(x) - (var(x) / 2)) < 0, 0, mean(x) - var(x)/2)\nie_optim <- function(x) optim(par = 0,\n                             fn = function(ncp) {\n                               -sum(dchisq(x, df = var(x)/2, ncp = ncp, log = TRUE))\n                             },\n                             method = \"Brent\",\n                             lower = 0,\n                             upper = 10 * var(x)/2)$par\n# both\nestimate_chisq_params <- function(data) {\n  # Negative log-likelihood function\n  negLogLik <- function(df, ncp) {\n    -sum(dchisq(data, df = df, ncp = ncp, log = TRUE))\n  }\n  \n  # Initial values (adjust based on your data if necessary)\n  start_vals <- list(df = trunc(var(data)/2), ncp = trunc(mean(data)))\n  \n  # MLE using bbmle\n  mle_fit <- bbmle::mle2(negLogLik, start = start_vals)\n  # Return estimated parameters as a named vector\n  df <- dplyr::tibble(\n    est_df = coef(mle_fit)[1],\n    est_ncp = coef(mle_fit)[2]\n  )\n  return(df)\n}\n\nsafe_estimates <- {\n  purrr::possibly(\n    estimate_chisq_params,\n    otherwise = NA_real_,\n    quiet = TRUE\n  )\n}\n```\n:::\n\n\n## Simulating and Evaluating: Putting the Functions to the Test\n\nTo assess the performance of our functions, we simulate chi-square data using the parameter grid and apply each function to estimate the parameters. We then compare these estimates to the true values and visualize the results using boxplots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate data ----\nset.seed(123)\ndff <- param_grid |>\n  mutate(x = pmap(pick(everything()), match.fun(\"rchisq\"))) |>\n  mutate(\n    safe_est_parms = map(x, safe_estimates),\n    dfa = map_dbl(x, mean_minus_1),\n    dfb = map_dbl(x, var_div_2),\n    dfc = map_dbl(x, length_minus_1),\n    ncpa = map_dbl(x, mean_minus_mean_minus_1),\n    ncpb = map_dbl(x, ie_mean_minus_var_div_2),\n    ncpc = map_dbl(x, ie_optim)\n  ) |>\n  select(-x) |>\n  filter(map_lgl(safe_est_parms, ~ any(is.na(.x))) == FALSE) |>\n  unnest(cols = safe_est_parms) |>\n  mutate(\n    dfa_resid = dfa - df,\n    dfb_resid = dfb - df,\n    dfc_resid = dfc - df,\n    dfd_resid = est_df - df,\n    ncpa_resid = ncpa - ncp,\n    ncpb_resid = ncpb - ncp,\n    ncpc_resid = ncpc - ncp,\n    ncpd_resid = est_ncp - ncp\n  )\n\nglimpse(dff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 987\nColumns: 19\n$ n          <dbl> 284, 284, 284, 284, 284, 284, 284, 284, 284, 284, 284, 284,…\n$ df         <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ ncp        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ est_df     <dbl> 1.1770904, 0.9905994, 0.9792179, 0.7781877, 1.5161669, 0.82…\n$ est_ncp    <dbl> 0.7231638, 1.9462325, 3.0371756, 4.2347494, 3.7611119, 6.26…\n$ dfa        <dbl> 0.9050589, 1.9826153, 3.0579375, 4.0515312, 4.2022289, 6.15…\n$ dfb        <dbl> 2.626501, 5.428382, 7.297746, 9.265272, 8.465838, 14.597976…\n$ dfc        <dbl> 283, 283, 283, 283, 283, 283, 283, 283, 283, 283, 283, 283,…\n$ ncpa       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ncpb       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ncpc       <dbl> 5.382789e-09, 8.170550e-09, 6.017177e-09, 8.618892e-09, 7.7…\n$ dfa_resid  <dbl> -0.09494109, 0.98261533, 2.05793748, 3.05153121, 3.20222890…\n$ dfb_resid  <dbl> 1.626501, 4.428382, 6.297746, 8.265272, 7.465838, 13.597976…\n$ dfc_resid  <dbl> 282, 282, 282, 282, 282, 282, 282, 282, 282, 282, 281, 281,…\n$ dfd_resid  <dbl> 0.177090434, -0.009400632, -0.020782073, -0.221812344, 0.51…\n$ ncpa_resid <dbl> 0, -1, -2, -3, -4, -5, -6, -7, -8, -9, 0, -1, -2, -3, -4, -…\n$ ncpb_resid <dbl> -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -1, -2, -3, -4, -5…\n$ ncpc_resid <dbl> -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -1, -2, -3, -4, -5…\n$ ncpd_resid <dbl> -0.27683618, -0.05376753, 0.03717560, 0.23474943, -1.238888…\n```\n:::\n:::\n\n\n## Visual Insights: Assessing Estimation Accuracy\n\nThe boxplots reveal interesting insights:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nboxplot(dff$dfa ~ dff$df, main = \"mean(x) -1 ~ df\")\nboxplot(dff$dfa_resid ~ dff$df, main = \"mean(x) -1 ~ df Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$dfb ~ dff$df, main = \"var(x) / 2 ~ df\")\nboxplot(dff$dfb_resid ~ dff$df, main = \"var(x) / 2 ~ df Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$dfc ~ dff$df, main = \"length(x) - 1 ~ df\")\nboxplot(dff$dfc_resid ~ dff$df, main = \"length(x) - 1 ~ df Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$est_df ~ dff$df, main = \"negloglik ~ df - Looks Good\")\nboxplot(dff$dfd_resid ~ dff$df, main = \"negloglik ~ df Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-4.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$ncpa ~ dff$ncp, main = \"mean(x) - (mean(x) - 1) ~ ncp\")\nboxplot(dff$ncpa_resid ~ dff$ncp, main = \"mean(x) - (mean(x) - 1) ~ ncp Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-5.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$ncpb ~ dff$ncp, main = \"mean(x) - var(x)/2 ~ nc\")\nboxplot(dff$ncpb_resid ~ dff$ncp, main = \"mean(x) - var(x)/2 ~ ncp Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-6.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$ncpc ~ dff$ncp, main = \"optim ~ ncp\")\nboxplot(dff$ncpc_resid ~ dff$ncp, main = \"optim ~ ncp Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-7.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$est_ncp ~ dff$ncp, main = \"negloglik ~ ncp - Looks Good\")\nboxplot(dff$ncpd_resid ~ dff$ncp, main = \"negloglik ~ ncp Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-8.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n_`df`_ Estimation:\n\n*   `mean_x - 1 and var(x) / 2` show potential as df estimators but exhibit bias depending on the true df value.\n*   `length(x) - 1` performs poorly, consistently underestimating df.\n*   The MLE approach from `estimate_chisq_params` demonstrates the most accurate and unbiased estimates across different df values.\n\n_`ncp`_ Estimation:\n\n*   The simple methods (`mean(x) - mean(x) - 1` and `mean(x) - var(x) / 2`) show substantial bias and variability.\n*   The optimization-based method (`optim`) performs better but still exhibits some bias.\n*   The MLE approach again emerges as the most reliable option, providing accurate and unbiased estimates across various ncp values.\n\n# Conclusion: The Power of Maximum Likelihood\n\nOur exploration highlights the effectiveness of MLE in estimating the parameters of a chi-square distribution. The estimate_chisq_params function, utilizing the bbmle package, provides a robust and accurate solution for this task. This function will be a valuable addition to the TidyDensity package, empowering users to delve deeper into the analysis of chi-square distributed data.\n\nStay tuned for further developments and exciting additions to the TidyDensity package!",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}