{
  "hash": "c84e5c92d1caadb25ea36f695b9ba421",
  "result": {
    "markdown": "---\ntitle: \"Decoding the Mystery: How to Interpret Regression Output in R Like a Champ\"\nauthor: \"Steven P. Sanderson II, MPH\"\ndate: \"2023-12-14\"\ncategories: [rtip, regression]\n---\n\n\n# Introduction\n\nEver run an R regression and stared at the output, feeling like you're deciphering an ancient scroll? Fear not, fellow data enthusiasts! Today, we'll crack the code and turn those statistics into meaningful insights. \n\n**Let's grab our trusty R arsenal and set up the scene:**\n\n* **Dataset:** `mtcars` (a classic car dataset in R)\n* **Regression:** Linear model with `mpg` as the dependent variable (miles per gallon) and all other variables as independent variables (predictors)\n\n# Step 1: Summon the Stats Gods with \"summary()\"\n\nFirst, cast your R spell with `summary(lm(mpg ~ ., data = mtcars))`. This incantation conjures a table of coefficients, p-values, and other stats. Don't panic if it looks like a cryptic riddle! We'll break it down:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mpg ~ ., data = mtcars)\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869,\tAdjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n```\n:::\n:::\n\n\n## Coefficients\n\nThese tell you how much, on average, the dependent variable changes for a one-unit increase in the corresponding independent variable (holding other variables constant). For example, a coefficient of 0.05 for `cyl` means for every one more cylinder, mpg is expected to increase by 0.05 miles per gallon, on average.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         cyl        disp          hp        drat          wt \n12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 \n       qsec          vs          am        gear        carb \n 0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925 \n```\n:::\n:::\n\n\n## P-values\n\nThese whisper secrets about significance. A p-value less than 0.05 (like for `wt`!) means the observed relationship between the variable and mpg is unlikely to be due to chance. The following are the individual p-values for each variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$coefficients[, 4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         cyl        disp          hp        drat          wt \n 0.51812440  0.91608738  0.46348865  0.33495531  0.63527790  0.06325215 \n       qsec          vs          am        gear        carb \n 0.27394127  0.88142347  0.23398971  0.66520643  0.81217871 \n```\n:::\n:::\n\n\nNow the overall p-value for the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_p <- function(.model) {\n  \n  # Get p-values\n  fstat <- summary(.model)$fstatistic\n  p <- pf(fstat[1], fstat[2], fstat[3], lower.tail = FALSE)\n  print(p)\n}\n\nmodel_p(.model = model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       value \n3.793152e-07 \n```\n:::\n:::\n\n\n# Step 2: Let's Talk Turkey - Interpreting the Numbers\n\n## Coefficients\n\nThink of them as slopes. A positive coefficient means the dependent variable increases with the independent variable. Negative? The opposite! For example, `disp` has a negative coefficient, so bigger engines (larger displacement) tend to have lower mpg.\n\n## P-values \n\nImagine a courtroom. A low p-value is like a strong witness, convincing you the relationship between the variables is real. High p-values (like for `am`!) are like unreliable witnesses, leaving us unsure.\n\n# Step 3: Zoom Out - The Bigger Picture\n\n## R-squared\n\nThis tells you how well the model explains the variation in mpg. A value close to 1 is fantastic, while closer to 0 means the model needs work. In our case, it's not bad, but there's room for improvement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8690158\n```\n:::\n:::\n\n\n## Residuals\n\nThese are the differences between the actual mpg values and the model's predictions. Analyzing them can reveal hidden patterns and model issues.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(model$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    model.residuals\nMazda RX4              -1.599505761\nMazda RX4 Wag          -1.111886079\nDatsun 710             -3.450644085\nHornet 4 Drive          0.162595453\nHornet Sportabout       1.006565971\nValiant                -2.283039036\nDuster 360             -0.086256253\nMerc 240D               1.903988115\nMerc 230               -1.619089898\nMerc 280                0.500970058\nMerc 280C              -1.391654392\nMerc 450SE              2.227837890\nMerc 450SL              1.700426404\nMerc 450SLC            -0.542224699\nCadillac Fleetwood     -1.634013415\nLincoln Continental    -0.536437711\nChrysler Imperial       4.206370638\nFiat 128                4.627094192\nHonda Civic             0.503261089\nToyota Corolla          4.387630904\nToyota Corona          -2.143103442\nDodge Challenger       -1.443053221\nAMC Javelin            -2.532181498\nCamaro Z28             -0.006021976\nPontiac Firebird        2.508321011\nFiat X1-9              -0.993468693\nPorsche 914-2          -0.152953961\nLotus Europa            2.763727417\nFord Pantera L         -3.070040803\nFerrari Dino            0.006171846\nMaserati Bora           1.058881618\nVolvo 142E             -2.968267683\n```\n:::\n:::\n\n\n**Bonus Tip:** Visualize the data! Scatter plots and other graphs can make relationships between variables pop.\n\n**Remember:** Interpreting regression output is an art, not a science. Use your domain knowledge, consider the context, and don't hesitate to explore further!\n\n**So next time you face regression output, channel your inner R wizard and remember:**\n\n* Coefficients whisper about slopes and changes.\n* P-values tell tales of significance, true or false.\n* R-squared unveils the model's explanatory magic.\n* Residuals hold hidden clues, waiting to be discovered.\n\nWith these tools in your belt, you'll be interpreting regression output like a pro in no time! Now go forth and conquer the data, fellow R adventurers!\n\n**Note:** This is just a brief example. For a deeper dive, explore specific diagnostics, model selection techniques, and other advanced topics to truly master the art of regression interpretation.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}