{
  "hash": "975cea5e010fa20c673aa1fd7390d515",
  "result": {
    "markdown": "---\ntitle: \"The new function on the block with tidyAML `extract_regression_residuals()`\"\nauthor: \"Steven P. Sanderson II, MPH\"\ndate: \"2024-01-19\"\ncategories: [code, rtip, tidyaml]\n---\n\n\n# Introduction\n\nYesterday I discussed the use of the function `internal_make_wflw_predictions()` in the `tidyAML` R package. Today I will discuss the use of the function `extract_wflw_pred()` and the brand new function `extract_regression_residuals()` in the `tidyAML` R package. We breifly saw yesterday the output of the function `internal_make_wflw_predictions()` which is a list of tibbles that are typically inside of a list column in the final output of `fast_regression()` and `fast_classification()`. The function `extract_wflw_pred()` takes this list of tibbles and extracts them from that output. The function `extract_regression_residuals()` also extracts those tibbles and has the added feature of also returning the residuals. Let's see how these functions work.\n\n# The new function\n\nFirst, we will go over the syntax of the new function `extract_regression_residuals()`. \n\n```R\nextract_regression_residuals(.model_tbl, .pivot_long = FALSE)\n```\n\nThe function takes two arguments. The first argument is `.model_tbl` which is the output of `fast_regression()` or `fast_classification()`. The second argument is `.pivot_long` which is a logical argument that defaults to `FALSE`. If `TRUE` then the output will be in a long format. If `FALSE` then the output will be in a wide format. Let's see how this works.\n\n# Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(tidyAML)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(multilevelmod) # for the gee model\n\ntidymodels_prefer() # good practice when using tidyAML\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"stan\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n  )\n```\n:::\n\n\nLet's break down the R code step by step:\n\n1. **Loading Libraries:**\n  ```R\n  library(tidyAML)\n  library(tidymodels)\n  library(tidyverse)\n  library(multilevelmod) # for the gee model\n  ```\nHere, the code is loading several R packages. These packages provide functions and tools for data analysis, modeling, and visualization. `tidyAML` and `tidymodels` are particularly relevant for modeling, while `tidyverse` is a collection of packages for data manipulation and visualization. `multilevelmod` is included for the Generalized Estimating Equations (gee) model.\n\n2. **Setting Preferences:**\n   ```R\n   tidymodels_prefer() # good practice when using tidyAML\n   ```\n\nThis line of code is setting preferences for the tidy modeling workflow using `tidymodels_prefer()`. It ensures that when using `tidyAML`, the tidy modeling conventions are followed. Tidy modeling involves an organized and consistent approach to modeling in R.\n\n3. **Creating a Recipe Object:**\n   ```R\n   rec_obj <- recipe(mpg ~ ., data = mtcars)\n   ```\n\nHere, a recipe object (`rec_obj`) is created using the `recipe` function from the `tidymodels` package. The formula `mpg ~ .` specifies that we want to predict the `mpg` variable based on all other variables in the dataset (`mtcars`).\n\n4. **Performing Fast Regression:**\n   ```R\n   frt_tbl <- fast_regression(\n     .data = mtcars, \n     .rec_obj = rec_obj, \n     .parsnip_eng = c(\"lm\",\"glm\",\"stan\",\"gee\"),\n     .parsnip_fns = \"linear_reg\"\n   )\n   ```\n\nThis part involves using the `fast_regression` function. It performs a fast regression analysis using various engines specified by `.parsnip_eng` and specific functions specified by `.parsnip_fns`. In this case, it includes linear models (`lm`), generalized linear models (`glm`), Stan models (`stan`), and the Generalized Estimating Equations model (`gee`). The results are stored in the `frt_tbl` table.\n\nIn summary, the code is setting up a tidy modeling workflow, creating a recipe for predicting `mpg` based on other variables in the `mtcars` dataset, and then performing a fast regression using different engines and functions. The choice of engines and functions allows flexibility in exploring different modeling approaches.\n\nNow that we have the output of `fast_regression()` stored in `frt_tbl`, we can use the function `extract_wflw_pred()` to extract the predictions and from the output. Let's see how this works. First, the syntax:\n\n```R\nextract_wflw_pred(.data, .model_id = NULL)\n```\n\nThe function takes two arguments. The first argument is `.data` which is the output of `fast_regression()` or `fast_classification()`. The second argument is `.model_id` which is a numeric vector that defaults to `NULL`. If `NULL` then the function will extract none of the predictions from the output. If a numeric vector is provided then the function will extract the predictions for the models specified by the numeric vector. Let's see how this works.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_wflw_pred(frt_tbl, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 64 × 4\n   .model_type     .data_category .data_type .value\n   <chr>           <chr>          <chr>       <dbl>\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 54 more rows\n```\n:::\n\n```{.r .cell-code}\nextract_wflw_pred(frt_tbl, 1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 128 × 4\n   .model_type     .data_category .data_type .value\n   <chr>           <chr>          <chr>       <dbl>\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 118 more rows\n```\n:::\n\n```{.r .cell-code}\nextract_wflw_pred(frt_tbl, 1:nrow(frt_tbl))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 256 × 4\n   .model_type     .data_category .data_type .value\n   <chr>           <chr>          <chr>       <dbl>\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 246 more rows\n```\n:::\n:::\n\n\nThe first line of code extracts the predictions for the first model in the output. The second line of code extracts the predictions for the first two models in the output. The third line of code extracts the predictions for all models in the output.\n\nNow, let's visualize the predictions for the models in the output and the actual values. We will use the `ggplot2` package for visualization. First, we will extract the predictions for all models in the output and store them in a table called `pred_tbl`. Then, we will use `ggplot2` to visualize the predictions and actual values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_tbl <- extract_wflw_pred(frt_tbl, 1:nrow(frt_tbl))\n\npred_tbl |>\n  group_split(.model_type) |>\n  map(\\(x) x |>\n        group_by(.data_category) |>\n        mutate(x = row_number()) |>\n        ungroup() |>\n        pivot_wider(names_from = .data_type, values_from = .value) |>\n        ggplot(aes(x = x, y = actual, group = .data_category)) +\n        geom_line(color = \"black\") +\n        geom_line(aes(x = x, y = training), linetype = \"dashed\", color = \"red\",\n                  linewidth = 1) +\n        geom_line(aes(x = x, y = testing), linetype = \"dashed\", color = \"blue\",\n                  linewidth = 1) +\n        theme_minimal() +\n        labs(\n          x = \"\",\n          y = \"Observed/Predicted Value\",\n          title = \"Observed vs. Predicted Values by Model Type\",\n          subtitle = x$.model_type[1]\n        )\n      )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[2]]\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[3]]\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[4]]\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n:::\n\n\nOr we can facet them by model type:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_tbl |>\n  group_by(.model_type, .data_category) |>\n  mutate(x = row_number()) |>\n  ungroup() |>\n  ggplot(aes(x = x, y = .value)) +\n  geom_line(data = . %>% filter(.data_type == \"actual\"), color = \"black\") +\n  geom_line(data = . %>% filter(.data_type == \"training\"), \n            linetype = \"dashed\", color = \"red\") +\n  geom_line(data = . %>% filter(.data_type == \"testing\"), \n            linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ .model_type, ncol = 2, scales = \"free\") +\n  labs(\n    x = \"\",\n    y = \"Observed/Predicted Value\",\n    title = \"Observed vs. Predicted Values by Model Type\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nOk, so what about this new function I talked about above? Well let's go over it here. We have already discussed it's syntax so no need to go over it again. Let's just jump right into an example. This function will return the residuals for all models. We will slice off just the first model for demonstration purposes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_regression_residuals(.model_tbl = frt_tbl, .pivot_long = FALSE)[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 × 4\n   .model_type     .actual .predicted .resid\n   <chr>             <dbl>      <dbl>  <dbl>\n 1 lm - linear_reg    15.2       17.3 -2.09 \n 2 lm - linear_reg    10.4       11.9 -1.46 \n 3 lm - linear_reg    33.9       30.8  3.06 \n 4 lm - linear_reg    32.4       28.0  4.35 \n 5 lm - linear_reg    16.4       15.0  1.40 \n 6 lm - linear_reg    21.5       22.3 -0.779\n 7 lm - linear_reg    15.8       17.2 -1.40 \n 8 lm - linear_reg    15         15.1 -0.100\n 9 lm - linear_reg    14.7       10.9  3.85 \n10 lm - linear_reg    10.4       10.8 -0.445\n# ℹ 22 more rows\n```\n:::\n:::\n\n\nNow let's set `.pivot_long = TRUE`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_regression_residuals(.model_tbl = frt_tbl, .pivot_long = TRUE)[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 96 × 3\n   .model_type     name       value\n   <chr>           <chr>      <dbl>\n 1 lm - linear_reg .actual    15.2 \n 2 lm - linear_reg .predicted 17.3 \n 3 lm - linear_reg .resid     -2.09\n 4 lm - linear_reg .actual    10.4 \n 5 lm - linear_reg .predicted 11.9 \n 6 lm - linear_reg .resid     -1.46\n 7 lm - linear_reg .actual    33.9 \n 8 lm - linear_reg .predicted 30.8 \n 9 lm - linear_reg .resid      3.06\n10 lm - linear_reg .actual    32.4 \n# ℹ 86 more rows\n```\n:::\n:::\n\n\nNow let's visualize the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid_tbl <- extract_regression_residuals(frt_tbl, TRUE)\n\nresid_tbl |>\n  map(\\(x) x |>\n        group_by(name) |>\n        mutate(x = row_number()) |>\n        ungroup() |>\n        mutate(plot_group = ifelse(name == \".resid\", \"Residuals\", \"Actual and Predictions\")) |>\n        ggplot(aes(x = x, y = value, group = name, color = name)) +\n        geom_line() +\n        theme_minimal() +\n        facet_wrap(~ plot_group, ncol = 1, scales = \"free\") +\n        labs(\n          x = \"\",\n          y = \"Value\",\n          title = \"Actual, Predicted, and Residual Values by Model Type\",\n          subtitle = x$.model_type[1],\n          color = \"Data Type\"\n        )\n      )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[2]]\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[3]]\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[4]]\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-4.png){width=672}\n:::\n:::\n\n\nAnd that's it!\n\nThank you for reading and I would love to hear your feedback. Please feel free to reach out to me.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}