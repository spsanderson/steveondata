{
  "hash": "95bca6a4b960545d794b935a49662288",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simplify Regression Modeling with tidyAML’s `fast_regression()`\"\nauthor: \"Steven P. Sanderson II, MPH\"\ndate: \"2024-07-18\"\ncategories: [code, rtip, tidyaml]\ntoc: TRUE\n---\n\n\n# Introduction\n\nIf you’ve ever faced the daunting task of setting up multiple regression models in R, you'll appreciate the convenience and efficiency that `tidyAML` brings to the table. Today, we're diving into one of its standout functions: `fast_regression()`. This function is designed to streamline the regression modeling process, allowing you to quickly create and evaluate a variety of model specifications with minimal code.\n\n# Introduction to `fast_regression()`\n\nThe `fast_regression()` function is part of the `tidyAML` package, a toolkit that simplifies machine learning workflows in R. This function takes your data and recipe object and generates multiple regression models, using a variety of engines and functions from the `parsnip` package.\n\n## Syntax\n\nHere’s a look at the function’s syntax:\n\n```r\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL,\n  .drop_na = TRUE\n)\n```\n\n## Arguments\n\n-   **.data**: The data frame to be used in the regression problem.\n-   **.rec_obj**: A recipe object from the `recipes` package that defines the pre-processing steps.\n-   **.parsnip_fns**: Specifies which parsnip functions to use. The default `\"all\"` will create all possible regression model specifications.\n-   **.parsnip_eng**: Specifies which parsnip engines to use. The default `\"all\"` will create all possible regression model specifications.\n-   **.split_type**: Defines the type of data split, with `\"initial_split\"` as the default. Other split types supported by `rsample` can also be used.\n-   **.split_args**: Additional arguments for the split type. When set to `NULL`, default parameters for the chosen split type are used.\n-   **.drop_na**: Determines whether to drop NA values from the data. Default is `TRUE`.\n\n# Example: Using `fast_regression()`\n\nLet’s see `fast_regression()` in action with a simple example. We’ll use the well-known `mtcars` dataset, and set up a basic recipe to predict miles per gallon (mpg).\n\nFirst, ensure you have the necessary packages loaded:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyAML)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(purrr)\n```\n:::\n\n\nNext, we create a recipe object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_obj <- recipe(mpg ~ ., data = mtcars)\n\nrec_obj\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 10\n```\n\n\n:::\n:::\n\n\nNow, we can run `fast_regression()` to create regression models using the `lm` and `glm` engines with a `linear_reg` parsnip function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfast_reg_tbl <- fast_regression(\n  .data = mtcars,\n  .rec_obj = rec_obj,\n  .parsnip_eng = c(\"lm\", \"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n```\n:::\n\n\n### Output\n\nThe function returns a tibble with details about the generated models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfast_reg_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec wflw      \n      <int> <chr>           <chr>         <chr>        <list>     <list>    \n1         1 lm              regression    linear_reg   <spec[+]>  <workflow>\n2         2 glm             regression    linear_reg   <spec[+]>  <workflow>\n# ℹ 2 more variables: fitted_wflw <list>, pred_wflw <list>\n```\n\n\n:::\n:::\n\n\n### Explanation\n\n-   **.model_id**: A unique identifier for each model.\n-   **.parsnip_engine**: The engine used by `parsnip` (e.g., `lm`, `glm`).\n-   **.parsnip_mode**: The mode of the model, typically `regression`.\n-   **.parsnip_fns**: The `parsnip` function used (e.g., `linear_reg`).\n-   **model_spec**: The model specification.\n-   **wflw**: The workflow object.\n-   **fitted_wflw**: The fitted workflow object.\n-   **pred_wflw**: The predictions from the fitted workflow.\n\n### Benefits of `fast_regression()`\n\n-   **Efficiency**: Quickly set up and evaluate multiple regression models with different specifications.\n-   **Flexibility**: Supports a wide range of engines and functions, allowing for comprehensive model exploration.\n-   **Integration**: Seamlessly integrates with the `recipes`, `parsnip`, and `rsample` packages, making it a versatile tool in your modeling arsenal.\n\n### Let's Inspect\n\nNow that we have our models, let's take a closer look at the results. We can use the `extract_regression_residuals()` function to extract the residuals from the fitted models along with the original and predicted data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_regression_residuals(fast_reg_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n# A tibble: 32 × 4\n   .model_type     .actual .predicted  .resid\n   <chr>             <dbl>      <dbl>   <dbl>\n 1 lm - linear_reg    21.4       21.1  0.276 \n 2 lm - linear_reg    21         23.9 -2.91  \n 3 lm - linear_reg    19.7       18.7  1.01  \n 4 lm - linear_reg    18.1       19.7 -1.55  \n 5 lm - linear_reg    14.7       12.0  2.73  \n 6 lm - linear_reg    24.4       23.7  0.694 \n 7 lm - linear_reg    22.8       24.3 -1.48  \n 8 lm - linear_reg    13.3       12.5  0.820 \n 9 lm - linear_reg    10.4       13.2 -2.85  \n10 lm - linear_reg    19.2       19.3 -0.0583\n# ℹ 22 more rows\n\n[[2]]\n# A tibble: 32 × 4\n   .model_type      .actual .predicted  .resid\n   <chr>              <dbl>      <dbl>   <dbl>\n 1 glm - linear_reg    21.4       21.1  0.276 \n 2 glm - linear_reg    21         23.9 -2.91  \n 3 glm - linear_reg    19.7       18.7  1.01  \n 4 glm - linear_reg    18.1       19.7 -1.55  \n 5 glm - linear_reg    14.7       12.0  2.73  \n 6 glm - linear_reg    24.4       23.7  0.694 \n 7 glm - linear_reg    22.8       24.3 -1.48  \n 8 glm - linear_reg    13.3       12.5  0.820 \n 9 glm - linear_reg    10.4       13.2 -2.85  \n10 glm - linear_reg    19.2       19.3 -0.0583\n# ℹ 22 more rows\n```\n\n\n:::\n:::\n\n\nNow of course we must now visualize! Let's plot the residuals to see how well our models are performing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_regression_residuals(fast_reg_tbl) |> \n  plot_regression_residuals()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n[[2]]\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\nNow the residuals are plotted, we can see how well our models are performing. This is a great way to visually inspect the quality of our models and identify any potential issues.\n\nLet's look at the predicted vs actual only now:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_wflw_pred(fast_reg_tbl,1:nrow(fast_reg_tbl)) |>\n  plot_regression_predictions()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n[[2]]\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\n# Conclusion\n\nThe `fast_regression()` function in `tidyAML` is a powerful addition to any data scientist’s toolkit, providing a streamlined approach to regression modeling. Whether you’re a seasoned pro or just getting started with machine learning in R, this function can save you time and effort, allowing you to focus on what really matters – interpreting and acting on your results.\n\nGive `fast_regression()` a try in your next project and experience the ease and efficiency it brings to regression modeling! If you have any questions or want to share your experience, feel free to leave a comment below.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}