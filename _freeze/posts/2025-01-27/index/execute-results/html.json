{
  "hash": "e51501cedc30bfc886dd672e4e15f9a2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How to Count Duplicates in R: A Comprehensive Guide with Base R, dplyr, and data.table Examples\"\nauthor: \"Steven P. Sanderson II, MPH\"\ndate: \"2025-01-27\"\ncategories: [code, rtip]\ntoc: TRUE\ndescription: \"Learn multiple methods to count duplicates in R using base R, dplyr, and data.table. Includes practical examples, performance tips, and best practices for efficient duplicate detection.\"\nkeywords: [Programming, Count duplicates in R, R programming duplicates, R data analysis, R data cleaning, R duplicate detection, dplyr count duplicates, data.table duplicate counting, base R duplicate functions, identify duplicates in R, R unique values, How to count duplicates in R using dplyr, Efficient methods for counting duplicates in large datasets in R, Step-by-step guide to finding duplicate values in R, Comparing base R and data.table for duplicate detection, Best practices for handling duplicates in R data frames]\n---\n\n\n\n# Introduction\n\nCounting duplicates is a fundamental task in data analysis and cleaning. As an R programmer working with healthcare data at Stony Brook Medicine, I've encountered numerous scenarios where identifying and counting duplicates is crucial for data quality assurance. This guide covers multiple approaches using base R, dplyr, and data.table.\n\n# Understanding Duplicates in R\n\nBefore diving into methods, let's create sample data to work with:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample patient data\npatient_data <- data.frame(\n  patient_id = c(101, 102, 101, 103, 102, 104),\n  visit_date = c(\"2025-01-01\", \"2025-01-01\", \"2025-01-02\", \n                 \"2025-01-02\", \"2025-01-03\", \"2025-01-03\")\n)\n```\n:::\n\n\n\n# Base R Methods\n\n## Using duplicated() Function\n\nThe most straightforward approach in base R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count all duplicates\nsum(duplicated(patient_data$patient_id))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get duplicate counts for each value\ntable(patient_data$patient_id)[table(patient_data$patient_id) > 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n101 102 \n  2   2 \n```\n\n\n:::\n:::\n\n\n\n## Using table() Function\n\nA more detailed view of frequencies:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get frequency count of all values\npatient_counts <- table(patient_data$patient_id)\nprint(patient_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n101 102 103 104 \n  2   2   1   1 \n```\n\n\n:::\n:::\n\n\n\n# Modern Approaches with dplyr\n\n## Using group_by() and count()\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\npatient_data %>%\n  group_by(patient_id) %>%\n  count() %>%\n  filter(n > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 2\n# Groups:   patient_id [2]\n  patient_id     n\n       <dbl> <int>\n1        101     2\n2        102     2\n```\n\n\n:::\n:::\n\n\n\n## Advanced dplyr Techniques\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count duplicates across multiple columns\npatient_data %>%\n  group_by(patient_id, visit_date) %>%\n  summarise(count = n(), .groups = 'drop') %>%\n  filter(count > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 0 Ã— 3\n# â„¹ 3 variables: patient_id <dbl>, visit_date <chr>, count <int>\n```\n\n\n:::\n:::\n\n\n\n# High-Performance Solutions with data.table\n\nFor large healthcare datasets, data.table offers superior performance:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\ndt_patients <- as.data.table(patient_data)\n\n# Count duplicates\ndt_patients[, .N, by = patient_id][N > 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   patient_id     N\n        <num> <int>\n1:        101     2\n2:        102     2\n```\n\n\n:::\n:::\n\n\n\n# Your Turn!\n\nTry this exercise:\n\nProblem: Create a function that returns both the count of duplicates and the duplicate values from a vector.\n\n``` r\n# Your code here\n```\n\n<details>\n\n<summary>Click here for Solution!</summary>\n\nSolution:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_duplicates <- function(x) {\n  dup_counts <- table(x)\n  list(\n    duplicate_values = names(dup_counts[dup_counts > 1]),\n    counts = dup_counts[dup_counts > 1]\n  )\n}\n\n# Test the function\ntest_vector <- c(1, 2, 2, 3, 3, 3, 4)\ncount_duplicates(test_vector)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$duplicate_values\n[1] \"2\" \"3\"\n\n$counts\nx\n2 3 \n2 3 \n```\n\n\n:::\n:::\n\n\n\n</details>\n\n# Quick Takeaways\n\n-   Base R's duplicated() is perfect for simple cases\n-   dplyr offers readable and chainable operations\n-   data.table provides the best performance for large datasets\n-   Consider memory usage when working with large healthcare datasets\n\n# Conclusion\n\nChoosing the right method for counting duplicates depends on your specific needs. For healthcare data analysis, I recommend using data.table for large datasets and dplyr for better code readability in smaller datasets.\n\n# Frequently Asked Questions\n\n1.  Q: Which method is fastest for large datasets? A: data.table consistently outperforms other methods for large datasets.\n\n2.  Q: Can these methods handle missing values? A: Yes, all methods can handle NA values, but you may need to specify na.rm = TRUE.\n\n3.  Q: How do I count duplicates across multiple columns? A: Use group_by() with multiple columns in dplyr or multiple columns in data.table's by parameter.\n\n4.  Q: Will these methods work with character vectors? A: Yes, all methods work with character, numeric, and factor data types.\n\n5.  Q: How can I improve performance when working with millions of rows? A: Use data.table and consider indexing frequently used columns.\n\n# Share Your Experience\n\nIf you've found this guide helpful, consider sharing it with your R programming colleagues. Have you discovered other efficient methods for counting duplicates? Share your approaches in the comments below.\n\n------------------------------------------------------------------------\n\nHappy Coding! ðŸš€\n\n![Duplicates?](todays_post.png)\n\n------------------------------------------------------------------------\n\n*You can connect with me at any one of the below*:\n\n*Telegram Channel here*: <https://t.me/steveondata>\n\n*LinkedIn Network here*: <https://www.linkedin.com/in/spsanderson/>\n\n*Mastadon Social here*: [https://mstdn.social/\\@stevensanderson](https://mstdn.social/@stevensanderson)\n\n*RStats Network here*: [https://rstats.me/\\@spsanderson](https://rstats.me/@spsanderson)\n\n*GitHub Network here*: <https://github.com/spsanderson>\n\n*Bluesky Network here*: <https://bsky.app/profile/spsanderson.com>\n\n*My Book: Extending Excel with Python and R* here: <https://packt.link/oTyZJ>\n\n------------------------------------------------------------------------\n\n\n\n```{=html}\n<script src=\"https://giscus.app/client.js\"\n        data-repo=\"spsanderson/steveondata\"\n        data-repo-id=\"R_kgDOIIxnLw\"\n        data-category=\"Comments\"\n        data-category-id=\"DIC_kwDOIIxnL84ChTk8\"\n        data-mapping=\"url\"\n        data-strict=\"0\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"top\"\n        data-theme=\"dark\"\n        data-lang=\"en\"\n        data-loading=\"lazy\"\n        crossorigin=\"anonymous\"\n        async>\n</script>\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}