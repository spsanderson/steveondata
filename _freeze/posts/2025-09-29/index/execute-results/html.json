{
  "hash": "e3b3177cffe3ed8cb81ae82d54fe25d7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data Imputation and Scaling with healthyR.ai: A Guide for R Programmers\"\nauthor: \"Steven P. Sanderson II, MPH\"\ndate: \"2025-09-29\"\ncategories: [code, rtip]\ntoc: TRUE\ndescription: \"Learn how to efficiently handle missing data and scale variables in R using healthyR.ai. This guide explains imputation and scaling methods with clear syntax, practical examples, and best practices for streamlined data preprocessing in your machine learning projects.\"\nkeywords: [Programming, data imputation, data scaling, healthyR.ai, R data preprocessing, R imputation methods, R scaling functions, missing data handling in R, hai_data_impute example, hai_data_scale tutorial, tidymodels data preprocessing, how to impute missing values in R with healthyR.ai, step by step data scaling using hai_data_scale in R, best practices for data preprocessing in R with healthyR.ai, rolling window imputation example using healthyR.ai package, combining imputation and scaling in R with healthyR.ai functions]\n---\n\n> This guide covers some data preprocessing techniques using healthyR.ai, focusing on imputation and scaling functions with clear syntax examples and best practices.\n\n# Introduction\n\nData preprocessing is a necessary step in any machine learning workflow. The healthyR.ai package offers user-friendly functions for imputation (filling missing values) and scaling (normalizing data) that integrate seamlessly with the tidymodels ecosystem in R. This guide explains the syntax and implementation of these functions in straightforward terms.\n\n# Data Imputation with hai_data_impute()\n\nImputation replaces missing values in your dataset. The `hai_data_impute()` function supports multiple imputation methods through a consistent interface.\n\n## Basic Syntax\n\n```r\nhai_data_impute(\n  .recipe_object = NULL,\n  ...,\n  .type_of_imputation = \"mean\",\n  .seed_value = 123,\n  # Additional parameters based on method\n)\n```\n\nKey arguments:\n\n- `.recipe_object`: Recipe object containing your data\n- `...`: Variables to impute (using selector functions)\n- `.type_of_imputation`: Method for imputation\n- Method-specific parameters (e.g., `.neighbors` for KNN)\n\n## Supported Imputation Methods\n\n| Method    | Description                            | Best For                        | Key Parameters         |\n|-----------|----------------------------------------|---------------------------------|------------------------|\n| \"mean\"    | Replace with column mean               | Normal distributions            | `.mean_trim`           |\n| \"median\"  | Replace with column median             | Skewed data, outliers present   | None                   |\n| \"mode\"    | Replace with most frequent value       | Categorical variables           | None                   |\n| \"knn\"     | K-nearest neighbors imputation         | Complex relationships           | `.neighbors` (default: 5) |\n| \"bagged\"  | Bagged tree imputation                 | Non-linear patterns             | `.number_of_trees` (default: 25) |\n| \"roll\"    | Rolling window statistic               | Time series data                | `.roll_window`, `.roll_statistic` |\n\n## Example: Rolling Median Imputation\n\n```r\nlibrary(healthyR.ai)\nlibrary(recipes)\nlibrary(dplyr)\n\n# Create recipe object\nrec_obj <- recipe(value ~ ., df_tbl)\n\n# Apply rolling median imputation\nimputed_data <- hai_data_impute(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_imputation = \"roll\",\n  .roll_statistic = median\n)$impute_rec_obj %>%\n  get_juiced_data()\n```\n\n# Data Scaling with hai_data_scale()\n\nScaling transforms variables to a common scale, which is important for many machine learning algorithms.\n\n## Basic Syntax\n\n```r\nhai_data_scale(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"center\",\n  .range_min = 0,\n  .range_max = 1,\n  .scale_factor = 1\n)\n```\n\nKey arguments:\n\n- `.recipe_object`: Recipe object containing your data\n- `...`: Variables to scale (using selector functions)\n- `.type_of_scale`: Method for scaling\n- `.range_min`, `.range_max`: Range bounds (for \"range\" method)\n- `.scale_factor`: Scale by 1 or 2 standard deviations (for interpretability)\n\n## Supported Scaling Methods\n\n| Method      | Description                  | Formula            | Result Range              |\n|-------------|------------------------------|--------------------|-----------------------------|\n| \"center\"    | Subtract mean                | x - mean(x)        | Mean = 0, original variance |\n| \"scale\"     | Divide by standard deviation | x / sd(x)          | Standard deviation = 1     |\n| \"normalize\" | Scale to unit norm           | x / \\|\\|x\\|\\|      | Vector length = 1          |\n| \"range\"     | Min-max scaling              | (x-min)/(max-min)  | [range_min, range_max]     |\n\n## Example: Standardization\n\n```r\nlibrary(healthyR.ai)\nlibrary(recipes)\nlibrary(dplyr)\n\n# Create recipe object\nrec_obj <- recipe(value ~ ., df_tbl)\n\n# Apply standardization (z-score)\nscaled_data <- hai_data_scale(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_scale = \"scale\"\n)$scale_rec_obj %>%\n  get_juiced_data()\n```\n\n# Combining Imputation and Scaling\n\nA typical preprocessing workflow combines both steps:\n\n```r\n# Create recipe\nrec_obj <- recipe(target ~ ., data_df)\n\n# Step 1: Impute missing values\nimputed <- hai_data_impute(\n  .recipe_object = rec_obj,\n  all_numeric(),\n  .type_of_imputation = \"median\"\n)$impute_rec_obj\n\n# Step 2: Scale the imputed data\nfinal_data <- hai_data_scale(\n  .recipe_object = imputed,\n  all_numeric(),\n  .type_of_scale = \"range\"\n)$scale_rec_obj %>%\n  get_juiced_data()\n```\n\n# Exampls\n\nI think things work best when you can see an example in action.\n\n## Imputation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(healthyR.ai)\nlibrary(recipes)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\n\nn <- 10L\nl <- 5L\nlo <- n * l\n\ndate_seq <- seq.Date(from = as.Date(\"2013-01-01\"), length.out = lo, by = \"month\")\ndate_seq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"2013-01-01\" \"2013-02-01\" \"2013-03-01\" \"2013-04-01\" \"2013-05-01\"\n [6] \"2013-06-01\" \"2013-07-01\" \"2013-08-01\" \"2013-09-01\" \"2013-10-01\"\n[11] \"2013-11-01\" \"2013-12-01\" \"2014-01-01\" \"2014-02-01\" \"2014-03-01\"\n[16] \"2014-04-01\" \"2014-05-01\" \"2014-06-01\" \"2014-07-01\" \"2014-08-01\"\n[21] \"2014-09-01\" \"2014-10-01\" \"2014-11-01\" \"2014-12-01\" \"2015-01-01\"\n[26] \"2015-02-01\" \"2015-03-01\" \"2015-04-01\" \"2015-05-01\" \"2015-06-01\"\n[31] \"2015-07-01\" \"2015-08-01\" \"2015-09-01\" \"2015-10-01\" \"2015-11-01\"\n[36] \"2015-12-01\" \"2016-01-01\" \"2016-02-01\" \"2016-03-01\" \"2016-04-01\"\n[41] \"2016-05-01\" \"2016-06-01\" \"2016-07-01\" \"2016-08-01\" \"2016-09-01\"\n[46] \"2016-10-01\" \"2016-11-01\" \"2016-12-01\" \"2017-01-01\" \"2017-02-01\"\n```\n\n\n:::\n\n```{.r .cell-code}\nval_seq <- replicate(n = l, c(runif(9), NA)) |> as.vector() |> as.double()\nval_seq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.74815520 0.62345014 0.98719405 0.98357823 0.64343460 0.38288945\n [7] 0.30782868 0.63132596 0.09734484         NA 0.79572696 0.08743225\n[13] 0.72841099 0.78703884 0.39553790 0.54639674 0.96807028 0.60125354\n[19] 0.74665373         NA 0.92237646 0.04457192 0.68444841 0.05388607\n[25] 0.24374963 0.73552094 0.84926348 0.55056715 0.77699405         NA\n[31] 0.55460139 0.24564446 0.24396533 0.60797386 0.71226179 0.93048958\n[37] 0.72179306 0.01549613 0.88487496         NA 0.41888816 0.08623630\n[43] 0.06213051 0.58266383 0.72425739 0.17659346 0.80285097 0.78684451\n[49] 0.68433082         NA\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_tbl <- tibble(\n  date_col = date_seq,\n  value = val_seq\n)\n\nrec_obj <- recipe(value ~ date_col, data = data_tbl)\nrec_obj\n\ndf_tbl <- tibble(\n  impute_type = c(\"bagged\",\"knn\",\"linear\",\"mean\",\"median\",\"roll\"),\n  rec_obj = list(rec_obj),\n  data = list(data_tbl)\n)\ndf_tbl[1,][[3]][[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 Ã— 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.748 \n 2 2013-02-01  0.623 \n 3 2013-03-01  0.987 \n 4 2013-04-01  0.984 \n 5 2013-05-01  0.643 \n 6 2013-06-01  0.383 \n 7 2013-07-01  0.308 \n 8 2013-08-01  0.631 \n 9 2013-09-01  0.0973\n10 2013-10-01 NA     \n# â„¹ 40 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_list <- df_tbl |>\n  group_split(impute_type)\n\ndata_impute_list <- data_list |>\n  imap(\n    .f = function(obj, id){\n      imp_type = obj |> pull(impute_type)\n      rec_obj = obj |> pull(rec_obj) |> pluck(1)\n      data = obj[[\"data\"]][[1]]\n      \n      imp_obj <- hai_data_impute(\n        .recipe_object = rec_obj,\n        value,\n        .type_of_imputation = imp_type,\n        .roll_statistic = median\n      )$impute_rec_obj\n\n      imputed_data <- get_juiced_data(imp_obj)\n\n      combined_tbl <- data |>\n        left_join(imputed_data, by = \"date_col\") |>\n        setNames(c(\"date_col\", \"original_value\", \"imputed_value\")) |>\n        mutate(rec_no = row_number()) |>\n        mutate(color_col = original_value,\n              size_col = original_value) |>\n        mutate(impute_type = imp_type)\n      \n      return(combined_tbl)\n    }\n  )\n\ncombined_tbl <- data_impute_list |>\n  list_rbind()\n\nimped_na_vals_tbl <- combined_tbl |>\n  filter(is.na(original_value)) |>\n  summarize(\n        avg_imputed_val = mean(imputed_value),\n        .by = impute_type\n  )\n\ncombined_tbl |>\n  summarize(\n        avg_imputed_val_col = mean(imputed_value),\n        avg_original_val_col = mean(original_value, na.rm = TRUE),\n        .by = impute_type\n  ) |>\nmutate(imputation_diff = avg_imputed_val_col - avg_original_val_col) |>\nleft_join(imped_na_vals_tbl, by = \"impute_type\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 5\n  impute_type avg_imputed_val_col avg_original_val_col imputation_diff\n  <chr>                     <dbl>                <dbl>           <dbl>\n1 bagged                    0.556                0.559        -0.00287\n2 knn                       0.557                0.559        -0.00199\n3 linear                    0.558                0.559        -0.00128\n4 mean                      0.559                0.559         0      \n5 median                    0.566                0.559         0.00721\n6 roll                      0.555                0.559        -0.00434\n# â„¹ 1 more variable: avg_imputed_val <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = combined_tbl,\n  aes(\n    x = date_col,\n    y = imputed_value,\n    color = color_col\n    )\n  ) + \n  facet_wrap(~ impute_type) +\n  geom_point(data = combined_tbl |> filter(is.na(original_value)), aes(shape = 'NA', size = 3)) +\n  scale_shape_manual(values = c('NA' = 3)) +\n  geom_line(aes(x = date_col, y = original_value), color = \"black\") +\n  geom_line(aes(x = date_col, y = imputed_value), color = \"red\", linetype = \"dashed\", alpha = .328) +\n  geom_vline(\n    data = combined_tbl[combined_tbl$original_value |> is.na(), ], \n    aes(xintercept = date_col), color = \"black\", linetype = \"dashed\"\n  ) +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"Original vs. Imputed Data using HealthyR.ai\",\n    subtitle = \"Function: hai_data_impute()\",\n    caption = \"Red line is the imputed data, blue line is the original data\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncombined_tbl |>\n  filter(is.na(original_value)) |>\n  ggplot(aes(x = impute_type, y = imputed_value, color = impute_type, group = impute_type)) +\n  geom_boxplot() +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"Original vs. Imputed Data using HealthyR.ai\",\n    subtitle = \"Function: hai_data_impute()\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n# Choosing the Right Method\n\nFor imputation:\n\n- **Continuous normal data**: Use \"mean\"\n- **Skewed data or outliers**: Use \"median\"\n- **Categorical data**: Use \"mode\"\n- **Time series**: Use \"roll\" with appropriate window\n- **Complex relationships**: Try \"knn\" or \"bagged\"\n\nFor scaling:\n\n- **Linear regression**: Use \"center\" or \"scale\"\n- **Distance-based algorithms** (KNN, SVM): Use \"scale\"\n- **Neural networks**: Use \"range\" [0,1] or \"normalize\"\n\n# Best Practices\n\n- Always load required libraries: `healthyR.ai`, `recipes`, `dplyr`\n- Create recipe object first: `recipe(target ~ ., data = df)`\n- Set `.seed_value` for reproducible results with stochastic methods\n- Extract processed data with `get_juiced_data()` function\n- Verify results with `summary()` after processing\n- Choose imputation method based on data characteristics and missingness pattern\n\n# Key Takeaways\n\n- healthyR.ai provides user-friendly wrappers around recipes functions for data preprocessing\n- Both imputation and scaling require a recipe object\n- Functions return a list containing the processed recipe object\n- Choose methods based on your data type and the requirements of your modeling approach\n- Chain operations (imputation â†’ scaling â†’ modeling) for a complete workflow\n\nThe goal of these healthyR.ai functions is to simplify data preprocessing with a consistent syntax and integration with the tidymodels ecosystem, making it an excellent choice for streamlining your machine learning pipeline.\n\n------------------------------------------------------------------------\n\nHappy Coding! ðŸš€\n\n------------------------------------------------------------------------\n\n*You can connect with me at any one of the below*:\n\n*Telegram Channel here*: <https://t.me/steveondata>\n\n*LinkedIn Network here*: <https://www.linkedin.com/in/spsanderson/>\n\n*Mastadon Social here*: [https://mstdn.social/\\@stevensanderson](https://mstdn.social/@stevensanderson)\n\n*RStats Network here*: [https://rstats.me/\\@spsanderson](https://rstats.me/@spsanderson)\n\n*GitHub Network here*: <https://github.com/spsanderson>\n\n*Bluesky Network here*: <https://bsky.app/profile/spsanderson.com>\n\n*My Book: Extending Excel with Python and R* here: <https://packt.link/oTyZJ>\n\n*You.com Referral Link*: <https://you.com/join/EHSLDTL6>\n\n------------------------------------------------------------------------\n\n```{=html}\n<script src=\"https://giscus.app/client.js\"\n        data-repo=\"spsanderson/steveondata\"\n        data-repo-id=\"R_kgDOIIxnLw\"\n        data-category=\"Comments\"\n        data-category-id=\"DIC_kwDOIIxnL84ChTk8\"\n        data-mapping=\"url\"\n        data-strict=\"0\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"top\"\n        data-theme=\"dark\"\n        data-lang=\"en\"\n        data-loading=\"lazy\"\n        crossorigin=\"anonymous\"\n        async>\n</script>\n```",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}