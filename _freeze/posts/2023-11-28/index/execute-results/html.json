{
  "hash": "9317865c84a6c360519ce17016204f1b",
  "result": {
    "markdown": "---\ntitle: \"Understanding and Implementing Robust Regression in R\"\nauthor: \"Steven P. Sanderson II, MPH\"\ndate: \"2023-11-28\"\ncategories: [rtip, regression]\n---\n\n\n# Introduction\n\nIf you're familiar with linear regression in R, you've probably encountered the traditional `lm()` function. While this is a powerful tool, it might not be the best choice when dealing with outliers or influential observations. In such cases, robust regression comes to the rescue, and in R, the `rlm()` function from the MASS package is a valuable resource. In this blog post, we'll delve into the step-by-step process of performing robust regression in R, using a dataset to illustrate the differences between the base R lm model and the robust rlm model.\n\n# The Dataset\n\nLet's start by loading the dataset into R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(\n  x1 = c(1, 3, 3, 4, 4, 6, 6, 8, 9, 3, 11, 16, 16, 18, 19, 20, 23, 23, 24, 25),\n  x2 = c(7, 7, 4, 29, 13, 34, 17, 19, 20, 12, 25, 26, 26, 26, 27, 29, 30, 31, 31, 32),\n  y = c(17, 170, 19, 194, 24, 2, 25, 29, 30, 32, 44, 60, 61, 63, 63, 64, 61, 67, 59, 70)\n)\n```\n:::\n\n\nThis dataset contains three variables: `x1`, `x2`, and `y`. Now, let's explore how to fit a linear regression model using both the traditional `lm()` function and the robust `rlm()` function.\n\n# Fitting a Model\n\n## Traditional Linear Regression (lm)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the lm model\nlm_model <- lm(y ~ x1 + x2, data = df)\n\n# Print the summary\nsummary(lm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-72.020 -27.290  -0.138   4.487 124.144 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   41.106     29.940   1.373    0.188\nx1            -0.605      2.066  -0.293    0.773\nx2             1.075      1.857   0.579    0.570\n\nResidual standard error: 49.42 on 17 degrees of freedom\nMultiple R-squared:  0.02203,\tAdjusted R-squared:  -0.09303 \nF-statistic: 0.1914 on 2 and 17 DF,  p-value: 0.8275\n```\n:::\n:::\n\n\nThe `lm()` function provides a standard linear regression model. However, it assumes that the data follows a normal distribution and is sensitive to outliers. This sensitivity can lead to biased coefficient estimates.\n\n## Robust Linear Regression (rlm)\n\nNow, let's contrast this with the robust approach using the `rlm()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the MASS package\nlibrary(MASS)\n\n# Fit the rlm model\nrobust_model <- rlm(y ~ x1 + x2, data = df)\n\n# Print the summary\nsummary(robust_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: rlm(formula = y ~ x1 + x2, data = df)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.9296  -6.1604  -0.5812   6.4648 170.4612 \n\nCoefficients:\n            Value   Std. Error t value\n(Intercept) 21.4109  5.9703     3.5862\nx1           2.3077  0.4121     5.6004\nx2          -0.2449  0.3703    -0.6615\n\nResidual standard error: 9.369 on 17 degrees of freedom\n```\n:::\n:::\n\n\nThe `rlm()` function, part of the MASS package, uses a robust M-estimation approach. It downplays the impact of outliers, making it more suitable for datasets with influential observations.\n\n# Understanding the Results\n\nWhen you compare the summaries of the two models, you'll notice differences in the coefficient estimates and standard errors. The robust model is less influenced by outliers, providing more reliable estimates in the presence of skewed or contaminated data.\n\n# View the Residuals\n\nLet's take a closer look at the residuals of the two models. First, we'll extract the residuals from the `lm()` model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_residuals <- residuals(lm_model)\n```\n:::\n\n\nNext, we'll extract the residuals from the `rlm()` model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrobust_residuals <- residuals(robust_model)\n```\n:::\n\n\nNow, let's plot the residuals from both models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new plot\nplot(\n  lm_residuals, \n  col = \"blue\", \n  pch = 16, \n  main = \"Residuals Comparison: lm vs. rlm\",\n  xlab = \"Observation\", \n  ylab = \"Residuals\"\n  )\n\n# Add robust residuals to the plot\npoints(\n  robust_residuals, \n  col = \"red\", \n  pch = 16\n  )\n\n# Add a legend\nlegend(\"topright\", legend = c(\"lm Residuals\", \"rlm Residuals\"), col = c(\"blue\", \"red\"), pch = 16)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n# Try it on Your Own\n\nNow that you've seen the contrast between the traditional `lm()` and the robust `rlm()` models, I encourage you to apply this knowledge to your own datasets. Experiment with different datasets, introduce outliers, and observe how the two models react. This hands-on approach will deepen your understanding of robust regression in R.\n\nRemember, robust regression is a powerful tool when facing real-world data challenges. As you explore, don't hesitate to tweak parameters, test assumptions, and refine your models. Happy coding, and may your regression analyses be robust and resilient!",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}