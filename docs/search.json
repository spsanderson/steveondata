[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steve On Data",
    "section": "",
    "text": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplify Your Code with R’s Powerful Functions: with() and within()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to subset list objects in R\n\n\n\n\n\n\n\nrtip\n\n\nlist\n\n\nsubset\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nEfficiently Finding Duplicate Rows in R: A Comparative Analysis\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\ndplyr\n\n\ndatatable\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nFinding Duplicate Values in a Data Frame in R: A Guide Using Base R and dplyr\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCovariance in R with the cov() Function\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying File Existence Checking in R with file.exists()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Data with colMeans() in R: A Programmer’s Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA Closer Look at the R Function identical()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying File Management in R: Introducing file.rename()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use a Windows .bat File to Execute an R Script\n\n\n\n\n\n\n\nrtip\n\n\nbatchfile\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Rolling Correlation with the rollapply Function: A Powerful Tool for Analyzing Time-Series Data\n\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe ave() Function in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nVisualization in R: Unleashing the Power of the abline() Function\n\n\n\n\n\n\n\nrtip\n\n\nabline\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrap Function in R: Resampling with the lapply and sample Functions\n\n\n\n\n\n\n\nrtip\n\n\nbootstrap\n\n\nlapply\n\n\nsample\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Repetition with R’s rep() Function: A Programmer’s Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnleashing the Power of Sampling in R: Exploring the Versatile sample() Function\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Aggregation with xtabs() in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering the Power of R’s diff() Function: A Programmer’s Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Linear Regression in R: Analyzing the mtcars Dataset with lm()\n\n\n\n\n\n\n\nrtip\n\n\nlinear\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPulling a formula from a recipe object\n\n\n\n\n\n\n\nrtip\n\n\nrecipes\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying Model Formulas with the R Function ‘reformulate()’\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding the file.info() Function in R: Listing Files by Date\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying Data Transformation with pivot_longer() in R’s tidyr Library\n\n\n\n\n\n\n\nrtip\n\n\ntidyr\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSorting, Ordering, and Ranking: Unraveling R’s Powerful Functions\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe do.call() function in R: Unlocking Efficiency and Flexibility\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying Regular Expressions: A Programmer’s Guide for Beginners\n\n\n\n\n\n\n\nrtip\n\n\nregex\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying Logical Operations with the R Function any()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWhy Check File Size Output for Different Methods?\n\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\nopenxlsx\n\n\nxlsx\n\n\nwritexl\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nComparing R Packages for Writing Excel Files: An Analysis of writexl, openxlsx, and xlsx in R\n\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\nopenxlsx\n\n\nxlsx\n\n\nwritexl\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Data with TidyDensity: A Guide to Using tidy_empirical() and tidy_four_autoplot() in R\n\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\ndplyr\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWhat is the sink() function? Capturing Output to External Files\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUpdate to {TidyDensity}\n\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering File Manipulation with R’s list.files() Function\n\n\n\n\n\n\n\nrtip\n\n\nfiles\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe which() Function in R\n\n\n\n\n\n\n\nrtip\n\n\nwhich\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times Pt 4\n\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times Pt 3\n\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times Pt 2: Finding the Next Mothers Day with Simplicity\n\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times Pt 1\n\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nVBA to R and Back Again: Running R from VBA Pt 2\n\n\n\n\n\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nVBA to R and Back Again: Running R from VBA\n\n\n\n\n\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUpdates to {healthyR.data}\n\n\n\n\n\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMaps with {shiny} Pt 2\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nmapping\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMaps with {shiny}\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nmapping\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Download a File from the Internet using download.file()\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nreadxl\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExtracting a model call from a fitted workflow in {tidymodels}\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 4\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 3\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 2\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 1\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny}, {TidyDensity} and {plotly} Part 5\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 4\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 3\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 2\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity}\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nStyling Tables for Excel with {styledTables}\n\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nReading in Multiple Excel Sheets with lapply and {readxl}\n\n\n\n\n\n\n\nrtip\n\n\nreadxl\n\n\nlapply\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA New Package for the African Stock Market {BRVM}\n\n\n\n\n\n\n\nrtip\n\n\nbrvm\n\n\nmarkets\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nLooking at Daily Log Returns with tidyquant, TidyDensity, and Shiny\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\ntidyquant\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA sample Shiny App to view Forecasts on the AirPassengers Data\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ndata\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA sample Shiny App to view CMS Healthcare Data\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ndata\n\n\nhealthcare\n\n\ncms\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA Bootstrapped Time Series Model with auto.arima() from {forecast}\n\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\nbootstrap\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow fast does a compressed file in Part 2\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\narrow\n\n\nduckdb\n\n\ndatatable\n\n\nreadr\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow fast does a compressed file in?\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow fast do the files read in?\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSome Examples of Cumulative Mean with {TidyDensity}\n\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGetting the CCI30 Index Current Makeup\n\n\n\n\n\n\n\ncrypto\n\n\ncci30\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUse of the apply family of functions\n\n\n\n\n\n\n\nthanks\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUse of the apply family of functions\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\napply\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMultiple Solutions to speedup tidy_bernoulli() with {data.table}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\ndata.table\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGetting NYS Home Heating Oil Prices with {rvest}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrvest\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\ntidy_bernoulli() with {data.table}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndata.table\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimple examples of imap() from {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimple examples of pmap() from {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Timeseries in a list with R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nText Processing Made Easy with {healthyR}’s sql_left(), sql_mid(), and sql_right() Functions in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nsql\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nOpen a File Folder in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nshell\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nQuickly Generate Nested Time Series Models\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nautoarima\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nData Preppers with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\npreprocessor\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCalibrate and Plot a Time Series with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nConverting a {tidyAML} tibble to a {workflowsets}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\nworkflowsets\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nOfficially on CRAN {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMoving Average Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAn example of using {box}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nbox\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nOff to CRAN! {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGet the Current Hospital Data Set from CMS with {healthyR.data}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating and Predicting Fast Regression Parsnip Models with {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating an R Project Directory\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSubsetting Named Lists in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlist\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCumulative Measurement Functions with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe Argument Matcher: A Function for Selecting the Right Arguments {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlist\n\n\ntidyaml\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDiverging Lollipop Chart: A Visual Tool for Comparing Data with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\nplots\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAttributes in R Functions: An Overview\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nmetadata\n\n\nattributes\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMedian: A Simple Way to Detect Excess Events Over Time with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\n{healthyR.ts}: The New and Improved Library for Time Series Analysis\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nService Line Grouping with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\naugment\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTransforming Your Data: A Guide to Popular Methods and How to Implement Them with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntransforms\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying List Filtering in R with purrr’s keep()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMaking Non Stationary Data Stationary\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nADF and Phillips-Perron Tests for Stationarity using lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\ntimeseries\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAnother Post on Lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBoilerplate XGBoost with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nxgboost\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGeometric Brownian Motion with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAugmenting a Brownian Motion to a Time Series with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAuto K-Means with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nkmeans\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe building of {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAn Update on {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\nautoml\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nReflecting on the Past Year: A LinkedIn Year in Review\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlinkedin\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nOptimal Break Points for Histograms with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\nhistograms\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nNew Release of {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBrownian Motion\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMore Randomwalks with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\nrandomwalk\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCalendar Heatmap with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nEvent Analysis with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGartner Magic Chart and its usefulness in healthcare analytics with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimulating Time Series Model Forecasts with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\ntimeseries\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nListing Functions and Parameters\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndplyr\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDistribution Statistics with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nRandom Walks with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrandomwalk\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nViewing Different Versions of the Same Statistical Distribution with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndistributions\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nModel Scedacity Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimple Moving Average Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDistribution Summaries with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMixture Distributions with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\nmixturemodels\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreate QQ Plots for Time Series Models with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreate a Faceted Historgram Plot with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhistograms\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreate Multiple {parsnip} Model Specs with {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nparsnip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nZ-Score Scaling Step Recipe with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nNaming Items in a List with {purrr}, {dplyr}, or {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAuto KNN with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nknn\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExtract Boilerplate Workflow Metrics with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGenerate Random Walk Data with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDefault Metric Sets with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSummary Statistics with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\ndatatable\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nData Preprocessing Scale/Normalize with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrap Modeling with Base R\n\n\n\n\n\n\n\ncode\n\n\nbootstrap\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUpdates to {healthyverse} packages\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyverse\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrap Modeling with {purrr} and {modler}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nmodelr\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCumulative Harmonic Mean with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAuto Prep data for XGBoost with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nxgboost\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nFind Skewed Features with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nskew\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Lag Correlation Plots\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nReading Multiple Files with {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMapping K-Means with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nkmeans\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHyperbolic Transform with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDiscrete Fourier Vec with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrapping and Plots with TidyDensity\n\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\nbootstrap\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCumulative Skewness\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPCA with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nControl Charts in healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCumulative Variance\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ncumulative\n\n\nsapply\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Clustering with healthyR.ts\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nhealthyR.ai Primer\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTidyDensity Primer\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimple lapply()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To Steve On Data\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/healthyrai-20221013/index.html",
    "href": "posts/healthyrai-20221013/index.html",
    "title": "healthyR.ai Primer",
    "section": "",
    "text": "This is going to serve as a sort of primer for my r packge {healthyR.ai}. The goal of this package is to help with producing uniform machine learning/ai models either from scratch or by way of one of the boilerplate functions.\nThis particular article is going to focus on k-means clustering with umap projection and visualization.\nFirst things first, lets load in the library:\n\nlibrary(healthyR.ai)\n\n\n== Welcome to healthyR.ai ===========================================================================\nIf you find this package useful, please leave a star: \n   https://github.com/spsanderson/healthyR.ai'\n\nIf you encounter a bug or want to request an enhancement please file an issue at:\n   https://github.com/spsanderson/healthyR.ai/issues\n\nThank you for using healthyR.ai\n\n\n\nInformation\nK-Means is a partition algorithm initially designed for signal processing. The goal is to partition n observations into k clusters where each n is in k. The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters.\nThe aim of this post is to showcase the use of the healthyR.ai wrapper for the kmeans function along with the wrapper and plot for the uwot::umap projection function. We will go through the entire workflow from getting the data to getting the final UMAP plot.\n\n\nGenerate some data\n\nsuppressPackageStartupMessages(library(healthyR.data))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(broom))\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata_tbl <- healthyR_data %>%\n    filter(ip_op_flag == \"I\") %>%\n    filter(payer_grouping != \"Medicare B\") %>%\n    filter(payer_grouping != \"?\") %>%\n    select(service_line, payer_grouping) %>%\n    mutate(record = 1) %>%\n    as_tibble()\n\ndata_tbl %>%\n  glimpse()\n\nRows: 116,823\nColumns: 3\n$ service_line   <chr> \"Medical\", \"Schizophrenia\", \"Syncope\", \"Pneumonia\", \"Ch…\n$ payer_grouping <chr> \"Blue Cross\", \"Medicare A\", \"Medicare A\", \"Medicare A\",…\n$ record         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nNow that we have our data we need to generate what is called a user item table. To do this we use the function hai_kmeans_user_item_tbl which takes in just a few arguments. The purpose of the user item table is to aggregate and normalize the data between the users and the items.\nThe data that we have generated is going to look for clustering amongst the service_lines (the user) and the payer_grouping (item) columns.\nLets now create the user item table.\n\n\nUser Item Tibble\n\nuit_tbl <- hai_kmeans_user_item_tbl(\n  data_tbl, \n  service_line, \n  payer_grouping, \n  record\n)\n\nuit_tbl\n\n# A tibble: 23 × 12\n   service_line   Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷\n   <chr>            <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n 1 Alcohol Abuse   0.0941 0.0321  5.25e-4 0.0116  0.0788 0.158    0.367   0.173 \n 2 Bariatric Sur…  0.317  0.0583  0       0.0518  0.168  0.00324  0.343   0.0485\n 3 Carotid Endar…  0.0845 0.0282  0       0       0.0141 0        0.0282  0.648 \n 4 Cellulitis      0.110  0.0339  1.18e-2 0.00847 0.0805 0.0869   0.192   0.355 \n 5 Chest Pain      0.144  0.0391  2.90e-3 0.00543 0.112  0.0522   0.159   0.324 \n 6 CHF             0.0295 0.00958 5.18e-4 0.00414 0.0205 0.0197   0.0596  0.657 \n 7 COPD            0.0493 0.0228  2.28e-4 0.00548 0.0342 0.0461   0.172   0.520 \n 8 CVA             0.0647 0.0246  1.07e-3 0.0107  0.0524 0.0289   0.0764  0.555 \n 9 GI Hemorrhage   0.0542 0.0175  1.25e-3 0.00834 0.0480 0.0350   0.0855  0.588 \n10 Joint Replace…  0.139  0.0179  3.36e-2 0.00673 0.0516 0        0.0874  0.5   \n# … with 13 more rows, 3 more variables: `Medicare HMO` <dbl>,\n#   `No Fault` <dbl>, `Self Pay` <dbl>, and abbreviated variable names\n#   ¹​`Blue Cross`, ²​Commercial, ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid,\n#   ⁶​`Medicaid HMO`, ⁷​`Medicare A`\n\n\nThe table is aggregated by item for the various users to which the algorithm will be applied.\nNow that we have this data we need to find what will be out optimal k (clusters). To do this we need to generate a table of data that will have a column of k and for that k apply the k-means function to the data with that k and return the total within sum of squares.\nTo do this there is a convienent function called hai_kmeans_mapped_tbl that takes as its sole argument the output from the hai_kmeans_user_item_tbl. There is an argument .centers where the default is set to 15.\n\n\nK-Means Mapped Tibble\n\nkmm_tbl <- hai_kmeans_mapped_tbl(uit_tbl)\nkmm_tbl\n\n# A tibble: 15 × 3\n   centers k_means  glance          \n     <int> <list>   <list>          \n 1       1 <kmeans> <tibble [1 × 4]>\n 2       2 <kmeans> <tibble [1 × 4]>\n 3       3 <kmeans> <tibble [1 × 4]>\n 4       4 <kmeans> <tibble [1 × 4]>\n 5       5 <kmeans> <tibble [1 × 4]>\n 6       6 <kmeans> <tibble [1 × 4]>\n 7       7 <kmeans> <tibble [1 × 4]>\n 8       8 <kmeans> <tibble [1 × 4]>\n 9       9 <kmeans> <tibble [1 × 4]>\n10      10 <kmeans> <tibble [1 × 4]>\n11      11 <kmeans> <tibble [1 × 4]>\n12      12 <kmeans> <tibble [1 × 4]>\n13      13 <kmeans> <tibble [1 × 4]>\n14      14 <kmeans> <tibble [1 × 4]>\n15      15 <kmeans> <tibble [1 × 4]>\n\n\nAs we see there are three columns, centers, k_means and glance. The k_means column is the k_means list object and glance is the tibble returned by the broom::glance function.\n\nkmm_tbl %>%\n  tidyr::unnest(glance)\n\n# A tibble: 15 × 6\n   centers k_means  totss tot.withinss betweenss  iter\n     <int> <list>   <dbl>        <dbl>     <dbl> <int>\n 1       1 <kmeans>  1.41       1.41    1.33e-15     1\n 2       2 <kmeans>  1.41       0.592   8.17e- 1     1\n 3       3 <kmeans>  1.41       0.372   1.04e+ 0     2\n 4       4 <kmeans>  1.41       0.276   1.13e+ 0     2\n 5       5 <kmeans>  1.41       0.202   1.21e+ 0     2\n 6       6 <kmeans>  1.41       0.159   1.25e+ 0     3\n 7       7 <kmeans>  1.41       0.124   1.28e+ 0     3\n 8       8 <kmeans>  1.41       0.0884  1.32e+ 0     2\n 9       9 <kmeans>  1.41       0.0745  1.33e+ 0     3\n10      10 <kmeans>  1.41       0.0576  1.35e+ 0     2\n11      11 <kmeans>  1.41       0.0460  1.36e+ 0     2\n12      12 <kmeans>  1.41       0.0363  1.37e+ 0     3\n13      13 <kmeans>  1.41       0.0293  1.38e+ 0     3\n14      14 <kmeans>  1.41       0.0202  1.39e+ 0     2\n15      15 <kmeans>  1.41       0.0161  1.39e+ 0     2\n\n\nAs stated we use the tot.withinss to decide what will become our k, an easy way to do this is to visualize the Scree Plot, also known as the elbow plot. This is done by ploting the x-axis as the centers and the y-axis as the tot.withinss.\n\n\nScree Plot and Data\n\nhai_kmeans_scree_plt(.data = kmm_tbl)\n\n\n\n\nIf we want to see the scree plot data that creates the plot then we can use another function hai_kmeans_scree_data_tbl.\n\nhai_kmeans_scree_data_tbl(kmm_tbl)\n\n# A tibble: 15 × 2\n   centers tot.withinss\n     <int>        <dbl>\n 1       1       1.41  \n 2       2       0.592 \n 3       3       0.372 \n 4       4       0.276 \n 5       5       0.202 \n 6       6       0.159 \n 7       7       0.124 \n 8       8       0.0884\n 9       9       0.0745\n10      10       0.0576\n11      11       0.0460\n12      12       0.0363\n13      13       0.0293\n14      14       0.0202\n15      15       0.0161\n\n\nWith the above pieces of information we can decide upon a value for k, in this instance we are going to use 3. Now that we have that we can go ahead with creating the umap list object where we can take a look at a great many things associated with the data.\n\n\nUMAP List Object\nNow lets go ahead and create our UMAP list object.\n\nump_lst <- hai_umap_list(.data = uit_tbl, kmm_tbl, 3)\n\nNow that it is created, lets take a look at each item in the list. The umap_list function returns a list of 5 items.\n\numap_obj\numap_results_tbl\nkmeans_obj\nkmeans_cluster_tbl\numap_kmeans_cluster_results_tbl\n\nSince we have the list object we can now inspect the kmeans_obj, first thing we will do is use the hai_kmeans_tidy_tbl function to inspect things.\n\nkm_obj <- ump_lst$kmeans_obj\nhai_kmeans_tidy_tbl(.kmeans_obj = km_obj, .data = uit_tbl, .tidy_type = \"glance\")\n\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  <dbl>        <dbl>     <dbl> <int>\n1  1.41        0.372      1.04     2\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"augment\")\n\n# A tibble: 23 × 2\n   service_line                  cluster\n   <chr>                         <fct>  \n 1 Alcohol Abuse                 1      \n 2 Bariatric Surgery For Obesity 1      \n 3 Carotid Endarterectomy        2      \n 4 Cellulitis                    3      \n 5 Chest Pain                    3      \n 6 CHF                           2      \n 7 COPD                          2      \n 8 CVA                           2      \n 9 GI Hemorrhage                 2      \n10 Joint Replacement             2      \n# … with 13 more rows\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"tidy\")\n\n# A tibble: 3 × 14\n  Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷ Medic…⁸ No Fa…⁹\n    <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1  0.150   0.0368 3.07e-4 0.0207  0.163   0.131   0.314    0.132  0.0319 0.00136\n2  0.0784  0.0218 4.32e-3 0.00620 0.0449  0.0368  0.0800   0.563  0.152  0.00348\n3  0.117   0.0314 1.02e-2 0.0139  0.0982  0.0856  0.147    0.354  0.105  0.00707\n# … with 4 more variables: `Self Pay` <dbl>, size <int>, withinss <dbl>,\n#   cluster <fct>, and abbreviated variable names ¹​`Blue Cross`, ²​Commercial,\n#   ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid, ⁶​`Medicaid HMO`,\n#   ⁷​`Medicare A`, ⁸​`Medicare HMO`, ⁹​`No Fault`\n\n\n\n\nUMAP Plot\nNow that we have all of the above data we can visualize our clusters that are colored by their cluster number.\n\nhai_umap_plot(.data = ump_lst, .point_size = 3, TRUE)"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html",
    "href": "posts/healthyrts-20221021/index.html",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "",
    "text": "There are two components to time-series clustering with {healthyR.ts}. There is the function that will create the clustering data along with a slew of other information and then there is a plotting function that will plot out the data in a time-series fashion colored by cluster.\nThe first function as mentioned is the function ts_feature_cluster(), and the next is ts_feature_cluster_plot()\nFunction Reference:\n\nts_feature_cluster()\nts_feature_cluster_plot()`\n\nWe are going to use the built-in AirPassengers data set for this example so let’s get right to it!"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster()",
    "text": "ts_feature_cluster()\nAs mentioned there are several outputs from the ts_feature_cluster(). Those are as follows:\nData Section\n\nts_feature_tbl\nuser_item_matrix_tbl\nmapped_tbl\nscree_data_tbl\ninput_data_tbl (the original data)\n\nPlots\n\nstatic_plot\nplotly_plot\n\nNow that we have our output, let’s take a look at each individual component of the output.\nts_feature_tbl\n\noutput$data$ts_feature_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nuser_item_matrix_tbl\n\noutput$data$user_item_matrix_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nmapped_tbl\n\noutput$data$mapped_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nscree_data_tbl\n\noutput$data$scree_data_tbl |> glimpse()\n\nRows: 3\nColumns: 2\n$ centers      <int> 1, 2, 3\n$ tot.withinss <dbl> 1.8324477, 0.7364934, 0.4571258\n\n\ninput_data_tbl\n\noutput$data$input_data_tbl |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nNow the plots.\nstatic_plot\n\noutput$plots$static_plot\n\n\n\n\nplotly_plot\n\noutput$plots$plotly_plot\n\n\n\n\n\nNow that we have seen the output of the ts_feature_cluster() function, let’s take a look at the output of the ts_feature_cluster_plot() function."
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster_plot()",
    "text": "ts_feature_cluster_plot()\nThis function itself returns a list object of a multitude of data. First before we get into that lets look at the function call itself:\n\nts_feature_cluster_plot(\n  .data,\n  .date_col,\n  .value_col,\n  ...,\n  .center = 3,\n  .facet_ncol = 3,\n  .smooth = FALSE\n)\n\nThe data that comes back from this function is:\nData Section\n\noriginal_data\nkmm_data_tbl\nuser_item_tbl\ncluster_tbl\n\nPlots\n\nstatic_plot\nplotly_plot\n\nK-Means Object\n\nk-means object\n\nWe will go through the same exercise and show the output of all the sections. First we have to create the output. The static plot will automatically print out.\n\nplot_out <- ts_feature_cluster_plot(\n  .data = output,\n  .date_col = date_col,\n  .value_col = value,\n  .center = 2,\n  group_id\n)\n\nJoining, by = \"group_id\"\n\n\n\n\n\n\nThe Data Section:\noriginal_data\n\nplot_out$data$original_data |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nkmm_data_tbl\n\nplot_out$data$kmm_data_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nuser_item_data\n\nplot_out$data$user_item_data |> glimpse()\n\n NULL\n\n\ncluster_tbl\n\nplot_out$data$cluster_tbl |> glimpse()\n\nRows: 12\nColumns: 9\n$ cluster        <int> 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\n\n\nThe plot data.\nstatic_plot\n\nplot_out$plot$static_plot\n\n\n\n\nplotly_plot\n\nplot_out$plot$plotly_plot\n\n\n\n\n\n\n\nThe K-Means Object\nkmeans_object\n\nplot_out$kmeans_object\n\n[[1]]\nK-means clustering with 2 clusters of sizes 5, 7\n\nCluster means:\n  ts_x_acf1 ts_x_acf10 ts_diff1_acf1 ts_diff1_acf10 ts_diff2_acf1 ts_seas_acf1\n1 0.7456468   1.568532     0.1172685      0.4858013    -0.1799728    0.2876449\n2 0.7387865   1.528308    -0.2909349      0.3638392    -0.5916245    0.2930543\n  ts_entropy\n1  0.4918321\n2  0.6438176\n\nClustering vector:\n [1] 1 1 2 2 2 1 1 1 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 0.3704304 0.3660630\n (between_SS / total_SS =  59.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Simple lapply()",
    "section": "",
    "text": "This is a simple lapply example to start things off.\n\n# Let l be some list of lists, where all elements of lists are numbers\nl <- list(\n  a = 1:10,\n  b = 11:20,\n  c = 21:30\n)\n\nNow let’s take a look at our list l and see it’s structure.\n\nl\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$b\n [1] 11 12 13 14 15 16 17 18 19 20\n\n$c\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nNow that we see the structure, we can use the lapply function to get the sum of each list element, the mean, etc.\n\nlapply(l, sum)\n\n$a\n[1] 55\n\n$b\n[1] 155\n\n$c\n[1] 255\n\nlapply(l, mean)\n\n$a\n[1] 5.5\n\n$b\n[1] 15.5\n\n$c\n[1] 25.5\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2021-01-11/index.html",
    "href": "posts/rtip-2021-01-11/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2022, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2023!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2022\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nfp <- \"linkedin_content.xlsx\"\n\nengagement_tbl <- read_excel(fp, sheet = \"ENGAGEMENT\") %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ntop_posts_tbl <- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %>%\n  clean_names()\n\nfollowers_tbl <- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ndemographics_tbl <- read_excel(fp, sheet = \"DEMOGRAPHICS\") %>%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 362\nColumns: 4\n$ date              <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 202…\n$ impressions       <dbl> 3088, 3911, 3303, 3134, 1118, 799, 3068, 1954, 2663,…\n$ engagements       <dbl> 31, 56, 51, 42, 8, 4, 43, 20, 33, 43, 14, 41, 5, 17,…\n$ `Engagement Rate` <dbl> 1.0038860, 1.4318589, 1.5440509, 1.3401404, 0.715563…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 5\n$ post_url_1  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ engagements <dbl> 241, 136, 123, 117, 117, 115, 107, 106, 104, 104, 95, 81, …\n$ x3          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_4  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ impressions <dbl> 52300, 33903, 30752, 29887, 25953, 24139, 23769, 18522, 18…\n\nglimpse(followers_tbl)\n\nRows: 362\nColumns: 2\n$ date          <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 2022-01…\n$ new_followers <dbl> 10, 10, 12, 5, 12, 13, 9, 8, 11, 4, 9, 6, 7, 9, 10, 11, …\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics <chr> \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            <chr> \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       <chr> \"0.054587073624134064\", \"0.035217467695474625\", \"0.02…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %>%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\n\nYou will notice that I placed a blue line where I started my telegram channel @steveondata and a red line where I started this blog. So far, not bad, it looks like the telegram channel helped a little bit but writing on the blog seems to maybe been helping the most.\nLet’s look at a cumulative view of things.\n\nengagement_tbl %>%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %>%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %>%\n  slice(1:12) %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %>%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %>%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %>%\n  slice(1:12) %>%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-24/index.html",
    "href": "posts/rtip-2022-10-24/index.html",
    "title": "Cumulative Variance",
    "section": "",
    "text": "Introducton\nThis is going to be a simple example on how we can make a function in #base #r that will crate a cumulative variance function. From base R we are going to use seq_along(), stats::var(), and sapply() inside of the function we will call cvar for cumulative variance.\n\n\nGenerate Data\nThe first thing we need to do in order to showcase this function is to generate some data. Lets do that below:\n\nl <- list(\n  a = rnorm(50),\n  b = rnorm(50, 1),\n  c = rnorm(50, 2)\n)\n\nl\n\n$a\n [1] -0.96548479  0.49276394  0.14030455  1.11786377 -1.47239834 -0.06906506\n [7] -1.51133985  1.48910665  0.09444727 -0.01216806  0.35365683 -1.13562871\n[13] -1.27899694  0.10963391 -0.00708945 -1.26718573  0.92143855  0.09716551\n[19] -0.28025814 -0.18046616 -1.75919633  0.01686201 -0.10204673  0.91791398\n[25] -1.70503761  1.50856724 -1.29433294  0.42665133 -0.78176459  0.16141529\n[31]  1.42536506 -0.42168041 -0.30222269  0.05129043 -0.73717680 -1.60823604\n[37] -0.11921815  0.08357566  0.23250949  0.50846618 -0.02674088  0.12101223\n[43]  0.10390867 -1.11476987 -1.42201791 -1.35493159  0.35703193 -1.08176152\n[49] -0.08189606  0.46341303\n\n$b\n [1]  1.67636555  1.22588224  0.44445597  2.06992723  1.82473269 -0.03321279\n [7]  1.29568923 -0.29542080  1.46614555  0.51617492  2.03383464  0.13835453\n[13]  3.18982479 -0.38493278  0.67450796  1.69715532  1.19963387  1.17294403\n[19]  0.83585415  1.49308994  0.53831112  1.76345465  1.80154859  0.47358491\n[25]  1.40422472  2.50254552 -0.07376997  0.38077031  1.13606122 -0.26052567\n[31]  0.88624336  1.89232197  1.37488657  2.53211686  1.77919794  3.42367520\n[37] -0.59175356 -0.04816522  2.08963807  1.40124074 -0.73135934  0.65282741\n[43]  0.87359580  0.14540086  1.52502012  0.52190806  2.29922084  0.62462975\n[49]  2.94462210  1.06173482\n\n$c\n [1]  1.01194711  1.36267530  1.37423091  1.14980487  1.39304340  3.26911528\n [7]  1.71184232  1.88096194  2.90461886  1.39510407  1.86157191  1.14906542\n[13]  1.90072693  1.78998258  1.61307934  0.76604158  2.92366827  2.32424523\n[19]  2.94645235  2.73102591  0.87949048  3.31239943  1.05720691  1.42571354\n[25]  1.79266828  1.84627335  0.81364549  0.25976918  1.48698512  1.10254109\n[31]  1.60219278  1.84545465  1.93508206  2.13570750  2.32733075  2.53404107\n[37]  1.25864169  3.28238628  1.98998276  1.44299079  2.26296491  3.86667748\n[43]  1.84651988  3.24765507  0.18464631 -0.01404234  2.78432762 -0.05193538\n[49]  0.35160392  2.58212054\n\n\n\n\nMake Function\nNow that we have our data, lets make the function:\n\ncvar <- function(.x){\n  sapply(seq_along(.x), function(k, z) stats::var(z[1:k]), z = .x)\n}\n\nOk, now that we have our function, lets take a look at it in use.\n\n\nUse Function\n\nsapply(l, cvar)\n\n              a         b          c\n [1,]        NA        NA         NA\n [2,] 1.0632447 0.1014676 0.06150513\n [3,] 0.5789145 0.3885272 0.04239889\n [4,] 0.7633500 0.4867186 0.03075658\n [5,] 1.1294646 0.4093271 0.02873772\n [6,] 0.9043498 0.6932616 0.69685950\n [7,] 1.0277904 0.5789892 0.58271798\n [8,] 1.2918409 0.7813852 0.50862439\n [9,] 1.1344452 0.7052323 0.62156290\n[10,] 1.0088029 0.6580963 0.56764373\n[11,] 0.9242084 0.6858993 0.51210764\n[12,] 0.9418512 0.7024341 0.49623990\n[13,] 0.9661294 1.0026509 0.45782344\n[14,] 0.8992042 1.1041314 0.42295247\n[15,] 0.8371837 1.0364119 0.39358167\n[16,] 0.8556585 0.9929979 0.42396425\n[17,] 0.8822275 0.9315646 0.49164277\n[18,] 0.8344918 0.8770439 0.48215682\n[19,] 0.7888762 0.8321667 0.52875406\n[20,] 0.7473648 0.7964123 0.54171586\n[21,] 0.8305355 0.7722667 0.56162921\n[22,] 0.7940780 0.7564316 0.63535849\n[23,] 0.7587189 0.7425071 0.63686713\n[24,] 0.7802954 0.7290301 0.61692337\n[25,] 0.8409644 0.7019443 0.59130379\n[26,] 0.9248957 0.7464413 0.56765490\n[27,] 0.9359290 0.7761117 0.58464117\n[28,] 0.9159283 0.7676951 0.64765859\n[29,] 0.8952420 0.7403040 0.62681485\n[30,] 0.8690093 0.7773175 0.61856072\n[31,] 0.9251735 0.7524213 0.59834912\n[32,] 0.8976913 0.7499102 0.57961326\n[33,] 0.8702922 0.7290409 0.56296663\n[34,] 0.8452302 0.7678835 0.55094641\n[35,] 0.8301013 0.7571526 0.54480209\n[36,] 0.8638223 0.8786798 0.54627250\n[37,] 0.8400510 0.9426489 0.53823917\n[38,] 0.8195803 0.9560736 0.58478212\n[39,] 0.8028106 0.9542482 0.57032971\n[40,] 0.7943867 0.9312335 0.55895977\n[41,] 0.7750385 0.9957722 0.55033290\n[42,] 0.7581242 0.9766789 0.63799874\n[43,] 0.7417073 0.9547108 0.62281006\n[44,] 0.7453949 0.9533619 0.65240410\n[45,] 0.7629119 0.9360654 0.70195202\n[46,] 0.7747320 0.9223139 0.76179573\n[47,] 0.7652088 0.9339432 0.76550157\n[48,] 0.7645076 0.9188787 0.82293002\n[49,] 0.7490587 0.9695572 0.84800554\n[50,] 0.7434405 0.9498710 0.84419808\n\nlapply(l, cvar)\n\n$a\n [1]        NA 1.0632447 0.5789145 0.7633500 1.1294646 0.9043498 1.0277904\n [8] 1.2918409 1.1344452 1.0088029 0.9242084 0.9418512 0.9661294 0.8992042\n[15] 0.8371837 0.8556585 0.8822275 0.8344918 0.7888762 0.7473648 0.8305355\n[22] 0.7940780 0.7587189 0.7802954 0.8409644 0.9248957 0.9359290 0.9159283\n[29] 0.8952420 0.8690093 0.9251735 0.8976913 0.8702922 0.8452302 0.8301013\n[36] 0.8638223 0.8400510 0.8195803 0.8028106 0.7943867 0.7750385 0.7581242\n[43] 0.7417073 0.7453949 0.7629119 0.7747320 0.7652088 0.7645076 0.7490587\n[50] 0.7434405\n\n$b\n [1]        NA 0.1014676 0.3885272 0.4867186 0.4093271 0.6932616 0.5789892\n [8] 0.7813852 0.7052323 0.6580963 0.6858993 0.7024341 1.0026509 1.1041314\n[15] 1.0364119 0.9929979 0.9315646 0.8770439 0.8321667 0.7964123 0.7722667\n[22] 0.7564316 0.7425071 0.7290301 0.7019443 0.7464413 0.7761117 0.7676951\n[29] 0.7403040 0.7773175 0.7524213 0.7499102 0.7290409 0.7678835 0.7571526\n[36] 0.8786798 0.9426489 0.9560736 0.9542482 0.9312335 0.9957722 0.9766789\n[43] 0.9547108 0.9533619 0.9360654 0.9223139 0.9339432 0.9188787 0.9695572\n[50] 0.9498710\n\n$c\n [1]         NA 0.06150513 0.04239889 0.03075658 0.02873772 0.69685950\n [7] 0.58271798 0.50862439 0.62156290 0.56764373 0.51210764 0.49623990\n[13] 0.45782344 0.42295247 0.39358167 0.42396425 0.49164277 0.48215682\n[19] 0.52875406 0.54171586 0.56162921 0.63535849 0.63686713 0.61692337\n[25] 0.59130379 0.56765490 0.58464117 0.64765859 0.62681485 0.61856072\n[31] 0.59834912 0.57961326 0.56296663 0.55094641 0.54480209 0.54627250\n[37] 0.53823917 0.58478212 0.57032971 0.55895977 0.55033290 0.63799874\n[43] 0.62281006 0.65240410 0.70195202 0.76179573 0.76550157 0.82293002\n[49] 0.84800554 0.84419808\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-26/index.html",
    "href": "posts/rtip-2022-10-26/index.html",
    "title": "Control Charts in healthyR.ai",
    "section": "",
    "text": "Sometimes you may be working with a time series or some process data and you will want to make a control chart. This is simple to do with the {healthyR.ai} package.\nIf you do not already have it, then you can follow the simple code below to get the latest version.\n\n\nYou can install the released version of healthyR.ai from CRAN with:\n\ninstall.packages(\"healthyR.ai\")\n\nAnd the development version from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/healthyR.ai\")\n\nNow that we have the latest version installed, lets get some data and then use the function."
  },
  {
    "objectID": "posts/rtip-2022-10-31/index.html",
    "href": "posts/rtip-2022-10-31/index.html",
    "title": "Cumulative Skewness",
    "section": "",
    "text": "Function\nIn this post we will make a function cum_skewness() that will generate a vector output of the cumulative skewness of some given vector. The full function call is simply:\n\ncum_skewness(.x)\n\nIt only takes in a numeric vector, we are not going to write type checks in the function as it won’t be necessary for this post.\n\ncum_skewness <- function(.x){\n  skewness <- function(.x){\n    sqrt(length(.x)) * sum((.x - mean(.x))^3 / (sum((.x))^2)^(3/2))\n  }\n  sapply(seq_along(.x), function(k, z) skewness(z[1:k]), z = .x)\n}\n\n\n\nData\nWe are going to use the mtcars data set and use the mpg column for this example. Let’s set x equal to mtcars$mpg\n\nx <- mtcars$mpg\n\n\n\nExample\nNow let’s see the function in use.\n\ncum_skewness(x)\n\n [1]  0.000000e+00  0.000000e+00  8.249747e-06  5.049149e-06 -1.113787e-05\n [6] -8.569220e-06 -1.134377e-04 -8.440629e-05 -8.280585e-05 -5.457236e-05\n[11] -3.209937e-05 -1.758922e-05 -5.567456e-06  1.436318e-07 -6.299325e-05\n[16] -8.605705e-05 -5.869380e-05  1.594511e-04  1.675837e-04  2.221143e-04\n[21]  1.855217e-04  1.936299e-04  1.998527e-04  2.082240e-04  1.897575e-04\n[26]  1.505425e-04  1.180971e-04  9.974055e-05  1.048461e-04  9.801797e-05\n[31]  1.024713e-04  9.107160e-05\n\n\nLet’s plot it out.\n\nplot(cum_skewness(x), type = \"l\")"
  },
  {
    "objectID": "posts/rtip-2022-11-07/index.html",
    "href": "posts/rtip-2022-11-07/index.html",
    "title": "Discrete Fourier Vec with healthyR.ai",
    "section": "",
    "text": "Introduction\nSometimes in modeling you may want to get a discrete 1/0 vector of a fourier transform of some input vector. With {healthyR.ai} we can do this easily.\n\n\nFunction\nHere is the full function call:\n\nhai_fourier_discrete_vec(\n  .x,\n  .period,\n  .order,\n  .scale_type = c(\"sin\", \"cos\", \"sincos\")\n)\n\nHere are the parameters to the function and what they expect:\n\n.x - A numeric vector\n.period - The number of observations that complete a cycle\n.order - The fourier term order\n.scale_type - A character of one of the following: sin,cos,sincos\n\nThe internal caluclation is straightforward:\n\nsin = sin(2 * pi * h * x), where h = .order/.period\ncos = cos(2 * pi * h * x), where h = .order/.period\nsincos = sin(2 * pi * h * x) * cos(2 * pi * h * x) where h = .order/.period\n\n\n\nExample\nLet’s work throught a quick and simple example.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(healthyR.ai)\nlibrary(tidyr)\n\nlen_out <- 24\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n  ),\n  a = rnorm(len_out, sd = 2),\n  fv_sin = hai_fourier_discrete_vec(a, 12, 1, \"sin\"),\n  fv_cos = hai_fourier_discrete_vec(a, 12, 1, \"cos\"),\n  fv_sc  = hai_fourier_discrete_vec(a, 12, 1, \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 24 × 5\n   date_col         a fv_sin fv_cos fv_sc\n   <date>       <dbl>  <dbl>  <dbl> <dbl>\n 1 2021-01-01 -0.486       0      1     0\n 2 2021-02-01 -0.708       0      1     0\n 3 2021-03-01 -0.119       0      1     0\n 4 2021-04-01  0.0405      1      1     1\n 5 2021-05-01  1.19        1      1     1\n 6 2021-06-01  1.88        1      1     1\n 7 2021-07-01 -1.32        0      1     0\n 8 2021-08-01 -0.0214      0      1     0\n 9 2021-09-01  2.80        1      1     1\n10 2021-10-01  1.67        1      1     1\n# … with 14 more rows\n\n\n\n\nVisual\nLet’s visualize.\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-08/index.html",
    "href": "posts/rtip-2022-11-08/index.html",
    "title": "Hyperbolic Transform with healthyR.ai",
    "section": "",
    "text": "Introduction\nIn data modeling there can be instanes where you will want some sort of hyperbolic transformation of your data. In {healthyR.ai} this is easy with the use of the function hai_hyperbolic_vec() along with it’s corresponding augment and step functions.\n\n\nFunction\nThe function takes in a numeric vector as it’s argument and will transform the data with one of the following:\n\nsin\ncos\ntan\nsincos This will do: value = sin(x) * cos(x)\n\nThe full function call is:\n\nhai_hyperbolic_vec(.x, .scale_type = c(\"sin\", \"cos\", \"tan\", \"sincos\"))\n\n\n\nExample\n\nlibrary(dplyr)\nlibrary(healthyR.ai)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nlen_out <- 25\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n    ),\n  b = runif(len_out),\n  fv_sin = hai_hyperbolic_vec(b, .scale_type = \"sin\"),\n  fv_cos = hai_hyperbolic_vec(b, .scale_type = \"cos\"),\n  fv_sc  = hai_hyperbolic_vec(b, .scale_type = \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 25 × 5\n   date_col        b fv_sin fv_cos  fv_sc\n   <date>      <dbl>  <dbl>  <dbl>  <dbl>\n 1 2021-01-01 0.961  0.820   0.573 0.470 \n 2 2021-02-01 0.418  0.406   0.914 0.371 \n 3 2021-03-01 0.0729 0.0728  0.997 0.0726\n 4 2021-04-01 0.426  0.413   0.911 0.376 \n 5 2021-05-01 0.851  0.752   0.659 0.496 \n 6 2021-06-01 0.824  0.734   0.679 0.499 \n 7 2021-07-01 0.659  0.612   0.791 0.484 \n 8 2021-08-01 0.683  0.631   0.776 0.490 \n 9 2021-09-01 0.173  0.172   0.985 0.169 \n10 2021-10-01 0.345  0.338   0.941 0.318 \n# … with 15 more rows\n\n\n\n\nVisual\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")"
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html",
    "href": "posts/rtip-2022-11-09/index.html",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "",
    "text": "K-Means is a clustering algorithm that can be used to find potential clusters in your data.\nThe algorithm does require that you look at different values of K in order to assess which is the optimal value.\nIn the R package {healthyR.ai} there is a utility to do this."
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html#parameters",
    "href": "posts/rtip-2022-11-09/index.html#parameters",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "Parameters",
    "text": "Parameters\nThe parameters take the following arguments:\n\n.data - This is the data that should be an output of the hai_user_item_tbl() or it’s synonym, or should at least be in the user item matrix format.\n.centers - The maximum amount of centers you want to map to the k-means function. The default is 15."
  },
  {
    "objectID": "posts/rtip-2022-11-10/index.html",
    "href": "posts/rtip-2022-11-10/index.html",
    "title": "Reading Multiple Files with {purrr}",
    "section": "",
    "text": "Introduction\nThere may be times when you have multiple structured files in the same folder, maybe they are .csv files. For this short tip, we will say that they are.\nI will show the short script and then discuss it.\n\n# Library Load ----\nlibrary(dplyr)\nlibrary(purrr)\n\n# Set file path ----\nfolder    <- \"FileFolder\"\npath      <- \"C:/Some/Root/Path/\"\nfull_path <- paste0(path,folder,\"/\")\n\n# File List ----\nfile_list <- dir(full_path\n                 , pattern = \"\\\\.csv$\"\n                 , full.names = T)\n\n# Read Files ----\nfiles <- file_list %>%\n  map(read.csv) %>%\n  map(as_tibble)\n\n# Clean File Names ----\nfile_names <- file_list %>%\n  str_remove(full_path) %>%\n  str_replace(\n    pattern = \"_OldStuff.csv\", \n    replacement = \"_NewStuff.csv\"\n  )\n\nnames(files) <- file_names\n\nWe load in {dplyr} for the pipe and the as_tibble function. After this we set out to create the file path. I have chosen to do this in two separate pieces as I have had experience with needing to go through different folders in the same root directory. While this could further be scripted I leave it as is.\nfolder is the folder that has the files of interest, in this case the .csv files. We then get the root path to that folder but not including it, this is defined as path in the above. After we have both folder and path we can create the full_path by using paste0\nNow after this we use the base R function of dir to list out all of the files that fit the specific format of .csv with a regex pattern. I always want the name of the file as it allows me to go back to the file later and lets me name the files in the upcoming list later on.\nSince these are .csv files I use purrr::map and then read.csv to read in all of the .csv files in the list that was created, we then used map again and this time used as_tibble to make sure that each file is a tibble and not something else like data.frame\nSince I provided the argument of T to dir, full.names I can then get a character vector of the names of the files which then is applied to the file list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-14/index.html",
    "href": "posts/rtip-2022-11-14/index.html",
    "title": "Find Skewed Features with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes we may want to quickly find skewed features in a data set. This is easily achiveable using the {healthyR.ai} library. There is a simple function called hai_skewed_features(). We are going to go over this function today.\n\n\nFunction\nLet’s first take a look at the function call.\n\nhai_skewed_features(\n  .data, \n  .threshold = 0.6, \n  .drop_keys = NULL\n  )\n\nNow let’s take a look at the arguments that go to the parameters of the function.\n\n.data - The data.frame/tibble you are passing in.\n.threshold - A level of skewness that indicates where you feel a column should be considered skewed.\n.drop_keys - A c() character vector of columns you do not want passed to the function.\n\n\n\nExample\nHere are a couple of examples.\n\nlibrary(healthyR.ai)\n\nhai_skewed_features(mtcars)\n\n[1] \"mpg\"  \"hp\"   \"carb\"\n\nhai_skewed_features(mtcars, .drop_keys = \"hp\")\n\n[1] \"mpg\"  \"carb\""
  },
  {
    "objectID": "posts/rtip-2022-11-15/index.html",
    "href": "posts/rtip-2022-11-15/index.html",
    "title": "Auto Prep data for XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes we may want to quickly format some data in order to just pass it through some algorithm just to see what happens, how crazy are things, just to get an idea of what may lie ahead…a lot of prep.\nWith my r package {healthyR.ai} there is a set of prepper functions that will automatically do a ‘best effort’ to format you data to be used in the algorithm you choose (should it be supported).\nToday we will talk about [hai_xgboost_data_prepper()\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\nNow let’s go over the arguments that are passed to the function.\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\n\n\nExample\nLet’s go over some examples.\n\nlibrary(ggplot2)\nlibrary(healthyR.ai)\n\n# Regression\nhai_xgboost_data_prepper(.data = diamonds, .recipe_formula = price ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nreg_obj <- hai_xgboost_data_prepper(diamonds, price ~ .)\nget_juiced_data(reg_obj)\n\n# A tibble: 53,940 × 27\n   carat depth table     x     y     z price  cut_1  cut_2  cut_3  cut_4   cut_5\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1  0.23  61.5    55  3.95  3.98  2.43   326  0.359 -0.109 -0.522 -0.567 -0.315 \n 2  0.21  59.8    61  3.89  3.84  2.31   326  0.120 -0.436 -0.298  0.378  0.630 \n 3  0.23  56.9    65  4.05  4.07  2.31   327 -0.359 -0.109  0.522 -0.567  0.315 \n 4  0.29  62.4    58  4.2   4.23  2.63   334  0.120 -0.436 -0.298  0.378  0.630 \n 5  0.31  63.3    58  4.34  4.35  2.75   335 -0.359 -0.109  0.522 -0.567  0.315 \n 6  0.24  62.8    57  3.94  3.96  2.48   336 -0.120 -0.436  0.298  0.378 -0.630 \n 7  0.24  62.3    57  3.95  3.98  2.47   336 -0.120 -0.436  0.298  0.378 -0.630 \n 8  0.26  61.9    55  4.07  4.11  2.53   337 -0.120 -0.436  0.298  0.378 -0.630 \n 9  0.22  65.1    61  3.87  3.78  2.49   337 -0.598  0.546 -0.373  0.189 -0.0630\n10  0.23  59.4    61  4     4.05  2.39   338 -0.120 -0.436  0.298  0.378 -0.630 \n# … with 53,930 more rows, and 15 more variables: color_1 <dbl>, color_2 <dbl>,\n#   color_3 <dbl>, color_4 <dbl>, color_5 <dbl>, color_6 <dbl>, color_7 <dbl>,\n#   clarity_1 <dbl>, clarity_2 <dbl>, clarity_3 <dbl>, clarity_4 <dbl>,\n#   clarity_5 <dbl>, clarity_6 <dbl>, clarity_7 <dbl>, clarity_8 <dbl>\n\n# Classification\nhai_xgboost_data_prepper(Titanic, Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\ncla_obj <- hai_xgboost_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(cla_obj)\n\n# A tibble: 32 × 7\n       n Survived Class_X2nd Class_X3rd Class_Crew Sex_Male Age_Child\n   <dbl> <fct>         <dbl>      <dbl>      <dbl>    <dbl>     <dbl>\n 1     0 No                0          0          0        1         1\n 2     0 No                1          0          0        1         1\n 3    35 No                0          1          0        1         1\n 4     0 No                0          0          1        1         1\n 5     0 No                0          0          0        0         1\n 6     0 No                1          0          0        0         1\n 7    17 No                0          1          0        0         1\n 8     0 No                0          0          1        0         1\n 9   118 No                0          0          0        1         0\n10   154 No                1          0          0        1         0\n# … with 22 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-16/index.html",
    "href": "posts/rtip-2022-11-16/index.html",
    "title": "Cumulative Harmonic Mean with {TidyDensity}",
    "section": "",
    "text": "Introduction\nThere can be times in which you may want to see a cumulative statistic, maybe in this particular case it is the harmonic mean. Well with the {TidyDensity} it is possible with a function called chmean()\nLet’s take a look at the function.\n\n\nFunction\nHere is the function call, it is very simple as it is a vectorized function.\n\nchmean(.x)\n\nThe only argument you provide to this function is a numeric vector. Let’s take a quick look at the construction of the function.\n\nchmean <- function(.x) {\n  1 / (cumsum(1 / .x))\n}\n\n\n\nExamples\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\nx <- mtcars$mpg\n\nchmean(x)\n\n [1] 21.0000000 10.5000000  7.1891892  5.3813575  4.1788087  3.3949947\n [7]  2.7436247  2.4663044  2.2255626  1.9943841  1.7934398  1.6166494\n[13]  1.4784877  1.3474251  1.1928760  1.0701322  0.9975150  0.9677213\n[19]  0.9378663  0.9126181  0.8754572  0.8286539  0.7858140  0.7419753\n[25]  0.7143688  0.6961523  0.6779989  0.6632076  0.6364908  0.6165699\n[31]  0.5922267  0.5762786\n\nmtcars %>%\n  select(mpg) %>%\n  mutate(cum_har_mean = chmean(mpg)) %>%\n  head(10)\n\n                   mpg cum_har_mean\nMazda RX4         21.0    21.000000\nMazda RX4 Wag     21.0    10.500000\nDatsun 710        22.8     7.189189\nHornet 4 Drive    21.4     5.381358\nHornet Sportabout 18.7     4.178809\nValiant           18.1     3.394995\nDuster 360        14.3     2.743625\nMerc 240D         24.4     2.466304\nMerc 230          22.8     2.225563\nMerc 280          19.2     1.994384\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-17/index.html",
    "href": "posts/rtip-2022-11-17/index.html",
    "title": "Bootstrap Modeling with {purrr} and {modler}",
    "section": "",
    "text": "Introduction\nMany times in modeling we want to get the uncertainty in the model, well, bootstrapping to the rescue!\nI am going to go over a very simple example on how to use purrr and modelr for this situation. We will use the mtcars dataset.\n\n\nFunctions\nThe main functions that we are going to showcase are purrr::map() and modelr::bootstrap()\n\n\nExamples\nLet’s get right into it.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndf <- mtcars\n\nfit_boots <- df %>% \n  modelr::bootstrap(n = 200, id = 'boot_num') %>%\n  group_by(boot_num) %>%\n  mutate(fit = map(strap, ~lm(mpg ~ ., data = data.frame(.))))\n\nfit_boots\n\n# A tibble: 200 × 3\n# Groups:   boot_num [200]\n   strap                boot_num fit   \n   <list>               <chr>    <list>\n 1 <resample [32 x 11]> 001      <lm>  \n 2 <resample [32 x 11]> 002      <lm>  \n 3 <resample [32 x 11]> 003      <lm>  \n 4 <resample [32 x 11]> 004      <lm>  \n 5 <resample [32 x 11]> 005      <lm>  \n 6 <resample [32 x 11]> 006      <lm>  \n 7 <resample [32 x 11]> 007      <lm>  \n 8 <resample [32 x 11]> 008      <lm>  \n 9 <resample [32 x 11]> 009      <lm>  \n10 <resample [32 x 11]> 010      <lm>  \n# … with 190 more rows\n\n\nNow lets get our parameter estimates.\n\n# get parameters ####\nparams_boot <- fit_boots %>%\n  mutate(tidy_fit = map(fit, tidy)) %>%\n  unnest(cols = tidy_fit) %>%\n  ungroup()\n\n# get predictions\npreds_boot <- fit_boots %>%\n  mutate(augment_fit = map(fit, augment)) %>%\n  unnest(cols = augment_fit) %>%\n  ungroup()\n\nTime to visualize.\n\nlibrary(patchwork)\n\n# plot distribution of estimated parameters\np1 <- ggplot(params_boot, aes(estimate)) +\n  geom_histogram(col = 'black', fill = 'white') +\n  facet_wrap(~ term, scales = 'free') +\n  theme_minimal()\n\n# plot points with predictions\np2 <- ggplot() +\n  geom_line(aes(mpg, .fitted, group = boot_num), preds_boot, alpha = .03) +\n  geom_point(aes(mpg, .fitted), preds_boot, col = 'steelblue', alpha = 0.05) +\n  theme_minimal()\n  \n# plot both\np1 + p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-21/index.html",
    "href": "posts/rtip-2022-11-21/index.html",
    "title": "Bootstrap Modeling with Base R",
    "section": "",
    "text": "Introduction\nI have previously written about bootstrap modeling with {purrr} and {modelr} here. What if you would like to do some simple bootstrap modeling without importing a library? This itself is easy too!\n\n\nExample\nWe will be using a very simple for loop to accomplish this. You will find an excellent post on this on Stats StackExchange from Francisco Jos Goerlich Gisbert\n\nn    <- 2000\ndf   <- mtcars\npred <- numeric(0)\n\nlibrary(tictoc) # for timing\n\ntic()\nset.seed(123)\nfor (i in 1:n){\n  boot    <- sample(nrow(df), n, replace = TRUE)\n  fit     <- lm(mpg ~ wt, data = df[boot,])\n  pred[i] <- predict(fit, newdata = df[boot,]) +\n    sample(resid(fit), size = 1)\n}\ntoc()\n\n6.8 sec elapsed\n\n\nSo we can see that the process ran pretty quickly and the loop itself is not a very difficult one. Let’s explain a little.\nSo the boot object is a sampling of df which in this case is the mtcars data set. We took a sample with replacement from this data set. We took 2000 samples and did this 2000 times.\nNext we made the fit object by fitting a simple linear model to the data where mpg is a function of wt. Once this is done, we made out predictions.\nThat’s it!"
  },
  {
    "objectID": "posts/rtip-2022-11-22/index.html",
    "href": "posts/rtip-2022-11-22/index.html",
    "title": "Data Preprocessing Scale/Normalize with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nA large portion of data modeling occurrs not only in the data cleaning phase but also in the data preprocessing phase. This can include things like scaling or normalizing data before proceeding to the modeling phase. I will discuss one such function from my r package {healthyR.ai}. In this post I will go over hai_data_scale()\nThis is a {recipes} style step function and is tidymodels compliant.\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_data_scale(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"center\",\n  .range_min = 0,\n  .range_max = 1,\n  .scale_factor = 1\n)\n\nNow let’s go over the arguments that get supplied to the parameters of this function.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“center”\n“normalize”\n“range”\n“scale”\n\nrange_min - A single numeric value for the smallest value in the range. This defaults to 0.\n.range_max - A single numeric value for the largeest value in the range. This defaults to 1.\n.scale_factor - A numeric value of either 1 or 2 that scales the numeric inputs by one or two standard deviations. By dividing by two standard deviations, the coefficients attached to continuous predictors can be interpreted the same way as with binary inputs. Defaults to 1.\n\n\n\nExample\nNow let’s see it in action!\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndate_seq <- seq.Date(\n  from = as.Date(\"2013-01-01\"), \n  length.out = 100, \n  by = \"month\"\n)\n\nval_seq <- rep(rnorm(10, mean = 6, sd = 2), times = 10)\n\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col   value\n   <date>     <dbl>\n 1 2013-01-01  6.66\n 2 2013-02-01  6.66\n 3 2013-03-01  5.09\n 4 2013-04-01  6.94\n 5 2013-05-01  5.96\n 6 2013-06-01  6.18\n 7 2013-07-01  3.62\n 8 2013-08-01  7.31\n 9 2013-09-01  4.58\n10 2013-10-01  7.29\n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nnew_rec_obj <- hai_data_scale(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_scale = \"center\"\n)$scale_rec_obj\n\nnew_rec_obj %>% \n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.633 \n 2 2013-02-01  0.630 \n 3 2013-03-01 -0.935 \n 4 2013-04-01  0.909 \n 5 2013-05-01 -0.0676\n 6 2013-06-01  0.149 \n 7 2013-07-01 -2.41  \n 8 2013-08-01  1.28  \n 9 2013-09-01 -1.45  \n10 2013-10-01  1.26  \n# … with 90 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-28/index.html",
    "href": "posts/rtip-2022-11-28/index.html",
    "title": "Default Metric Sets with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nWhen modeling it is always good to understand your model performance against some metric The {tidymodels} package {yardstick} is a great resource for this.\nIn my R package {healthyR.ai} there are two functions that allow you to either minimize or maximize some cost function against your modeling problem.\nThese functions are: * hai_default_regression_metric_set() * hai_default_classification_metric_set()\n\n\nFunction\nThe functions themselves are {yardstick} metric set functions. Let’s take a look at them.\n\nlibrary(healthyR.ai)\n\nhai_default_classification_metric_set()\n\n# A tibble: 11 × 3\n   metric       class        direction\n   <chr>        <chr>        <chr>    \n 1 sensitivity  class_metric maximize \n 2 specificity  class_metric maximize \n 3 recall       class_metric maximize \n 4 precision    class_metric maximize \n 5 mcc          class_metric maximize \n 6 accuracy     class_metric maximize \n 7 f_meas       class_metric maximize \n 8 kap          class_metric maximize \n 9 ppv          class_metric maximize \n10 npv          class_metric maximize \n11 bal_accuracy class_metric maximize \n\nhai_default_regression_metric_set()\n\n# A tibble: 6 × 3\n  metric class          direction\n  <chr>  <chr>          <chr>    \n1 mae    numeric_metric minimize \n2 mape   numeric_metric minimize \n3 mase   numeric_metric minimize \n4 smape  numeric_metric minimize \n5 rmse   numeric_metric minimize \n6 rsq    numeric_metric maximize \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-29/index.html",
    "href": "posts/rtip-2022-11-29/index.html",
    "title": "Working with Lists",
    "section": "",
    "text": "Introduction\nIn R there are many times where we will work with lists. I won’t go into why lists are great or really the structure of a list but rather simply working with them.\n\n\nExample\nFirst let’s make a list.\n\nl <- list(\n  letters,\n  1:26,\n  rnorm(26)\n)\n\nl\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26\n\n[[3]]\n [1] -1.5647537840 -1.3080486753  1.3331315389 -0.5490502644 -0.4467608750\n [6] -1.5876952894  0.2292049732 -0.2885449316  1.4614499298 -0.0864987690\n[11]  0.5686850031 -0.3897819578  0.1776603862 -1.1326372302 -1.8651290164\n[16]  1.2676006036  0.2405115523 -1.0506728047  1.4069277686 -1.0125778892\n[21] -0.7687818102 -0.1325350681  0.3639485041  0.0005700058 -1.0698214370\n[26]  1.1972767040\n\n\nNow let’s look at somethings we can do with lists. First, let’s see if we can get the class of each item in the list. We are going to use lapply() for this.\n\nlapply(l, class)\n\n[[1]]\n[1] \"character\"\n\n[[2]]\n[1] \"integer\"\n\n[[3]]\n[1] \"numeric\"\n\n\nNow, let’s perform some simple operations on each item of the list.\n\nlapply(l, length)\n\n[[1]]\n[1] 26\n\n[[2]]\n[1] 26\n\n[[3]]\n[1] 26\n\ntry(lapply(l, sum))\n\nError in FUN(X[[i]], ...) : invalid 'type' (character) of argument\n\n\nOk so we see taking the sum of the first element of the list in lapply() did not work because of a class type mismatch. Let’s see how we can get around this an only apply the sum function to a numeric type. To do this we can rely on {purrr} by using a function map_if()\n\nlibrary(purrr)\n\nmap_if(l, is.numeric, sum)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 351\n\n[[3]]\n[1] -5.006323\n\n\n\nmap_if(l, is.numeric, mean)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 13.5\n\n[[3]]\n[1] -0.1925509\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-30/index.html",
    "href": "posts/rtip-2022-11-30/index.html",
    "title": "Generate Random Walk Data with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nGenerating random walk data for timesieries analysis does not have to be difficult, and in fact is not. It can be generated for multiple simulations and have a tidy output. How? ts_random_walk() from the {healthyR.ts} package. Let’s take a look at the function.\n\n\nFunction\nHere is the full function call.\n\nts_random_walk(\n  .mean = 0,\n  .sd = 0.1,\n  .num_walks = 100,\n  .periods = 100,\n  .initial_value = 1000\n)\n\nNow let’s look at the arguments to the parameters.\n\n.mean - The desired mean of the random walks\n.sd - The standard deviation of the random walks\n.num_walks - The number of random walks you want generated\n.periods - The length of the random walk(s) you want generated\n.initial_value - The initial value where the random walks should start\n\nThe underlying data of this function is generated by rnorm()\n\n\nExample\nLet’s take a look at an example and see some visuals.\n\nlibrary(healthyR.ts)\nlibrary(ggplot2)\n\ndf <- ts_random_walk(.num_walks = 100)\n\ndf\n\n# A tibble: 10,000 × 4\n     run     x        y cum_y\n   <dbl> <dbl>    <dbl> <dbl>\n 1     1     1 -0.144    856.\n 2     1     2  0.00648  862.\n 3     1     3  0.0726   924.\n 4     1     4 -0.152    784.\n 5     1     5  0.0228   802.\n 6     1     6 -0.0455   765.\n 7     1     7  0.0972   840.\n 8     1     8 -0.234    643.\n 9     1     9 -0.0501   611.\n10     1    10 -0.0358   589.\n# … with 9,990 more rows\n\n\nThere are attributes attached to the output of this function, let’s see what they are.\n\natb <- attributes(df)\n\nnames_to_print <- names(atb)[which(names(atb) != \"row.names\")]\n\natb[names_to_print]\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$names\n[1] \"run\"   \"x\"     \"y\"     \"cum_y\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 0.1\n\n$.num_walks\n[1] 100\n\n$.periods\n[1] 100\n\n$.initial_value\n[1] 1000\n\n\nNow lets visualize.\n\ndf %>%\n   ggplot(\n       mapping = aes(\n           x = x\n           , y = cum_y\n           , color = factor(run)\n           , group = factor(run)\n        )\n    ) +\n    geom_line(alpha = 0.8) +\n    ts_random_walk_ggplot_layers(df)\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-01/index.html",
    "href": "posts/rtip-2022-12-01/index.html",
    "title": "Extract Boilerplate Workflow Metrics with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nWhen working with the {tidymodels} framework there are ways to pull model metrics from a workflow, since {healthyR.ai} is built on and around the {tidyverse} and {tidymodels} we can do the same. This post will focus on the function hai_auto_wflw_metrics()\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_auto_wflw_metrics(.data)\n\nThe only parameter is .data and this is strictly the output object of one of the hai_auto_ boiler plate functions\n\n\nExample\nSince this function requires the input from an hai_auto function, we will walk through an example with the iris data set. We are going to use the hai_auto_knn() to classify the Species.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_knn_data_prepper(data, Species ~ .)\n\nauto_knn <- hai_auto_knn(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\",\n  .grid_size = 2,\n  .num_cores = 4\n)\n\nhai_auto_wflw_metrics(auto_knn)\n\n# A tibble: 22 × 9\n   neighbors weight_func dist_power .metric  .esti…¹  mean     n std_err .config\n       <int> <chr>            <dbl> <chr>    <chr>   <dbl> <int>   <dbl> <chr>  \n 1         8 rank             0.888 accuracy multic… 0.95     25 0.00652 Prepro…\n 2         8 rank             0.888 bal_acc… macro   0.962    25 0.00471 Prepro…\n 3         8 rank             0.888 f_meas   macro   0.947    25 0.00649 Prepro…\n 4         8 rank             0.888 kap      multic… 0.922    25 0.0102  Prepro…\n 5         8 rank             0.888 mcc      multic… 0.925    25 0.00964 Prepro…\n 6         8 rank             0.888 npv      macro   0.975    25 0.00351 Prepro…\n 7         8 rank             0.888 ppv      macro   0.949    25 0.00663 Prepro…\n 8         8 rank             0.888 precisi… macro   0.949    25 0.00663 Prepro…\n 9         8 rank             0.888 recall   macro   0.949    25 0.00633 Prepro…\n10         8 rank             0.888 sensiti… macro   0.949    25 0.00633 Prepro…\n# … with 12 more rows, and abbreviated variable name ¹​.estimator\n\n\nAs we see this pulls out the full metric table from the workflow.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-05/index.html",
    "href": "posts/rtip-2022-12-05/index.html",
    "title": "Naming Items in a List with {purrr}, {dplyr}, or {healthyR}",
    "section": "",
    "text": "Introduction\nMany times when we are working with a data set we will want to break it up into groups and place them into a list and work with them in that fashion. With this it can be useful to the elements of the list named by the column that the data was split upon. Let’s use the iris set as an example where we split on Species.\nThere are two main functions that we will use in this scenario, namely purrr:map() and dplyr::group_split(), you could also use the split function from base r for this.\nWe will also go over how simple this is using the {healthyR} package. Let’s look at the function from {healthyR}\n\n\nFunction\nFull function call.\n\nnamed_item_list(.data, .group_col)\n\nThere are only two arguments to supply.\n\n.data - The data.frame/tibble.\n.group_col - The column that contains the groupings.\n\nThat’s it.\n\n\nExamples\nLet’s jump into it.\n\nlibrary(purrr)\nlibrary(dplyr)\n\ndata_tbl <- iris\n\ndata_tbl_list <- data_tbl %>%\n  group_split(Species)\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n[[1]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n[[2]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n[[3]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\ndata_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\n[[1]]\n[1] \"setosa\"\n\n[[2]]\n[1] \"versicolor\"\n\n[[3]]\n[1] \"virginica\"\n\n\nNow lets go ahead and apply the names.\n\nnames(data_tbl_list) <- data_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nLet’s now see how we do this in {healthyR}\n\nlibrary(healthyR)\n\nnamed_item_list(iris, Species)\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nIf you use this in conjunction with the healthyR function save_to_excel() then it will write an excel file with a tab for each named item in the list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-06/index.html",
    "href": "posts/rtip-2022-12-06/index.html",
    "title": "Z-Score Scaling Step Recipe with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes one may find it useful or necessary to scale their data during a modeling or analysis phase. One of these such transformations is the z-score scaling.\nThis is done simply by performing the below transform where x is simply some numeric vector:\n\\[ z_x = (x - mu(x))/sd(x) \\]\nLet’s take a look at the recipe function called step_hai_scale_zscore\n\n\nFunction\nHere is the full function call:\n\nstep_hai_scale_zscore(\n  recipe,\n  ...,\n  role = \"predictor\",\n  trained = FALSE,\n  columns = NULL,\n  skip = FALSE,\n  id = rand_id(\"hai_scale_zscore\")\n)\n\nHere are the arguments to the function.\n\nrecipe - A recipe object. The step will be added to the sequence of operations for this recipe.\n... - One or more selector functions to choose which variables that will be used to create the new variables. The selected variables should have class numeric\nrole - For model terms created by this step, what analysis role should they be assigned?. By default, the function assumes that the new variable columns created by the original variables will be used as predictors in a model.\ntrained - A logical to indicate if the quantities for preprocessing have been estimated.\ncolumns - A character string of variables that will be used as inputs. This field is a placeholder and will be populated once recipes::prep() is used.\nskip - A logical. Should the step be skipped when the recipe is baked by bake.recipe()? While all operations are baked when prep.recipe() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations.\nid - A character string that is unique to this step to identify it.\n\n\n\nExample\nHere is a simple example.\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndf <- iris |>\n  as_tibble() |>\n  select(Species, Sepal.Length)\n\nrec_obj <- recipe(Sepal.Length ~ ., data = df) %>%\n  step_hai_scale_zscore(Sepal.Length)\n\nrec_obj\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nZero-One Scale Transformation on Sepal.Length\n\nsummary(rec_obj)\n\n# A tibble: 2 × 4\n  variable     type      role      source  \n  <chr>        <list>    <chr>     <chr>   \n1 Species      <chr [3]> predictor original\n2 Sepal.Length <chr [2]> outcome   original\n\n\nNow let’s take a look at the differences.\n\nlibrary(ggplot2)\nlibrary(plotly)\n\ndf_tbl <- get_juiced_data(rec_obj)\n\ndf_tbl |>\n  purrr::set_names(\"Species\",\"Sepal_Length\",\"Scaled_Sepal_Length\") |>\n  ggplot(aes(x = Sepal_Length)) +\n  geom_histogram(color = \"black\", fill = \"lightgreen\") +\n  geom_histogram(aes(x = Scaled_Sepal_Length), \n                 color = \"black\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    y = \"Count\",\n    x = \"Sepal Length\",\n    title = \"Speal.Length: Original vs. Z-Score Scaled\",\n    subtitle = \"Original (Light Green) Scaled (Steelblue)\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-07/index.html",
    "href": "posts/rtip-2022-12-07/index.html",
    "title": "Create Multiple {parsnip} Model Specs with {purrr}",
    "section": "",
    "text": "Introduction\nIf you want to generate multiple parsnip model specifications at the same time then it’s really not to hard. This sort of thing is being addressed in an upcoming package of mine called {tidyaml}\nThis post is going to be quick and simple, I will showcase how you can generate many different model specifications in one go. I will also discuss the function create_model_spec() that will allow you to do this with a simple function call once the package is actually released.\n\n\nFunction\nHere is the function call for the create_model_spec() for once it is release.\n\ncreate_model_spec(\n  .parsnip_eng = list(\"lm\"),\n  .mode = list(\"regression\"),\n  .parsnip_fns = list(\"linear_reg\"),\n  .return_tibble = TRUE\n)\n\nHere are the arguments to the function. * .parsnip_eng - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c(‘lm’, ‘glm’) * .mode- The input must be a list. The default is ‘regression’ * .parsnip_fns - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(“linear_reg”,“cubist_rules”) * .return_tibble - The default is TRUE. FALSE will return a list object.\n\n\nExample\nHere is the function at work.\n\nlibrary(tidyaml)\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow that we have seen what is to come in the future, let’s take a look at a pseudo solution that is easy to replicate now.\n\n# Load the purrr package\nlibrary(purrr)\nlibrary(parsnip)\n\n# Create a list of parsnip engines\nengines <- list(\n  engine1 = \"lm\",\n  engine2 = \"glm\",\n  engine3 = \"randomForest\"\n)\n\n# Create a list of parsnip call names\nparsnip_calls <- list(\n  call1 = \"linear_reg\",\n  call2 = \"linear_reg\",\n  call3 = \"rand_forest\"\n)\n\n# Use pmap() to create a list of parsnip model specs from the list of engines\n# and parsnip call names\n# Set the mode argument to \"regression\"\nmodel_specs <- pmap(list(engines, parsnip_calls), function(engine, call) {\n  match.fun(call)(engine = engine, mode = \"regression\")\n})\n\n# Print the list of model specs to the console\nmodel_specs\n\n$engine1\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$engine2\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$engine3\nRandom Forest Model Specification (regression)\n\nComputational engine: randomForest \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-08/index.html",
    "href": "posts/rtip-2022-12-08/index.html",
    "title": "Create a Faceted Historgram Plot with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nOne of the most important steps in data analysis is visualizing the distribution of your data. This can help you identify patterns, outliers, and trends in your data, and can also provide valuable insights into the relationships between different variables.\nOne way to visualize data distributions is by using histograms. A histogram is a graphical representation of the distribution of a numeric variable. It shows the number of observations (or the frequency) within each bin or range of values.\nIn this blog post, we will showcase the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create faceted histograms of numeric and factor data in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nhai_histogram_facet_plot(\n  .data,\n  .bins = 10,\n  .scale_data = FALSE,\n  .ncol = 5,\n  .fct_reorder = FALSE,\n  .fct_rev = FALSE,\n  .fill = \"steelblue\",\n  .color = \"white\",\n  .scale = \"free\",\n  .interactive = FALSE\n)\n\nHere are the parameters and the arguments that get passed to them.\n\n.data - The data you want to pass to the function.\n.bins - The number of bins for the histograms.\n.scale_data - This is a boolean set to FALSE. TRUE will use hai_scale_zero_one_vec() to [0, 1] scale the data.\n.ncol - The number of columns for the facet_warp argument.\n.fct_reorder - Should the factor column be reordered? TRUE/FALSE, default of FALSE\n.fct_rev - Should the factor column be reversed? TRUE/FALSE, default of FALSE\n.fill - Default is steelblue\n.color - Default is ‘white’\n.scale - Default is ‘free’\n.interactive - Default is FALSE, TRUE will produce a {plotly} plot.\n\n\n\nExamples\nLet’s take a look at some example.\n\nlibrary(healthyR.ai, quietly = TRUE)\n\nhai_histogram_facet_plot(mtcars)\n\n\n\n\nNow lets scale the data and review.\n\nhai_histogram_facet_plot(mtcars, .scale_data = TRUE)\n\n\n\n\nLet’s take a look the iris data set now.\n\noutput <- hai_histogram_facet_plot(iris, .interactive = TRUE)\noutput$plot\n\n\n\n\n\nIn this blog post, we showcased the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create histogram plots and faceted histograms in R. The hai_histogram_facet_plot() function allows you to quickly and easily visualize the distribution of your data, and can provide valuable insights into the relationships between different variables\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-13/index.html",
    "href": "posts/rtip-2022-12-13/index.html",
    "title": "Mixture Distributions with {TidyDensity}",
    "section": "",
    "text": "Introduction\nA mixture distribution is a type of probability distribution that is created by combining two or more simpler distributions. This allows us to model complex data that may have multiple underlying patterns. For example, a mixture distribution could be used to model a dataset that includes both continuous and discrete variables.\nTo create a mixture distribution, we first need to specify the individual distributions that will be combined, as well as the weights that determine how much each distribution contributes to the overall mixture. Once we have these components, we can use them to calculate the probability of any given value occurring in the mixture distribution.\nMixture distributions can be useful in a variety of applications, such as data analysis and machine learning. In data analysis, they can be used to model data that is not well-described by a single distribution, and in machine learning, they can be used to improve the performance of predictive models. Overall, mixture distributions are a powerful tool for understanding and working with complex data.\n\n\nFunction\nLet’s take a look a function in {TidyDensity} that allows us to do this. At this moment, weights are not a parameter to the function.\n\ntidy_mixture_density(...)\n\nNow let’s take a look at the arguments that get supplied to the ... parameter.\n\n... - The random data you want to pass. Example rnorm(50,0,1) or something like tidy_normal(.mean = 5, .sd = 1)\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\n\noutput <- tidy_mixture_density(\n  rnorm(100, 0, 1), \n  tidy_normal(.mean = 5, .sd = 1)\n)\n\nAs you can see, you can enter a function that outputs a numeric vector or you can use a {TidyDensity} distribution function.\nLet’s take a look at the outputs.\n\noutput$data\n\n$dist_tbl\n# A tibble: 150 × 2\n       x       y\n   <int>   <dbl>\n 1     1  0.442 \n 2     2  1.80  \n 3     3  0.571 \n 4     4 -0.0365\n 5     5  0.854 \n 6     6 -0.634 \n 7     7 -0.189 \n 8     8 -0.415 \n 9     9  1.36  \n10    10  0.107 \n# … with 140 more rows\n\n$dens_tbl\n# A tibble: 150 × 2\n       x         y\n   <dbl>     <dbl>\n 1 -4.17 0.0000995\n 2 -4.08 0.000145 \n 3 -3.98 0.000207 \n 4 -3.89 0.000294 \n 5 -3.79 0.000413 \n 6 -3.70 0.000574 \n 7 -3.60 0.000788 \n 8 -3.51 0.00107  \n 9 -3.41 0.00145  \n10 -3.32 0.00193  \n# … with 140 more rows\n\n$input_data\n$input_data$`rnorm(100, 0, 1)`\n  [1]  0.44169781  1.80418306  0.57133927 -0.03649729  0.85387119 -0.63383074\n  [7] -0.18854658 -0.41451222  1.36023418  0.10726858  0.08526992 -0.64879496\n [13]  0.69255412 -0.75735669  0.19705920 -0.17721516 -0.63079170 -1.39983310\n [19]  1.01755199 -0.83631414  0.72912414 -0.14737137  1.27082258 -1.04753889\n [25] -0.16141490  0.22198899  2.83598596 -0.22484669 -0.58487594 -0.62746477\n [31] -0.81873031  1.74559087  1.36529721  1.45023471 -0.06258668  2.14467649\n [37]  0.10043517 -0.67990809  2.85050168 -1.45216256  0.01049808  0.22827703\n [43] -0.51146361  0.43143915 -0.59915348  1.61324991 -0.58580448 -0.46120961\n [49]  0.98191810 -0.31593955  0.86164296  1.18808250  1.09066101  0.39150090\n [55]  0.50730674  1.88640675  1.55522681 -0.65149477 -0.27561149 -0.31867192\n [61]  0.08555271 -1.00047014  1.12127311 -1.23597493  0.96384070  0.99097697\n [67] -0.25932523  0.25407058 -0.35294377 -0.72055148 -0.40429088 -0.08843004\n [73]  0.95498089 -0.68453125  1.67531797 -0.20665261  0.57318766 -0.12758793\n [79] -0.38044927  1.81833828  1.05959931  0.08519174  0.16865694 -0.15828443\n [85]  0.08736815  0.70222886  1.27180668  0.76483122 -0.43573173  0.02909088\n [91] -1.31286933 -0.09244617  0.22188836 -0.88909052  1.22243358  0.48397190\n [97]  0.82291445  0.46595188  0.68619052 -1.65739185\n\n$input_data$`tidy_normal(.mean = 5, .sd = 1)`\n# A tibble: 50 × 7\n   sim_number     x     y    dx       dy      p     q\n   <fct>      <int> <dbl> <dbl>    <dbl>  <dbl> <dbl>\n 1 1              1  6.20  1.24 0.000230 0.886   6.20\n 2 1              2  3.95  1.40 0.000639 0.146   3.95\n 3 1              3  4.59  1.55 0.00158  0.342   4.59\n 4 1              4  3.16  1.70 0.00349  0.0326  3.16\n 5 1              5  5.03  1.86 0.00689  0.513   5.03\n 6 1              6  4.89  2.01 0.0122   0.455   4.89\n 7 1              7  5.49  2.16 0.0195   0.687   5.49\n 8 1              8  6.78  2.32 0.0283   0.962   6.78\n 9 1              9  5.17  2.47 0.0376   0.566   5.17\n10 1             10  6.36  2.62 0.0464   0.913   6.36\n# … with 40 more rows\n\n\nAnd now the visuals that come with it.\n\noutput$plots\n\n$line_plot\n\n\n\n\n\n\n$dens_plot\n\n\n\n\n\nThe function also lists the input functions as well.\n\noutput$input_fns\n\n[[1]]\nrnorm(100, 0, 1)\n\n[[2]]\ntidy_normal(.mean = 5, .sd = 1)\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-14/index.html",
    "href": "posts/rtip-2022-12-14/index.html",
    "title": "Distribution Summaries with {TidyDensity}",
    "section": "",
    "text": "Introduction\n{TidyDensity} is an R package that provides tools for working with probability distributions in a tidy data format. One of the key functions in the package is tidy_distribution_summary_tbl(), which allows users to quickly and easily get summary information about a probability distribution.\nThe tidy_distribution_summary_tbl() function takes a vector of data as input and returns a table with basic statistics about the distribution of the data. This includes the mean, standard deviation, kurtosis, and skewness of the data, as well as other useful information.\nUsing tidy_distribution_summary_tbl(), users can easily get a high-level overview of their data, which can be useful for exploratory data analysis, data visualization, and other tasks. The function is designed to work seamlessly with the other tools in the {TidyDensity} package, making it easy to combine with other operations and build complex data analysis pipelines.\nOverall, TidyDensity and its tidy_distribution_summary_tbl() function are valuable tools for anyone working with probability distributions in R. Whether you are a seasoned data scientist or a beginner, TidyDensity can help you quickly and easily explore and understand your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_distribution_summary_tbl(.data, ...)\n\nHere are the arguments that go to the parameters.\n\n.data - The data that is going to be passed from a a tidy_ distribution function.\n... - This is the grouping variable that gets passed to dplyr::group_by() and dplyr::select().\n\n\n\nExample\nNow let’s go over a simple example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <- tidy_normal(.num_sims = 5)\ntb <- tidy_beta(.num_sims = 5)\n\ntidy_distribution_summary_tbl(tn) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> -0.044964\n$ median_val <dbl> -0.0266966\n$ std_val    <dbl> 1.020322\n$ min_val    <dbl> -2.834123\n$ max_val    <dbl> 3.336879\n$ skewness   <dbl> 0.03115634\n$ kurtosis   <dbl> 2.772527\n$ range      <dbl> 6.171002\n$ iqr        <dbl> 1.447849\n$ variance   <dbl> 1.041057\n$ ci_low     <dbl> -1.873091\n$ ci_high    <dbl> 1.868382\n\ntidy_distribution_summary_tbl(tn, sim_number) |>\n  glimpse()\n\nRows: 5\nColumns: 13\n$ sim_number <fct> 1, 2, 3, 4, 5\n$ mean_val   <dbl> -0.09684833, -0.13886169, 0.23257556, -0.32487778, 0.103192…\n$ median_val <dbl> -0.1358051, -0.2550682, 0.3069263, -0.1334922, 0.2898412\n$ std_val    <dbl> 1.1231699, 1.0954659, 0.8902380, 0.9270631, 0.9919932\n$ min_val    <dbl> -2.834123, -2.340575, -1.963215, -2.396105, -1.827744\n$ max_val    <dbl> 3.336879, 1.987640, 2.066451, 1.526231, 2.093211\n$ skewness   <dbl> 0.352771389, 0.132723834, -0.282840344, -0.191853538, 0.006…\n$ kurtosis   <dbl> 3.652828, 2.169309, 2.749967, 2.332081, 2.409223\n$ range      <dbl> 6.171002, 4.328215, 4.029666, 3.922336, 3.920956\n$ iqr        <dbl> 1.5256470, 1.6335396, 0.9368546, 1.3968485, 1.3469671\n$ variance   <dbl> 1.2615106, 1.2000455, 0.7925236, 0.8594460, 0.9840505\n$ ci_low     <dbl> -1.834548, -1.844197, -1.428713, -2.193065, -1.626225\n$ ci_high    <dbl> 1.860755, 1.858576, 1.644153, 1.090125, 1.976371\n\ndata_tbl <- tidy_combine_distributions(tn, tb)\n\ntidy_distribution_summary_tbl(data_tbl) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> 0.2413251\n$ median_val <dbl> 0.3687409\n$ std_val    <dbl> 0.8030476\n$ min_val    <dbl> -2.834123\n$ max_val    <dbl> 3.336879\n$ skewness   <dbl> -0.7608556\n$ kurtosis   <dbl> 4.248452\n$ range      <dbl> 6.171002\n$ iqr        <dbl> 0.7835065\n$ variance   <dbl> 0.6448855\n$ ci_low     <dbl> -1.695096\n$ ci_high    <dbl> 1.585147\n\ntidy_distribution_summary_tbl(data_tbl, dist_type) |>\n  glimpse()\n\nRows: 2\nColumns: 13\n$ dist_type  <fct> \"Gaussian c(0, 1)\", \"Beta c(1, 1, 0)\"\n$ mean_val   <dbl> -0.0449640, 0.5276142\n$ median_val <dbl> -0.0266966, 0.5301650\n$ std_val    <dbl> 1.0203220, 0.2944871\n$ min_val    <dbl> -2.834123047, 0.001236575\n$ max_val    <dbl> 3.3368786, 0.9992146\n$ skewness   <dbl> 0.03115634, -0.08744219\n$ kurtosis   <dbl> 2.772527, 1.751248\n$ range      <dbl> 6.171002, 0.997978\n$ iqr        <dbl> 1.447849, 0.511105\n$ variance   <dbl> 1.04105699, 0.08672268\n$ ci_low     <dbl> -1.87309115, 0.04220623\n$ ci_high    <dbl> 1.8683817, 0.9771898"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html",
    "href": "posts/rtip-2022-12-15/index.html",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "The R package {healthyR.ts}, is an R package that allows users to easily plot and analyze their time series data. The package includes a variety of functions, but one of the standout features is the ts_sma_plot() function, which allows users to quickly visualize their time series data and any number of simple moving averages (SMAs) of their choosing.\nSMAs are a common tool used by analysts and investors to smooth out short-term fluctuations in data and identify longer-term trends. By overlaying SMAs of different time periods on top of the original time series data, the ts_sma_plot() function makes it easy to compare and contrast different time periods and identify potential trends and patterns in the data.\nWith {healthyR.ts} and the ts_sma_plot() function, users can quickly and easily gain valuable insights into their time series data and make more informed decisions based on the trends and patterns they uncover.\nOk enough of that, let’s see the function."
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#data",
    "href": "posts/rtip-2022-12-15/index.html#data",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Data",
    "text": "Data\n\nout$data\n\n# A tibble: 288 × 5\n   index     date_col   value sma_order sma_value\n   <yearmon> <date>     <dbl> <fct>         <dbl>\n 1 Jan 1949  1949-01-01   112 3               NA \n 2 Feb 1949  1949-02-01   118 3              121.\n 3 Mar 1949  1949-03-01   132 3              126.\n 4 Apr 1949  1949-04-01   129 3              127.\n 5 May 1949  1949-05-01   121 3              128.\n 6 Jun 1949  1949-06-01   135 3              135.\n 7 Jul 1949  1949-07-01   148 3              144.\n 8 Aug 1949  1949-08-01   148 3              144 \n 9 Sep 1949  1949-09-01   136 3              134.\n10 Oct 1949  1949-10-01   119 3              120.\n# … with 278 more rows"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#plots",
    "href": "posts/rtip-2022-12-15/index.html#plots",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Plots",
    "text": "Plots\n\nout$plots$static_plot\n\nWarning: Removed 7 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nout$plots$interactive_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-19/index.html",
    "href": "posts/rtip-2022-12-19/index.html",
    "title": "Viewing Different Versions of the Same Statistical Distribution with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIn statistics, it is often useful to view different versions of the same statistical distribution. For example, when working with the normal distribution, it may be helpful to see how the distribution changes as the mean and standard deviation are varied.\nOne way to do this is by using the R library {TidyDensity}, which has a function called tidy_multi_single_dist(). This function allows a user to easily generate multiple versions of the same statistical distribution which can be plotted on the same graph, with each version representing a different combination of mean and standard deviation.\nTo use this function, the user simply needs to specify the distribution they want to plot (e.g. “normal”), the range of values for the mean and standard deviation, and the number of versions they want to plot. The function will then generate a plot showing the different versions of the distribution, with each version represented by a different color.\nThere are several reasons why it might be a good idea to view different versions of the same statistical distribution. For one, it can help the user understand how the shape of the distribution changes as the mean and standard deviation are varied. This can be particularly useful for distributions that have a wide range of possible values for the mean and standard deviation, such as the normal distribution.\nIn addition, viewing different versions of the same distribution can also help the user identify patterns and trends in the data. For example, the user may notice that the distribution becomes more spread out as the standard deviation increases, or that the distribution shifts to the left or right as the mean changes.\nOverall, the TidyDensity function tidy_multi_single_dist() is a useful tool for anyone interested in visualizing different versions of the same statistical distribution. Whether you are a student learning about statistics for the first time, or an experienced data scientist looking to better understand your data, this function can help you gain a deeper understanding of the underlying distribution and identify patterns and trends in your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_multi_single_dist(\n  .tidy_dist = NULL, \n  .param_list = list()\n  )\n\nNow let’s look at the arguments that go to the parameters.\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the {TidyDensity} ‘tidy_’ distribution function.\n\n\n\nExample\nLet’s run through an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <-tidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 200,\n    .mean = c(-1, 0, 1),\n    .sd = 1,\n    .num_sims = 3\n  )\n)\n\nNow that we have generated the data, let’s take a look and see if these different distributions have indeed been created.\n\ntidy_distribution_summary_tbl(tn, dist_name) |>\n  select(dist_name, mean_val, std_val)\n\n# A tibble: 3 × 3\n  dist_name         mean_val std_val\n  <fct>                <dbl>   <dbl>\n1 Gaussian c(-1, 1)  -1.03     0.988\n2 Gaussian c(0, 1)    0.0136   1.01 \n3 Gaussian c(1, 1)    0.990    1.02 \n\n\nLook’s good there, now let’s visualize.\n\ntn %>%\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-20/index.html",
    "href": "posts/rtip-2022-12-20/index.html",
    "title": "Random Walks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a type of stochastic process that can be used to model the movement of a particle or system over time. At each time step, the position of the particle is updated based on a random step drawn from a given probability distribution. This process can be represented as a sequence of independent and identically distributed (i.i.d.) random variables, and the resulting path traced by the particle is known as a random walk.\nRandom walks have a wide range of applications, including modeling the movement of stock prices, animal migration, and the spread of infectious diseases. They are also a fundamental concept in probability and statistics, and have been studied extensively in the literature.\nThe {TidyDensity} package provides a convenient way to generate and visualize random walks using the tidy_random_walk() function. This function takes a probability distribution as an argument, and generates a random walk by sampling from this distribution at each time step. For example, to generate a random walk with normally distributed steps, we can use the tidy_normal() function as follows:\n\nlibrary(TidyDensity)\n\n# Generate a random walk with normally distributed steps\nrw <- tidy_random_walk(tidy_normal())\n\nThe resulting object rw is a tibble with the typical tidy_ distribution columns and one augmented column called random_walk_value. The columns that are output are:\n\nsim_number The current simulation number from the tidy_ distribution\nx (You can think of this as time t)\ny The randomly generated value.\ndx & dy The density estimates of y at x\np & q The probability and quantile values of y\nrandom_walk_value The random walk value generated from tidy_random_walk() (You can think of this as the position of the particle at time t or the x)\n\nTo visualize the random walk, we can use the tidy_random_walk_autoplot() function, which creates a ggplot object showing the position of the particle at each time step. For example:\n\n# Visualize the random walk\ntidy_random_walk_autoplot(rw)\n\nThis will produce a plot showing the trajectory of the particle over time. You can customize the appearance of the plot by passing additional arguments to the tidy_random_walk_autoplot() function, such as the geom argument to specify the type of plot to use (e.g. geom = “line” for a line plot, or geom = “point” for a scatter plot).\nIn summary, the {TidyDensity} package provides a convenient and user-friendly interface for generating and visualizing random walks. With the tidy_random_walk() and tidy_random_walk_autoplot() functions, you can easily explore the behavior of random walks and their applications in a wide range of contexts.\nLet’s take a look at these functions.\n\n\nFunction\nFirstly we will look at the tidy_random_walk() function.\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\nHere are the arguments that get provided to the parameters of this function.\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\nNow let’s do the same with the tidy_random_walk_autoplot() function.\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\nHere are the arguments that get provided to the parameters.\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExamples\nLet’s go over some examples.\n\nlibrary(TidyDensity)\n\ndist_data <- tidy_normal(.sd = .1, .num_sims = 5)\n\ntidy_random_walk(.data = dist_data, .value_type = \"cum_sum\") %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nAnd another.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nNow let’s get an interactive one.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot(.interactive = TRUE)\n\n\n\n\n\nOne last example, let’s use a different distribution. Let’s use a cauchy distribution.\n\ntidy_cauchy(.num_sims = 9, .location = .5) %>%\n  tidy_random_walk(.value_type = \"cum_sum\", .sample = TRUE) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-21/index.html",
    "href": "posts/rtip-2022-12-21/index.html",
    "title": "Distribution Statistics with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re working with statistical distributions in R, you may be interested in the {TidyDensity} package. This package provides a set of functions for creating, manipulating, and visualizing probability distributions in a tidy format. One of these functions is tidy_chisquare(), which allows you to create a chi-square distribution with a specified number of degrees of freedom and a non-centrality parameter.\nOnce you’ve created a chi-square distribution using tidy_chisquare(), you may want to get some summary statistics about the distribution. This is where the util_chisquare_stats_tbl() function comes in handy. This function takes a chi-square distribution (created with tidy_chisquare()) as input and returns a tibble with several statistics about the distribution.\nSome of the statistics included in the table are:\n\nMean: The mean of the chi-square distribution, also known as the expected value.\nVariance: The variance of the chi-square distribution, which is a measure of how spread out the data is.\nSkewness: The skewness of the chi-square distribution, which is a measure of the symmetry of the data.\nKurtosis: The kurtosis of the chi-square distribution, which is a measure of the peakedness of the data.\n\nTo use the util_chisquare_stats_tbl() function, you’ll need to install and load the {TidyDensity} package first. Then, you can create a chi-square distribution using tidy_chisquare() and pass it to util_chisquare_stats_tbl() like this:\n\n# install and load TidyDensity\ninstall.packages(\"TidyDensity\")\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# create a chi-square distribution with 5 degrees of freedom\ndistribution <- tidy_chisquare(.df = 5)\n\n# get statistics about the distribution\nutil_chisquare_stats_tbl(distribution) |>\n  glimpse()\n\nThe output will be a table with the mean, variance, skewness, and kurtosis of the chi-square distribution. These statistics can be useful for understanding the characteristics of the distribution and making statistical inferences.\nOverall, the {TidyDensity} package is a useful tool for working with statistical distributions in R. The util_chisquare_stats_tbl() function is just one of many functions available in the package that can help you analyze and understand your data. Give it a try and see how it can help with your statistical analysis!\n\n\nFunction\nLet’s take a look at the full function call.\n\nutil_chisquare_stats_tbl(.data)\n\nLet’s take a look at the arguments that get supplied to the function parameters.\n\n.data - The data being passed from a tidy_ distribution function.\n\n\n\nExample\nNow for a full example with output.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntidy_chisquare() %>%\n  util_chisquare_stats_tbl() %>%\n  glimpse()\n\nRows: 1\nColumns: 17\n$ tidy_function     <chr> \"tidy_chisquare\"\n$ function_call     <chr> \"Chisquare c(1, 1)\"\n$ distribution      <chr> \"Chisquare\"\n$ distribution_type <chr> \"continuous\"\n$ points            <dbl> 50\n$ simulations       <dbl> 1\n$ mean              <dbl> 1\n$ median            <dbl> 0.3333333\n$ mode              <chr> \"undefined\"\n$ std_dv            <dbl> 1.414214\n$ coeff_var         <dbl> 1.414214\n$ skewness          <dbl> 2.828427\n$ kurtosis          <dbl> 15\n$ computed_std_skew <dbl> 1.132669\n$ computed_std_kurt <dbl> 3.894553\n$ ci_lo             <dbl> 0.002189912\n$ ci_hi             <dbl> 6.521727\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-22/index.html",
    "href": "posts/rtip-2022-12-22/index.html",
    "title": "Listing Functions and Parameters",
    "section": "",
    "text": "Introduction\nI got a little bored one day and decided I wanted to list out all of the functions inside of a package along with their parameters in a tibble. Not sure if this serves any particular purpose or not, I was just bored.\nThis does not work for packages that have data as an export like {healthyR} or {healthyR.data} but it will work for packages like {TidyDensity}.\nLet’s run through it\n\n\nExamples\nHere we go.\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:TidyDensity\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nbootstrap_density_augment\n.data\nbootstrap_density_augment(.data)\n\n\nbootstrap_p_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_p_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_p_vec\n.x\nbootstrap_p_vec(.x)\n\n\nbootstrap_q_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_q_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_q_vec\n.x\nbootstrap_q_vec(.x)\n\n\nbootstrap_stat_plot\nc(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\nbootstrap_stat_plot(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\n\n\nbootstrap_unnest_tbl\n.data\nbootstrap_unnest_tbl(.data)\n\n\ncgmean\n.x\ncgmean(.x)\n\n\nchmean\n.x\nchmean(.x)\n\n\nci_hi\nc(“.x”, “.na_rm”)\nci_hi(“.x”, “.na_rm”)\n\n\nci_lo\nc(“.x”, “.na_rm”)\nci_lo(“.x”, “.na_rm”)\n\n\nckurtosis\n.x\nckurtosis(.x)\n\n\ncmean\n.x\ncmean(.x)\n\n\ncmedian\n.x\ncmedian(.x)\n\n\ncolor_blind\nNULL\ncolor_blind(NULL)\n\n\ncsd\n.x\ncsd(.x)\n\n\ncskewness\n.x\ncskewness(.x)\n\n\ncvar\n.x\ncvar(.x)\n\n\ndist_type_extractor\n.x\ndist_type_extractor(.x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\ntd_scale_color_colorblind\nc(“…”, “theme”)\ntd_scale_color_colorblind(“…”, “theme”)\n\n\ntd_scale_fill_colorblind\nc(“…”, “theme”)\ntd_scale_fill_colorblind(“…”, “theme”)\n\n\ntidy_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_bernoulli\nc(“.n”, “.prob”, “.num_sims”)\ntidy_bernoulli(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_beta\nc(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\ntidy_beta(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\n\n\ntidy_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_bootstrap\nc(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\ntidy_bootstrap(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\n\n\ntidy_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_cauchy\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_cauchy(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_chisquare\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_chisquare(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_combine_distributions\n…\ntidy_combine_distributions(…)\n\n\ntidy_combined_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_combined_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_distribution_comparison\nc(“.x”, “.distribution_type”)\ntidy_distribution_comparison(“.x”, “.distribution_type”)\n\n\ntidy_distribution_summary_tbl\nc(“.data”, “…”)\ntidy_distribution_summary_tbl(“.data”, “…”)\n\n\ntidy_empirical\nc(“.x”, “.num_sims”, “.distribution_type”)\ntidy_empirical(“.x”, “.num_sims”, “.distribution_type”)\n\n\ntidy_exponential\nc(“.n”, “.rate”, “.num_sims”)\ntidy_exponential(“.n”, “.rate”, “.num_sims”)\n\n\ntidy_f\nc(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\ntidy_f(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\n\n\ntidy_four_autoplot\nc(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_four_autoplot(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_gamma\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_gamma(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_beta\nc(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_beta(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_pareto\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_pareto(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_hypergeometric\nc(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\ntidy_hypergeometric(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\n\n\ntidy_inverse_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_exponential\nc(“.n”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_exponential(“.n”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_gamma\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_gamma(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_normal\nc(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\ntidy_inverse_normal(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\n\n\ntidy_inverse_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_inverse_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_weibull\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_weibull(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_kurtosis_vec\n.x\ntidy_kurtosis_vec(.x)\n\n\ntidy_logistic\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_logistic(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_lognormal\nc(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\ntidy_lognormal(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\n\n\ntidy_mixture_density\n…\ntidy_mixture_density(…)\n\n\ntidy_multi_dist_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_multi_dist_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_multi_single_dist\nc(“.tidy_dist”, “.param_list”)\ntidy_multi_single_dist(“.tidy_dist”, “.param_list”)\n\n\ntidy_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_normal\nc(“.n”, “.mean”, “.sd”, “.num_sims”)\ntidy_normal(“.n”, “.mean”, “.sd”, “.num_sims”)\n\n\ntidy_paralogistic\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_paralogistic(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_pareto1\nc(“.n”, “.shape”, “.min”, “.num_sims”)\ntidy_pareto1(“.n”, “.shape”, “.min”, “.num_sims”)\n\n\ntidy_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\ntidy_random_walk\nc(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\ntidy_random_walk(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\n\n\ntidy_random_walk_autoplot\nc(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\ntidy_random_walk_autoplot(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\n\n\ntidy_range_statistic\n.x\ntidy_range_statistic(.x)\n\n\ntidy_scale_zero_one_vec\n.x\ntidy_scale_zero_one_vec(.x)\n\n\ntidy_skewness_vec\n.x\ntidy_skewness_vec(.x)\n\n\ntidy_stat_tbl\nc(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\ntidy_stat_tbl(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\n\n\ntidy_t\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_t(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_uniform\nc(“.n”, “.min”, “.max”, “.num_sims”)\ntidy_uniform(“.n”, “.min”, “.max”, “.num_sims”)\n\n\ntidy_weibull\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_weibull(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_zero_truncated_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_zero_truncated_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_zero_truncated_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\nutil_bernoulli_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_bernoulli_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_bernoulli_stats_tbl\n.data\nutil_bernoulli_stats_tbl(.data)\n\n\nutil_beta_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_beta_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_beta_stats_tbl\n.data\nutil_beta_stats_tbl(.data)\n\n\nutil_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_binomial_stats_tbl\n.data\nutil_binomial_stats_tbl(.data)\n\n\nutil_cauchy_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_cauchy_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_cauchy_stats_tbl\n.data\nutil_cauchy_stats_tbl(.data)\n\n\nutil_chisquare_stats_tbl\n.data\nutil_chisquare_stats_tbl(.data)\n\n\nutil_exponential_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_exponential_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_exponential_stats_tbl\n.data\nutil_exponential_stats_tbl(.data)\n\n\nutil_f_stats_tbl\n.data\nutil_f_stats_tbl(.data)\n\n\nutil_gamma_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_gamma_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_gamma_stats_tbl\n.data\nutil_gamma_stats_tbl(.data)\n\n\nutil_geometric_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_geometric_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_geometric_stats_tbl\n.data\nutil_geometric_stats_tbl(.data)\n\n\nutil_hypergeometric_param_estimate\nc(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\nutil_hypergeometric_param_estimate(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\n\n\nutil_hypergeometric_stats_tbl\n.data\nutil_hypergeometric_stats_tbl(.data)\n\n\nutil_logistic_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_logistic_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_logistic_stats_tbl\n.data\nutil_logistic_stats_tbl(.data)\n\n\nutil_lognormal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_lognormal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_lognormal_stats_tbl\n.data\nutil_lognormal_stats_tbl(.data)\n\n\nutil_negative_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_negative_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_negative_binomial_stats_tbl\n.data\nutil_negative_binomial_stats_tbl(.data)\n\n\nutil_normal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_normal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_normal_stats_tbl\n.data\nutil_normal_stats_tbl(.data)\n\n\nutil_pareto_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_pareto_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_pareto_stats_tbl\n.data\nutil_pareto_stats_tbl(.data)\n\n\nutil_poisson_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_poisson_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_poisson_stats_tbl\n.data\nutil_poisson_stats_tbl(.data)\n\n\nutil_t_stats_tbl\n.data\nutil_t_stats_tbl(.data)\n\n\nutil_uniform_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_uniform_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_uniform_stats_tbl\n.data\nutil_uniform_stats_tbl(.data)\n\n\nutil_weibull_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_weibull_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_weibull_stats_tbl\n.data\nutil_weibull_stats_tbl(.data)\n\n\n\n\n\nAnother example.\n\nlibrary(healthyverse)\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:healthyverse\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\nexpr\nexpr\nexpr(expr)\n\n\nhealthyverse_conflicts\nNULL\nhealthyverse_conflicts(NULL)\n\n\nhealthyverse_deps\nc(“recursive”, “repos”)\nhealthyverse_deps(“recursive”, “repos”)\n\n\nhealthyverse_packages\ninclude_self\nhealthyverse_packages(inlude_self)\n\n\nhealthyverse_sitrep\nNULL\nhealthyverse_sitrep(NULL)\n\n\nhealthyverse_update\nc(“recursive”, “repos”)\nhealthyverse_update(“recursive”, “repos”)\n\n\nsym\nx\nsym(x)\n\n\nsyms\nx\nsyms(x)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-29/index.html",
    "href": "posts/rtip-2022-12-29/index.html",
    "title": "Gartner Magic Chart and its usefulness in healthcare analytics with {healthyR}",
    "section": "",
    "text": "Introduction\nThe Gartner Magic Chart is a powerful tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. It was developed by Dr. James Gartner in the early 2000s as a way to visualize the relationship between two key metrics, for example: Excess Length of Stay (ELOS) and Excess Readmit Rate.\nIn healthcare, length of stay (LOS) refers to the amount of time a patient spends in the hospital. Excess LOS is the difference between the actual LOS of a patient and the expected LOS for that patient, based on their diagnosis and other factors. Excess readmit rate is the percentage of patients who are readmitted to the hospital within a certain time period after being discharged, above and beyond what is expected based on their diagnosis and other factors.\nThe Gartner Magic Chart can plot excess LOS on the x-axis and excess readmit rate on the y-axis. The resulting chart is divided into four quadrants, with the top right quadrant representing high excess LOS and high excess readmit rate, the bottom left quadrant representing low excess LOS and low excess readmit rate, and the other two quadrants representing intermediate values of these metrics.\nOne of the key benefits of the Gartner Magic Chart is that it allows healthcare professionals to quickly and easily identify areas of concern and opportunities for improvement. For example, if a hospital has a high excess LOS and a high excess readmit rate, it may be an indication that the hospital is not effectively managing patient care and is instead relying on costly and unnecessary readmissions to address problems that could have been avoided in the first place.\nThe Gartner Magic Chart can also be used to identify trends over time, allowing healthcare professionals to track progress and see the impact of changes they have made to patient care processes.\nIf you are interested in creating a Gartner Magic Chart for your own healthcare data, the R package {healthyR} has a convenient function called gartner_magic_chart_plt() that allows you to easily create this chart from data supplied by the end user. Simply input your excess LOS and excess readmit rate data, and the function will generate the chart for you.\nIn summary, the Gartner Magic Chart is a valuable tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. By using the gartner_magic_chart_plt() function from the {healthyR} package, you can easily create this chart for your own data and start using it to improve patient care and outcomes.\n\n\nFunction\nLet’s take a look at the full function call for gartner_magic_chart_plt().\n\ngartner_magic_chart_plt(\n  .data,\n  .x_col,\n  .y_col,\n  .point_size_col = NULL,\n  .y_lab,\n  .x_lab,\n  .plt_title,\n  .tl_lbl,\n  .tr_lbl,\n  .br_lbl,\n  .bl_lbl\n)\n\nNow let’s take a look at the arguments to the parameters.\n\n.data - The data set you want to plot\n.x_col - The x-axis for the plot\n.y_col - The y-axis for the plot\n.point_size_col - The default is NULL, if you want to size the dots by a column in the data.frame/tibble then enter the column name here.\n.y_lab - The y-axis label\n.x_lab - The x-axis label\n.plt_title - The title of the plot\n.tl_lbl - The top left label\n.tr_lbl - The top right label\n.br_lbl - The bottom right label\n.bl_lbl - The bottom left label\n\n\n\nExample\nLet’s see the function in action.\n\nlibrary(dplyr)\nlibrary(healthyR)\n\ndata_tbl <- tibble(\n    x = rnorm(100, 0, 1),\n    y = rnorm(100, 0, 1),\n    z = abs(x) + abs(y)\n )\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = z,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)\n\n\n\n\nExample two.\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = NULL,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)"
  },
  {
    "objectID": "posts/rtip-2023-01-03/index.html",
    "href": "posts/rtip-2023-01-03/index.html",
    "title": "Calendar Heatmap with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nCalendar heatmaps are a useful visualization tool for understanding patterns and trends in time series data. They are particularly useful for displaying daily data, as they allow for the visualization of multiple weeks or months at a time.\nThe ts_calendar_heatmap_plot() function from the R library {healthyR.ts} is a powerful tool for creating calendar heatmaps. This function takes in a time series object and creates a heatmap plot with daily data plotted on the calendar. The intensity of the color on each day corresponds to the value of the data for that day.\nOne application of calendar heatmaps is in understanding daily patterns in data such as website traffic or sales. For example, a business owner could use a calendar heatmap to identify trends in their daily sales data and make informed decisions about their operations. Similarly, a website owner could use a calendar heatmap to understand the daily traffic patterns on their site and optimize their content strategy accordingly.\nCalendar heatmaps are also useful for identifying anomalies or unusual patterns in time series data. For example, a calendar heatmap could be used to identify unexpected spikes or dips in daily sales data, or to identify unusual patterns in website traffic.\nIn addition to their practical applications, calendar heatmaps are also aesthetically pleasing and can be a fun way to visualize data. The ts_calendar_heatmap_plot() function allows for customization of the color palette and other visual options, making it easy to create visually appealing heatmaps.\nOverall, calendar heatmaps are a useful tool for understanding patterns and trends in daily time series data. The ts_calendar_heatmap_plot() function from the R library healthyR.ts is a powerful tool for creating calendar heatmaps and can be easily customized to suit the needs of the user.\n\n\nFunction\nLet’s take a look at the function.\n\nts_calendar_heatmap_plot(\n  .data,\n  .date_col,\n  .value_col,\n  .low = \"red\",\n  .high = \"green\",\n  .plt_title = \"\",\n  .interactive = TRUE\n)\n\nNow let’s see the arguments to the parameters.\n\n.data - The time-series data with a date column and value column.\n.date_col - The column that has the datetime values\n.value_col - The column that has the values\n.low - The color for the low value, must be quoted like “red”. The default is “red”\n.high - The color for the high value, must be quoted like “green”. The default is “green”\n.plt_title - The title of the plot\n.interactive - Default is TRUE to get an interactive plot using plotly::ggplotly(). It can be set to FALSE to get a ggplot plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\n\ndata_tbl <- data.frame(\n  date_col = seq.Date(\n    from = as.Date(\"2020-01-01\"),\n    to   = as.Date(\"2022-06-01\"),\n    length.out = 365*2 + 180\n    ),\n  value = rnorm(365*2+180, mean = 100)\n)\n\nts_calendar_heatmap_plot(\n  .data          = data_tbl\n  , .date_col    = date_col\n  , .value_col   = value\n  , .interactive = TRUE\n)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-05/index.html",
    "href": "posts/rtip-2023-01-05/index.html",
    "title": "More Randomwalks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a mathematical concept that have found various applications in fields such as economics, biology, and computer science. At a high level, a random walk refers to a process in which a set of objects move in a random direction at each time step. The path that the objects take over time forms a random walk.\nOne of the main uses of random walks is in modeling the behavior of stock prices. In the stock market, prices can be thought of as performing a random walk because they are influenced by a variety of unpredictable factors such as market trends, news events, and investor sentiment. By modeling stock prices as a random walk, it is possible to make predictions about future price movements and to understand the underlying factors that drive these movements.\nAnother application of random walks is in studying the movement patterns of animals. For example, biologists have used random walk models to understand the foraging behavior of ants and the migration patterns of animals such as birds and whales.\nOne interesting aspect of random walks is that they can be generated with different statistical distributions. For example, a random walk could be generated with a standard normal distribution (mean = 0, standard deviation = 1) or with a distribution that has a different mean and standard deviation. By looking at random walks with different distributional parameters, it is possible to understand how the underlying distribution affects the overall shape and pattern of the random walk.\nTo generate random walks with different distributional parameters, you can use the R package {TidyDensity}. This package provides functions for generating random walks and visualizing them using density plots. With {TidyDensity}, you can easily compare random walks with different mean and standard deviation values to see how these parameters affect the shape of the random walk.\nIn summary, random walks are a useful tool for modeling the behavior of various systems over time. They are particularly useful for understanding the movement patterns of stock prices and animals, and can be generated with different statistical distributions using the R package {TidyDensity}.\n\n\nFunctions\nThere are a couple of functions that we are going to use, below you will find them with their full function call and parameter arguments.\ntidy_multi_single_dist()\n\ntidy_multi_single_dist(.tidy_dist = NULL, .param_list = list())\n\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the TidyDensity tidy_ distribution function.\n\ntidy_random_walk()\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\ntidy_random_walk_autoplot()\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExample\n\nlibrary(TidyDensity)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 250,\n    .mean = 0,\n    .sd = c(.025, .05, .1, .15),\n    .num_sims = 25\n  )\n) %>%\n  tidy_random_walk(.initial_value = 1000, .value_type = \"cum_prod\") %>%\n  tidy_random_walk_autoplot() +\n  facet_wrap(~ dist_name, scales = \"free\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-09/index.html",
    "href": "posts/rtip-2023-01-09/index.html",
    "title": "New Release of {healthyR.ts}",
    "section": "",
    "text": "Introduction\nHello R users!\nI am excited to announce a new update to the {healthyR.ts} package: the ts_brownian_motion() function.\nThis function allows you to easily simulate brownian motion, also known as a Wiener process, using just a few parameters. You can specify the length of the simulation using the ‘.time’ parameter, the number of simulations to run using the ‘.num_sims’ parameter, the time step size (standard deviation) using the ‘.delta_time’ parameter, and the initial value (which is set to 0 by default) using the ‘.initial_value’ parameter.\nBut what is brownian motion, and why might you want to simulate it? Brownian motion is a random process that describes the movement of particles suspended in a fluid. It is named after the botanist Robert Brown, who observed the random movement of pollen grains suspended in water under a microscope in the 19th century.\nIn finance, brownian motion is often used to model the movement of stock prices over time. By simulating brownian motion, you can get a sense of how prices might fluctuate in the future, and use this information to inform your investment decisions.\nI hope that the ts_brownian_motion() function will be a useful tool for anyone interested in simulating brownian motion, whether for financial modeling or any other application. Give it a try and see what you can do with it!\nRight now the function is a bit slow at .num_sims > 500 so I am working on optimizing it. I will also later on be introducing the Geometric Brownian Motion to {healthyR.ts}\nAs always, we welcome feedback and suggestions for new features and improvements. Thank you for using the {healthyR.ts} package, and happy simulating!\n\n\nFunction\nHere is the full function call:\n\nts_brownian_motion(\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = 1,\n  .initial_value = 0\n)\n\n\n\nExample\nA simple example of the output.\n\nlibrary(healthyR.ts)\n\nts_brownian_motion()\n\n# A tibble: 1,010 × 3\n   sim_number     t     y\n   <fct>      <dbl> <dbl>\n 1 1              0  0   \n 2 1              1  1.46\n 3 1              2  2.68\n 4 1              3  2.78\n 5 1              4  3.07\n 6 1              5  3.43\n 7 1              6  3.05\n 8 1              7  4.43\n 9 1              8  6.04\n10 1              9  6.89\n# … with 1,000 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-10/index.html",
    "href": "posts/rtip-2023-01-10/index.html",
    "title": "Optimal Break Points for Histograms with {healthyR}",
    "section": "",
    "text": "Introduction\nHistogram binning is a technique used in data visualization to group continuous data into a set of discrete bins, or intervals. The purpose of histogram binning is to represent the distribution of a dataset in a graphical format, allowing for easy identification of patterns and outliers. However, there are several challenges that can arise when working with histogram binning.\nOne major challenge is determining the appropriate number of bins to use. If there are too few bins, the histogram may not accurately represent the underlying distribution of the data. On the other hand, if there are too many bins, the histogram may become cluttered and difficult to interpret. To overcome this challenge, there are several strategies that can be employed, such as the Scott’s normal reference rule, Freedman-Diaconis rule, or the Rice rule, to determine the optimal number of bins.\nAnother challenge with histogram binning is dealing with outliers. If a dataset has outliers, they can greatly skew the distribution of the data and make it difficult to interpret the histogram. One strategy to handle outliers is to use a log-scale on the x-axis, which can help to reduce their impact on the histogram. Alternatively, one could remove the outlier data points before creating the histogram.\nA further challenge is to choose the width of the bin that best represents the data. Too narrow bins might cause overfitting and too wide bins may cause loss of information. Different widths of bin can lead to a different representation of the data and hence a different conclusion. To overcome this, one could use the Freedman-Diaconis rule which take into consideration the range and the size of the sample to provide a robust and adaptive way to choose the width of the bin\nA simple solution to these challenges is the opt_bin() function in the {healthyR} library for R. This function uses an optimal binning algorithm to automatically determine the number of bins and bin widths that best represent the data. This can save a lot of time and effort when working with histograms and can help to ensure that the resulting histograms are accurate and easy to interpret.\nIn conclusion, histogram binning is a useful technique for visualizing the distribution of data, but it can be challenging to determine the appropriate number of bins and bin widths. Strategies such as Scott’s normal reference rule, Freedman-Diaconis rule, or the Rice rule can be used to determine the optimal number of bins. Outliers and bin width selection also can be challenges to take into account, and a function such as opt_bin() in the {healthyR} library can be used to overcome these challenges and create high-quality histograms with ease.\n\n\nFunction\nHere is the full call.\n\nopt_bin(.data, .value_col, .iters = 30)\n\nHere are the arguments that get passed to the parameters.\n\n.data - The data set in question\n.value_col - The column that holds the values\n.iters - How many times the cost function loop should run\n\nNow under I will provide some code and it’s explanation under the hood that exaplains how this works.\nHere is a breakdown of what each part of the code is doing:\n\nn <- 2:iters: this line creates a sequence of numbers starting at 2 and ending at the number specified by the variable “iters”\nc <- base::numeric(base::length(n)) and d <- c: These lines create two empty numeric vectors (arrays) called “c” and “d” with the same length as the sequence “n”\nfor (i in 1:length(n)) {...}: This is a for loop that iterates through each number in the sequence “n”\nd[i] <- diff(range( data ) ) / n[i]: Inside the loop, this line calculates the width of the bin for the current iteration by dividing the range of the data by the current number in the sequence “n”\nhp <- graphics::hist(data, breaks = edges, plot = FALSE) and ki <- hp$counts: This creates a histogram of the data using the current bin width and then gets the count of data points in each bin\nk <- mean(ki) and v <- sum((ki-k)^2/n[i]): this line uses the counts from the previous step to calculate the average count across all bins (k) and the variance of the counts across all bins (v)\nc[i] <- (2*k - v)/d[i]^2: this line calculates the cost function for the current bin width, which is based on k and v\nidx <- which.min(c) and opt_d <- d[idx]: this line finds the index in the “c” vector where the cost function is the lowest and stores that value in the variable “idx”\nedges <- seq(min(data), max(data), length = n[idx]) and edges <- tibble::as_tibble(edges): this line creates a new sequence of numbers representing the edges of the bins with the optimal bin width\nreturn(edges): this line returns the sequence of optimal bin edges that the function has determined\n\nOverall, this function will return an optimal set of bin edges for a histogram of the given data, the function uses an iterative process and consider the balance between the number of bins and the width of the bins to find the optimal width of the bins for the histogram. This could save a lot of time and effort for the data analysts as it can help to ensure that the resulting histograms are accurate and easy to interpret.\n\n\nExample\nLet’s look at some examples.\n\nlibrary(healthyR)\nlibrary(tidyverse)\n\ndf_tbl <- rnorm(n = 1000, mean = 0, sd = 1)\ndf_tbl <- df_tbl %>%\n  as_tibble()\n\nopt_bin(\n  .data = df_tbl,\n  .value_col = value\n  , .iters = 100\n)\n\n# A tibble: 6 × 1\n   value\n   <dbl>\n1 -3.04 \n2 -1.81 \n3 -0.585\n4  0.644\n5  1.87 \n6  3.10 \n\n\nNow lets user a smaller n to see how the output changes\n\nopt_bin(\n  .data = as_tibble(rnorm(n = 50)),\n  value,\n  100\n)\n\n# A tibble: 11 × 1\n     value\n     <dbl>\n 1 -1.69  \n 2 -1.24  \n 3 -0.797 \n 4 -0.351 \n 5  0.0944\n 6  0.540 \n 7  0.986 \n 8  1.43  \n 9  1.88  \n10  2.32  \n11  2.77  \n\n\nLet’s visualize.\n\nrn <- rnorm(50)\nhist(rn)\n\n\n\n\nNow let’s use opt_bin()\n\nhist(rn, breaks = opt_bin(as_tibble(rn), value) %>% pull())\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-12/index.html",
    "href": "posts/rtip-2023-01-12/index.html",
    "title": "An Update on {tidyAML}",
    "section": "",
    "text": "Introduction\nI have been doing a lot of work on a new package called {tidyAML}. {tidyAML} is a new R package that makes it easy to use the {tidymodels} ecosystem to perform automated machine learning (AutoML). This package provides a simple and intuitive interface that allows users to quickly generate machine learning models without worrying about the underlying details. It also includes a safety mechanism that ensures that the package will fail gracefully if any required extension packages are not installed on the user’s machine. With {tidyAML}, users can easily build high-quality machine learning models in just a few lines of code. Whether you are a beginner or an experienced machine learning practitioner, {tidyAML} has something to offer.\nSome ideas are that we should be able to generate regression models on the fly without having to actually go through the process of building the specification, especially if it is a non-tuning model, meaning we are not planing on tuning hyper-parameters like penalty and cost.\nThe idea is not to re-write the excellent work the {tidymodels} team has done (because it’s not possible) but rather to try and make an enhanced easy to use set of functions that do what they say and can generate many models and predictions at once.\nThis is similar to the great {h2o} package, but, {tidyAML} does not require java to be setup properly like {h2o} because {tidyAML} is built on {tidymodels}.\nThis package is not yet release, so you can only install from GitHub with the following:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/tidyAML\")\n\n\n\nExample\n\nlibrary(tidyAML)\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n11        11 stan_glmer      regression    linear_reg   <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 glm             regression    linear_reg   <spec[+]> \n3         3 glm             regression    poisson_reg  <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\",\"gee\"), \n                                 .parsnip_fns = \"linear_reg\")\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 gee             regression    linear_reg   <spec[+]> \n3         3 glm             regression    linear_reg   <spec[+]> \n\n\nAs shown we can easily select the models we want either by choosing the supported parsnip function like linear_reg() or by choose the desired engine, you can also use them both in conjunction with each other!\nNow, what if you want to create a non-tuning model spec without using the fast_regression_parsnip_spec_tbl() function. Well, you can. The function is called create_model_spec().\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow the reason we are here. Let’s take a look at the first function for modeling with tidyAML, fast_regression().\n\nlibrary(recipes)\nlibrary(dplyr)\nlibrary(purrr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(frt_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nNow lets take a look at a few different things in the frt_tbl.\n\nnames(frt_tbl)\n\n[1] \".model_id\"       \".parsnip_engine\" \".parsnip_mode\"   \".parsnip_fns\"   \n[5] \"model_spec\"      \"wflw\"            \"fitted_wflw\"     \"pred_wflw\"      \n\n\nLet’s look at a single model spec.\n\nfrt_tbl %>% slice(1) %>% select(model_spec) %>% pull() %>% pluck(1)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the wflw column.\n\nfrt_tbl %>% slice(1) %>% select(wflw) %>% pull() %>% pluck(1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe fitted wflw object.\n\nfrt_tbl %>% slice(1) %>% select(fitted_wflw) %>% pull() %>% pluck(1)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   11.77621      0.59296      0.01626     -0.03191     -0.55350     -5.30785  \n       qsec           vs           am         gear         carb  \n    0.97840      2.64023      1.68549      0.87059      0.58785  \n\nfrt_tbl %>% slice(1) %>% select(fitted_wflw) %>% pull() %>% pluck(1) %>%\n  broom::glance() %>%\n  glimpse()\n\nRows: 1\nColumns: 12\n$ r.squared     <dbl> 0.9085669\n$ adj.r.squared <dbl> 0.8382337\n$ sigma         <dbl> 2.337527\n$ statistic     <dbl> 12.91804\n$ p.value       <dbl> 3.367361e-05\n$ df            <dbl> 10\n$ logLik        <dbl> -47.07551\n$ AIC           <dbl> 118.151\n$ BIC           <dbl> 132.2877\n$ deviance      <dbl> 71.03241\n$ df.residual   <int> 13\n$ nobs          <int> 24\n\n\nAnd finally the predictions (this one I am probably going to change up).\n\nfrt_tbl %>% slice(1) %>% select(pred_wflw) %>% pull() %>% pluck(1)\n\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  17.4\n 2  28.4\n 3  17.2\n 4  10.7\n 5  13.4\n 6  17.0\n 7  22.8\n 8  14.3\n 9  22.4\n10  15.5\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-16/index.html",
    "href": "posts/rtip-2023-01-16/index.html",
    "title": "Auto K-Means with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nToday’s post is going to center around the automatic k-means functionality of {healthyR.ai}. I am not going to get into what it is or how it works, but rather the function call itself and how it works and what it puts out. The function is called hai_kmeans_automl. This function is a wrapper around the h2o::h2o.kmeans() function, but also does some processing to enhance the output at the end. Let’s get to it!\n\n\nFunction\nHere is the full function call.\n\nhai_kmeans_automl(\n  .data,\n  .split_ratio = 0.8,\n  .seed = 1234,\n  .centers = 10,\n  .standardize = TRUE,\n  .print_model_summary = TRUE,\n  .predictors,\n  .categorical_encoding = \"auto\",\n  .initialization_mode = \"Furthest\",\n  .max_iterations = 100\n)\n\nNow let’s go over the function arguments:\n\n.data - The data that is to be passed for clustering.\n.split_ratio - The ratio for training and testing splits.\n.seed - The default is 1234, but can be set to any integer.\n.centers - The default is 1. Specify the number of clusters (groups of data) in a data set.\n.standardize - The default is set to TRUE. When TRUE all numeric columns will be set to zero mean and unit variance.\n.print_model_summary - This is a boolean and controls if the model summary is printed to the console. The default is TRUE.\n.predictors - This must be in the form of c(“column_1”, “column_2”, … “column_n”)\n.categorical_encoding - Can be one of the following:\n\n“auto”\n“enum”\n“one_hot_explicit”\n“binary”\n“eigen”\n“label_encoder”\n“sort_by_response”\n“enum_limited”\n\n.initialization_mode - This can be one of the following:\n\n“Random”\n“Furthest (default)\n“PlusPlus”\n\n.max_iterations - The default is 100. This specifies the number of training iterations\n\n\n\nExamples\nTime for some examples.\n\nlibrary(healthyR.ai)\nlibrary(h2o)\n\nh2o.init()\n\noutput <- hai_kmeans_automl(\n  .data = iris,\n  .predictors = c(\n    \"Sepal.Width\", \"Sepal.Length\", \"Petal.Width\", \"Petal.Length\"\n    ),\n  .standardize = TRUE,\n  .split_ratio = .8\n)\n\nh2o.shutdown(prompt = FALSE)\n\nNow let’s take a look at the output. There are going to be 4 pieces of main output. Here they are:\n\ndata\nauto_kmeans_obj\nmodel_id\nscree_plt\n\nLet’s take a look at each one. First the data output which itself has 6 different objects in it.\n\noutput$data\n\n$splits\n$splits$training_tbl\n# A tibble: 123 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          5           3.6          1.4         0.2 setosa \n 5          5.4         3.9          1.7         0.4 setosa \n 6          4.6         3.4          1.4         0.3 setosa \n 7          5           3.4          1.5         0.2 setosa \n 8          4.4         2.9          1.4         0.2 setosa \n 9          5.4         3.7          1.5         0.2 setosa \n10          4.8         3.4          1.6         0.2 setosa \n# … with 113 more rows\n\n$splits$validate_tbl\n# A tibble: 27 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.6         3.1          1.5         0.2 setosa \n 2          4.9         3.1          1.5         0.1 setosa \n 3          5.8         4            1.2         0.2 setosa \n 4          5.1         3.5          1.4         0.3 setosa \n 5          5.7         3.8          1.7         0.3 setosa \n 6          5.1         3.8          1.5         0.3 setosa \n 7          5.4         3.4          1.7         0.2 setosa \n 8          5.1         3.7          1.5         0.4 setosa \n 9          5           3.4          1.6         0.4 setosa \n10          4.7         3.2          1.6         0.2 setosa \n# … with 17 more rows\n\n\n$metrics\n$metrics$training_metrics\n# A tibble: 2 × 3\n  centroid  size within_cluster_sum_of_squares\n     <dbl> <dbl>                         <dbl>\n1        1    87                         145. \n2        2    36                          38.3\n\n$metrics$validation_metrics\n# A tibble: 2 × 3\n  centroid  size within_cluster_sum_of_squares\n     <dbl> <dbl>                         <dbl>\n1        1    13                          35.7\n2        2    14                          13.4\n\n$metrics$cv_metric_summary\n# A tibble: 5 × 8\n  metric_name   mean    sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid cv_5_va…¹\n  <chr>        <dbl> <dbl>      <dbl>      <dbl>      <dbl>      <dbl>     <dbl>\n1 betweenss     62.5 12.4        66.7       72.7       71.6       42.5      59.1\n2 mse          NaN    0         NaN        NaN        NaN        NaN       NaN  \n3 rmse         NaN    0         NaN        NaN        NaN        NaN       NaN  \n4 tot_withinss  33.6  7.36       45.6       29.7       34.8       26.5      31.3\n5 totss         96.1 17.1       112.       102.       106.        69.0      90.3\n# … with abbreviated variable name ¹​cv_5_valid\n\n\n$original_data\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n[150 rows x 5 columns] \n\n$scree_data_tbl\n# A tibble: 2 × 2\n  centers   wss\n    <dbl> <dbl>\n1       1  745.\n2       2  226.\n\n$scoring_history_tbl\n# A tibble: 6 × 6\n  timestamp           duration     iterations number_of_clusters numbe…¹ withi…²\n  <chr>               <chr>             <dbl>              <dbl>   <dbl>   <dbl>\n1 2023-01-16 09:41:25 \" 0.241 sec\"          0                  0     NaN    NaN \n2 2023-01-16 09:41:25 \" 0.246 sec\"          1                  1     123   1003.\n3 2023-01-16 09:41:25 \" 0.247 sec\"          2                  1       0    488 \n4 2023-01-16 09:41:25 \" 0.250 sec\"          3                  2      26    311.\n5 2023-01-16 09:41:25 \" 0.251 sec\"          4                  2       2    184.\n6 2023-01-16 09:41:25 \" 0.251 sec\"          5                  2       0    183.\n# … with abbreviated variable names ¹​number_of_reassigned_observations,\n#   ²​within_cluster_sum_of_squares\n\n$model_summary_tbl\n# A tibble: 7 × 2\n  name                           value\n  <chr>                          <dbl>\n1 number_of_rows                  123 \n2 number_of_clusters                2 \n3 number_of_categorical_columns     0 \n4 number_of_iterations              5 \n5 within_cluster_sum_of_squares   183.\n6 total_sum_of_squares            488 \n7 between_cluster_sum_of_squares  305.\n\n\nNow lets take a look at the auto_kmeans_obj\n\noutput$auto_kmeans_obj\n\nModel Details:\n==============\n\nH2OClusteringModel: kmeans\nModel ID:  KMeans_model_R_1673880074548_1 \nModel Summary: \n  number_of_rows number_of_clusters number_of_categorical_columns\n1            123                  2                             0\n  number_of_iterations within_cluster_sum_of_squares total_sum_of_squares\n1                    5                     183.42511            488.00000\n  between_cluster_sum_of_squares\n1                      304.57489\n\n\nH2OClusteringMetrics: kmeans\n** Reported on training data. **\n\n\nTotal Within SS:  183.4251\nBetween SS:  304.5749\nTotal SS:  488 \nCentroid Statistics: \n  centroid     size within_cluster_sum_of_squares\n1        1 87.00000                     145.14706\n2        2 36.00000                      38.27805\n\nH2OClusteringMetrics: kmeans\n** Reported on validation data. **\n\n\nTotal Within SS:  49.05625\nBetween SS:  53.9303\nTotal SS:  102.9865 \nCentroid Statistics: \n  centroid     size within_cluster_sum_of_squares\n1        1 13.00000                      35.67618\n2        2 14.00000                      13.38007\n\nH2OClusteringMetrics: kmeans\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\n\nTotal Within SS:  167.8887\nBetween SS:  320.1113\nTotal SS:  488 \nCentroid statistics are not available.\nCross-Validation Metrics Summary: \n                  mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\nbetweenss    62.496490 12.417224  66.671760  72.679665  71.595474  42.470100\nmse                 NA  0.000000         NA         NA         NA         NA\nrmse                NA  0.000000         NA         NA         NA         NA\ntot_withinss 33.577747  7.357984  45.607327  29.705257  34.811970  26.512373\ntotss        96.074234 17.148642 112.279080 102.384926 106.407450  68.982475\n             cv_5_valid\nbetweenss     59.065456\nmse                  NA\nrmse                 NA\ntot_withinss  31.251806\ntotss         90.317260\n\n\nThe model id:\n\noutput$model_id\n\n[1] \"KMeans_model_R_1673880074548_1\"\n\n\nAnd finally the scree_plt.\n\noutput$scree_plt\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-17/index.html",
    "href": "posts/rtip-2023-01-17/index.html",
    "title": "Augmenting a Brownian Motion to a Time Series with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series analysis is a crucial tool for forecasting and understanding trends in various industries, including finance, economics, and engineering. However, traditional time series analysis methods can be limiting, and they may not always capture the complex dynamics of real-world data. That’s where the R package {healthyR.ts} comes in.\nThe {healthyR.ts} package is a powerful tool for time series analysis that offers a wide range of functions for cleaning, transforming, and analyzing time series data. One of its standout features is the ts_brownian_motion_augment() function, which allows you to add a brownian motion to a given time series dataset. This powerful tool can be used to simulate more realistic and complex scenarios, making it an invaluable tool for forecasters and data analysts.\nBrownian motion is a random walk process that can be used to model the movement of particles in a fluid. It has been widely used in mathematical finance, physics, and engineering to model the random movements of stock prices, pollutant concentrations, and other phenomena. By adding a brownian motion to a time series dataset, the ts_brownian_motion_augment() function allows users to capture the unpredictable and random nature of real-world data, making time series analysis more accurate and reliable.\nThe ts_brownian_motion_augment() function is easy to use and requires no prior knowledge of brownian motion or advanced mathematics. With just a few lines of code, users can quickly add a brownian motion to their time series dataset and begin analyzing the data with greater precision and confidence.\nThis set of functionality will be included in the next release which will be coming soon as it also speeds up the current ts_brownian_motion() function by 49x!\n\n\nFunction\nHere is the full function call.\n\nts_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = NULL\n)\n\nLet’s take a look at the arguments for the parameters.\n\n.data - The data.frame/tibble being augmented.\n.date_col - The column that holds the date.\n.value_col - The value that is going to get augmented. The last value of this column becomes the initial value internally.\n.time - How many time steps ahead.\n.num_sims - How many simulations should be run.\n.delta_time - Time step size.\n\n\n\nExample\nNow for an example.\n\nlibrary(tidyquant)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\ndf <- FANG %>%\n  filter(symbol == \"FB\") %>%\n  select(symbol, date, adjusted) %>%\n  filter_by_time(.date_var = date, .start_date = \"2016-01-01\") %>%\n  tq_mutate(select = adjusted, mutate_fun = periodReturn,\n            period = \"daily\", type = \"log\",\n            col_rename = \"daily_returns\")\n\nLet’s take a look at our initial data.\n\ndf\n\n# A tibble: 252 × 4\n   symbol date       adjusted daily_returns\n   <chr>  <date>        <dbl>         <dbl>\n 1 FB     2016-01-04    102.        0      \n 2 FB     2016-01-05    103.        0.00498\n 3 FB     2016-01-06    103.        0.00233\n 4 FB     2016-01-07     97.9      -0.0503 \n 5 FB     2016-01-08     97.3      -0.00604\n 6 FB     2016-01-11     97.5       0.00185\n 7 FB     2016-01-12     99.4       0.0189 \n 8 FB     2016-01-13     95.4      -0.0404 \n 9 FB     2016-01-14     98.4       0.0302 \n10 FB     2016-01-15     95.0      -0.0352 \n# … with 242 more rows\n\n\nNow let’s augment it with the brownian motion and see that data set before we visualize it.\n\ndf %>%\n  ts_brownian_motion_augment(\n    .date_col = date,\n    .num_sims = 50,\n    .value_col = daily_returns,\n    .delta_time = 0.00005\n  )\n\n# A tibble: 5,302 × 3\n   sim_number  date       daily_returns\n   <fct>       <date>             <dbl>\n 1 actual_data 2016-01-04       0      \n 2 actual_data 2016-01-05       0.00498\n 3 actual_data 2016-01-06       0.00233\n 4 actual_data 2016-01-07      -0.0503 \n 5 actual_data 2016-01-08      -0.00604\n 6 actual_data 2016-01-11       0.00185\n 7 actual_data 2016-01-12       0.0189 \n 8 actual_data 2016-01-13      -0.0404 \n 9 actual_data 2016-01-14       0.0302 \n10 actual_data 2016-01-15      -0.0352 \n# … with 5,292 more rows\n\n\nAs you see the function preserves the names of the input columns!\nNow, let’s see it!\n\ndf %>%\n  ts_brownian_motion_augment(\n    .date_col = date,\n    .num_sims = 50,\n    .value_col = daily_returns,\n    .delta_time = 0.00005\n  ) %>%\n  ggplot(aes(x = date, y = daily_returns\n             , group = sim_number, color = sim_number)) +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"FB Log Daily Returns for 2016\",\n    x = \"Date\",\n    y = \"Log Daily Returns\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-18/index.html",
    "href": "posts/rtip-2023-01-18/index.html",
    "title": "Geometric Brownian Motion with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nGeometric Brownian motion (GBM) is a widely used model in financial analysis for modeling the behavior of stock prices. It is a stochastic process that describes the evolution of a stock price over time, assuming that the stock price follows a random walk with a drift term and a volatility term.\nOne of the advantages of GBM is that it can capture the randomness and volatility of stock prices, which is a key feature of financial markets. GBM can also be used to estimate the expected return and volatility of a stock, which are important inputs for financial decision making.\nAnother advantage of GBM is that it can be used to generate simulations of future stock prices. These simulations can be used to estimate the probability of different outcomes, such as the probability of a stock price reaching a certain level in the future. This can be useful for risk management and for evaluating investment strategies.\nGBM is also very easy to implement, making it a popular choice among financial analysts and traders.\nThe equation for GBM is: \\[\ndS(t) = μS(t)dt + σS(t)dW(t)\n\\] Where:\n\\(dS(t)\\) is the change in the stock price at time \\(t\\)\n\\(S(t)\\) is the stock price at time \\(t\\)\n\\(μ\\) is the expected return of the stock\n\\(σ\\) is the volatility of the stock\n\\(dW(t)\\) is a Wiener process (a random variable that describes the rate of change of a random variable over time)\nIt’s important to keep in mind that GBM is a model and not always a perfect fit to real-world stock prices. However, it’s a widely accepted model due to its capability to captures the key characteristics of stock prices and its mathematical tractability.\nAttention R users! Are you looking for a reliable and accurate way to model stock prices? We have some exciting news for you! The next release of the R package {healthyR.ts} will include a new function, ts_geometric_brownian_motion(). This powerful function utilizes the geometric Brownian motion model to simulate stock prices, providing you with valuable insights and predictions for your financial analysis.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\nNow let’s go over the arguments to the parameters.\n\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.initial_value - Integer representing the initial value.\n.delta_time - Time step size.\n.return_tibble The default is TRUE. If set to FALSE then an object of class matrix will be returned.\n\n\n\nExample\nLet’s go over a few examples.\n\nlibrary(healthyR.ts)\n\nts_geometric_brownian_motion()\n\n# A tibble: 2,600 × 3\n   sim_number        t     y\n   <fct>         <int> <dbl>\n 1 sim_number 1      1   100\n 2 sim_number 2      1   100\n 3 sim_number 3      1   100\n 4 sim_number 4      1   100\n 5 sim_number 5      1   100\n 6 sim_number 6      1   100\n 7 sim_number 7      1   100\n 8 sim_number 8      1   100\n 9 sim_number 9      1   100\n10 sim_number 10     1   100\n# … with 2,590 more rows\n\n\nNow without returning a tibble object.\n\nts_geometric_brownian_motion(.num_sims = 5, .return_tibble = FALSE)\n\n      sim_number 1 sim_number 2 sim_number 3 sim_number 4 sim_number 5\n [1,]    100.00000     100.0000    100.00000    100.00000     100.0000\n [2,]    101.04170     100.6583    100.46420     99.68513     100.3776\n [3,]    101.58155     100.8959    100.03621     98.91656     101.5732\n [4,]    100.91680     100.7494     99.47735     98.57117     101.1525\n [5,]     99.96787     101.3298     98.70899     99.03101     101.1557\n [6,]     99.29069     101.4187     98.32176     98.33018     101.5584\n [7,]     99.40451     101.5124     98.26237     97.79356     101.4934\n [8,]     99.35345     101.0328     98.69587     97.46604     101.9630\n [9,]     97.94177     100.9534     98.32630     96.95231     102.1643\n[10,]     97.95812     101.3813     98.36934     96.64048     101.8546\n[11,]     98.47820     101.8262     98.21492     96.12851     102.5529\n[12,]     99.53016     102.5522     97.92270     95.97443     102.8912\n[13,]     98.82850     102.7482     96.66348     96.26008     103.1899\n[14,]     99.87335     102.9351     96.69635     96.15058     103.9259\n[15,]    101.03605     103.3796     96.60162     96.63562     103.3790\n[16,]    101.83475     103.1900     97.63875     96.00162     103.0422\n[17,]    102.10155     103.5851     97.12873     95.99579     103.0913\n[18,]    102.16085     103.2966     96.26772     95.95174     103.7034\n[19,]    102.35736     103.7429     96.37355     96.02805     102.8406\n[20,]    102.49297     104.5301     96.44318     96.28293     103.3507\n[21,]    102.36953     105.1809     96.87639     97.32625     104.0307\n[22,]    103.30672     104.7480     96.90017     97.16507     104.0751\n[23,]    103.55433     104.9848     97.40063     97.49375     102.6901\n[24,]    103.44429     104.3553     97.35982     97.39390     102.8163\n[25,]    103.23952     102.9840     97.30287     97.66737     103.2160\n[26,]    103.48365     103.6117     97.96290     97.91773     103.0579\nattr(,\".time\")\n[1] 25\nattr(,\".num_sims\")\n[1] 5\nattr(,\".mean\")\n[1] 0\nattr(,\".sigma\")\n[1] 0.1\nattr(,\".initial_value\")\n[1] 100\nattr(,\".delta_time\")\n[1] 0.002739726\nattr(,\".return_tibble\")\n[1] FALSE\n\n\nLet’s visualize the GBM at different levels of volatility.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ngbm <- rbind(\n  ts_geometric_brownian_motion(.sigma = 0.05) %>%\n    mutate(volatility = as.factor(\"A) Sigma = 5%\")),\n  ts_geometric_brownian_motion(.sigma = 0.1) %>%\n    mutate(volatility = as.factor(\"B) Sigma = 10%\")),\n  ts_geometric_brownian_motion(.sigma = .15) %>%\n    mutate(volatility = as.factor(\"C) Sigma = 15%\")),\n  ts_geometric_brownian_motion(.sigma = .2) %>%\n    mutate(volatility = as.factor(\"D) Sigma = 20%\"))\n)\n\ngbm %>%\n  ggplot(aes(x = t, y = y, group = sim_number, color = sim_number)) + \n  facet_wrap(~ volatility, scales = \"free\") +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-19/index.html",
    "href": "posts/rtip-2023-01-19/index.html",
    "title": "Boilerplate XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nXGBoost, short for “eXtreme Gradient Boosting,” is a powerful and popular machine learning library that is specifically designed for gradient boosting. It is an open-source library and is available in many programming languages, including R.\nGradient boosting is a technique that combines the predictions of multiple weak models to create a strong, more accurate model. XGBoost is an optimized version of gradient boosting that is designed to run faster and more efficiently than other implementations.\nLet’s take a look at a simple example of how to use XGBoost in R. We will use the iris dataset, a well-known dataset that contains 150 observations of iris flowers, each with four features (sepal length, sepal width, petal length, and petal width) and one target variable (the species of iris). Our goal is to train a model to predict the species of an iris flower based on its features.\nFirst, we need to install the “xgboost” package in R:\n\ninstall.packages(\"xgboost\")\n\nNext, we load the iris dataset and split it into training and test sets:\n\ndata(iris)\nset.seed(123)\nindices <- sample(1:nrow(iris), 0.8*nrow(iris))\ntrain_data <- iris[indices, 1:4]\ntrain_label <- iris[indices, 5]\ntest_data <- iris[-indices, 1:4]\ntest_label <- iris[-indices, 5]\n\nNow we can train our XGBoost model:\n\nlibrary(xgboost)\nxgb_model <- xgboost(\n  data = train_data, \n  label = train_label, \n  nrounds = 100, \n  objective = \"multi:softmax\", \n  num_class = 3\n  )\n\nHere, we specified the training data, labels, number of rounds (iterations) to run, the objective (multiclass classification) and the number of classes.\nFinally, we can use the trained model to make predictions on the test set:\n\npredictions <- predict(xgb_model, test_data)\n\nWe can also evaluate the performance of our model by comparing the predicted labels to the true labels using metrics such as accuracy:\n\naccuracy <- mean(predictions == test_label)\n\nIn this example, we used XGBoost to train a model to predict the species of iris flowers based on their features. We saw that XGBoost is a powerful and efficient library for gradient boosting, and it can be easily integrated into a R script.\nKeep in mind that this is a simple example, and in real-world scenarios, more preprocessing and parameter tuning is necessary to achieve optimal performance. Also, the dataset is small, and the number of rounds used is also small, which is not ideal for real-world scenarios. But this example shows the basic usage of XGBoost in R.\nOk, so, what’s the point? Is there a possibly easier way to do this…yes! You can use the boilerplace function hai_auto_xgboost() and it’s data prep helper hai_xgboost_data_prepper() from the {healthyR.ai} library. Let’s see how that works.\n\n\nFunction\nHere is the data prepper function and it’s arguments.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\nHere is the boilerplate function\n\nhai_auto_xgboost(\n  .data,\n  .rec_obj,\n  .splits_obj = NULL,\n  .rsamp_obj = NULL,\n  .tune = TRUE,\n  .grid_size = 10,\n  .num_cores = 1,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\"\n)\n\nHere are it’s arguments.\n\n.data - The data being passed to the function. The time-series object.\n.rec_obj - This is the recipe object you want to use. You can use hai_xgboost_data_prepper() an automatic recipe_object.\n.splits_obj - NULL is the default, when NULL then one will be created.\n.rsamp_obj - NULL is the default, when NULL then one will be created. It will default to creating an rsample::mc_cv() object.\n.tune - Default is TRUE, this will create a tuning grid and tuned workflow\n.grid_size - Default is 10\n.num_cores - Default is 1\n.best_metric - Default is “f_meas”. You can choose a metric depending on the model_type used. If regression then see hai_default_regression_metric_set(), if classification then see hai_default_classification_metric_set().\n.model_type - Default is classification, can also be regression.\n\n\n\nExample\nLet’s take a look at an example and it’s output. This is using {parsnip} under the hood.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_xgboost_data_prepper(data, Species ~ .)\n\nauto_xgb <- hai_auto_xgboost(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .num_cores = 1\n)\n\nThere are three main outputs to this function, which are:\n\nrecipe_info\nmodel_info\ntuned_info\n\nLet’s take a look at each. First the recipe_info\n\nauto_xgb$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nNow the model_info\n\nauto_xgb$model_info\n\n$model_spec\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune::tune()\n  min_n = tune::tune()\n  tree_depth = tune::tune()\n  learn_rate = tune::tune()\n  loss_reduction = tune::tune()\n  sample_size = tune::tune()\n\nComputational engine: xgboost \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune::tune()\n  min_n = tune::tune()\n  tree_depth = tune::tune()\n  learn_rate = tune::tune()\n  loss_reduction = tune::tune()\n  sample_size = tune::tune()\n\nComputational engine: xgboost \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 2.5 Mb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.10962507492329, max_depth = 13L, \n    gamma = 0.000498577409120534, colsample_bytree = 1, colsample_bynode = 1, \n    min_child_weight = 3L, subsample = 0.594320066112559), data = x$data, \n    nrounds = 1240L, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"multi:softprob\", num_class = 3L)\nparams (as set within xgb.train):\n  eta = \"0.10962507492329\", max_depth = \"13\", gamma = \"0.000498577409120534\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"3\", subsample = \"0.594320066112559\", nthread = \"1\", objective = \"multi:softprob\", num_class = \"3\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 4 \nniter: 1240\nnfeatures : 4 \nevaluation_log:\n    iter training_mlogloss\n       1        0.96929822\n       2        0.85785438\n---                       \n    1239        0.07815044\n    1240        0.07808817\n\n$was_tuned\n[1] \"tuned\"\n\nNow the tuned_info\n\nauto_xgb$tuned_info\n\n$tuning_grid\n# A tibble: 10 × 6\n   trees min_n tree_depth learn_rate loss_reduction sample_size\n   <int> <int>      <int>      <dbl>          <dbl>       <dbl>\n 1   926     6          2    0.0246        2.21e- 1       0.952\n 2  1510    25         14    0.00189       1.01e+ 1       0.424\n 3  1077    29          9    0.195         1.34e- 5       0.319\n 4   795    32          3    0.00102       1.64e- 3       0.686\n 5   368    22          4    0.00549       2.97e- 7       0.735\n 6  1240     3         13    0.110         4.99e- 4       0.594\n 7  1839    18          5    0.0501        1.67e- 7       0.273\n 8   139    11         10    0.0153        1.17e- 2       0.483\n 9   470    40          8    0.0906        6.79e-10       0.168\n10  1732    16         11    0.00667       9.19e- 9       0.883\n\n$cv_obj\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n$tuned_results\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics            .notes          \n   <list>          <chr>      <list>              <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 10]> <tibble [1 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 10]> <tibble [1 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 10]> <tibble [1 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 10]> <tibble [1 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 10]> <tibble [1 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 10]> <tibble [1 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 10]> <tibble [1 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 10]> <tibble [1 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 10]> <tibble [1 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 10]> <tibble [1 × 3]>\n# … with 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n$grid_size\n[1] 10\n\n$best_metric\n[1] \"f_meas\"\n\n$best_result_set\n# A tibble: 1 × 12\n  trees min_n tree_depth learn_rate loss_r…¹ sampl…² .metric .esti…³  mean     n\n  <int> <int>      <int>      <dbl>    <dbl>   <dbl> <chr>   <chr>   <dbl> <int>\n1  1240     3         13      0.110 0.000499   0.594 f_meas  macro   0.944    25\n# … with 2 more variables: std_err <dbl>, .config <chr>, and abbreviated\n#   variable names ¹​loss_reduction, ²​sample_size, ³​.estimator\n\n$tuning_grid_plot\n\n\n\n\nTuning Grid\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-23/index.html",
    "href": "posts/rtip-2023-01-23/index.html",
    "title": "ADF and Phillips-Perron Tests for Stationarity using lists",
    "section": "",
    "text": "Introduction\nA time series is a set of data points collected at regular intervals of time. Sometimes, the data points in a time series change over time in a predictable way. This is called a stationary time series. Other times, the data points change in an unpredictable way. This is called a non-stationary time series.\nImagine you are playing a game of catch with a friend. If you throw the ball back and forth at the same speed and distance, that’s like a stationary time series. But if you keep throwing the ball harder and farther, that’s like a non-stationary time series.\nThere are two tests that we can use to see if a time series is stationary or non-stationary. The first test is called the ADF test, which stands for Augmented Dickey-Fuller test. The second test is called the Phillips-Perron test.\nThe ADF test looks at the data points and checks to see if the average value of the data points is the same over time. If the average value is the same, then the time series is stationary. If the average value is not the same, then the time series is non-stationary.\nThe Phillips-Perron test is similar to the ADF test, but it is a bit more advanced. It checks to see if the data points are changing in a predictable way. If the data points are changing in a predictable way, then the time series is stationary. If the data points are changing in an unpredictable way, then the time series is non-stationary.\nSo, in short, The ADF test checks if the mean of the time series is constant over time and Phillips-Perron test checks if the variance of the time series is constant over time.\nNow, you can use these two tests to see if the time series you are studying is stationary or non-stationary, just like how you can use the game of catch to see if your throws are the same or different.\n\n\nFunction\nTo perform these test we can use two libraries, one is the {tseries} library for the adf.test() and the other is the {aTSA} for the pp.test()\nLet’s see some examples.\n\n\nExamples\nLet’s first make our time series obejcts and place them in a list.\n\nlibrary(tseries)\nlibrary(aTSA)\n\n# create time series objects\nts1 <- ts(rnorm(100), start = c(1990,1), frequency = 12)\nts2 <- ts(rnorm(100), start = c(1995,1), frequency = 12)\nts3 <- ts(rnorm(100), start = c(2000,1), frequency = 12)\n\n# create list of time series\nts_list <- list(ts1, ts2, ts3)\n\nNow let’s make our functions.\n\n# function to test for stationarity\nadf_is_stationary <- function(x) {\n  adf.test(x)$p.value > 0.05\n}\n\npp_is_stationary <- function(x) {\n  pp_df <- pp.test(x) |> as.data.frame() \n  pp_df$p.value > 0.05\n}\n\nTime to use lapply()!\n\n# apply function to each time series in list\nlapply(ts_list, adf_is_stationary)\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -10.97    0.01\n[2,]   1  -7.70    0.01\n[3,]   2  -5.23    0.01\n[4,]   3  -4.05    0.01\n[5,]   4  -4.03    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -11.48    0.01\n[2,]   1  -8.32    0.01\n[3,]   2  -5.81    0.01\n[4,]   3  -4.59    0.01\n[5,]   4  -4.63    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -11.42    0.01\n[2,]   1  -8.28    0.01\n[3,]   2  -5.77    0.01\n[4,]   3  -4.56    0.01\n[5,]   4  -4.59    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -10.60    0.01\n[2,]   1  -7.88    0.01\n[3,]   2  -5.96    0.01\n[4,]   3  -5.26    0.01\n[5,]   4  -4.90    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -10.64    0.01\n[2,]   1  -7.98    0.01\n[3,]   2  -6.08    0.01\n[4,]   3  -5.41    0.01\n[5,]   4  -5.08    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -10.58    0.01\n[2,]   1  -7.94    0.01\n[3,]   2  -6.06    0.01\n[4,]   3  -5.39    0.01\n[5,]   4  -5.07    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -9.19    0.01\n[2,]   1 -6.65    0.01\n[3,]   2 -5.41    0.01\n[4,]   3 -5.33    0.01\n[5,]   4 -4.77    0.01\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -9.14    0.01\n[2,]   1 -6.62    0.01\n[3,]   2 -5.39    0.01\n[4,]   3 -5.30    0.01\n[5,]   4 -4.75    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -9.11    0.01\n[2,]   1 -6.59    0.01\n[3,]   2 -5.36    0.01\n[4,]   3 -5.29    0.01\n[5,]   4 -4.73    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\n[[1]]\nlogical(0)\n\n[[2]]\nlogical(0)\n\n[[3]]\nlogical(0)\n\nlapply(ts_list, pp_is_stationary)\n\nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3  -111    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3  -110    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3  -110    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3  -101    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3  -101    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3  -101    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3 -92.9    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3 -92.9    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3   -93    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \n\n\n[[1]]\n[1] FALSE FALSE FALSE\n\n[[2]]\n[1] FALSE FALSE FALSE\n\n[[3]]\n[1] FALSE FALSE FALSE\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-24/index.html",
    "href": "posts/rtip-2023-01-24/index.html",
    "title": "Making Non Stationary Data Stationary",
    "section": "",
    "text": "Introduction\nIn the most basic sense for time series, a series is stationary if the properties of the generating process (the process that generates the data) do not change over time, the process remains constant. This does not mean the data does not change, it simply means the process does not change. You can bake a vanilla cake or a chocolate cake but you still cook it in the oven.\nA non-stationary time series is like a toy car that doesn’t run in a straight line. Sometimes it goes fast and sometimes it goes slow, so it’s hard to predict what it will do next. But, just like how you can fix a toy car by adjusting it, we can fix a non-stationary time series by making it “stationary.”\nOne way we can do this is by taking the difference in the time series vector. This is like taking the toy car apart and looking at how each piece moves. By subtracting one piece from another, we can see if they are moving at the same speed or not. If they are not, we can adjust them so they are moving at the same speed. This makes it easier to predict what the toy car will do next because it’s moving at a steady pace.\nAnother way we can make a non-stationary time series stationary is by taking the second difference of the log of the data. This is like looking at the toy car from a different angle. By taking the log of the data, we can see how much each piece has changed over time. Then, by taking the second difference, we can see if the changes are happening at the same rate or not. If they are not, we can adjust them so they are happening at the same rate.\nIn simple terms, these methods help to stabilize the time series by making the data move at a consistent speed, which allows for better predictions.\nIn summary, a non-stationary time series is like a toy car that doesn’t run in a straight line. By taking the difference in the time series vector or taking the second difference of the log of the data, we can fix the toy car and make it run in a straight line. This is helpful for making accurate predictions.\n\n\nFunction\nWe are going to use the adf.test() function from the {aTSA} library. Here is the function:\n\nadf.test(x, nlag = NULL, output = TRUE)\n\nHere are the arugments to the parameters.\n\nx- a numeric vector or time series.\nalternative - the lag order with default to calculate the test statistic. See details for the default.\noutput - a logical value indicating to print the test results in R console. The default is TRUE.\n\n\n\nExamples\nAs an example, we are going to use the R built in data set AirPassengers as our timeseries. This data is both cyclical and trending so it is good for this purpose.\n\nlibrary(aTSA)\n\nplot(AirPassengers)\n\n\n\n\nNow that we know what it looks like, lets see if it is stationary right off the bat.\n\nadf.test(AirPassengers)\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag      ADF p.value\n[1,]   0  0.04712   0.657\n[2,]   1 -0.35240   0.542\n[3,]   2 -0.00582   0.641\n[4,]   3  0.26034   0.718\n[5,]   4  0.82238   0.879\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -1.748   0.427\n[2,]   1 -2.345   0.194\n[3,]   2 -1.811   0.402\n[4,]   3 -1.536   0.509\n[5,]   4 -0.986   0.701\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -4.64    0.01\n[2,]   1 -7.65    0.01\n[3,]   2 -7.09    0.01\n[4,]   3 -6.94    0.01\n[5,]   4 -5.95    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nSo we can see that right off the bat that “Type 1” and “Type 2” fail as there is significant trend in this data as we can plainly see. Let’s see what happens when we take a simpmle diff() of the series.\n\nplot(diff(AirPassengers))\n\n\n\n\nLooking like its still going to fail, but let’s run the test anyways.\n\nadf.test(diff(AirPassengers))\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -8.58    0.01\n[2,]   1 -8.68    0.01\n[3,]   2 -8.13    0.01\n[4,]   3 -8.48    0.01\n[5,]   4 -6.59    0.01\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -8.58    0.01\n[2,]   1 -8.69    0.01\n[3,]   2 -8.17    0.01\n[4,]   3 -8.60    0.01\n[5,]   4 -6.70    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -8.55    0.01\n[2,]   1 -8.66    0.01\n[3,]   2 -8.14    0.01\n[4,]   3 -8.57    0.01\n[5,]   4 -6.69    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nThe adf.test comes back with a p.value <= 0.01 as the data is no longer presenting a trend, but as we can plainly see, the data has non constant variance overtime which we know we need. Here we will use the {TidyDensity} package to use the cvar() (cumulative variance) function to see the ongoing variance.\n\nlibrary(TidyDensity)\n\nplot(cvar(diff(AirPassengers)), type = \"l\")\n\n\n\n\nReject the null that the data is stationary. So lets proceed with a diff diff log of the data and see what we get. First let’s visualize.\n\nplot(diff(diff(log(AirPassengers))))\n\n\n\nplot(cvar(diff(diff(log(AirPassengers)))), type = \"l\")\n\n\n\n\nLooking good!\n\nadf.test(diff(diff(log(AirPassengers))))\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -15.90    0.01\n[2,]   1 -12.78    0.01\n[3,]   2  -9.28    0.01\n[4,]   3 -10.76    0.01\n[5,]   4  -9.72    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -15.85    0.01\n[2,]   1 -12.73    0.01\n[3,]   2  -9.24    0.01\n[4,]   3 -10.73    0.01\n[5,]   4  -9.68    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -15.79    0.01\n[2,]   1 -12.68    0.01\n[3,]   2  -9.21    0.01\n[4,]   3 -10.68    0.01\n[5,]   4  -9.64    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nVoila!\n\n\nReferences\n\nhttps://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322\nhttps://www.statology.org/dickey-fuller-test-in-r/"
  },
  {
    "objectID": "posts/rtip-2023-01-25/index.html",
    "href": "posts/rtip-2023-01-25/index.html",
    "title": "Simplifying List Filtering in R with purrr’s keep()",
    "section": "",
    "text": "Introduction\nThe {purrr} package in R is a powerful tool for working with lists and other data structures. One particularly useful function in the package is keep(), which allows you to filter a list by keeping only the elements that meet certain conditions.\nThe keep() function takes two arguments: the list to filter, and a function that returns a logical value indicating whether each element of the list should be kept. The function can be specified as an anonymous function or a named function, and it should take a single argument (the current element of the list).\nFor example, let’s say we have a list of numbers and we want to keep only the even numbers. We could use the keep() function with an anonymous function that checks the remainder of the current element divided by 2:\n\nlibrary(purrr)\n\nnumbers <- c(1, 2, 3, 4, 5, 6)\neven_numbers <- keep(numbers, function(x) x %% 2 == 0)\neven_numbers\n\n[1] 2 4 6\n\n\nWe see that this keeps [1] 2 4 6.\nThe purrr package also provides a convenient shorthand for this operation, .p, which can be used inside the keep function to return the element.\n\neven_numbers <- keep(numbers, ~ .x %% 2 == 0)\neven_numbers\n\n[1] 2 4 6\n\n\nYou can also use the keep() function to filter a list of other types of objects, such as strings or lists. For example, you could use it to keep only the strings that are longer than a certain length:\n\nwords <- c(\"cat\", \"dog\", \"elephant\", \"bird\")\nlong_words <- keep(words, function(x) nchar(x) > 4)\nlong_words\n\n[1] \"elephant\"\n\n\nWe see that this keeps “elephant” & “bird”.\nIn summary, the {purrr} package’s keep() function is a powerful tool for filtering lists in R, and the .p parameter can be used as a shorthand. It can be used to keep only the items in a list that meet a user-given condition, and it can be used with a variety of data types.\n\n\nFunction\nHere is the keep() function and it’s parameters.\n\nkeep(.x, .p, ...)\n\nHere are the arguments to the parameters.\n\n.x - A list or vector.\n.p - A predicate function (i.e. a function that returns either TRUE or FALSE) specified in one of the following ways:\n\nA named function, e.g. is.character.\nAn anonymous function, e.g. \\(x) all(x < 0) or function(x) all(x < 0).\nA formula, e.g. ~ all(.x < 0). You must use .x to refer to the first argument). Only recommended if you require backward compatibility with older versions of R.\n\n... - Additional arguments passed on to .p.\n\n\n\nExamples\nI recently came across wanting to filter a list that is given as an argument to a parameter. The function I am working for my upcoming {tidyAML} package has a function called create_workflow_set() that has a parameter .recipe_list which is set to list(). The user must only place recipes in this list or else I want it to fail. So I was able to write a quick check using keep() like so:\n\n# Checks ----\n# only keep() recipes\nrec_list <- purrr::keep(rec_list, ~ inherits(.x, \"recipe\"))\n\nVoila!"
  },
  {
    "objectID": "posts/tidydensity-20221007/index.html",
    "href": "posts/tidydensity-20221007/index.html",
    "title": "TidyDensity Primer",
    "section": "",
    "text": "This is going to serve as a sort of primer for the {TidyDensity} package.\nThe goal of {TidyDensity} is to make working with random numbers from different distributions easy. All tidy_ distribution functions provide the following components:\n\n[r_]\n[d_]\n[q_]\n[p_]\n\n\nInstallation\nYou can install the released version of {TidyDensity} from CRAN with:\ninstall.packages(\"TidyDensity\")\nAnd the development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/TidyDensity\")\n\n\nExample Data\nThis is a basic example which shows you how to solve a common problem, which is, how do we generate randomly generated data from a normal distribution of some mean, and some standard deviation with n points and sims number of simulations?\nWith the function tidy_normal() we can generate such data. All functions that are condsidered tidy_ distribution functions, meaning those that generate randomly generated data from some distribution, have the same API call structure.\nFor example, using tidy_normal() the full function call at it’s default is as follows:\ntidy_normal(.n = 50, .mean = 0, .sd = 1, .num_sims = 1).\nWhat this means is that we want to generate 50 points from a standard normal distribution of mean 0 and with a standard deviation of 1, and we want to generate a single simulation of this data.\nLet’s see an example below:\n\nsuppressPackageStartupMessages(library(TidyDensity))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(ggplot2))\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nset.seed(123)\ntidy_normal()\n\n# A tibble: 50 × 7\n   sim_number     x       y    dx       dy     p       q\n   <fct>      <int>   <dbl> <dbl>    <dbl> <dbl>   <dbl>\n 1 1              1 -0.560  -3.11 0.000256 0.288 -0.560 \n 2 1              2 -0.230  -2.98 0.000691 0.409 -0.230 \n 3 1              3  1.56   -2.85 0.00167  0.940  1.56  \n 4 1              4  0.0705 -2.72 0.00362  0.528  0.0705\n 5 1              5  0.129  -2.59 0.00707  0.551  0.129 \n 6 1              6  1.72   -2.45 0.0125   0.957  1.72  \n 7 1              7  0.461  -2.32 0.0201   0.678  0.461 \n 8 1              8 -1.27   -2.19 0.0298   0.103 -1.27  \n 9 1              9 -0.687  -2.06 0.0415   0.246 -0.687 \n10 1             10 -0.446  -1.93 0.0552   0.328 -0.446 \n# … with 40 more rows\n\n\nWhat comes back we see is a tibble. This is true for all functions in the {TidyDensity} library. It was a goal to return items that are consistent with the tidyverse.\nNow let’s talk a bit about what was actually returned. There are a few columns that are returned, these are referred to as the r, d, p, and q\n\n[r_] Shows as y and is the randomly generated data from the underlying distribution.\n[d_] Two components come back, dx and dy where these are generated from the [stats::density()] function with n set to .n from the function input.\n[p_] Shows as p and is the results of the p_ function, in this case pnorm() where the x of the input goes from 0-1 with .n points.\n[q_] Shows as q and is the results of the q_ function, in this case qnorm() where the x of the input goes from 0-1 with .n points.\n\nNow you will also see two more columns, namely, sim_number a factor column and x an integer column. The sim_number column represents the current simulation for which data was drawn, and the x represents the nth point in that simulation.\n\n\nVisualization Example\nWith data typically comes the need to see it! Show me the data! TidyDensity has a variety of autoplot functionality that will present only data from a tidy_ distribution function. We will take a look at output from tidy_normal() and set a see otherwise everytime this site is rendered the data would change.\n\nset.seed(123)\ntn <- tidy_normal(.n = 100, .num_sims = 6)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe can see that the plots are faily informative. There are the regular density plot, the quantile plot, probability and qq plots. The title and subtitle of these plots are generated from attributes that are attached to the output of the tidy_ distribution function. Let’s take a look at the attributes of tn\n\nattributes(tn)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n[235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n[253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n[271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n[289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n[307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n[325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n[343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n[361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n[379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n[397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n[415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n[433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n[451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n[469] 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n[487] 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n[505] 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n[523] 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n[541] 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n[559] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n[577] 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n[595] 595 596 597 598 599 600\n\n$names\n[1] \"sim_number\" \"x\"          \"y\"          \"dx\"         \"dy\"        \n[6] \"p\"          \"q\"         \n\n$distribution_family_type\n[1] \"continuous\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 1\n\n$.n\n[1] 100\n\n$.num_sims\n[1] 6\n\n$tibble_type\n[1] \"tidy_gaussian\"\n\n$ps\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$qs\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$param_grid\n# A tibble: 1 × 2\n  .mean   .sd\n  <dbl> <dbl>\n1     0     1\n\n$param_grid_txt\n[1] \"c(0, 1)\"\n\n$dist_with_params\n[1] \"Gaussian c(0, 1)\"\n\n\nI won’t go over them but as you can see, the attribute list can get long and has a lot of great information in it.\nNow what if we have simulations over 9? The legend would get fairly large making the visualization difficult to read.\nLet’s take a look at 20 simulations.\n\ntn <- tidy_normal(.n = 100, .num_sims = 20)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe see that the legend disappears! That’s great, but what if we still want to see what simulation is what? Well, make the plot interactive!\n\ntidy_autoplot(tn, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-2022-12-23/index.html",
    "href": "posts/weekly-rtip-2022-12-23/index.html",
    "title": "Simulating Time Series Model Forecasts with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series models are a powerful tool for forecasting future values of a time-dependent variable. These models are commonly used in a variety of fields, including finance, economics, and engineering, to predict future outcomes based on past data.\nOne important aspect of time series modeling is the ability to simulate model forecasts. This allows us to evaluate the performance of different forecasting methods and to compare the results of different models. Simulating forecasts also allows us to assess the uncertainty associated with our predictions, which can be especially useful when making important decisions based on the forecast.\nThere are several benefits to simulating time series model forecasts:\n\nImproved accuracy: By simulating forecasts, we can identify the best forecasting method for a given time series and optimize its parameters. This can lead to more accurate forecasts, especially for long-term predictions.\nEnhanced understanding: Simulating forecasts helps us to understand how different factors, such as seasonality and trend, affect the prediction. This understanding can inform our decision-making and allow us to make more informed predictions.\nImproved communication: Simulating forecasts allows us to present the uncertainty associated with our predictions, which can be useful for communicating the potential risks and benefits of different courses of action.\n\nThe R package {healthyR.ts} includes a function called ts_forecast_simulator() that can be used to simulate time series model forecasts. This function allows users to specify the forecasting method, the number of simulations to run, and the length of the forecast horizon. It also provides options for visualizing the results, including plots of the forecast distribution and summary statistics such as the mean and standard deviation of the forecasts.\nIn summary, simulating time series model forecasts is a valuable tool for improving the accuracy and understanding of our predictions, as well as for communicating the uncertainty associated with these forecasts. The ts_forecast_simulator() function in the {healthyR.ts} package is a useful tool for performing these simulations in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_forecast_simulator(\n  .model,\n  .data,\n  .ext_reg = NULL,\n  .frequency = NULL,\n  .bootstrap = TRUE,\n  .horizon = 4,\n  .iterations = 25,\n  .sim_color = \"steelblue\",\n  .alpha = 0.05\n)\n\nNow let’s take a look at the arguments that get provided to the parameters.\n\n.model - A forecasting model of one of the following from the forecast package:\n\nArima\nauto.arima\nets\nnnetar\nArima() with xreg\n\n.data - The data that is used for the .model parameter. This is used with timetk::tk_index()\n.ext_reg - A tibble or matrix of future xregs that should be the same length as the horizon you want to forecast.\n.frequency - This is for the conversion of an internal table and should match the time frequency of the data.\n.bootstrap - A boolean value of TRUE/FALSE. From forecast::simulate.Arima() Do simulation using resampled errors rather than normally distributed errors.\n.horizon - An integer defining the forecast horizon.\n.iterations - An integer, set the number of iterations of the simulation.\n.sim_color - Set the color of the simulation paths lines.\n.alpha - Set the opacity level of the simulation path lines.\n\nGreat, now let’s take a look at examples.\n\n\nExamples\n\nlibrary(healthyR.ts)\nlibrary(forecast)\n\nfit <- auto.arima(AirPassengers)\ndata_tbl <- ts_to_tbl(AirPassengers)\n\n# Simulate 50 possible forecast paths, with .horizon of 12 months\noutput <- ts_forecast_simulator(\n  .model        = fit\n  , .horizon    = 12\n  , .iterations = 50\n  , .data       = data_tbl\n)\n\nOk, so now we have our output object, which is a list object. Let’s see what it contains.\n\n\n\nForecast Simulation Output\n\n\nNow, let’s explore each element.\nFirst the forecast simulation data.\n\noutput$forecast_sim\n\n            sim_1    sim_2    sim_3    sim_4    sim_5    sim_6    sim_7\nJan 1961 445.7399 434.7175 462.6173 447.8849 453.5069 443.0944 464.4749\nFeb 1961 424.1606 423.3814 431.9512 420.4460 426.8245 423.4784 443.1545\nMar 1961 444.0015 467.6276 459.3452 450.2936 453.6116 461.3751 462.4351\nApr 1961 490.2370 504.9129 492.0632 491.2198 494.6883 489.6674 498.7902\nMay 1961 502.0907 517.7794 504.5420 503.7614 504.8544 504.1816 521.9044\nJun 1961 552.6359 588.4650 560.1199 567.5266 563.5353 552.8411 543.5723\nJul 1961 655.1872 666.8450 642.9804 653.4262 646.1559 648.1923 639.6587\nAug 1961 633.4657 635.5080 644.1405 650.1333 631.2688 630.4410 623.3784\nSep 1961 536.3259 551.6068 548.9176 554.8289 533.3672 553.5334 529.2182\nOct 1961 486.2815 513.2851 514.8842 513.2190 481.4636 495.1295 490.7402\nNov 1961 406.9061 428.0550 437.2211 435.3443 426.4892 425.9885 416.1940\nDec 1961 454.1048 478.1804 477.3923 475.0052 504.0761 486.3584 472.2933\n            sim_8    sim_9   sim_10   sim_11   sim_12   sim_13   sim_14\nJan 1961 444.3152 444.3203 453.5722 438.0253 438.0253 425.2956 458.6829\nFeb 1961 418.1653 418.2569 423.9229 413.1814 404.0688 403.3318 445.9250\nMar 1961 450.8166 437.2694 452.5463 433.7963 450.5365 433.6445 448.2279\nApr 1961 489.4801 501.5545 504.4428 488.2243 503.9232 476.7711 495.1634\nMay 1961 499.7523 499.2059 508.6748 496.3915 520.8837 489.1480 502.1013\nJun 1961 562.7791 574.2660 571.6261 560.3126 578.8797 564.5855 581.8097\nJul 1961 653.2719 655.1924 655.0359 647.2704 664.1505 660.7023 654.6549\nAug 1961 649.6533 639.4611 655.1187 610.5014 661.2034 606.4399 654.0751\nSep 1961 542.5110 548.6498 560.1903 505.3303 552.4413 515.2890 547.9581\nOct 1961 505.5032 490.4686 495.1026 464.1546 503.7735 479.0465 501.3405\nNov 1961 423.0761 414.9520 445.9182 399.1634 435.6692 405.7617 447.0409\nDec 1961 454.4311 452.3740 473.5319 435.0425 468.3699 449.6422 474.2666\n           sim_15   sim_16   sim_17   sim_18   sim_19   sim_20   sim_21\nJan 1961 455.4787 451.0585 459.4839 444.3242 435.1001 443.7523 444.9274\nFeb 1961 439.4881 422.0447 442.7488 427.5273 412.6110 430.0844 418.5412\nMar 1961 473.0626 449.9922 479.0381 454.4156 443.6824 454.0246 455.8716\nApr 1961 497.0203 501.5911 517.4542 495.3593 476.1640 489.3426 492.7721\nMay 1961 515.2215 526.5615 506.5715 487.1920 504.6323 500.4116 506.1908\nJun 1961 576.5544 581.4492 560.9466 560.1496 557.4915 551.0780 580.6906\nJul 1961 662.4222 675.8571 643.9878 645.8877 634.9354 649.0339 655.5332\nAug 1961 648.2923 653.8785 639.1892 624.4315 643.3871 630.6072 638.5270\nSep 1961 548.8370 547.7029 548.0178 529.7305 552.4617 533.3734 541.2910\nOct 1961 500.8130 500.6410 509.7552 481.4202 500.3374 484.8628 513.4386\nNov 1961 448.3088 427.7597 434.1320 424.8617 468.9838 421.0139 460.0366\nDec 1961 479.2161 468.6392 473.0734 460.6548 492.5949 445.3285 483.7701\n           sim_22   sim_23   sim_24   sim_25   sim_26   sim_27   sim_28\nJan 1961 431.9061 436.9803 445.6030 470.3241 438.1303 438.0355 442.8814\nFeb 1961 405.5096 406.4255 431.2780 451.1454 438.4957 414.3992 406.9653\nMar 1961 437.8384 437.3900 455.2998 472.2864 457.3227 452.7172 439.0830\nApr 1961 480.2212 479.1317 491.4019 520.8334 492.1962 497.7494 495.6963\nMay 1961 507.5101 491.4699 502.3217 522.6236 504.1139 498.8398 490.4093\nJun 1961 565.3556 555.2957 562.9435 569.6062 575.3107 565.8427 558.4052\nJul 1961 631.8589 643.0705 649.3241 656.5773 699.1111 660.5835 656.7441\nAug 1961 636.9621 620.3416 635.3089 658.5513 666.7588 655.0455 634.5556\nSep 1961 533.4397 546.0061 537.0957 552.6849 578.5525 563.2023 557.6656\nOct 1961 493.0531 483.6816 489.7577 517.3445 535.8427 509.2783 506.7000\nNov 1961 421.6430 429.6623 419.7854 427.1834 456.2713 429.0018 434.0667\nDec 1961 452.2728 457.5337 460.9375 470.2428 514.2062 482.2928 490.3966\n           sim_29   sim_30   sim_31   sim_32   sim_33   sim_34   sim_35\nJan 1961 455.3775 444.3272 461.2236 433.8962 464.4749 438.0355 439.1887\nFeb 1961 426.9441 418.2812 447.4852 424.0317 409.7277 415.1395 433.4188\nMar 1961 457.7136 440.5773 468.7090 447.5754 438.9636 443.0306 420.3961\nApr 1961 505.5824 483.8879 508.5020 490.2527 475.6410 497.7693 480.1376\nMay 1961 502.1128 495.2827 526.9527 488.3082 510.2163 524.8456 504.3970\nJun 1961 564.2560 565.1836 571.4699 554.6782 548.1972 582.0468 557.3177\nJul 1961 675.1975 656.7991 674.3476 643.9736 631.1844 665.9546 650.1157\nAug 1961 642.3301 604.3293 649.4941 612.8896 632.0786 640.5119 626.3405\nSep 1961 546.0228 520.2840 550.9040 533.3240 544.4996 540.7844 555.0268\nOct 1961 517.2212 480.8524 494.4332 467.0373 505.1177 495.0939 511.8599\nNov 1961 436.9353 409.0263 423.5813 407.0598 433.9868 420.4208 435.3652\nDec 1961 477.7085 444.2295 463.2785 447.8678 471.4819 477.0888 486.3902\n           sim_36   sim_37   sim_38   sim_39   sim_40   sim_41   sim_42\nJan 1961 458.2243 453.5069 442.9633 437.7823 456.7181 439.1887 444.2746\nFeb 1961 437.0087 430.5385 443.4436 434.5281 441.0503 415.1167 410.6375\nMar 1961 450.4477 456.5806 462.1142 454.3194 453.8004 433.9958 450.8734\nApr 1961 507.2234 499.3962 524.0761 495.3350 498.6223 465.6067 489.4286\nMay 1961 521.1416 518.5159 541.6983 514.5651 504.7369 474.7817 504.5533\nJun 1961 579.8129 580.3206 598.3710 558.8196 564.7209 544.2263 569.3150\nJul 1961 685.2838 663.2947 685.4005 652.3353 664.4497 622.2188 654.6807\nAug 1961 671.5436 671.9516 655.8918 630.8036 628.7970 615.9232 637.1259\nSep 1961 566.4307 567.5127 576.3038 539.0021 523.1752 530.4702 525.4079\nOct 1961 506.4949 510.5275 513.7364 488.4346 483.6546 479.5688 482.5080\nNov 1961 455.9362 453.4418 460.7156 417.2549 413.5535 408.8823 397.0385\nDec 1961 487.4205 481.4040 488.4877 455.6214 481.9331 453.4355 448.3142\n           sim_43   sim_44   sim_45   sim_46   sim_47   sim_48   sim_49\nJan 1961 456.7181 448.9127 444.1944 439.1887 444.3762 443.9540 449.6957\nFeb 1961 415.5481 417.4993 432.0998 418.8621 411.7694 419.4677 432.5796\nMar 1961 441.1190 446.8376 453.2327 448.5254 448.2663 470.9662 455.3371\nApr 1961 484.0184 474.0491 510.7521 487.2088 508.1952 505.5636 510.9074\nMay 1961 474.5845 493.5429 520.5754 493.9236 521.0746 494.3940 527.4129\nJun 1961 540.3995 553.2761 571.1053 570.3663 579.1209 553.5803 571.8349\nJul 1961 649.1646 657.5268 653.2605 657.5574 676.0876 607.8539 654.0949\nAug 1961 643.3467 635.9533 636.3980 656.9203 673.6242 590.6108 632.1774\nSep 1961 539.6460 532.2568 541.7711 546.2181 564.7882 492.5588 501.7329\nOct 1961 490.9799 483.5566 507.1510 500.1024 505.8764 471.2807 448.5220\nNov 1961 412.2859 415.6316 428.0756 413.5517 434.7768 384.6290 382.9796\nDec 1961 461.1189 422.9218 457.3822 459.4905 484.2661 434.8731 429.3802\n           sim_50\nJan 1961 442.9633\nFeb 1961 382.3990\nMar 1961 411.8363\nApr 1961 459.1659\nMay 1961 458.3228\nJun 1961 524.1910\nJul 1961 615.5164\nAug 1961 614.8716\nSep 1961 506.6345\nOct 1961 444.9304\nNov 1961 383.3882\nDec 1961 426.0341\n\noutput$forecast_sim_tbl\n\n# A tibble: 600 × 4\n       x     y n        id\n   <dbl> <dbl> <chr> <int>\n 1 1961.  446. sim_1     1\n 2 1961.  424. sim_1     2\n 3 1961.  444. sim_1     3\n 4 1961.  490. sim_1     4\n 5 1961.  502. sim_1     5\n 6 1961.  553. sim_1     6\n 7 1962.  655. sim_1     7\n 8 1962.  633. sim_1     8\n 9 1962.  536. sim_1     9\n10 1962.  486. sim_1    10\n# … with 590 more rows\n\n\nThe time series that was used.\n\noutput$time_series\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nThe fitted values in two different formats\n\noutput$fitted_values\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1949 111.9353 117.9664 131.9662 128.9774 120.9892 134.9782 147.9692 147.9731\n1950 115.4270 121.3807 138.5312 137.2522 127.5180 139.7865 158.9159 166.3207\n1951 132.8130 151.5147 164.7662 165.5934 153.1413 186.9835 201.0564 197.1598\n1952 171.2150 174.9573 205.3873 181.9983 189.5111 191.9692 229.3659 230.2887\n1953 197.4932 205.0669 211.9492 214.7030 229.4790 261.9978 259.3789 272.3394\n1954 205.8921 206.1940 236.4993 236.1932 227.3492 247.7618 281.4459 303.3972\n1955 229.7559 221.0431 274.5806 260.1968 270.5411 299.1164 345.0987 346.2332\n1956 283.9172 273.9637 307.7212 313.9008 312.5977 358.8094 415.5070 394.8641\n1957 313.6903 307.2137 343.5753 347.2471 353.0923 409.4687 455.6903 452.6105\n1958 346.2681 328.3116 377.2767 360.7205 361.5534 432.0632 479.8147 490.8438\n1959 346.5194 335.2349 385.8185 384.9676 406.8419 485.3013 531.0698 553.0149\n1960 418.4120 397.2579 454.0138 420.2105 468.2327 523.6974 603.6761 623.9211\n          Sep      Oct      Nov      Dec\n1949 135.9874 119.0049 104.0187 118.0778\n1950 156.2023 139.1631 119.1047 128.9974\n1951 185.1990 158.4085 140.6879 169.2312\n1952 220.7255 190.3056 172.8446 191.9035\n1953 238.8716 218.7628 194.3686 207.1910\n1954 261.6971 232.5912 199.3564 222.5606\n1955 309.8394 277.5670 246.5073 264.0691\n1956 363.2702 318.0959 271.0990 310.9291\n1957 409.6920 354.9782 312.2741 341.2514\n1958 438.0323 360.3301 317.3131 346.5759\n1959 454.6235 412.1130 357.5358 384.6475\n1960 513.8591 450.7760 410.8955 439.9468\n\noutput$fitted_values_tbl\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949   112.\n 2 Feb 1949   118.\n 3 Mar 1949   132.\n 4 Apr 1949   129.\n 5 May 1949   121.\n 6 Jun 1949   135.\n 7 Jul 1949   148.\n 8 Aug 1949   148.\n 9 Sep 1949   136.\n10 Oct 1949   119.\n# … with 134 more rows\n\n\nThe residual values in two different formats\n\noutput$residual_values\n\n               Jan           Feb           Mar           Apr           May\n1949   0.064663218   0.033565844   0.033806149   0.022551853   0.010753877\n1950  -0.426993657   4.619296276   2.468817592  -2.252222539  -2.517970381\n1951  12.187004737  -1.514734464  13.233788623  -2.593406722  18.858703025\n1952  -0.215049450   5.042657391 -12.387298452  -0.998279160  -6.511066526\n1953  -1.493243603  -9.066863638  24.050789583  20.296981218  -0.479038408\n1954  -1.892145409 -18.194023466  -1.499295070  -9.193240870   6.650775967\n1955  12.244087324  11.956865808  -7.580632008   8.803192574  -0.541058565\n1956   0.082775370   3.036286487   9.278782426  -0.900756132   5.402300489\n1957   1.309660582  -6.213658164  12.424731881   0.752860797   1.907693986\n1958  -6.268081829 -10.311568418 -15.276674427 -12.720532082   1.446551672\n1959  13.480639348   6.765147100  20.181485627  11.032373098  13.158068772\n1960  -1.412019863  -6.257914445 -35.013829003  40.789527421   3.767286916\n               Jun           Jul           Aug           Sep           Oct\n1949   0.021825883   0.030792890   0.026927580   0.012574909  -0.004856125\n1950   9.213518228  11.084130208   3.679258769   1.797714396  -6.163078961\n1951  -8.983477860  -2.056432959   1.840247841  -1.199018264   3.591516610\n1952  26.030752750   0.634055619  11.711292178 -11.725472510   0.694363698\n1953 -18.997830355   4.621111100  -0.339432546  -1.871647726  -7.762821892\n1954  16.238163730  20.554095186 -10.397216564  -2.697121208  -3.591173672\n1955  15.883576851  18.901348128   0.766758698   2.160561460  -3.567021842\n1956  15.190550278  -2.507027255  10.135908860  -8.270245332 -12.095862166\n1957  12.531339373   9.309735715  14.389487605  -5.692026179  -7.978198549\n1958   2.936791273  11.185298228  14.156225803 -34.032306461  -1.330101426\n1959 -13.301336524  16.930226311   5.985059029   8.376509562  -5.112953069\n1960  11.302583143  18.323916561 -17.921058274  -5.859106651  10.223989361\n               Nov           Dec\n1949  -0.018746667  -0.077775679\n1950  -5.104668785  11.002554045\n1951   5.312079856  -3.231170377\n1952  -0.844622054   2.096450492\n1953 -14.368618246  -6.190983141\n1954   3.643587684   6.439397882\n1955  -9.507327199  13.930943896\n1956  -0.099013624  -4.929146809\n1957  -7.274091927  -5.251369244\n1958  -7.313133943  -9.575869505\n1959   4.464243872  20.352533059\n1960 -20.895479201  -7.946822359\n\noutput$residual_values_tbl\n\n# A tibble: 144 × 2\n   index        value\n   <yearmon>    <dbl>\n 1 Jan 1949   0.0647 \n 2 Feb 1949   0.0336 \n 3 Mar 1949   0.0338 \n 4 Apr 1949   0.0226 \n 5 May 1949   0.0108 \n 6 Jun 1949   0.0218 \n 7 Jul 1949   0.0308 \n 8 Aug 1949   0.0269 \n 9 Sep 1949   0.0126 \n10 Oct 1949  -0.00486\n# … with 134 more rows\n\n\nThe input data itself\n\noutput$input_data\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949    112\n 2 Feb 1949    118\n 3 Mar 1949    132\n 4 Apr 1949    129\n 5 May 1949    121\n 6 Jun 1949    135\n 7 Jul 1949    148\n 8 Aug 1949    148\n 9 Sep 1949    136\n10 Oct 1949    119\n# … with 134 more rows\n\n\nThe time series simulations\n\noutput$sim_ts_tbl\n\n# A tibble: 600 × 5\n   index         x     y n        id\n   <yearmon> <dbl> <dbl> <chr> <int>\n 1 Jan 1961  1961.  446. sim_1     1\n 2 Feb 1961  1961.  424. sim_1     2\n 3 Mar 1961  1961.  444. sim_1     3\n 4 Apr 1961  1961.  490. sim_1     4\n 5 May 1961  1961.  502. sim_1     5\n 6 Jun 1961  1961.  553. sim_1     6\n 7 Jul 1961  1962.  655. sim_1     7\n 8 Aug 1961  1962.  633. sim_1     8\n 9 Sep 1961  1962.  536. sim_1     9\n10 Oct 1961  1962.  486. sim_1    10\n# … with 590 more rows\n\n\nNow, the visuals, first the static ggplot\n\noutput$ggplot\n\n\n\n\nThe interactive plotly plot.\n\noutput$plotly_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-2023-01-06/index.html",
    "href": "posts/weekly-rtip-2023-01-06/index.html",
    "title": "Brownian Motion",
    "section": "",
    "text": "Introduction\nBrownian motion, also known as the random motion of particles suspended in a fluid, is a phenomenon that was first described by Scottish botanist Robert Brown in 1827. It occurs when a particle is subjected to a series of random collisions with the molecules in the fluid.\nThe motion of the particle can be described mathematically using the following equation:\n\\[ \\frac{dx_t}{dt} = \\mu + \\sigma \\cdot W_t \\]\nWhere \\(x_t\\) represents the position of the particle at time t, \\(\\mu\\) is the drift coefficient, \\(\\sigma\\) is the diffusion coefficient, and \\(W_t\\) is a Wiener process (a type of random process).\nBrownian motion has a number of important applications, including in the field of finance. It is used to model the random movements of financial assets, such as stocks, over time. It can also be used to estimate the volatility of an asset, as well as to calculate the prices of financial derivatives such as options.\nIn physics, Brownian motion is used to study the behavior of small particles suspended in a fluid, as well as to understand the properties of fluids at the molecular level. It has also been used to study the motion of biological molecules, such as proteins, within cells.\nOverall, Brownian motion is a fundamental concept that has wide-ranging applications in a variety of fields, including finance, physics, and biology.\n\n\nFunction\nLet’s take a look at a function to produce such results. This type of functionality will be coming to my R packages {TidyDensity} and to {healthyR.ts}\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(forcats)\n\nbrownian_motion <- function(T, N, delta_t) {\n  # T: total time of the simulation (in seconds)\n  # N: number of simulations to generate\n  # delta_t: time step size (in seconds)\n  \n  # Initialize empty data.frame to store the simulations\n  sim_data <- data.frame()\n  \n  # Generate N simulations\n  for (i in 1:N) {\n    # Initialize the current simulation with a starting value of 0\n    sim <- c(0)\n    \n    # Generate the brownian motion values for each time step\n    for (t in 1:(T / delta_t)) {\n      sim <- c(sim, sim[t] + rnorm(1, mean = 0, sd = sqrt(delta_t)))\n    }\n    \n    # Bind the time steps, simulation values, and simulation number together\n    # in a data.frame and add it to the result\n    sim_data <- rbind(\n      sim_data, \n      data.frame(\n        t = seq(0, T, delta_t), \n        y = sim, \n        sim_number = i\n        )\n      ) %>%\n      as_tibble()\n  }\n  \n  sim_data <- sim_data %>%\n    mutate(sim_number = as_factor(sim_number)\n                  )\n  return(sim_data)\n}\n\nWe see that the internal variable sim is set to 0, this in the future will be set to an initial value that a user can provide.\n\n\nExamples\nLet’s take a look at a couple of examples.\n\nbrownian_motion(40, 25, .2) %>%\n  ggplot(aes(x = t, y = y, group = sim_number, color = sim_number)) +\n  geom_line() +\n  geom_smooth(se = FALSE, \n              aes(group = FALSE), \n              color = \"black\", \n              linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    x = \"Time t\",\n    y = \"Brownian Motion Value y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nNow lets take a look at the change in a few different ones at the same time.\n\nbm_tbl <- rbind(\n  brownian_motion(40, 25, .2) %>%\n    mutate(label = \"20% Volatility\"),\n  brownian_motion(40, 25, .1) %>%\n    mutate(label = \"10% Volatility\"),\n  brownian_motion(40, 25, .05) %>%\n    mutate(label = \"5% Volatility\"),\n  brownian_motion(40, 25, .025) %>%\n    mutate(label = \"2.5% Volatility\")\n)\n\nggplot(bm_tbl, aes(x = t, y = y, group = sim_number, color = sim_number)) +\n  geom_line() +\n  facet_wrap(~ label, scales = \"free\") +\n    geom_smooth(se = FALSE, \n              aes(group = FALSE), \n              color = \"black\", \n              linetype = \"dashed\") +\n  theme_minimal() +\n    labs(\n    x = \"Time t\",\n    y = \"Brownian Motion Value y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "title": "PCA with healthyR.ai",
    "section": "",
    "text": "In this post we are going to talk about how you can perform principal component analysis in R with {healthyR.ai} in a tidyverse compliant fashion.\nThe specific function we are going to discuss on this post is pca_your_recipe()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "title": "PCA with healthyR.ai",
    "section": "Library Load",
    "text": "Library Load\n\npacman::p_load(\n  \"healthyR.ai\",\n  \"healthyR.data\",\n  \"timetk\",\n  \"dplyr\",\n  \"purrr\",\n  \"rsample\",\n  \"recipes\"\n)\n\nNow that we have our libraries loaded lets get the data."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "title": "PCA with healthyR.ai",
    "section": "Data",
    "text": "Data\n\ndata_tbl <- healthyR_data %>%\n  select(visit_end_date_time) %>%\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by       = \"month\",\n    value     = n()\n  ) %>%\n  set_names(\"date_col\", \"value\") %>%\n  filter_by_time(\n    .date_var = date_col,\n    .start_date = \"2013\",\n    .end_date = \"2020\"\n  )\n\nhead(data_tbl, 5)\n\n# A tibble: 5 × 2\n  date_col            value\n  <dttm>              <int>\n1 2013-01-01 00:00:00  2082\n2 2013-02-01 00:00:00  1719\n3 2013-03-01 00:00:00  1796\n4 2013-04-01 00:00:00  1865\n5 2013-05-01 00:00:00  2028\n\n\nNow for the splits object."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "title": "PCA with healthyR.ai",
    "section": "Splits",
    "text": "Splits\n\nsplits <- initial_split(data = data_tbl, prop = 0.8)\n\nsplits\n\n<Training/Testing/Total>\n<76/19/95>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "title": "PCA with healthyR.ai",
    "section": "Recipe and Output",
    "text": "Recipe and Output\nNow it is time for the recipe and the output objects.\n\nrec_obj <- recipe(value ~ ., training(splits)) %>%\n  step_timeseries_signature(date_col) %>%\n  step_rm(matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\"))\n\noutput_list <- pca_your_recipe(rec_obj, .data = data_tbl)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "title": "PCA with healthyR.ai",
    "section": "PCA Transform",
    "text": "PCA Transform\n\noutput_list$pca_transform\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nTimeseries signature features from date_col\nVariables removed matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\")\nCentering for recipes::all_numeric()\nScaling for recipes::all_numeric()\nSparse, unbalanced variable filter on recipes::all_numeric()\nPCA extraction with recipes::all_numeric_predictors()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "title": "PCA with healthyR.ai",
    "section": "Variable Loadings",
    "text": "Variable Loadings\n\noutput_list$variable_loadings\n\n# A tibble: 169 × 4\n   terms                 value component id       \n   <chr>                 <dbl> <chr>     <chr>    \n 1 date_col_index.num -0.00137 PC1       pca_bVc37\n 2 date_col_year       0.0529  PC1       pca_bVc37\n 3 date_col_half      -0.385   PC1       pca_bVc37\n 4 date_col_quarter   -0.434   PC1       pca_bVc37\n 5 date_col_month     -0.437   PC1       pca_bVc37\n 6 date_col_wday      -0.0159  PC1       pca_bVc37\n 7 date_col_qday      -0.0608  PC1       pca_bVc37\n 8 date_col_yday      -0.437   PC1       pca_bVc37\n 9 date_col_mweek      0.0537  PC1       pca_bVc37\n10 date_col_week      -0.438   PC1       pca_bVc37\n# … with 159 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "title": "PCA with healthyR.ai",
    "section": "Variable Variance",
    "text": "Variable Variance\n\noutput_list$variable_variance\n\n# A tibble: 52 × 4\n   terms       value component id       \n   <chr>       <dbl>     <int> <chr>    \n 1 variance 5.14             1 pca_bVc37\n 2 variance 2.08             2 pca_bVc37\n 3 variance 1.47             3 pca_bVc37\n 4 variance 1.40             4 pca_bVc37\n 5 variance 1.07             5 pca_bVc37\n 6 variance 0.684            6 pca_bVc37\n 7 variance 0.583            7 pca_bVc37\n 8 variance 0.519            8 pca_bVc37\n 9 variance 0.0534           9 pca_bVc37\n10 variance 0.000231        10 pca_bVc37\n# … with 42 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Estimates",
    "text": "PCA Estimates\n\noutput_list$pca_estimates\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nTraining data contained 76 data points and no missing data.\n\nOperations:\n\nTimeseries signature features from date_col [trained]\nVariables removed date_col_year.iso, date_col_month.xts, date_col_hour, d... [trained]\nCentering for value, date_col_index.num, date_col_year, date_... [trained]\nScaling for value, date_col_index.num, date_col_year, date_... [trained]\nSparse, unbalanced variable filter removed date_col_day, date_col_mday, date_col_m... [trained]\nPCA extraction with date_col_index.num, date_col_year, date_col_half... [trained]"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Juiced Estimates",
    "text": "PCA Juiced Estimates\n\noutput_list$pca_juiced_estimates\n\n# A tibble: 76 × 8\n   date_col              value date_col…¹ date_…²     PC1     PC2     PC3    PC4\n   <dttm>                <dbl> <ord>      <ord>     <dbl>   <dbl>   <dbl>  <dbl>\n 1 2018-06-01 00:00:00  0.676  June       Friday   0.733   1.23   -1.30    0.536\n 2 2016-01-01 00:00:00 -0.133  January    Friday   3.51   -0.102  -0.500  -0.556\n 3 2013-05-01 00:00:00  1.67   May        Wednes…  1.15   -2.07   -1.68   -0.319\n 4 2018-10-01 00:00:00  0.337  October    Monday  -2.30    0.499   1.91   -1.92 \n 5 2016-09-01 00:00:00 -0.130  September  Thursd… -1.08    0.0972  0.410   2.64 \n 6 2016-07-01 00:00:00 -0.364  July       Friday  -0.0591  0.0211 -0.333   1.11 \n 7 2020-02-01 00:00:00 -0.646  February   Saturd…  3.08    2.48   -0.0249  0.214\n 8 2020-08-01 00:00:00 -1.42   August     Saturd… -0.491   2.60    0.142   1.88 \n 9 2018-03-01 00:00:00  0.243  March      Thursd…  2.74    0.848  -1.53    0.260\n10 2015-05-01 00:00:00 -0.0148 May        Friday   1.18   -0.636  -1.88   -0.147\n# … with 66 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "title": "PCA with healthyR.ai",
    "section": "PCA Baked Data",
    "text": "PCA Baked Data\n\noutput_list$pca_baked_data\n\n# A tibble: 95 × 8\n   date_col            value date_col_month…¹ date_…²    PC1   PC2    PC3    PC4\n   <dttm>              <dbl> <ord>            <ord>    <dbl> <dbl>  <dbl>  <dbl>\n 1 2013-01-01 00:00:00 1.86  January          Tuesday  3.60  -2.64  1.13  -0.871\n 2 2013-02-01 00:00:00 0.596 February         Friday   2.93  -1.76 -0.532  0.424\n 3 2013-03-01 00:00:00 0.864 March            Friday   2.62  -1.95 -2.24   0.643\n 4 2013-04-01 00:00:00 1.10  April            Monday   2.09  -2.68  1.39  -0.867\n 5 2013-05-01 00:00:00 1.67  May              Wednes…  1.15  -2.07 -1.68  -0.319\n 6 2013-06-01 00:00:00 0.923 June             Saturd…  0.612 -1.57 -2.01   0.919\n 7 2013-07-01 00:00:00 1.29  July             Monday  -0.669 -2.41  2.29  -0.420\n 8 2013-08-01 00:00:00 1.18  August           Thursd… -0.628 -1.76 -0.165  1.96 \n 9 2013-09-01 00:00:00 0.714 September        Sunday  -1.11  -2.19  0.910  2.25 \n10 2013-10-01 00:00:00 1.40  October          Tuesday -2.55  -1.91 -0.128 -1.48 \n# … with 85 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Data Frame",
    "text": "PCA Variance Data Frame\n\noutput_list$pca_variance_df\n\n# A tibble: 13 × 6\n   PC    var_explained var_pct_txt cum_var_pct cum_var_pct_txt ou_threshold\n   <chr>         <dbl> <chr>             <dbl> <chr>           <fct>       \n 1 PC1   0.395         39.55%            0.395 39.55%          Under       \n 2 PC2   0.160         16.02%            0.556 55.57%          Under       \n 3 PC3   0.113         11.29%            0.669 66.86%          Under       \n 4 PC4   0.107         10.75%            0.776 77.61%          Over        \n 5 PC5   0.0824        8.24%             0.858 85.85%          Over        \n 6 PC6   0.0526        5.26%             0.911 91.11%          Over        \n 7 PC7   0.0449        4.49%             0.956 95.59%          Over        \n 8 PC8   0.0400        4.00%             0.996 99.59%          Over        \n 9 PC9   0.00411       0.41%             1.00  100.00%         Over        \n10 PC10  0.0000178     0.00%             1.00  100.00%         Over        \n11 PC11  0.000000712   0.00%             1.00  100.00%         Over        \n12 PC12  0.000000273   0.00%             1.00  100.00%         Over        \n13 PC13  0.00000000196 0.00%             1     100.00%         Over"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Rotation Data Frame",
    "text": "PCA Rotation Data Frame\n\noutput_list$pca_rotation_df\n\n# A tibble: 13 × 13\n        PC1      PC2      PC3     PC4     PC5     PC6      PC7     PC8      PC9\n      <dbl>    <dbl>    <dbl>   <dbl>   <dbl>   <dbl>    <dbl>   <dbl>    <dbl>\n 1 -0.00137  0.671    0.116   -0.0521 -0.183   0.0165  0.0471   0.0277 -0.0177 \n 2  0.0529   0.667    0.116   -0.0610 -0.173   0.0146  0.0466   0.0313  0.00904\n 3 -0.385    0.0115   0.222    0.173   0.140   0.217   0.250   -0.0112  0.802  \n 4 -0.434    0.00824  0.0752  -0.0427  0.0615  0.135   0.00843  0.0332 -0.272  \n 5 -0.437    0.00278 -0.00879  0.0737 -0.0734  0.0167  0.00135 -0.0264 -0.213  \n 6 -0.0159   0.265   -0.406    0.274   0.468   0.254  -0.520   -0.366   0.0484 \n 7 -0.0608  -0.0200  -0.325    0.480  -0.542  -0.476  -0.0318  -0.244   0.187  \n 8 -0.437    0.00435 -0.00655  0.0740 -0.0733  0.0149  0.00231 -0.0303 -0.216  \n 9  0.0537  -0.164    0.562   -0.0242 -0.414   0.267  -0.572   -0.285   0.0519 \n10 -0.438    0.00638 -0.00736  0.0676 -0.0704  0.0236  0.00470 -0.0266 -0.219  \n11  0.250   -0.0238   0.208    0.420   0.0436  0.301   0.544   -0.492  -0.293  \n12 -0.0474   0.0762   0.516    0.142   0.460  -0.676  -0.108   -0.144  -0.0636 \n13  0.123    0.00776  0.150    0.666   0.0183  0.152  -0.161    0.676  -0.111  \n# … with 4 more variables: PC10 <dbl>, PC11 <dbl>, PC12 <dbl>, PC13 <dbl>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Scree Plot",
    "text": "PCA Variance Scree Plot\n\noutput_list$pca_variance_scree_plt"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Loadings Plot",
    "text": "PCA Loadings Plot\n\noutput_list$pca_loadings_plt\n\n\n\noutput_list$pca_loadings_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "title": "PCA with healthyR.ai",
    "section": "Top N Loadings Plots",
    "text": "Top N Loadings Plots\n\noutput_list$pca_top_n_loadings_plt\n\n\n\noutput_list$pca_top_n_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "",
    "text": "Minimal coding ML is not something that is unheard of and is rather prolific, think h2o and pycaret just to name two. There is also no shortage available for R with the h2o interface, and tidyfit. There are also similar low-code workflows in my r package {healthyR.ai}. Today I will specifically go through the workflow for Automatic KNN classification for the Iris data set where we will classify the Species."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Recipe Output",
    "text": "Recipe Output\n\nauto_knn$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\nCentering and scaling for recipes::all_numeric()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Model Info",
    "text": "Model Info\n\nauto_knn$model_info$was_tuned\n\n[1] \"tuned\"\n\n\n\nauto_knn$model_info$model_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$wflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$fitted_wflw\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(13L,     data, 5), distance = ~1.69935879141092, kernel = ~\"rank\")\n\nType of response variable: nominal\nMinimal misclassification: 0.03571429\nBest kernel: rank\nBest k: 13"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Tuning Info",
    "text": "Tuning Info\n\nauto_knn$tuned_info$tuning_grid\n\n# A tibble: 10 × 3\n   neighbors weight_func  dist_power\n       <int> <chr>             <dbl>\n 1         4 triangular        0.764\n 2        11 rectangular       0.219\n 3         5 gaussian          1.35 \n 4        14 triweight         0.351\n 5         5 biweight          1.05 \n 6         9 optimal           1.87 \n 7         7 cos               0.665\n 8        11 inv               1.18 \n 9        13 rank              1.70 \n10         1 epanechnikov      1.58 \n\n\n\nauto_knn$tuned_info$cv_obj\n\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$tuned_results\n\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics           .notes          \n   <list>          <chr>      <list>             <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 7]> <tibble [0 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 7]> <tibble [0 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 7]> <tibble [0 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 7]> <tibble [0 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 7]> <tibble [0 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 7]> <tibble [0 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 7]> <tibble [0 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 7]> <tibble [0 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 7]> <tibble [0 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 7]> <tibble [0 × 3]>\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$grid_size\n\n[1] 10\n\n\n\nauto_knn$tuned_info$best_metric\n\n[1] \"f_meas\"\n\n\n\nauto_knn$tuned_info$best_result_set\n\n# A tibble: 1 × 9\n  neighbors weight_func dist_power .metric .estima…¹  mean     n std_err .config\n      <int> <chr>            <dbl> <chr>   <chr>     <dbl> <int>   <dbl> <chr>  \n1        13 rank              1.70 f_meas  macro     0.957    25 0.00655 Prepro…\n# … with abbreviated variable name ¹​.estimator\n\n\n\nauto_knn$tuned_info$tuning_grid_plot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nauto_knn$tuned_info$plotly_grid_plot\n\n\n\n\n\nVoila!\nThank you for reading."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "title": "Time Series Lag Correlation Plots",
    "section": "",
    "text": "In time series analysis there is something called a lag. This simply means we take a look at some past event from some point in time t. This is a non-statistical method for looking at a relationship between a timeseries and its lags.\n{healthyR.ts} has a function called ts_lag_correlation(). This function, as described by it’s name, provides more than just a simple lag plot.\nThis function provides a lot of extra information for the end user. First let’s go over the function call."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Call",
    "text": "Function Call\nHere is the full call:\n\nts_lag_correlation(\n  .data,\n  .date_col,\n  .value_col,\n  .lags = 1,\n  .heatmap_color_low = \"white\",\n  .heatmap_color_hi = \"steelblue\"\n)\n\nHere are the arguments that get supplied to the different parameters.\n\n.data - A tibble of time series data\n.date_col - A date column\n.value_col - The value column being analyzed\n.lags - This is a vector of integer lags, ie 1 or c(1,6,12)\n.heatmap_color_low - What color should the low values of the heatmap of the correlation matrix be, the default is ‘white’\n.heatmap_color_hi - What color should the low values of the heatmap of the correlation matrix be, the default is ‘steelblue’"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Return",
    "text": "Function Return\nThe function itself returns a list object. The list has the following elements in it:\nData Elements\n\nlag_list\nlag_tbl\ncorrelation_lag_matrix\ncorrelation_lag_tbl\n\nPlot Elements\n\nlag_plot\nplotly_lag_plot\ncorrelation_heatmap\nplotly_heatmap"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Data Elements",
    "text": "Data Elements\nHere are the data elements.\n\noutput$data$lag_list\n\n[[1]]\n# A tibble: 143 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 1       118          112\n 2 1       132          118\n 3 1       129          132\n 4 1       121          129\n 5 1       135          121\n 6 1       148          135\n 7 1       148          148\n 8 1       136          148\n 9 1       119          136\n10 1       104          119\n# … with 133 more rows\n\n[[2]]\n# A tibble: 141 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 3       129          112\n 2 3       121          118\n 3 3       135          132\n 4 3       148          129\n 5 3       148          121\n 6 3       136          135\n 7 3       119          148\n 8 3       104          148\n 9 3       118          136\n10 3       115          119\n# … with 131 more rows\n\n[[3]]\n# A tibble: 138 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 6       148          112\n 2 6       148          118\n 3 6       136          132\n 4 6       119          129\n 5 6       104          121\n 6 6       118          135\n 7 6       115          148\n 8 6       126          148\n 9 6       141          136\n10 6       135          119\n# … with 128 more rows\n\n[[4]]\n# A tibble: 132 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 12      115          112\n 2 12      126          118\n 3 12      141          132\n 4 12      135          129\n 5 12      125          121\n 6 12      149          135\n 7 12      170          148\n 8 12      170          148\n 9 12      158          136\n10 12      133          119\n# … with 122 more rows\n\n\nThis is a list of all the tibbles of the different lags that were chosen.\n\noutput$data$lag_tbl\n\n# A tibble: 554 × 4\n   lag   value lagged_value lag_title\n   <fct> <dbl>        <dbl> <fct>    \n 1 1       118          112 Lag: 1   \n 2 1       132          118 Lag: 1   \n 3 1       129          132 Lag: 1   \n 4 1       121          129 Lag: 1   \n 5 1       135          121 Lag: 1   \n 6 1       148          135 Lag: 1   \n 7 1       148          148 Lag: 1   \n 8 1       136          148 Lag: 1   \n 9 1       119          136 Lag: 1   \n10 1       104          119 Lag: 1   \n# … with 544 more rows\n\n\nThis is the long lag tibble with all of the lags in it.\n\noutput$data$correlation_lag_matrix\n\n                value value_lag1 value_lag3 value_lag6 value_lag12\nvalue       1.0000000  0.9542938  0.8186636  0.7657001   0.9905274\nvalue_lag1  0.9542938  1.0000000  0.8828054  0.7726530   0.9492382\nvalue_lag3  0.8186636  0.8828054  1.0000000  0.8349550   0.8218493\nvalue_lag6  0.7657001  0.7726530  0.8349550  1.0000000   0.7780911\nvalue_lag12 0.9905274  0.9492382  0.8218493  0.7780911   1.0000000\n\n\nThis is the correlation matrix.\n\noutput$data$correlation_lag_tbl\n\n# A tibble: 25 × 3\n   name        data_names value\n   <fct>       <fct>      <dbl>\n 1 value       value      1    \n 2 value_lag1  value      0.954\n 3 value_lag3  value      0.819\n 4 value_lag6  value      0.766\n 5 value_lag12 value      0.991\n 6 value       value_lag1 0.954\n 7 value_lag1  value_lag1 1    \n 8 value_lag3  value_lag1 0.883\n 9 value_lag6  value_lag1 0.773\n10 value_lag12 value_lag1 0.949\n# … with 15 more rows\n\n\nThis is the correlation lag tibble"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Plot Elements",
    "text": "Plot Elements\n\noutput$plots$lag_plot\n\n\n\n\nThe Lag Plot itself.\n\noutput$plots$plotly_lag_plot\n\n\n\n\n\nA plotly version of the lag plot.\n\noutput$plots$correlation_heatmap\n\n\n\n\nA heatmap of the correlations.\n\noutput$plots$plotly_heatmap\n\n\n\n\n\nA plotly version of the correlation heatmap.\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "title": "Create QQ Plots for Time Series Models with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nA Q-Q plot, or quantile-quantile plot, is a graphical tool for comparing two sets of data to assess whether they come from the same distribution. In the context of time series modeling, a Q-Q plot can be used to check whether the residuals of a fitted time series model follow the normal distribution. This is important because many time series models, such as the autoregressive moving average (ARMA) model, assume that the residuals are normally distributed.\nTo create a Q-Q plot, the data are first sorted in ascending order and then divided into quantiles. The quantiles of the first dataset are then plotted against the quantiles of the second dataset. If the two datasets come from the same distribution, the points on the Q-Q plot will fall approximately on a straight line. Deviations from this line can indicate departures from the assumed distribution.\nFor example, if we have a time series dataset and we fit an ARMA model to it, we can use a Q-Q plot to check whether the residuals of the fitted model are normally distributed. If the Q-Q plot shows that the residuals do not follow the normal distribution, we may need to consider using a different time series model that does not assume normality of the residuals.\nIn summary, Q-Q plots are a useful tool for assessing the distribution of a dataset and for checking whether a time series model has produced satisfactory residuals.\nIn the R package {healthyR.ts} there is a function to view the QQ plot. This function is called ts_qq_plot() and it is meant to work with a calibration tibble from the excellent {modeltime} which is a {parsnip} extension package.\n\n\nFunction\nLet’s take a look at the full function call and the arguments that get provided to the parameters.\n\nts_qq_plot(\n  .calibration_tbl, \n  .model_id = NULL, \n  .interactive = FALSE\n  )\n\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nLet’s work through an example, and since we already spoke about ARMA let’s try out an ARMA model.\n\nlibrary(healthyR.ts)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(recipes)\nlibrary(parsnip)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_qq_plot(calibration_tbl, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "title": "Model Scedacity Plots with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nScedacity plots are a useful tool for evaluating the performance of time series models and identifying trends or patterns in the data. They are a type of scatter plot that compares the predicted values produced by a model to the observed values in the data, with a diagonal reference line indicating perfect agreement between the two.\nThe {healthyR.ts} package in R provides a convenient function for creating scedacity plots for time series data, called ts_scedacity_scatter_plot(). This function takes as input a calibration tibble which you would get from using the {modeltime} library, and produces a scedacity plot showing the predicted values against the observed values.\nOne of the main benefits of using a scedacity plot is that it allows you to visualize the accuracy of the model’s predictions. If the points on the plot fall close to the reference line, it indicates that the model is able to accurately predict the values in the data. On the other hand, if the points are scattered far from the reference line, it suggests that the model is not performing well and may need to be improved or refined.\nIn addition to evaluating the accuracy of the model, scedacity plots can also be used to identify trends or patterns in the data. For example, if there is a clear upward or downward trend in the points on the plot, it may indicate that the model is over- or under-estimating the values in the data. By identifying these trends, you can adjust the model or try different approaches to improve its performance.\nOverall, scedacity plots are a useful tool for evaluating the performance of time series models and identifying trends or patterns in the data. The ts_scedacity_scatter_plot() function in the {healthyR.ts} package makes it easy to create these plots and gain insights into the performance of your time series models.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_scedacity_scatter_plot(\n  .calibration_tbl,\n  .model_id = NULL,\n  .interactive = FALSE\n)\n\nLet’s take a look at the arguments that get provided to the parameters.\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(recipes)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nmodel_spec_mars <- mars(mode = \"regression\") %>%\n  set_engine(\"earth\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nwflw_fit_mars <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_mars) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima, wflw_fit_mars)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_scedacity_scatter_plot(calibration_tbl)\n\n\n\n\nNow the interactive plot.\n\nts_scedacity_scatter_plot(calibration_tbl, .interactive = TRUE)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "title": "Event Analysis with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime-to-event analysis, also known as survival analysis, is a statistical technique used to analyze the length of time until an event occurs. This type of analysis is often used in fields such as healthcare, engineering, and finance to understand the factors that influence the likelihood of an event occurring and to make predictions about future events.\nIn economics, an event study is a statistical technique used to analyze the effect of a specific event on a particular market or financial instrument. Event studies are commonly used in finance to understand how events, such as the announcement of a new product, the release of financial earnings, or a change in government policy, may impact the price or performance of a company’s stock or other financial instruments.\nTo conduct an event study, analysts typically collect data on the performance of a market or financial instrument before and after the event in question. This data is then used to estimate the effect of the event on the market or instrument.\nThere are several different methods that can be used to conduct an event study, including the market model, the abnormal return method, and the buy-and-hold abnormal return method. These methods allow analysts to quantify the effect of the event on the market or instrument and to identify any changes in market behavior that may have occurred as a result of the event.\nOverall, event studies are a valuable tool for understanding how specific events may impact financial markets and instruments, and are widely used in finance and economics to inform investment decisions and to better understand market behavior.\nIn this post we are going to examine a function from the R package {healthyR.ts} has a function called ts_time_event_analysis_tbl() that will help us understand what happens after a specified event, in this instance it will always be some percentage decrease or increase in a value.\nThere is a great article from Investopedia on this economic topic here\n\n\nFunction\nThe function is ts_time_event_analysis_tbl() and it’s complimentary plotting function ts_event_analysis_plot().\nHere is the tibble data return function.\n\nts_time_event_analysis_tbl(\n  .data,\n  .date_col,\n  .value_col,\n  .percent_change = 0.05,\n  .horizon = 12,\n  .precision = 2,\n  .direction = \"forward\",\n  .filter_non_event_groups = TRUE\n)\n\nLet’s take a look at the arguments to the parameters for this one.\n\n.data - The date.frame/tibble that holds the data.\n.date_col - The column with the date value.\n.value_col - The column with the value you are measuring.\n.percent_change - This defaults to 0.05 which is a 5% increase in the value_col.\n.horizon - How far do you want to look back or ahead.\n.precision - The default is 2 which means it rounds the lagged 1 value percent change to 2 decimal points. You may want more for more finely tuned results, this will result in fewer groupings.\n.direction - The default is forward. You can supply either forward, backwards or both.\nfilter_non_event_groups - The default is TRUE, this drops groupings with no events on the rare occasion it does occur.\n\nNow the plotting function.\n\nts_event_analysis_plot(\n  .data,\n  .plot_type = \"mean\",\n  .plot_ci = TRUE,\n  .interactive = FALSE\n)\n\n\n.data - The data that comes from the ts_time_event_analysis_tbl()\n.plot_type - The default is “mean” which will show the mean event change of the output from the analysis tibble. The possible values for this are: mean, median, and individual.\n.plot_ci - The default is TRUE. This will only work if you choose one of the aggregate plots of either “mean” or “median”\n.interactive - The default is FALSE. TRUE will return a plotly plot.\n\n\n\nExamples\nLet’s go through a couple examples using the AirPassengers data. We will first transform it into a tibble and then we will use a look period of 6. Let’s see the data output and then we will visualize.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndf <- ts_to_tbl(AirPassengers) %>% select(-index)\n\nevent_tbl <- ts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"both\"\n)\n\nglimpse(event_tbl)\n\nRows: 33\nColumns: 18\n$ rowid                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ date_col             <date> 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value                <dbl> 118, 132, 129, 121, 135, 148, 148, 199, 184, 162,…\n$ lag_val              <dbl> 112, 118, 132, 129, 121, 135, 148, 199, 199, 184,…\n$ adj_diff             <dbl> 6, 14, -3, -8, 14, 13, 0, 0, -15, -22, -16, 20, 5…\n$ relative_change_raw  <dbl> 0.05357143, 0.11864407, -0.02272727, -0.06201550,…\n$ relative_change      <dbl> 0.05, 0.12, -0.02, -0.06, 0.12, 0.10, 0.00, 0.00,…\n$ pct_chg_mark         <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ event_base_change    <dbl> 0.00000000, 0.11864407, -0.02272727, -0.06201550,…\n$ group_number         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ numeric_group_number <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ group_event_number   <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ x                    <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ mean_event_change    <dbl> 0.00000000, 0.03849647, -0.06815622, -0.04991040,…\n$ median_event_change  <dbl> 0.00000000, 0.07222222, -0.06217617, -0.06201550,…\n$ event_change_ci_low  <dbl> 0.00000000, -0.06799693, -0.11669576, -0.09692794…\n$ event_change_ci_high <dbl> 0.000000000, 0.116322976, -0.024699717, 0.0073964…\n$ event_type           <fct> Before, Before, Before, Before, Before, Before, A…\n\n\nLet’s visualize!\n\nts_event_analysis_plot(\n  .data = event_tbl\n)\n\n\n\n\nLet’s see the median now.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\"\n)\n\n\n\n\nNow let’s see it as an interactive plot.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\",\n  .interactive = TRUE\n)\n\n\n\n\n\nNow let’s see all the individual groups.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"individual\",\n  .interactive = TRUE\n)\n\n\n\n\n\nSingle direction plotting.\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"backward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nAnd…\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"forward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-lists-2023-01-20/index.html",
    "href": "posts/weekly-rtip-lists-2023-01-20/index.html",
    "title": "Another Post on Lists",
    "section": "",
    "text": "Introduction\nManipulating lists in R is a powerful tool for organizing and analyzing data. Here are a few common ways to manipulate lists:\n\nIndexing: Lists can be indexed using square brackets “[ ]” and numeric indices. For example, to access the first element of a list called “mylist”, you would use the expression “mylist[1]”.\nSubsetting: Lists can be subsetted using the same square bracket notation, but with a logical vector indicating which elements to keep. For example, to select all elements of “mylist” that are greater than 5, you would use the expression “mylist[mylist > 5]”.\nModifying elements: Elements of a list can be modified by assigning new values to them using the assignment operator “<-”. For example, to change the third element of “mylist” to 10, you would use the expression “mylist[3] <- 10”.\nAdding elements: New elements can be added to a list using the concatenation operator “c()” or the “append()” function. For example, to add the number 7 to the end of “mylist”, you would use the expression “mylist <- c(mylist, 7)”.\nRemoving elements: Elements can be removed from a list using the “-” operator. For example, to remove the second element of “mylist”, you would use the expression “mylist <- mylist[-2]”.\n\n\n\nExamples\nHere is an example of how these methods can be used to manipulate a list in R:\n\nmylist <- list(1,2,3,4,5)\n\n# Indexing\nmylist[[1]] # Returns 1\n\n[1] 1\n\n# Subsetting\nmylist[mylist > 3] # Returns 4 & 5\n\n[[1]]\n[1] 4\n\n[[2]]\n[1] 5\n\n# Modifying elements\nmylist[[3]] <- 10\nmylist # Returns 1 2 10 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n# Adding elements\nmylist <- c(mylist, 7)\nmylist # Returns 1 2 10 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n[[6]]\n[1] 7\n\n# Removing elements\nmylist[-3]\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 4\n\n[[4]]\n[1] 5\n\n[[5]]\n[1] 7\n\nmylist # Returns 1 2 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n[[6]]\n[1] 7\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidyaml-2023-01-13/index.html",
    "href": "posts/weekly-rtip-tidyaml-2023-01-13/index.html",
    "title": "The building of {tidyAML}",
    "section": "",
    "text": "Introduction\nYesterday I posted on An Update to {tidyAML} where I was discussing some of my thought process and how things could potentially work for the package.\nToday I want to showcase how the function fast_regression_parsnip_spec_tbl() and it’s complimentary function fast_classification_parsnip_spec_tbl() actually work or maybe don’t work for that matter.\nWe are going to pick on fast_regression_parsnip_spec_tbl() in today’s post. The point of it is that it creates a tibble of parsnip regression model specifications. This will create a tibble of 46 different regression model specifications which can be filtered. The model specs are created first and then filtered out. This will only create models for regression problems. To find all of the supported models in this package you can visit the parsnip search page\n\n\nFunction\nFirst let’s take a look at the function call itself.\n\nfast_regression_parsnip_spec_tbl(\n  .parsnip_fns = \"all\", \n  .parsnip_eng = \"all\"\n  )\n\nNow let’s take a look at the arguments:\n\n.parsnip_fns - The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(\"linear_reg\",\"cubist_rules\")\n.parsnip_eng - The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c('lm', 'glm')\n\nThe workhorse to this function is the internal_make_spec_tbl() function. This is the one that will be the subject of the post. Let’s take a look at it’s inner workings, afterall this is open source.\n\ninternal_make_spec_tbl <- function(.data){\n\n  # Checks ----\n  df <- dplyr::as_tibble(.data)\n\n  nms <- unique(names(df))\n\n  if (!\".parsnip_engine\" %in% nms | !\".parsnip_mode\" %in% nms | !\".parsnip_fns\" %in% nms){\n    rlang::abort(\n      message = \"The model tibble must come from the class/reg to parsnip function.\",\n      use_cli_format = TRUE\n    )\n  }\n\n  # Make tibble ----\n  mod_spec_tbl <- df %>%\n    dplyr::mutate(\n      model_spec = purrr::pmap(\n        dplyr::cur_data(),\n        ~ match.fun(..3)(mode = ..2, engine = ..1)\n      )\n    ) %>%\n    # add .model_id column\n    dplyr::mutate(.model_id = dplyr::row_number()) %>%\n    dplyr::select(.model_id, dplyr::everything())\n\n  # Return ----\n  return(mod_spec_tbl)\n\n}\n\nLet’s examine this (and it is currently changing form in a github issue). Firstly, we are taking in a data.frame/tibble that has to have certain names in it (this is going to change and look for a class instead). Once this determination is TRUE we then proceed to the meat and potatoes of it. The internal mod_spec_tbl is made using mutate, pmap, cur_data and match.fun. What this does essentially is the following:\n\nmutate a column called model_spec\nUse the {purrr} function pmap which maps over several columns in parallel to create the model spec.\nInside of the pmap we use cur_data() to get the current line where we match the function using match.fun (which takes a character string of the function, this means the library needs to be loaded) we supply the column it is in and then we supply the arguments we want.\nWe give it a numeric model id\nWe then ensure that the .model_id column is first.\n\n\n\nExample\nLet’s see it in action!\n\nlibrary(tidyAML) # Not yet available, you can install from GitHub though\n\nfast_regression_parsnip_spec_tbl()\n\n# A tibble: 46 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n# … with 36 more rows\n\n\nSo we see we get a nicely generated tibble of output that matchs a model spec to the .model_id and to the appropriate parsnip engine and mode\nWe can also choose the models we may want by giving either arguments to the .parsnip_engine parameter or .parsnip_fns or both.\n\nlibrary(dplyr)\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n11        11 stan_glmer      regression    linear_reg   <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 glm             regression    linear_reg   <spec[+]> \n3         3 glm             regression    poisson_reg  <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = \"glm\") %>%\n  pull(model_spec)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n[[2]]\nPoisson Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "",
    "text": "Many times someone may want to see a summary or cumulative statistic for a given set of data or even from several simulations of data. I went over bootstrap plotting earlier this month, and this is a form of what we will go over today although slightly more restrictive.\nI have decided to make today my weekly r-tip because tomorrow is Thanksgiving here in the US and I am taking an extended holiday so I won’t be back until Monday.\nToday’s function and weekly tip is on tidy_stat_tbl(). It is meant to be used with a tidy_ distribution function. Let’s take a look."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Single Simulation",
    "text": "Single Simulation\nLet’s go over some examples. Firstly, we will go over all the different .return_type’s of a single simulation of tidy_normal() using the quantile function.\nVector Output BE CAREFUL IT USES SAPPLY\n\nlibrary(TidyDensity)\n\nset.seed(123)\ntn <- tidy_normal()\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = quantile,\n  na.rm = TRUE,\n  probs = c(0.025, 0.5, 0.975)\n  )\n\n      sim_number_1\n2.5%   -1.59190149\n50%    -0.07264039\n97.5%   1.77074730\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n$sim_number_1\n       2.5%         50%       97.5% \n-1.59190149 -0.07264039  1.77074730 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nNow let’s take a look with multiple simulations."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Multiple Simulations",
    "text": "Multiple Simulations\nLet’s set our simulation count to 5. While this is not a large amount it will serve as a good illustration on the outputs.\n\nns <- 5\nf  <- quantile\nnr <- TRUE\np  <- c(0.025, 0.975)\n\nOk let’s run the same simulations but with the updated params.\nVector Output BE CAREFUL IT USES SAPPLY\n\nset.seed(123)\ntn <- tidy_normal(.num_sims = ns)\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = f,\n  na.rm = nr,\n  probs = p\n  )\n\n      sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n2.5%     -1.591901    -1.474945    -1.656679    -1.258156    -1.309749\n97.5%     1.770747     1.933653     1.894424     2.098923     1.943384\n\ntidy_stat_tbl(\n  tn, y, .return_type = \"vector\",\n  .fns = f, na.rm = nr\n)\n\n     sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n0%    -1.96661716   -2.3091689   -2.0532472  -1.31080153   -1.3598407\n25%   -0.55931702   -0.3612969   -0.9505826  -0.49541417   -0.7140627\n50%   -0.07264039    0.1525789   -0.3048700  -0.07675993   -0.2240352\n75%    0.69817699    0.6294358    0.2900859   0.55145766    0.5287605\n100%   2.16895597    2.1873330    2.1001089   3.24103993    2.1988103\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\n$sim_number_2\n        0%        25%        50%        75%       100% \n-2.3091689 -0.3612969  0.1525789  0.6294358  2.1873330 \n\n$sim_number_3\n        0%        25%        50%        75%       100% \n-2.0532472 -0.9505826 -0.3048700  0.2900859  2.1001089 \n\n$sim_number_4\n         0%         25%         50%         75%        100% \n-1.31080153 -0.49541417 -0.07675993  0.55145766  3.24103993 \n\n$sim_number_5\n        0%        25%        50%        75%       100% \n-1.3598407 -0.7140627 -0.2240352  0.5287605  2.1988103 \n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr, \n  probs = p\n)\n\n$sim_number_1\n     2.5%     97.5% \n-1.591901  1.770747 \n\n$sim_number_2\n     2.5%     97.5% \n-1.474945  1.933653 \n\n$sim_number_3\n     2.5%     97.5% \n-1.656679  1.894424 \n\n$sim_number_4\n     2.5%     97.5% \n-1.258156  2.098923 \n\n$sim_number_5\n     2.5%     97.5% \n-1.309749  1.943384 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <chr>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <chr> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <fct>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <fct> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nOk, now that we have shown that, let’s ratchet up the simulations so we can see the true difference in using the .use_data_tbl parameter when simulations are large. We are going to use {rbenchmark} for"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Benchmarking",
    "text": "Benchmarking\nHere we go. We are going to make a tidy_bootstrap() of the mtcars$mpg data which will produce 2000 simulations, we will replicate this 25 times.\n\nlibrary(rbenchmark)\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# Get the interesting vector, well for this anyways\nx <- mtcars$mpg\n\n# Bootstrap the vector (2k simulations is default)\ntb <- tidy_bootstrap(x) %>%\n  bootstrap_unnest_tbl()\n\nbenchmark(\n  \"tibble\" = {\n    tidy_stat_tbl(tb, y, IQR, \"tibble\")\n  },\n  \"data.table\" = {\n    tidy_stat_tbl(tb, y, IQR, .use_data_table = TRUE, type = 7)\n  },\n  \"sapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"vector\")\n  },\n  \"lapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"list\")\n  },\n  replications = 25,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) %>%\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1 data.table           25    4.11    1.000      3.33     0.11\n2     lapply           25   24.14    5.873     20.02     0.38\n3     sapply           25   25.11    6.109     21.01     0.28\n4     tibble           25   33.18    8.073     27.45     0.51\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html",
    "title": "Updates to {healthyverse} packages",
    "section": "",
    "text": "I have made several updates to {healthyverse}, this has resulted in new releases to CRAN for {healthyR.ai}, {healthyR.ts}, and {TidyDesnsity}."
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "title": "Updates to {healthyverse} packages",
    "section": "TidyDensity",
    "text": "TidyDensity\nFor TidyDensity a new distribution was added, welcome tidy_bernoulli(). This distribution also comes with the standard util_distname_param_estimate() and the util_distname_stats_tbl() functions. Let’s take a look at the function calls.\n\ntidy_bernoulli(.n = 50, .prob = 0.1, .num_sims = 1)\n\nutil_bernoulli_param_estimate(.x, .auto_gen_empirical = TRUE)\n\nutil_bernoulli_stats_tbl(.data)\n\nLet’s see them in use.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntb <- tidy_bernoulli()\n\ntb\n\n# A tibble: 50 × 7\n   sim_number     x     y      dx     dy     p     q\n   <fct>      <int> <int>   <dbl>  <dbl> <dbl> <dbl>\n 1 1              1     0 -0.338  0.0366   0.9     0\n 2 1              2     0 -0.304  0.0866   0.9     0\n 3 1              3     0 -0.270  0.187    0.9     0\n 4 1              4     0 -0.236  0.369    0.9     0\n 5 1              5     0 -0.201  0.663    0.9     0\n 6 1              6     0 -0.167  1.09     0.9     0\n 7 1              7     0 -0.133  1.63     0.9     0\n 8 1              8     1 -0.0988 2.22     1       1\n 9 1              9     0 -0.0646 2.76     0.9     0\n10 1             10     0 -0.0304 3.14     0.9     0\n# … with 40 more rows\n\nutil_bernoulli_param_estimate(tb$y)\n\n$combined_data_tbl\n# A tibble: 100 × 8\n   sim_number     x     y      dx     dy     p     q dist_type\n   <fct>      <int> <dbl>   <dbl>  <dbl> <dbl> <dbl> <fct>    \n 1 1              1     0 -0.338  0.0366  0.92     0 Empirical\n 2 1              2     0 -0.304  0.0866  0.92     0 Empirical\n 3 1              3     0 -0.270  0.187   0.92     0 Empirical\n 4 1              4     0 -0.236  0.369   0.92     0 Empirical\n 5 1              5     0 -0.201  0.663   0.92     0 Empirical\n 6 1              6     0 -0.167  1.09    0.92     0 Empirical\n 7 1              7     0 -0.133  1.63    0.92     0 Empirical\n 8 1              8     1 -0.0988 2.22    1        0 Empirical\n 9 1              9     0 -0.0646 2.76    0.92     0 Empirical\n10 1             10     0 -0.0304 3.14    0.92     0 Empirical\n# … with 90 more rows\n\n$parameter_tbl\n# A tibble: 1 × 8\n  dist_type samp_size   min   max  mean variance sum_x  prob\n  <chr>         <int> <dbl> <dbl> <dbl>    <dbl> <dbl> <dbl>\n1 Bernoulli        50     0     1  0.08   0.0736     4  0.08\n\nutil_bernoulli_stats_tbl(tb) %>%\n  glimpse()\n\nRows: 1\nColumns: 18\n$ tidy_function      <chr> \"tidy_bernoulli\"\n$ function_call      <chr> \"Bernoulli c(0.1)\"\n$ distribution       <chr> \"Bernoulli\"\n$ distribution_type  <chr> \"discrete\"\n$ points             <dbl> 50\n$ simulations        <dbl> 1\n$ mean               <dbl> 0.1\n$ mode               <chr> \"0\"\n$ coeff_var          <dbl> 0.09\n$ skewness           <dbl> 2.666667\n$ kurtosis           <dbl> 5.111111\n$ mad                <dbl> 0.5\n$ entropy            <dbl> 0.325083\n$ fisher_information <dbl> 11.11111\n$ computed_std_skew  <dbl> 3.096281\n$ computed_std_kurt  <dbl> 10.58696\n$ ci_lo              <dbl> 0\n$ ci_hi              <dbl> 1"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ai",
    "text": "healthyR.ai\nThis was a minor patch release that exported some previously internal only functions and fixed an error with the custom recipe steps. One of the functions that has been exported is hai_data_impute()\nLet’s take a look.\n\nhai_data_impute(\n  .recipe_object = NULL,\n  ...,\n  .seed_value = 123,\n  .type_of_imputation = \"mean\",\n  .number_of_trees = 25,\n  .neighbors = 5,\n  .mean_trim = 0,\n  .roll_statistic,\n  .roll_window = 5\n)\n\nLet’s take a look at an example of it’s use.\n\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(healthyR.ai)\n\ndate_seq <- seq.Date(from = as.Date(\"2013-01-01\"), length.out = 100, by = \"month\")\nval_seq <- rep(c(rnorm(9), NA), times = 10)\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 NA     \n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nhai_data_impute(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_imputation = \"roll\",\n  .roll_statistic = median\n)$impute_rec_obj %>%\n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 -0.322 \n# … with 90 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ts",
    "text": "healthyR.ts\nThis was a minor patch release fixing the function ts_lag_correlation() when the column that was the value was not explicitly called…value.\nThank you!"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "",
    "text": "Many times in the real world we have a data set which is actually a sample as we typically do not know what the actual population is. This is where bootstrapping tends to come into play. It allows us to get a hold on what the possible parameter values are by taking repeated samples of the data that is available to us.\nAt it’s core it is a resampling method with replacement where it assigns measures of accuracy to the sample estimates. Here is the Wikipedia Article for bootstrapping.\nIn this post I am going to go over how to use the bootstrap function set with {TidyDensity}. You can find the pkgdown site with all function references here: TidyDensity"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Mean",
    "text": "Cumulative Mean\n\ntb %>%\n  bootstrap_stat_plot(.value = y)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE)\n\n\n\n\n\nYou can see from this output that the statistic you choose is printed in the chart title and on the y axis, the caption will also tell you how many simulations are present. Lets look at skewness as another example."
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Skewness",
    "text": "Cumulative Skewness\n\nsc <- \"cskewness\"\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE,\n                      .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .stat = sc,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE,\n                      .show_groups = TRUE,\n                      .stat = sc)\n\n\n\n\n\nVolia!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Steve On Data",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nI am using this as a site to host all of the tips and tricks for R/SQL and data that I will post to my LinkedIn, Twitter and Telegram channels."
  },
  {
    "objectID": "posts/rtip-2023-01-26/index.html",
    "href": "posts/rtip-2023-01-26/index.html",
    "title": "Transforming Your Data: A Guide to Popular Methods and How to Implement Them with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nTransforming data refers to the process of changing the scale or distribution of a variable in order to make it more suitable for analysis. There are many different methods for transforming data, and each has its own specific use case.\n\nBox-Cox: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses a power transformation to adjust the scale of the data.\nBasis Spline: This is a type of non-parametric regression that uses splines (piecewise polynomials) to model the relationship between a dependent variable and one or more independent variables.\nLog: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses the logarithm function to adjust the scale of the data.\nLogit: This is a method for transforming binary data (i.e., data with only two possible values) into a continuous scale. It uses the logistic function to adjust the scale of the data.\nNatural Spline: This is a type of non-parametric regression that uses splines (piecewise polynomials) to model the relationship between a dependent variable and one or more independent variables, where the splines are chosen to be as smooth as possible.\nRectified Linear Unit (ReLU): This is a type of activation function used in artificial neural networks. It is used to introduce non-linearity in the output of a neuron.\nSquare Root: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses the square root function to adjust the scale of the data.\nYeo-Johnson: This is a power transformation that works well for data that is positively or negatively skewed. It is a generalization of the Box-Cox transformation and handles zero and negative data.\n\nThe R library {healthyR.ai} provides a function called hai_data_transform() that allows users to easily apply any of these transforms to their data. The function takes in the data and the type of transformation as arguments, and returns the transformed data. This makes it easy for users to experiment with different transformations and see which one works best for their data.\n\n\nFunction\nLet’s take a look at the full function call.\n\nhai_data_transform(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"log\",\n  .bc_limits = c(-5, 5),\n  .bc_num_unique = 5,\n  .bs_deg_free = NULL,\n  .bs_degree = 3,\n  .log_base = exp(1),\n  .log_offset = 0,\n  .logit_offset = 0,\n  .ns_deg_free = 2,\n  .rel_shift = 0,\n  .rel_reverse = FALSE,\n  .rel_smooth = FALSE,\n  .yj_limits = c(-5, 5),\n  .yj_num_unique = 5\n)\n\nNow let’s go over the arguments to the parameters.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“boxcox”\n“bs”\n“log”\n“logit”\n“ns”\n“relu”\n“sqrt”\n“yeojohnson\n\n.bc_limits - A length 2 numeric vector defining the range to compute the transformation parameter lambda.\n.bc_num_unique - An integer to specify minimum required unique values to evaluate for a transformation\n.bs_deg_free - The degrees of freedom for the spline. As the degrees of freedom for a spline increase, more flexible and complex curves can be generated. When a single degree of freedom is used, the result is a rescaled version of the original data.\n.bs_degree - Degree of polynomial spline (integer).\n.log_base - A numeric value for the base.\n.log_offset - An optional value to add to the data prior to logging (to avoid log(0))\n.logit_offset - A numeric value to modify values of the columns that are either one or zero. They are modifed to be x - offset or offset respectively.\n.ns_deg_free - The degrees of freedom for the natural spline. As the degrees of freedom for a natural spline increase, more flexible and complex curves can be generated. When a single degree of freedom is used, the result is a rescaled version of the original data.\n.rel_shift - A numeric value dictating a translation to apply to the data.\n.rel_reverse - A logical to indicate if the left hinge should be used as opposed to the right hinge.\n.rel_smooth - A logical indicating if hte softplus function, a smooth approximation to the rectified linear transformation, should be used.\n.yj_limits - A length 2 numeric vector defining the range to compute the transformation parameter lambda.\n.yj_num_unique - An integer where data that have less possible values will not be evaluated for a transformation.\n\n\n\nExamples\nLet’s look over some examples. For an example data set we are going to pick on the mtcars data set as the histogram will prove to be skewed which makes it a good candidate to test these transformations on.\n\ninstall.packages(\"healthyR.ai\")\n\nNow that we have {healthyR.ai} installed we can get to work. It does use the {recipes} package underneath so you will need to have that installed as well. Let’s look at the histogram of mtcars now.\n\nmpg_vec <- mtcars$mpg\n\nhist(mpg_vec)\n\n\n\nplot(density(mpg_vec))\n\n\n\n\nFirst up, Box-Cox\n\nlibrary(healthyR.ai)\nlibrary(recipes)\n\nro <- recipe(mpg ~ wt, data = mtcars)\n\nboxcox_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"boxcox\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(boxcox_vec))\n\n\n\n\nBasis Spline\n\nbs_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"bs\"\n)$scale_rec_obj %>%\n  get_juiced_data()\n\nplot(density(bs_vec$mpg_bs_1))\n\n\n\nplot(density(bs_vec$mpg_bs_2))\n\n\n\nplot(density(bs_vec$mpg_bs_3))\n\n\n\n\nLog\n\nlog_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"log\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(log_vec))\n\n\n\n\nYeo-Johnson\n\nyj_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"yeojohnson\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(yj_vec))\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyr-2023-01-27/index.html",
    "href": "posts/weekly-rtip-healthyr-2023-01-27/index.html",
    "title": "Service Line Grouping with {healthyR}",
    "section": "",
    "text": "Introduction\nHealthcare data analysis can be a complex and time-consuming task, but it doesn’t have to be. Meet {healthyR}, your new go-to R package for all things healthcare data analysis. With {healthyR}, you can easily and efficiently analyze your healthcare data, and make sense of the information it contains.\nOne of the key features of {healthyR} is the service_line_augment() function. This function is designed to help you quickly and easily append a vector to a data.frame or tibble that is passed to the .data parameter. In order to use this function, you will need a data.frame or tibble with a principal diagnosis column, a principal procedure column, and a column for the DRG number. These are needed so that the function can join the dx_cc_mapping and px_cc_mapping columns to provide the service line.\nThe service_line_augment() function is especially useful for analyzing healthcare data that is coded using ICD Version 10. This version of the ICD coding system is widely used in the healthcare industry, and the service_line_augment() function is specifically designed to work with it. With this function, you can quickly and easily append a vector to your data.frame or tibble that provides the service line for each visit.\nIn addition to the service_line_augment() function, {healthyR} also includes a wide range of other useful tools and functions for healthcare data analysis. Whether you’re looking to analyze claims data, clinical data, or any other type of healthcare data, {healthyR} has you covered.\nSo why wait? Download {healthyR} today and start making sense of your healthcare data! With {healthyR}, you can easily and efficiently analyze your healthcare data, and make sense of the information it contains.\n\n\nFunction\nLet’s take a look at the full function call.\n\nservice_line_augment(.data, .dx_col, .px_col, .drg_col)\n\nNow let’s look at the arguments to the parameters.\n\n.data - The data being passed that will be augmented by the function.\n.dx_col - The column containing the Principal Diagnosis for the discharge.\n.px_col - The column containing the Principal Coded Procedure for the discharge. It is possible that this could be blank.\n.drg_col - The DRG Number coded to the inpatient discharge.\n\nNow for some examples.\n\n\nExample\nFirst if you have not already, install {healthyR}\n\ninstall.packages(\"healthyR\")\n\nHere we go.\n\nlibrary(healthyR)\n\ndf <- data.frame(\n  dx_col = \"F10.10\",\n  px_col = NA,\n  drg_col = \"896\"\n)\n\nservice_line_augment(\n  .data = df,\n  .dx_col = dx_col,\n  .px_col = px_col,\n  .drg_col = drg_col\n)\n\n# A tibble: 1 × 4\n  dx_col px_col drg_col service_line \n  <chr>  <lgl>  <chr>   <chr>        \n1 F10.10 NA     896     alcohol_abuse\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-30/index.html",
    "href": "posts/rtip-2023-01-30/index.html",
    "title": "{healthyR.ts}: The New and Improved Library for Time Series Analysis",
    "section": "",
    "text": "Introduction\nAre you looking for a powerful and efficient library for time series analysis? Look no further than {healthyR.ts}! This library has recently been updated with new functions and improvements, making it easier for you to analyze and visualize your time series data.\nOne of the new functions in {healthyR.ts} is ts_geometric_brownian_motion(). This function allows you to generate multiple Brownian motion simulations at once, saving you time and effort. With this feature, you can easily generate multiple simulations to compare and analyze different scenarios.\nAnother new function, ts_brownian_motion_augment(), enables you to add a Brownian motion to a time series that you provide. This is a great tool for analyzing the impact of random variations on your data.\nThe ts_geometric_brownian_motion_augment() function generates a geometric Brownian motion, allowing you to study the effects of compounding growth or decay in your time series data. And, with the ts_brownian_motion_plot() function, you can easily plot both augmented and non-augmented Brownian motion plots, giving you a visual representation of your data.\nIn addition to the new functions, {healthyR.ts} has also made several minor fixes and improvements. For example, the ts_brownian_motion() function has been updated and optimized, resulting in a 49x speedup due to vectorization. Additionally, all Brownian motion functions now have an attribute of .motion_type, making it easier to track and identify your data.\nWith all of these new features and improvements, {healthyR.ts} is the ideal library for anyone looking to analyze and visualize time series data. So, if you want to take your time series analysis to the next level, install {healthyR.ts} today!\n\n\nFunction\nLet’s take a look at the new functions.\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\nIts arguments.\n\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.initial_value - Integer representing the initial value.\n.delta_time - Time step size.\n.return_tibble - The default is TRUE. If set to FALSE then an object of class matrix will be returned.\n\n\nts_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = NULL\n)\n\nIts arguments.\n\n.data - The data.frame/tibble being augmented.\n.date_col - The column that holds the date.\n.value_col - The value that is going to get augmented. The last value of this column becomes the initial value internally.\n.time - How many time steps ahead.\n.num_sims - How many simulations should be run.\n.delta_time - Time step size.\n\n\nts_geometric_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .num_sims = 10,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .delta_time = 1/365\n)\n\nIts arguments.\n\n.data - The data you are going to pass to the function to augment.\n.date_col - The column that holds the date\n.value_col - The column that holds the value\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.delta_time - Time step size.\n\n\nts_brownian_motion_plot(\n  .data, \n  .date_col, \n  .value_col, \n  .interactive = FALSE\n)\n\nIts arguments.\n\n.data - The data you are going to pass to the function to augment.\n.date_col - The column that holds the date\n.value_col - The column that holds the value\n.interactive - The default is FALSE, TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nFirst make sure you install {healthyR.ts} if you do not yet already have it, otherwise update it to gain th enew functionality.\n\ninstall.packages(\"healthyR.ts\")\n\nNow let’s take a look at ts_geometric_brownian_motion().\n\nlibrary(healthyR.ts)\n\nts_geometric_brownian_motion()\n\n# A tibble: 2,600 × 3\n   sim_number        t     y\n   <fct>         <int> <dbl>\n 1 sim_number 1      1   100\n 2 sim_number 2      1   100\n 3 sim_number 3      1   100\n 4 sim_number 4      1   100\n 5 sim_number 5      1   100\n 6 sim_number 6      1   100\n 7 sim_number 7      1   100\n 8 sim_number 8      1   100\n 9 sim_number 9      1   100\n10 sim_number 10     1   100\n# … with 2,590 more rows\n\n\nNow let’s take a look at ts_brownian_motion_augment().\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value\n)\n\n# A tibble: 1,041 × 3\n   sim_number  date_col    value\n   <fct>       <date>      <dbl>\n 1 actual_data 2022-01-01 -0.303\n 2 actual_data 2022-01-02 -1.17 \n 3 actual_data 2022-01-03 -1.44 \n 4 actual_data 2022-01-04 -0.682\n 5 actual_data 2022-01-05 -2.31 \n 6 actual_data 2022-01-06 -1.19 \n 7 actual_data 2022-01-07 -0.454\n 8 actual_data 2022-01-08 -1.83 \n 9 actual_data 2022-01-09  0.659\n10 actual_data 2022-01-10 -0.150\n# … with 1,031 more rows\n\n\nNow ts_geometric_brownian_motion_augment().\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_geometric_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value\n)\n\n# A tibble: 291 × 3\n   sim_number  date_col    value\n   <fct>       <date>      <dbl>\n 1 actual_data 2022-01-01 -1.47 \n 2 actual_data 2022-01-02 -1.63 \n 3 actual_data 2022-01-03  1.01 \n 4 actual_data 2022-01-04  1.44 \n 5 actual_data 2022-01-05 -1.05 \n 6 actual_data 2022-01-06 -0.599\n 7 actual_data 2022-01-07 -0.393\n 8 actual_data 2022-01-08  1.06 \n 9 actual_data 2022-01-09 -0.121\n10 actual_data 2022-01-10 -0.349\n# … with 281 more rows\n\n\nNow for ts_brownian_motion_plot().\n\nts_geometric_brownian_motion() %>%\n  ts_brownian_motion_plot(.date_col = t, .value_col = y)\n\n\n\n\n\nts_brownian_motion() %>%\n  ts_brownian_motion_plot(t, y, .interactive = TRUE)\n\n\n\n\n\nAnd with the augmenting functions\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value,\n  .time = 30,\n  .num_sims = 30\n) %>%\n  ts_brownian_motion_plot(date_col, value, TRUE)\n\n\n\n\n\nAnd with a static ggplot2 plot.\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value,\n  .time = 30,\n  .num_sims = 30\n) %>%\n  ts_brownian_motion_plot(date_col, value)\n\n\n\n\nThank you for reading, and Voila!"
  },
  {
    "objectID": "posts/rtip-2023-01-31/index.html",
    "href": "posts/rtip-2023-01-31/index.html",
    "title": "Median: A Simple Way to Detect Excess Events Over Time with {healthyR}",
    "section": "",
    "text": "Introduction\nAs we collect data over time, it’s important to look for patterns and trends that can help us understand what’s happening. One common way to do this is to look at the median of the data. The median is the middle value of a set of numbers, and it can be a useful tool for detecting whether there is an excess of events, either positive or negative, occurring over time.\nBenefits of Looking at Median:\n\nShows the central tendency: The median gives us a good idea of the central tendency of the data. This can help us understand what’s typical and what’s not.\nResistant to outliers: Unlike the mean, the median is not affected by outliers. This means that if there are a few extreme values in the data, the median will not be skewed by them.\nEasy to understand: The median is easy to understand, even for people who are not familiar with statistics.\n\nUsing the R Library {healthyR} provides a convenient way to perform median analysis. The function ts_median_excess_plt() can be used to plot the median of an event over time and detect any excess events that may be occurring. This function is designed to be user-friendly, so even if you’re not an expert in statistics, you can still use it to gain valuable insights into your data.\nIn conclusion, looking at the median of an event over time can be a useful tool for detecting excess events, either positive or negative. The R library {healthyR} provides a convenient way to perform this analysis with the function ts_median_excess_plt(). Give it a try and see what insights you can uncover in your own data!\n\n\nFunction\nHere is the full function call.\n\nts_median_excess_plt(\n  .data,\n  .date_col,\n  .value_col,\n  .x_axis,\n  .ggplot_group_var,\n  .years_back\n)\n\nHere are its arguments.\n\n.data - The data that is being analyzed, data must be a tibble/data.frame.\n.date_col - The column of the tibble that holds the date.\n.value_col - The column that holds the value of interest.\n.x_axis - What is the be the x-axis, day, week, etc.\n.ggplot_group_var - The variable to group the ggplot on.\n.years_back - How many yeas back do you want to go in order to compute the median value.\n\n\n\nExample\nFirst make sure you have the package installed.\n\ninstall.packages(\"healthyR\")\n\nNow for an example. The data is required to be in a certain format, this function is dated, meaning it was one of the first ones I wrote so I will be taking time to improve it in the future. We are using data from my {healthyR.data]} package.\n\nlibrary(healthyR.data)\nlibrary(lubridate)\nlibrary(healthyR)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(ggplot2)\n\ndf <- healthyR_data %>%\n  filter_by_time(\n    .date_var = visit_start_date_time,\n    .start_date = \"2012\",\n    .end_date = \"2019\"\n  ) %>%\n  filter(ip_op_flag == \"I\") %>%\n  select(visit_id, visit_start_date_time) %>%\n  mutate(\n    visit_start_date_time = as.Date(visit_start_date_time, \"%Y%M%D\"),\n    record = 1\n    ) %>%\n  summarise_by_time(\n    .date_var = visit_start_date_time,\n    visits = sum(record)\n  ) %>%\n  ts_signature_tbl(\n    .date_col = visit_start_date_time\n  )\n\nOk now that we have our data, let’s take a look at it using glimpse()\n\nglimpse(df)\n\nRows: 2,922\nColumns: 30\n$ visit_start_date_time <date> 2012-01-01, 2012-01-02, 2012-01-03, 2012-01-04,…\n$ visits                <dbl> 34, 52, 53, 44, 46, 55, 42, 29, 50, 55, 50, 43, …\n$ index.num             <dbl> 1325376000, 1325462400, 1325548800, 1325635200, …\n$ diff                  <dbl> NA, 86400, 86400, 86400, 86400, 86400, 86400, 86…\n$ year                  <int> 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ year.iso              <int> 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ half                  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ quarter               <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ month                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ month.xts             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ month.lbl             <ord> January, January, January, January, January, Jan…\n$ day                   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ hour                  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ minute                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ second                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hour12                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ am.pm                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ wday                  <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, …\n$ wday.xts              <int> 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, …\n$ wday.lbl              <ord> Sunday, Monday, Tuesday, Wednesday, Thursday, Fr…\n$ mday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ qday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ yday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ mweek                 <int> 5, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, …\n$ week                  <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, …\n$ week.iso              <int> 52, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3,…\n$ week2                 <int> 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ week3                 <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, …\n$ week4                 <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, …\n$ mday7                 <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, …\n\n\nNow to visualize it.\n\ndf %>%\n  ts_median_excess_plt(\n    .date_col = visit_start_date_time,\n    .value_col = visits,\n    .x_axis = month.lbl,\n    .ggplot_group_var = year,\n    .years_back = 3\n  ) +\n  labs(\n    y = \"Excess Visits\",\n    title = \"Excess Visits by Month YoY\"\n  ) + \n  theme(axis.text.x=element_text(angle = -90, hjust = 0))\n\n\n\n\nSo from here what we can see is that looking back in time over the visits data that the current year (the max year in the data) shows that it is significantly under previous years median values by month.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-01/index.html",
    "href": "posts/rtip-2023-02-01/index.html",
    "title": "Attributes in R Functions: An Overview",
    "section": "",
    "text": "R is a powerful programming language that is widely used for data analysis, visualization, and machine learning. One of the features of R that makes it versatile and flexible is the ability to assign attributes to functions. Attributes are metadata associated with an object in R, and they can be used to store additional information about the function or to modify the behavior of the function.\nIn this blog post, we will discuss what attributes are, how they can be useful, and how they can be used inside R functions.\n\n\nAttributes are pieces of information that are stored alongside an object in R. Functions are objects in R, and they can have attributes associated with them. Some of the common attributes associated with functions in R include:\n\nformals: This attribute stores the arguments of the function and their default values.\nsrcref: This attribute stores the source code of the function, including the line numbers of the code.\nenvironment: This attribute stores the environment in which the function was defined.\n\n\n\n\nAttributes can be useful in R functions in several ways, including:\n\nDebugging: Attributes can be used to store information that can be used to debug functions. For example, the srcref attribute can be used to retrieve the source code of the function and the line numbers of the code, which can be useful when trying to identify the source of an error.\nMetadata: Attributes can be used to store metadata about the function, such as the author, version, and date of creation. This information can be used to keep track of the function and to provide information about its purpose and usage.\nModifying Function Behavior: Attributes can be used to modify the behavior of the function. For example, the environment attribute can be used to set the environment in which the function is executed. This can be useful when creating closures or when using functions in a specific context.\n\n\n\n\nTo access or modify the attributes of a function in R, you can use the attributes() function. For example, to retrieve the formals attribute of a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\nformals(f)\n\n$x\n\n\n$y\n\n\nTo add an attribute to a function, you can use the attr() function. For example, to add a version attribute to a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattr(f, \"version\") <- \"1.0\"\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\n$version\n[1] \"1.0\"\n\n\nTo remove an attribute from a function, you can use the attributes() function with the NULL value. For example, to remove the version attribute from a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattr(f, \"version\") <- \"1.0\"\nattributes(f)$version <- NULL\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\n\nConclusion\nAttributes are a useful feature in R functions that can be used to store additional information about the function, to debug the function, and to modify its behavior. By using attributes, you can make your functions more versatile, flexible, and easier to work with."
  },
  {
    "objectID": "posts/rtip-2023-02-02/index.html",
    "href": "posts/rtip-2023-02-02/index.html",
    "title": "Diverging Lollipop Chart: A Visual Tool for Comparing Data with {healthyR}",
    "section": "",
    "text": "Introduction\nA diverging lollipop chart is a useful tool for comparing data that falls into two categories, usually indicated by different colors. This type of chart is particularly well-suited for comparing the differences between two data sets and for identifying which data points are contributing most to the differences.\nThe R package {healthyR} offers a function called diverging_lollipop_plt() that can be used to create a diverging lollipop chart. This function has several parameters that can be used to customize the chart to meet your specific needs.\nIn conclusion, the diverging lollipop chart is a useful tool for comparing data sets and can provide insights into the differences between two sets of data. The diverging_lollipop_plt() function from the {healthyR} package is a great option for creating this type of chart, as it offers a range of customization options to meet your specific needs. Whether you’re working with data related to business, finance, or any other field, a diverging lollipop chart can be a valuable tool in your visual analysis toolkit.\n\n\nFunction\nLet’s take a look at the full function call.\n\ndiverging_lollipop_plt(\n  .data,\n  .x_axis,\n  .y_axis,\n  .plot_title = NULL,\n  .plot_subtitle = NULL,\n  .plot_caption = NULL,\n  .interactive = FALSE\n)\n\nNow lets see the arguments that get provided to the parameters.\n\n.data - The data to pass to the function, must be a tibble/data.frame.\n.x_axis - The data that is passed to the x-axis. This will also be the x and xend parameters of the geom_segment\n.y_axis - The data that is passed to the y-axis. This will also equal the parameters of yend and label\n.plot_title - Default is NULL\n.plot_subtitle - Default is NULL\n.plot_caption - Default is NULL\n.interactive - Default is FALSE. TRUE returns a plotly plot\n\n\n\nExample\nLet’s see an example.\n\nlibrary(healthyR)\n\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata(\"mtcars\")\nmtcars$car_name <- rownames(mtcars)\nmtcars$mpg_z <- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), 2)\nmtcars$mpg_type <- ifelse(mtcars$mpg_z < 0, \"below\", \"above\")\nmtcars <- mtcars[order(mtcars$mpg_z), ]  # sort\nmtcars$car_name <- factor(mtcars$car_name, levels = mtcars$car_name)\n\ndiverging_lollipop_plt(\n  .data = mtcars, \n  .x_axis = car_name,\n  .y_axis = mpg_z\n)\n\n\n\n\nNow let’s also see the interactive chart.\n\ndiverging_lollipop_plt(\n  .data = mtcars, \n  .x_axis = car_name,\n  .y_axis = mpg_z,\n  .interactive = TRUE\n)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-03/index.html",
    "href": "posts/rtip-2023-02-03/index.html",
    "title": "The Argument Matcher: A Function for Selecting the Right Arguments {tidyAML}",
    "section": "",
    "text": "Introduction\nI am working on finishing up a few things with my new R package {tidyAML} before I release it to CRAN. One of those things is the ability of a user to build a model using a command that might be something like generate_model(). One of the things that is necessary to do is to match the function arguments from the generate_model() to the actual parsnip call.\nThis is where and argument matcher of sorts may come in handy. I am doing this because it will take one most step of abstraction away, and instead of say calling linear_reg() or mars() or something like that, you can just instead use generate_model() and type in your engine or the parsnip function call there.\nNow I am not one hundred percent certain that I’ll actually implement this or not, but the exercise was fun enough that I decided to share it. So let’s get into it.\n\n\nFunction\nHere is the current state of the function.\n\nargument_matcher <- function(.f = \"linear_reg\", .args = list()){\n  \n  # TidyEval ----\n  fns <- as.character(.f)\n  \n  fns_args <- formalArgs(fns)\n  fns_args_list <- as.list(fns_args)\n  names(fns_args_list) <- fns_args\n  \n  arg_list <- .args\n  arg_list_names <- unique(names(arg_list))\n  \n  l <- list(arg_list, fns_args_list)\n  \n  arg_idx <- which(arg_list_names %in% fns_args_list)\n  bad_arg_idx <- which(!arg_list_names %in% fns_args_list)\n  \n  bad_args <- arg_list[bad_arg_idx]\n  bad_arg_names <- unique(names(bad_args))\n  \n  final_args <- arg_list[arg_idx]\n  \n  # Return ----\n  if (length(bad_arg_names > 0)){\n    rlang::inform(\n      message = paste0(\"bad arguments passed: \", bad_arg_names),\n      use_cli_format = TRUE\n    )\n  }\n\n  return(final_args)\n}\n\nWhen working with R functions, it’s not uncommon to encounter a situation where you need to pass arguments to another function. This can be especially challenging when the arguments are not properly matched. Fortunately, the argument_matcher function provides an elegant solution to this problem.\nThe argument_matcher function takes two arguments: .f and .args. The .f argument is a string that specifies the name of the function you want to pass arguments to, while the .args argument is a list that contains the arguments you want to pass to the specified function.\nThe argument_matcher function first uses the formalArgs function to extract the formal arguments of the specified function and store them in fns_args. The names of the formal arguments are then used to create a list, fns_args_list.\nNext, the function extracts the names of the arguments in .args and stores them in arg_list_names. It then checks if the names of the arguments in .args match the names of the formal arguments of the specified function, and stores the matching arguments in final_args. Any arguments that don’t match the formal arguments are stored in bad_args, and a warning message is printed indicating that bad arguments were passed.\nThe final step is to return the final_args list, which contains only the arguments that match the formal arguments of the specified function.\nIn conclusion, the argument_matcher function is a useful tool for ensuring that arguments are properly matched when passed to another function. Whether you’re working with linear regression models or any other type of function, the argument_matcher function will help you select the right arguments and avoid common errors.\n\n\nExample\nLet’s see a simple example.\n\nsuppressPackageStartupMessages(library(tidymodels))\n\nargument_matcher(\n  .args = list(\n    mode = \"regression\", \n    engine = \"lm\",\n    cost = 0.5,\n    trees = 1, \n    mtry = 1\n    )\n  )\n\nbad arguments passed: cost\nbad arguments passed: trees\nbad arguments passed: mtry\n\n\n$mode\n[1] \"regression\"\n\n$engine\n[1] \"lm\"\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-06/inex.html",
    "href": "posts/rtip-2023-02-06/inex.html",
    "title": "Cumulative Measurement Functions with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re looking for an easy-to-use package to calculate cumulative statistics in R, you may want to check out the TidyDensity package. This package offers several functions to calculate cumulative measurements, including mean, median, standard deviation, variance, skewness, kurtosis, harmonic mean, and geometric mean.\n\ncgmean(): Cumulative Geometric Mean\n\nThe cgmean() function calculates the cumulative geometric mean of a set of values. This is the nth root of the product of the first n elements of the set. It’s a useful measurement for sets of values that are multiplied together, such as growth rates.\n\nchmean(): Cumulative Harmonic Mean\n\nThe chmean() function calculates the cumulative harmonic mean of a set of values. This is the inverse of the arithmetic mean of the reciprocals of the values. It’s commonly used for sets of values that represent rates, such as speeds.\n\nckurtosis(): Cumulative Kurtosis\n\nThe ckurtosis() function calculates the cumulative kurtosis of a set of values. Kurtosis is a measure of the peakedness of a distribution, relative to a normal distribution. The cumulative kurtosis calculates the kurtosis of a set of values up to a specific point in the set.\n\ncmean(): Cumulative Mean\n\nThe cmean() function calculates the cumulative mean of a set of values. It’s a measure of the average of the values up to a specific point in the set.\n\ncmedian(): Cumulative Median\n\nThe cmedian() function calculates the cumulative median of a set of values. It’s the value that separates the lower half of the set from the upper half, up to a specific point in the set.\n\ncsd(): Cumulative Standard Deviation\n\nThe csd() function calculates the cumulative standard deviation of a set of values. Standard deviation is a measure of the spread of values in a set. The cumulative standard deviation calculates the standard deviation up to a specific point in the set.\n\ncskewness(): Cumulative Skewness\n\nThe cskewness() function calculates the cumulative skewness of a set of values. Skewness is a measure of the asymmetry of a distribution. The cumulative skewness calculates the skewness up to a specific point in the set.\n\ncvar(): Cumulative Variance\n\nThe cvar() function calculates the cumulative variance of a set of values. Variance is a measure of the spread of values in a set. The cumulative variance calculates the variance up to a specific point in the set.\nIn conclusion, the {TidyDensity} package offers several functions for calculating cumulative statistics, including mean, median, standard deviation, variance, skewness, kurtosis, harmonic mean, and geometric mean. These functions make it easy to calculate cumulative statistics for sets of values in R.\n\n\nFunctions\nAll of the functions perform work strictly on a vector. Because of this I will not go over the function calls separately because they all follow the vectorized for of fun(.x) where .x is the argument passed to the cumulative function.\n\n\nExamples\nHere I will go over some examples of each function use the AirPassengers data set.\n\nlibrary(TidyDensity)\n\nv <- AirPassengers\n\nLet’s start at the top.\nCumulative Geometric Mean:\n\nhead(cgmean(v))\n\n[1] 112.0000 114.9609 120.3810 122.4802 122.1827 124.2311\n\ntail(cgmean(v))\n\n[1] 249.6135 251.1999 252.4577 253.5305 254.2952 255.2328\n\nplot(cgmean(v), type = \"l\")\n\n\n\n\nCumulative Harmonic Mean:\n\nhead(chmean(v))\n\n[1] 112.00000  57.46087  40.03378  30.55222  24.39304  20.66000\n\ntail(chmean(v))\n\n[1] 1.636832 1.632423 1.627194 1.621471 1.614757 1.608744\n\nplot(chmean(v), type = \"l\")\n\n\n\n\nCumulative Kurtosis:\n\nhead(ckurtosis(v))\n\n[1]      NaN 1.000000 1.500000 1.315839 1.597316 1.597850\n\ntail(ckurtosis(v))\n\n[1] 2.668951 2.795314 2.733117 2.674195 2.649894 2.606228\n\nplot(ckurtosis(v), type = \"l\")\n\n\n\n\nCumulative Mean:\n\nhead(cmean(v))\n\n[1] 112.0000 115.0000 120.6667 122.7500 122.4000 124.5000\n\ntail(cmean(v))\n\n[1] 273.1367 275.5143 277.1631 278.4577 279.2378 280.2986\n\nplot(cmean(v), type = \"l\")\n\n\n\n\nCumulative Median:\n\nhead(cmedian(v))\n\n[1] 112.0 115.0 118.0 123.5 121.0 125.0\n\ntail(cmedian(v))\n\n[1] 259.0 261.5 264.0 264.0 264.0 265.5\n\nplot(cmedian(v), type = \"l\")\n\n\n\n\nCumulative Standard Deviation:\n\nhead(csd(v))\n\n[1]        NA  4.242641 10.263203  9.358597  8.142481  8.916277\n\ntail(csd(v))\n\n[1] 115.0074 117.9956 119.1924 119.7668 119.7083 119.9663\n\nplot(csd(v), type = \"l\")\n\n\n\n\nCumulative Skewness:\n\nhead(cskewness(v))\n\n[1]         NaN  0.00000000  0.44510927 -0.14739157 -0.02100016 -0.18544758\n\ntail(cskewness(v))\n\n[1] 0.5936970 0.6471651 0.6349071 0.6145579 0.5972102 0.5770682\n\nplot(cskewness(v), type = \"l\")\n\n\n\n\nCumulative Variance:\n\nhead(cvar(v))\n\n[1]        NA  18.00000 105.33333  87.58333  66.30000  79.50000\n\ntail(cvar(v))\n\n[1] 13226.70 13922.96 14206.84 14344.08 14330.07 14391.92\n\nplot(cvar(v), type = \"l\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-07/index.html",
    "href": "posts/rtip-2023-02-07/index.html",
    "title": "Subsetting Named Lists in R",
    "section": "",
    "text": "Introduction\nIn R, lists are a fundamental data structure that allows us to store multiple objects of different data types under a single name. Often times, we want to extract certain elements of a list based on their names, and this can be accomplished through the use of the subset function. In this blog post, we will take a look at how to use the grep function to subset named lists in R.\nFirst, we will create a list object as follows:\n\nasc_list &lt;- list(\n  Facility = 1:10,\n  State = 11:20,\n  National = 21:30\n)\n\nWe now have a list with three elements, each with a different name. Next, we want to make sure that our list does not contain any 0 length items. This can be achieved by using the lapply function and the length function:\n\nasc_list &lt;- asc_list[lapply(asc_list, length) &gt; 0]\n\nThe lapply function applies the length function to each element of the list, and returns a logical vector indicating whether each element is of length greater than 0. By using the square bracket operator, we can extract only those elements for which the logical value is TRUE.\nNext, we create a character vector of possible items that we want to match on:\n\npatterns &lt;- c(\"state\",\"faci\")\n\nWe can now pass this vector of patterns to the grep function, along with the names of our list and the ignore.case argument set to TRUE. The grep function returns the indices of the elements in our list that match the given pattern:\n\nasc_list[grep(\n  paste(patterns, collapse = \"|\"),\n  names(asc_list),\n  ignore.case = TRUE\n  )]\n\n$Facility\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$State\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nThe result of this code is a new list that contains only the elements of our original list whose names match either “state” or “faci”. The paste function is used to join the patterns in the vector into a single string, with the | character separating each pattern. This allows us to search for multiple patterns at once.\nIn conclusion, the grep function is a powerful tool for sub-setting named lists in R, especially when we have multiple patterns that we want to match on. By combining the grep function with other R functions such as lapply and length, we can extract specific elements from our lists with ease."
  },
  {
    "objectID": "posts/rtip-2023-02-08/index.html",
    "href": "posts/rtip-2023-02-08/index.html",
    "title": "Creating an R Project Directory",
    "section": "",
    "text": "Introduction\nWhen working in R I find it best to create a new project when working on something. This keeps all of the data and scripts in one location. This also means that if you are not careful the directory you have your project in can become quite messy. This used to happen to me with regularity, then I got smart and wrote a script that would standardize how projects are built for me.\nI find it important to have different fodlers for different parts of a project. This does not mean I will use them all for every project but that is fine, you can either comment that portion out or just delete the files that are created.\n\n\nFunction\nHere is what I do broken down into different steps. First, I see if the package {fs} is installed, and if not, then install it, and finally load it.\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\nNext we create a character vector of folder paths that will exist inside of the main project folder itself.\n\nfolders <- c(\n  \"00_Scripts\"\n  , \"00_Data\"\n  , \"01_Queries\"\n  , \"02_Data_Manipulation\"\n  , \"03_Viz\"\n  , \"04_TS_Modeling\"\n  , \"99_Automations\"\n)\n\nNow that the folders we want are spelt out, we can create them.\n\nfs::dir_create(\n  path = folders\n)\n\nNow that is done, it’s off to creating a few files that I personally almost always use. I do a lot of work out of a data warehouse so a connection file is needed. We also need a disconnection function.\n\n# DSS Connection \ndb_connect <- function() {\n  db_con <- LICHospitalR::db_connect()\n  \n  return(db_con)\n  \n}\n\n# Disconnect from Database\ndb_disconnect <- function(.connection) {\n  \n  DBI::dbDisconnect(\n    conn = db_connect()\n  )\n  \n}\n\nNow, let’s load in the typical libraries. You can modify this to suit your own needs.\n\n# Library Load\n\nlibrary_load <- function(){\n  \n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"DBI\"\n    , \"odbc\"\n    , \"janitor\"\n    , \"dplyr\"\n    , \"tibble\"\n    , \"tidyr\"\n    , \"LICHospitalR\"\n    , \"modeltime\"\n  )\n  \n}\n\nOk so now the functions have been created, let’s dump them!\n\ndb_funs <- c(\"db_connect\",\"db_disconnect\")\ndump(\n  list = db_funs,\n  file = \"00_Scripts/db_con_obj.R\"\n)\n\nlib_funs <- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Scripts/library_load.R\"\n)\n\n\n\nExample\nHere is the full script!\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\nfolders <- c(\n  \"00_Scripts\"\n  , \"00_Data\"\n  , \"01_Queries\"\n  , \"02_Data_Manipulation\"\n  , \"03_Viz\"\n  , \"04_TS_Modeling\"\n  , \"99_Automations\"\n)\n\nfs::dir_create(\n  path = folders\n)\n\n\nfile_create(\"01_Queries/query_functions.R\")\nfile_create(\"02_Data_Manipulation/data_functions.R\")\nfile_create(\"03_Viz/viz_functions.R\")\nfile_create(\"04_TS_Modeling/ts_functions.R\")\n\n# DSS Connection \ndb_connect <- function() {\n  db_con <- LICHospitalR::db_connect()\n  \n  return(db_con)\n  \n}\n\n# Disconnect from Database\ndb_disconnect <- function(.connection) {\n  \n  DBI::dbDisconnect(\n    conn = db_connect()\n  )\n  \n}\n\n# Library Load\n\nlibrary_load <- function(){\n  \n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"DBI\"\n    , \"odbc\"\n    , \"janitor\"\n    , \"dplyr\"\n    , \"tibble\"\n    , \"tidyr\"\n    , \"LICHospitalR\"\n    , \"modeltime\"\n  )\n  \n}\n\ndb_funs <- c(\"db_connect\",\"db_disconnect\")\ndump(\n  list = db_funs,\n  file = \"00_Scripts/db_con_obj.R\"\n)\n\nlib_funs <- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Scripts/library_load.R\"\n)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-09/index.html",
    "href": "posts/rtip-2023-02-09/index.html",
    "title": "Creating and Predicting Fast Regression Parsnip Models with {tidyAML}",
    "section": "",
    "text": "Introduction\nI am almost ready for a first release of my R package {tidyAML}. The purpose of this is to act as a way of quickly generating models using the parsnip package and keeping things inside of the tidymodels framework allowing users to seamlessly create models in tidyAML but pluck and move them over to tidymodels should they prefer. This is because I believe that software should be interchangeable and work well with other libraries. Today I am going to showcase how the function fast_regression()\n\n\nFunction\nLet’s take a look at the function.\n\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL\n)\n\nHere are the arguments to the function:\n\n.data - The data being passed to the function for the regression problem\n.rec_obj - The recipe object being passed.\n.parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported.\n.parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported.\n.split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample\n.split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type.\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(tidyAML)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(purrr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfast_reg_tbl <- fast_regression(\n  .data = mtcars,\n  .rec_obj = rec_obj,\n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(fast_reg_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nLet’s take a look at the model spec.\n\nfast_reg_tbl %>% slice(1) %>% pull(model_spec) %>% pluck(1)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the wflw column.\n\nfast_reg_tbl %>% slice(1) %>% pull(wflw) %>% pluck(1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe Fitted workflow.\n\nfast_reg_tbl %>% slice(1) %>% pull(fitted_wflw) %>% pluck(1)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n -15.077267     1.107474     0.001161    -0.001014     4.010199    -1.280324  \n       qsec           vs           am         gear         carb  \n   0.512318    -0.488014     2.430052     4.353568    -2.546043  \n\n\nAnd lastly tne predicted workflow column.\n\nfast_reg_tbl %>% slice(1) %>% pull(pred_wflw) %>% pluck(1)\n\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.7\n 2  28.2\n 3  18.9\n 4  12.0\n 5  14.8\n 6  15.4\n 7  14.7\n 8  20.0\n 9  11.2\n10  19.1\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrdata-2023-02-10/index.html",
    "href": "posts/weekly-rtip-healthyrdata-2023-02-10/index.html",
    "title": "Get the Current Hospital Data Set from CMS with {healthyR.data}",
    "section": "",
    "text": "Introduction\nGetting data for health care in the US can sometimes be hard. With my R package {healthyR.data} I am hoping to alleviate some of that pain.\nRight now the package is bring actively developed from what was a simple yet sleepy simulated administrative data set is getting supercharged into a a full blow package that will retrieve data from outside sources. One such source is CMS.\nAt the start, and this is going to be a long road, I have started to build some functionality around getting the current hospital data from CMS. Let’s take a look at how it works.\n\n\nFunction\nHere is the function which has no parameters. This function will download the current and the official hospital data sets from the CMS.gov website.\nThe function makes use of a temporary directory and file to save and unzip the data. This will grab the current Hospital Data Files, unzip them and return a list of tibbles with each tibble named after the data file.\nThe function returns a list object with all of the current hospital data as a tibble. It does not save the data anywhere so if you want to save it you will have to do that manually.\nThis also means that you would have to store the data as a variable in order to access the data later on. It does have a given attributes and a class so that it can be piped into other functions.\n\ncurrent_hosp_data()\n\nNow let’s see it in action.\n\n\nExample\nWe will download the current hospital data sets and take a look.\n\nlibrary(healthyR.data)\nlibrary(dplyr)\n\ncurrent_hospital_dataset <- current_hosp_data()\n\nThis function downloads 70 files. Let’s see which ones have been downloaded.\n\nnames(current_hospital_dataset)\n\n [1] \"ASC_Facility.csv\"                                                \n [2] \"ASC_National.csv\"                                                \n [3] \"ASC_State.csv\"                                                   \n [4] \"ASCQR_OAS_CAHPS_BY_ASC.csv\"                                      \n [5] \"ASCQR_OAS_CAHPS_NATIONAL.csv\"                                    \n [6] \"ASCQR_OAS_CAHPS_STATE.csv\"                                       \n [7] \"CJR_PY6_Quality_Reporting_July_2022_Production_File.csv\"         \n [8] \"CMS_PSI_6_decimal_file.csv\"                                      \n [9] \"Complications_and_Deaths_Hospital.csv\"                           \n[10] \"Complications_and_Deaths_National.csv\"                           \n[11] \"Complications_and_Deaths_State.csv\"                              \n[12] \"Data_Updates_January_2023.csv\"                                   \n[13] \"Footnote_Crosswalk.csv\"                                          \n[14] \"FY_2023_HAC_Reduction_Program_Hospital.csv\"                      \n[15] \"FY_2023_Hospital_Readmissions_Reduction_Program_Hospital.csv\"    \n[16] \"FY2021_Distribution_of_Net_Change_in_Base_Op_DRG_Payment_Amt.csv\"\n[17] \"FY2021_Net_Change_in_Base_Op_DRG_Payment_Amt.csv\"                \n[18] \"FY2021_Percent_Change_in_Medicare_Payments.csv\"                  \n[19] \"FY2021_Value_Based_Incentive_Payment_Amount.csv\"                 \n[20] \"HCAHPS_Hospital.csv\"                                             \n[21] \"HCAHPS_National.csv\"                                             \n[22] \"HCAHPS_State.csv\"                                                \n[23] \"Healthcare_Associated_Infections_Hospital.csv\"                   \n[24] \"Healthcare_Associated_Infections_National.csv\"                   \n[25] \"Healthcare_Associated_Infections_State.csv\"                      \n[26] \"Hospital_General_Information.csv\"                                \n[27] \"HOSPITAL_QUARTERLY_MSPB_6_DECIMALS.csv\"                          \n[28] \"hvbp_clinical_outcomes.csv\"                                      \n[29] \"hvbp_efficiency_and_cost_reduction.csv\"                          \n[30] \"hvbp_person_and_community_engagement.csv\"                        \n[31] \"hvbp_safety.csv\"                                                 \n[32] \"hvbp_tps.csv\"                                                    \n[33] \"IPFQR_QualityMeasures_Facility.csv\"                              \n[34] \"IPFQR_QualityMeasures_National.csv\"                              \n[35] \"IPFQR_QualityMeasures_State.csv\"                                 \n[36] \"Maternal_Health_Hospital.csv\"                                    \n[37] \"Maternal_Health_National.csv\"                                    \n[38] \"Maternal_Health_State.csv\"                                       \n[39] \"Measure_Dates.csv\"                                               \n[40] \"Medicare_Hospital_Spending_by_Claim.csv\"                         \n[41] \"Medicare_Hospital_Spending_Per_Patient_Hospital.csv\"             \n[42] \"Medicare_Hospital_Spending_Per_Patient_National.csv\"             \n[43] \"Medicare_Hospital_Spending_Per_Patient_State.csv\"                \n[44] \"OAS_CAHPS_Footnotes.csv\"                                         \n[45] \"OQR_OAS_CAHPS_BY_HOSPITAL.csv\"                                   \n[46] \"OQR_OAS_CAHPS_NATIONAL.csv\"                                      \n[47] \"OQR_OAS_CAHPS_STATE.csv\"                                         \n[48] \"Outpatient_Imaging_Efficiency_Hospital.csv\"                      \n[49] \"Outpatient_Imaging_Efficiency_National.csv\"                      \n[50] \"Outpatient_Imaging_Efficiency_State.csv\"                         \n[51] \"Payment_National.csv\"                                            \n[52] \"Payment_State.csv\"                                               \n[53] \"Payment_and_Value_of_Care_Hospital.csv\"                          \n[54] \"PCH_HCAHPS_HOSPITAL.csv\"                                         \n[55] \"PCH_HCAHPS_NATIONAL.csv\"                                         \n[56] \"PCH_HCAHPS_STATE.csv\"                                            \n[57] \"PCH_HEALTHCARE_ASSOCIATED_INFECTIONS_HOSPITAL.csv\"               \n[58] \"PCH_ONCOLOGY_CARE_MEASURES_HOSPITAL.csv\"                         \n[59] \"PCH_OUTCOMES_HOSPITAL.csv\"                                       \n[60] \"PCH_OUTCOMES_NATIONAL.csv\"                                       \n[61] \"Timely_and_Effective_Care_Hospital.csv\"                          \n[62] \"Timely_and_Effective_Care_National.csv\"                          \n[63] \"Timely_and_Effective_Care_State.csv\"                             \n[64] \"Unplanned_Hospital_Visits_Hospital.csv\"                          \n[65] \"Unplanned_Hospital_Visits_National.csv\"                          \n[66] \"Unplanned_Hospital_Visits_State.csv\"                             \n[67] \"VA_IPF.csv\"                                                      \n[68] \"VA_TE.csv\"                                                       \n[69] \"Value_of_Care_National.csv\"                                      \n[70] \"Veterans_Health_Administration_Provider_Level_Data.csv\"          \n\n\nMore to come in the future!"
  },
  {
    "objectID": "posts/rtip-2023-02-13/index.html",
    "href": "posts/rtip-2023-02-13/index.html",
    "title": "Off to CRAN! {tidyAML}",
    "section": "",
    "text": "Introduction\nAre you tired of spending hours tuning and testing different machine learning models for your regression or classification problems? The new R package {tidyAML} is here to simplify the process for you! tidyAML is a simple interface for automatic machine learning that fits the tidymodels framework, making it easier for you to solve regression and classification problems.\nThe tidyAML package has been designed with the goal of providing a simple API that automates the entire machine learning pipeline, from data preparation to model selection, training, and prediction. This means that you no longer have to spend hours tuning and testing different models; tidyAML will do it all for you, saving you time and effort.\nIn this initial release (version 0.0.1), tidyAML introduces a number of new features and minor fixes to improve the overall user experience. Here are some of the updates in this release:\nNew Features:\n\nmake_regression_base_tbl() and make_classification_base_tbl() functions for creating base tables for regression and classification problems, respectively.\ninternal_make_spec_tbl() function for making the specification table for the machine learning pipeline.\ninternal_set_args_to_tune() function for setting arguments to tune the models. This has not yet been implemented in a true working fashion but might be useful for feedback in this initial release.\ncreate_workflow_set() function for creating a set of workflows to test different models.\nget_model(), extract_model_spec(), extract_wflw(), extract_wflw_fit(), and extract_wflw_pred() functions for extracting different parts of the machine learning pipeline.\nmatch_args() function for matching arguments between the base and specification tables.\n\nMinor Fixes and Improvements:\n\nUpdates to fast_classification_parsnip_spec_tbl() and fast_regression_parsnip_spec_tbl() to use the make_regression and make_classification functions and the internal_make_spec_tbl() function.\nAddition of a class for the base table functions and using that class in internal_make_spec_tbl().\nUpdate to the DESCRIPTION for R >= 3.4.0.\n\nIn conclusion, tidyAML is a game-changer for those looking to automate the machine learning pipeline. It provides a simple API that eliminates the need for manual tuning and testing of different models. With the updates in this initial release, the tidyAML package is sure to make your machine learning journey easier and more efficient.\n\n\nFunction\nThere are too many functions to go over in this post so you can find them all here\n\n\nExamples\nEven though there are many functions to go over, we can showcase some with a small useful example. So let’s get at it!\n\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(dplyr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\n\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n  )\n\nglimpse(frt_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nNow let’s go through the extractors.\nThe get_model() function.\n\nget_model(frt_tbl, 2) |>\n  glimpse()\n\nRows: 1\nColumns: 8\n$ .model_id       <int> 2\n$ .parsnip_engine <chr> \"glm\"\n$ .parsnip_mode   <chr> \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, glm, TRUE…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>]\n\n\nThe extract_model_spec() function.\n\nextract_model_spec(frt_tbl, 1)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOr do multiples:\n\nextract_model_spec(frt_tbl, 1:2)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nThe extract_wflw() function.\n\nextract_wflw(frt_tbl, 1)\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOr do multiples:\n\nextract_wflw(frt_tbl, c(1, 2))\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n[[2]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nThe extract_wflw_fit() function.\n\nextract_wflw_fit(frt_tbl, 1)\n\n[[1]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\n\nOr do multiples:\n\nextract_wflw_fit(frt_tbl, 1:2)\n\n[[1]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\n\n[[2]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::gaussian, data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\nDegrees of Freedom: 23 Total (i.e. Null);  13 Residual\nNull Deviance:      935.1 \nResidual Deviance: 121.5    AIC: 131\n\n\nFinally the extract_wflw_pred() function.\n\nextract_wflw_pred(frt_tbl, 2)\n\n[[1]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n\nOr do multiples:\n\nextract_wflw_pred(frt_tbl, 1:2)\n\n[[1]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n[[2]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-14/index.html",
    "href": "posts/rtip-2023-02-14/index.html",
    "title": "An example of using {box}",
    "section": "",
    "text": "Introduction\nToday I am going to make a short post on the R package {box} which was showcased to me quite nicely by Michael Miles. It was informative and I was able to immediately see the usefulness of the {box} library.\nSo what is ‘box’? Well here is the description straight from their site:\n\n‘box’ allows organising R code in a more modular way, via two mechanisms:\n\nIt enables writing modular code by treating files and folders of R code as independent (potentially nested) modules, without requiring the user to wrap reusable code into packages.\nIt provides a new syntax to import reusable code (both from packages and from modules) which is more powerful and less error-prone than library or require, by limiting the number of names that are made available.\n\n\nSo let’s see how it all works.\n\n\nFunction\nThe main portion of the script looks like this:\n\n# Main script\n\n# Script setup --------------------------------------\n\n# Load box modules\nbox::use(. / box / global_options / global_options)\nbox::use(. / box / io / imports)\nbox::use(. / box / io / exports)\nbox::use(. / box / mod / mod)\n\n# Load global options\nglobal_options$set_global_options() \n\n\n# Main script ---------------------------------------\n\n# Load data, process it, and export results\nall_data <- getOption('data_dir') |> \n  \n  # Load all data\n  imports$load_all() |> \n  \n  # Modify dataset\n  mod$modify_data() |> \n  \n  # Export data\n  exports$export_data()\n\nSo what does this do? Well it is grabbing data from a predefined location, modifying it and then re-exporting it. Now let’s look at all the code that is behind it, which allows us to do these things and then you will see the power of using box\n\n\nExample\nLet’s take a look at the global options settings.\n\n# Set global options\n#' @export\nset_global_options <- function() {\n  options(\n    look_ups = 'look-ups/',\n    data_dir = 'data/input/'\n  )\n}\n\nOk 6 lines, boxed down to one.\nNow the import function.\n\n# Function for importing data\n\n#' @export\nload_all <- function(file_path) {\n  \n  box::use(purrr)\n  box::use(vroom)\n  \n  file_path |> \n    \n    # Get all csv files from folder\n    list.files(full.names = TRUE) |> \n    \n    # Set list names\n    purrr$set_names(\\(file) basename(file)) |> \n    \n    # Load all csvs into list\n    purrr$map(\\(file) vroom$vroom(file))\n\n}\n\nNow the modify_data function.\n\n# Function for modifying data\n\n#' @export\nmodify_data <- function(df_list) {\n  \n  box::use(dplyr)\n  box::use(purrr)\n  \n  map_fun <- function(df) {\n    \n    df |> \n      dplyr$select(name:mass) |> \n      dplyr$mutate(lol = height * mass) |> \n      dplyr$filter(lol > 1500)\n  }\n  \n  # Apply mapping function to list\n  purrr$map(df_list, map_fun)\n  \n}\n\nOk again, a big savings here, instead of the above we simply call mod$modify_data() which makes things clearner and also modular in that we can go to a very specific spot in our proejct to fix an error or add/subtract functionality.\nLastly the export.\n\n# Function for exporting data\n\n#' @export\nexport_data <- function(df_list) {\n  \n  box::use(vroom)\n  box::use(purrr)\n  \n  # Export data\n  purrr$map2(.x = df_list,\n             .y = names(df_list),\n             ~vroom$vroom_write(x = .x,\n                               file = paste0('data/output/', \n                                             .y),\n                               delim = ','))\n  \n}\n\nVoila! I think to even a fresh user, the power of boxing your functions is fairly apparent and to the advanced user, eyes are most likely glowing!"
  },
  {
    "objectID": "posts/rtip-2023-02-15/index.html",
    "href": "posts/rtip-2023-02-15/index.html",
    "title": "Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nAre you interested in visualizing time series data in a clear and concise way? The R package {healthyR.ts} provides a variety of tools for time series analysis and visualization, including the ts_ma_plot() function.\nThe ts_ma_plot() function is designed to help you quickly and easily create moving average plots for time series data. This function takes several arguments, including the data you want to visualize, the date column from your data, the value column from your data, and the frequency of the aggregation.\nOne of the great features of ts_ma_plot() is that it can handle both weekly and monthly data frequencies, making it a flexible tool for analyzing a variety of time series data. If you pass in a frequency other than “weekly” or “monthly”, the function will default to weekly, so it’s important to ensure that your data is aggregated at the appropriate frequency.\nWith ts_ma_plot(), you can create a variety of plots to help you better understand your time series data. The function allows you to add up to three different titles to your plot, helping you to organize and communicate your findings effectively. The main_title argument sets the title for the main plot, while the secondary_title and tertiary_title arguments set the titles for the second and third plots, respectively.\nIf you’re interested in using ts_ma_plot() for your own time series data, you’ll first need to preprocess your data so that it’s in the appropriate format for this function. Once you’ve done that, though, ts_ma_plot() can help you to quickly identify trends and patterns in your data that might not be immediately apparent from a raw data set.\nIn summary, ts_ma_plot() is a powerful and flexible tool for visualizing time series data. Whether you’re working with weekly or monthly data, this function can help you to quickly and easily create moving average plots that can help you to better understand your data. If you’re interested in time series analysis, be sure to check out {healthyR.ts} and give ts_ma_plot() a try!\n\n\nFunction\nHere is the full function call.\n\nts_ma_plot(\n  .data,\n  .date_col,\n  .value_col,\n  .ts_frequency = \"monthly\",\n  .main_title = NULL,\n  .secondary_title = NULL,\n  .tertiary_title = NULL\n)\n\nNow for the arguments to the parameters.\n\n.data: the data you want to visualize, which should be pre-processed and the aggregation should match the .frequency argument.\n.date_col: the data column from the .data argument that contains the dates for your time series.\n.value_col: the data column from the .data argument that contains the values for your time series.\n.ts_frequency: the frequency of the aggregation, which should be quoted as “weekly” or “monthly”. If not specified, the function defaults to weekly.\n.main_title: the title of the main plot.\n.secondary_title: the title of the second plot.\n.tertiary_title: the title of the third plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndata_tbl <- ts_to_tbl(AirPassengers) |>\n  select(-index)\n\noutput <- ts_ma_plot(\n  .data = data_tbl,\n  .date_col = date_col,\n  .value_col = value\n)\n\nLet’s take a look at each piece of the output.\n\noutput$data_trans_xts |> head()\n\n           value ma12\n1949-01-01   112   NA\n1949-02-01   118   NA\n1949-03-01   132   NA\n1949-04-01   129   NA\n1949-05-01   121   NA\n1949-06-01   135   NA\n\n\n\noutput$data_diff_xts_a |> head()\n\n              diff_a\n1949-01-01        NA\n1949-02-01  5.357143\n1949-03-01 11.864407\n1949-04-01 -2.272727\n1949-05-01 -6.201550\n1949-06-01 11.570248\n\n\n\noutput$data_diff_xts_b |> head()\n\n           diff_b\n1949-01-01     NA\n1949-02-01     NA\n1949-03-01     NA\n1949-04-01     NA\n1949-05-01     NA\n1949-06-01     NA\n\n\n\noutput$data_summary_tbl\n\n# A tibble: 144 × 5\n   date_col   value  ma12 diff_a diff_b\n   <date>     <dbl> <dbl>  <dbl>  <dbl>\n 1 1949-01-01   112    NA   0         0\n 2 1949-02-01   118    NA   5.36      0\n 3 1949-03-01   132    NA  11.9       0\n 4 1949-04-01   129    NA  -2.27      0\n 5 1949-05-01   121    NA  -6.20      0\n 6 1949-06-01   135    NA  11.6       0\n 7 1949-07-01   148    NA   9.63      0\n 8 1949-08-01   148    NA   0         0\n 9 1949-09-01   136    NA  -8.11      0\n10 1949-10-01   119    NA -12.5       0\n# … with 134 more rows\n\n\n\noutput$pgrid\n\n\n\n\n\noutput$xts_plt"
  },
  {
    "objectID": "posts/rtip-2023-02-16/index.html",
    "href": "posts/rtip-2023-02-16/index.html",
    "title": "Officially on CRAN {tidyAML}",
    "section": "",
    "text": "Introduction\nI’m excited to announce that the R package {tidyAML} is now officially available on CRAN! This package is designed to make it easy for users to perform automated machine learning (AutoML) using the tidymodels ecosystem. With a simple and intuitive interface, tidyAML allows users to quickly generate high-quality machine learning models without worrying about the underlying details.\nOne of the key features of tidyAML is its ability to generate regression models on the fly, without the need to build a full specification or tune hyper-parameters. This makes it ideal for users who want to quickly build a machine learning model without spending a lot of time on the setup process.\ntidyAML is also designed to be easy to use, with a set of functions that are straightforward and can generate many models and predictions at once. And because it’s built on top of the tidymodels ecosystem, users don’t need to worry about setting up additional packages or dependencies.\nWe’re also happy to announce that tidyAML will be added to the R package {healthyverse} and pushed to CRAN this week. This means that users who install {healthyverse} will automatically get access to tidyAML, as well as other popular packages like ggplot2, dplyr, and tidyr.\nWhether you’re a beginner or an experienced machine learning practitioner, tidyAML is a powerful tool that can help you quickly generate high-quality models with minimal setup. We hope you’ll give it a try and let us know what you think!"
  },
  {
    "objectID": "posts/rtip-2023-02-17/index.html",
    "href": "posts/rtip-2023-02-17/index.html",
    "title": "Converting a {tidyAML} tibble to a {workflowsets}",
    "section": "",
    "text": "The {tidyAML} package is an R package that provides a set of tools for building regression/classification models on the fly with minimal input required. In this post we will discuss the create_workflow_set() function.\nThe create_workflow_set function is a function in the tidyAML package that is used to create a workflowset object from the workflowsets package. A workflow is a sequence of tasks that can be executed in a specific order, and is often used in data analysis and machine learning to automate data processing and model fitting. The create_workflow_set function takes as input a YAML specification of a set of workflows, and returns a list of workflow objects that can be executed using the tidymodels package and its associated packages.\nThe create_workflow_set function is particularly useful when working with the tidymodels package and the parsnip framework. The tidymodels package is a collection of packages for modeling and machine learning in R that provides a consistent interface for building, tuning, and evaluating machine learning models. The parsnip package is part of the tidymodels ecosystem and provides a way to specify a wide range of models in a consistent manner.\n\n\nTo use the create_workflow_set function with tidymodels andparsnip, you will need to provide a recipe or recipes as a list to the .recipe_list parameter and a model_spec tibble that you would get from something like fast_regression_parsnip_spec_tbl(), other classes will be supported in the future.\nThe reason this was done was because I did not want to force users to remain inside of tidyAML perhaps and most likely there are other packages out there that are more suited to an end users specific problem at hand."
  },
  {
    "objectID": "posts/rtip-2023-02-22/index.html",
    "href": "posts/rtip-2023-02-22/index.html",
    "title": "Calibrate and Plot a Time Series with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nIn time series analysis, it is common to split the data into training and testing sets to evaluate the accuracy of a model. However, it is important to ensure that the model is calibrated on the training set before evaluating its performance on the testing set. The {healthyR.ts} library provides a function called calibrate_and_plot() that simplifies this process.\n\n\nFunction\nHere is the full function call:\n\ncalibrate_and_plot(\n  ...,\n  .type = \"testing\",\n  .splits_obj,\n  .data,\n  .print_info = TRUE,\n  .interactive = FALSE\n)\n\nHere are the arguments to the parameters:\n\n... - The workflow(s) you want to add to the function.\n.type - Either the training(splits) or testing(splits) data.\n.splits_obj - The splits object.\n.data - The full data set.\n.print_info - The default is TRUE and will print out the calibration accuracy tibble and the resulting plotly plot.\n.interactive - The defaults is FALSE. This controls if a forecast plot is interactive or not via plotly.\n\n\n\nExample\nBy default, calibrate_and_plot() will print out a calibration accuracy tibble and a resulting plotly plot. This can be controlled with the print_info argument, which is set to TRUE by default. If you prefer a non-interactive forecast plot, you can set the interactive argument to FALSE.\nHere’s an example of how to use the calibrate_and_plot() function:\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(workflows)\nlibrary(rsample)\n\n# Get the Data\ndata <- ts_to_tbl(AirPassengers) |>\n  select(-index)\n\n# Split the data into training and testing sets\nsplits <- time_series_split(\n   data\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\n# Make the recipe object\nrec_obj <- recipe(value ~ ., data = training(splits))\n\n# Make the Model\nmodel_spec <- linear_reg(\n   mode = \"regression\"\n   , penalty = 0.5\n   , mixture = 0.5\n) |>\n   set_engine(\"lm\")\n\n# Make the workflow object\nwflw <- workflow() |>\n   add_recipe(rec_obj) |>\n   add_model(model_spec) |>\n   fit(training(splits))\n\n# Get our output\noutput <- calibrate_and_plot(\n  wflw\n  , .type = \"training\"\n  , .splits_obj = splits\n  , .data = data\n  , .print_info = FALSE\n  , .interactive = TRUE\n )\n\nThe resulting output will include a calibration accuracy tibble and a plotly plot showing the original time series data along with the fitted values for the training set.\nLet’s take a look at the output.\n\noutput$calibration_tbl\n\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc .type .calibration_data \n      <int> <list>     <chr>       <chr> <list>            \n1         1 <workflow> LM          Test  <tibble [132 × 4]>\n\n\n\noutput$model_accuracy\n\n# A tibble: 1 × 9\n  .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq\n      <int> <chr>       <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 LM          Test   31.4  12.0  1.31  11.9  41.7 0.846\n\n\nAnd…\n\noutput$plot\n\n\n\n\n\nOverall, the calibrate_and_plot() function is a useful tool for simplifying the process of calibrating time series models on a training set and evaluating their performance on a testing set.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-23/index.html",
    "href": "posts/rtip-2023-02-23/index.html",
    "title": "Data Preppers with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nThere are many different methods that one can choose from in order to model their data. This brings with it a fundamental issue of how to prepare your data for the specified algorithm. With the [{healthyR.ai}] package there are many different functions in this family that will help solve this issue for some algorithms but of course not all, that would be utterly exhausting for me to do on my own.\nIn healthyR.ai I call these Data Preppers because they prep the data you supply to the format necessary for the algorithm to function properly.\nLet’s take a look at one.\n\n\nFunction\nHere we are going to use the hai_c50_data_prepper(.data, .recipe_formula) function.\n\nhai_c50_data_prepper(.data, .recipe_formula)\n\nHere are the simple arguments:\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::recipe() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the iris data then the formula would most likely be something like Species ~ .\n\n\n\nExample\nHere is a small example:\n\nlibrary(healthyR.ai)\n\nhai_c50_data_prepper(.data = Titanic, .recipe_formula = Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\n\nrec_obj <- hai_c50_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(rec_obj)\n\n# A tibble: 32 × 5\n   Class Sex    Age       n Survived\n   <fct> <fct>  <fct> <dbl> <fct>   \n 1 1st   Male   Child     0 No      \n 2 2nd   Male   Child     0 No      \n 3 3rd   Male   Child    35 No      \n 4 Crew  Male   Child     0 No      \n 5 1st   Female Child     0 No      \n 6 2nd   Female Child     0 No      \n 7 3rd   Female Child    17 No      \n 8 Crew  Female Child     0 No      \n 9 1st   Male   Adult   118 No      \n10 2nd   Male   Adult   154 No      \n# … with 22 more rows\n\n\nHere are the rest of the data-preppers at the time of writing this article:\n\nhai_c50_data_prepper()\nhai_cubist_data_prepper()\nhai_earth_data_prepper()\nhai_glmnet_data_prepper()\nhai_knn_data_prepper()\nhai_ranger_data_prepper()\nhai_svm_poly_data_prepper()\nhai_svm_rbf_data_prepper()\nhai_xgboost_data_prepper()\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-27/index.html",
    "href": "posts/rtip-2023-02-27/index.html",
    "title": "Quickly Generate Nested Time Series Models",
    "section": "",
    "text": "Introduction\nThere are many approaches to modeling time series data in R. One of the types of data that we might come across is a nested time series. This means the data is grouped simply by one or more keys. There are many methods in which to accomplish this task. This will be a quick post, but if you want a longer more detailed and quite frankly well written out one, then this is a really good article\n\n\nExampmle\nLet’s just get to it with a very simple example, the motivation here isn’t to be all encompassing, but rather to just showcase it is possible for those who may not know it is.\n\nlibrary(healthyR.data)\nlibrary(dplyr)\nlibrary(timetk)\n\nts_tbl <- healthyR_data |> \n  filter(ip_op_flag == \"I\") |> \n  select(visit_end_date_time, service_line, length_of_stay) |>\n  mutate(visit_end_date_time = as.Date(visit_end_date_time)) |>\n  group_by(service_line) |>\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by = \"month\",\n    los = mean(length_of_stay)\n  ) |>\n  ungroup()\n\nglimpse(ts_tbl)\n\nRows: 2,148\nColumns: 3\n$ service_line        <chr> \"Alcohol Abuse\", \"Alcohol Abuse\", \"Alcohol Abuse\",…\n$ visit_end_date_time <date> 2011-09-01, 2011-10-01, 2011-11-01, 2011-12-01, 2…\n$ los                 <dbl> 3.666667, 3.181818, 4.380952, 3.464286, 3.677419, …\n\n\n\nlibrary(forecast)\nlibrary(broom)\nlibrary(tidyr)\n\nglanced_models <- ts_tbl |> \n  nest_by(service_line) |> \n  mutate(AA = list(auto.arima(data$los))) |> \n  mutate(perf = list(glance(AA))) |> \n  unnest(cols = c(perf))\n\nglanced_models |>\n  select(-data)\n\n# A tibble: 23 × 7\n# Groups:   service_line [23]\n   service_line                  AA         sigma logLik   AIC   BIC  nobs\n   <chr>                         <list>     <dbl>  <dbl> <dbl> <dbl> <int>\n 1 Alcohol Abuse                 <fr_ARIMA> 2.22  -241.   493.  506.   109\n 2 Bariatric Surgery For Obesity <fr_ARIMA> 0.609  -80.1  168.  178.    88\n 3 CHF                           <fr_ARIMA> 0.963 -152.   309.  314.   110\n 4 COPD                          <fr_ARIMA> 0.987 -155.   315.  320.   110\n 5 CVA                           <fr_ARIMA> 1.50  -201.   407.  412.   110\n 6 Carotid Endarterectomy        <fr_ARIMA> 6.27  -166.   335.  339.    51\n 7 Cellulitis                    <fr_ARIMA> 1.07  -163.   329.  335.   110\n 8 Chest Pain                    <fr_ARIMA> 0.848 -139.   281.  287.   110\n 9 GI Hemorrhage                 <fr_ARIMA> 1.21  -179.   361.  366.   111\n10 Joint Replacement             <fr_ARIMA> 1.65  -196.   396.  401.   102\n# … with 13 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-28/index.html",
    "href": "posts/rtip-2023-02-28/index.html",
    "title": "Open a File Folder in R",
    "section": "",
    "text": "Inroduction\nWhen writing a function, it is possible that you may want to ask the user where they want the data stored and if they want to open the file folder after the download has taken place. Well we can do this in R by invoking the shell.exec() command where we use a variable like f_path that is the path to the folder. We are going to go over a super simple example.\n\n\nFunction\nHere is the function:\n\nshell.exec(file)\n\nHere are the arguments.\n\nfile - file, directory or URL to be opened.\n\nNow let’s go over a simple example\n\n\nExample\nHere we go.\n\n# Create a temporary file to store the zip file\nf_path <- utils::choose.dir()\n\n# Open file folder?\nif (.open_folder){\n    shell.exec(f_path)\n}\n\nIf in our function creation we make a variable .open_folder and set it equal to TRUE then the if statement will execute and shell.exec(f_path) will open the specified path set by utils::choose.dir()\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-01/index.html",
    "href": "posts/rtip-2023-03-01/index.html",
    "title": "Text Processing Made Easy with {healthyR}’s sql_left(), sql_mid(), and sql_right() Functions in R",
    "section": "",
    "text": "Introduction\nAre you tired of manually manipulating text data in R? Do you find yourself frequently needing to extract substrings from long strings or to grab just the first few characters of a string? If so, you’re in luck! The {healthyR} library has three functions that will make your text processing tasks much easier: sql_left(), sql_mid(), and sql_right().\n\n\nFunction\nHere are the function calls, I will also make the source avilable in the same cell so steal this code!!\n\n# LEFT\nsql_left(\"text\", 3)\n\nsql_left <- function(.text, .num_char) {\n    base::substr(.text, 1, .num_char)\n}\n\n# MID\nsql_mid(\"this is some text\", 6, 2)\n\nsql_mid <- function(.text, .start_num, .num_char) {\n    base::substr(.text, .start_num, .start_num + .num_char - 1)\n}\n\n# RIGHT\nsql_right(\"this is some more text\", 3)\n\nsql_right <- function(.text, .num_char) {\n    base::substr(.text, base::nchar(.text) - (.num_char-1), base::nchar(.text))\n}\n\n\n\nExample\nLet’s start with sql_left(). This function is similar to the LEFT() function in SQL and Excel, in that it returns the specified number of characters from the beginning of a string. For example, if we have the string “Hello, world!”, and we want to grab just the first three characters, we can use sql_left() like this:\n\nlibrary(healthyR)\nsql_left(\"Hello, world!\", 3)\n\n[1] \"Hel\"\n\n\nThis will return the string “Hel”.\nNext up is sql_mid(). This function is similar to the SUBSTRING() and MID() functions in SQL and Excel, in that it returns a specified portion of a string. The first argument is the string itself, the second argument is the starting position of the substring, and the third argument is the length of the substring. For example, if we have the string “This is some text”, and we want to grab the two characters starting at position six, we can use sql_mid() like this:\n\nsql_mid(\"This is some text\", 6, 2)\n\n[1] \"is\"\n\n\nThis will return the string “is”.\nFinally, we have sql_right(). This function is similar to the RIGHT() function in SQL and Excel, in that it returns the specified number of characters from the end of a string. For example, if we have the string “This is some more text”, and we want to grab just the last three characters, we can use sql_right() like this:\n\nsql_right(\"This is some more text\", 3)\n\n[1] \"ext\"\n\n\nThis will return the string “ext”.\nThese three functions can be extremely helpful when working with text data in R. They can save you time and effort, and make your code more concise and readable. So next time you find yourself needing to manipulate text data, remember to reach for sql_left(), sql_mid(), and sql_right()!\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-02/index.html",
    "href": "posts/rtip-2023-03-02/index.html",
    "title": "Forecasting Timeseries in a list with R",
    "section": "",
    "text": "Introduction\nIn this article, we will discuss how to perform an ARIMA forecast on nested data or data that is in a list using R programming language. This is a common scenario in which we have data stored in a list format, where each element of the list corresponds to a different time series. We will use the R programming language, specifically the “forecast” package, to perform the ARIMA forecast.\nFirst, we will need to load the required packages and data. For this example, we will use the “AirPassengers” dataset which is included in the “datasets” package. This dataset contains the number of international airline passengers per month from 1949 to 1960. We will then create a list containing subsets of this data for each year.\n\nlibrary(forecast)\n\nyearly_data <- split(AirPassengers, f = ceiling(seq_along(AirPassengers)/12))\n\nyearly_data\n\n$`1`\n [1] 112 118 132 129 121 135 148 148 136 119 104 118\n\n$`2`\n [1] 115 126 141 135 125 149 170 170 158 133 114 140\n\n$`3`\n [1] 145 150 178 163 172 178 199 199 184 162 146 166\n\n$`4`\n [1] 171 180 193 181 183 218 230 242 209 191 172 194\n\n$`5`\n [1] 196 196 236 235 229 243 264 272 237 211 180 201\n\n$`6`\n [1] 204 188 235 227 234 264 302 293 259 229 203 229\n\n$`7`\n [1] 242 233 267 269 270 315 364 347 312 274 237 278\n\n$`8`\n [1] 284 277 317 313 318 374 413 405 355 306 271 306\n\n$`9`\n [1] 315 301 356 348 355 422 465 467 404 347 305 336\n\n$`10`\n [1] 340 318 362 348 363 435 491 505 404 359 310 337\n\n$`11`\n [1] 360 342 406 396 420 472 548 559 463 407 362 405\n\n$`12`\n [1] 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nIn the above code, we use the “split” function to split the data into yearly subsets. The “f” parameter is used to specify the grouping variable which, in this case, is the sequence of numbers from 1 to the length of the dataset divided by 12, rounded up to the nearest integer. This creates a list of 12 elements, one for each year.\n\n\nFunction\nNext, we will define a function that takes a single element of the list, fits an ARIMA model, and generates a forecast.\n\narima_forecast <- function(x){\n  fit <- auto.arima(x)\n  forecast(fit)\n}\n\nThis function takes a single argument “x” which is one of the elements of the list. We use the “auto.arima” function from the “forecast” package to fit an ARIMA model to the data. The “forecast” function is then used to generate a forecast based on this model.\n\n\nExample\nWe can now use the “lapply” function to apply this function to each element of the list.\n\nforecasts <- lapply(yearly_data, arima_forecast)\n\nThe “lapply” function applies the “arima_forecast” function to each element of the “yearly_data” list and returns a list of forecasts.\nFinally, we can extract and plot the forecasts for a specific year.\n\nplot(forecasts[[5]])\n\n\n\n\nNow lets take a look at them all.\n\npar(mfrow = c(2,1))\n\npurrr::map(forecasts, plot)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$`1`\n$`1`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 132.2237 126.4744 126.4744 126.4744 126.4744 126.4744 126.4744 126.4744\n [9] 126.4744 126.4744\n\n$`1`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 120.1608 113.7751\n14 110.0828 101.4056\n15 110.0828 101.4056\n16 110.0828 101.4056\n17 110.0828 101.4056\n18 110.0828 101.4056\n19 110.0828 101.4056\n20 110.0828 101.4056\n21 110.0828 101.4056\n22 110.0828 101.4056\n\n$`1`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 144.2865 150.6722\n14 142.8660 151.5432\n15 142.8660 151.5432\n16 142.8660 151.5432\n17 142.8660 151.5432\n18 142.8660 151.5432\n19 142.8660 151.5432\n20 142.8660 151.5432\n21 142.8660 151.5432\n22 142.8660 151.5432\n\n\n$`2`\n$`2`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 153.8708 139.5919 139.5919 139.5919 139.5919 139.5919 139.5919 139.5919\n [9] 139.5919 139.5919\n\n$`2`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 136.3778 127.1175\n14 115.8789 103.3260\n15 115.8789 103.3260\n16 115.8789 103.3260\n17 115.8789 103.3260\n18 115.8789 103.3260\n19 115.8789 103.3260\n20 115.8789 103.3260\n21 115.8789 103.3260\n22 115.8789 103.3260\n\n$`2`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 171.3638 180.6240\n14 163.3048 175.8577\n15 163.3048 175.8577\n16 163.3048 175.8577\n17 163.3048 175.8577\n18 163.3048 175.8577\n19 163.3048 175.8577\n20 163.3048 175.8577\n21 163.3048 175.8577\n22 163.3048 175.8577\n\n\n$`3`\n$`3`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 173.6413 170.0479 170.0479 170.0479 170.0479 170.0479 170.0479 170.0479\n [9] 170.0479 170.0479\n\n$`3`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 153.5404 142.8995\n14 146.6452 134.2565\n15 146.6452 134.2565\n16 146.6452 134.2565\n17 146.6452 134.2565\n18 146.6452 134.2565\n19 146.6452 134.2565\n20 146.6452 134.2565\n21 146.6452 134.2565\n22 146.6452 134.2565\n\n$`3`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 193.7423 204.3831\n14 193.4506 205.8393\n15 193.4506 205.8393\n16 193.4506 205.8393\n17 193.4506 205.8393\n18 193.4506 205.8393\n19 193.4506 205.8393\n20 193.4506 205.8393\n21 193.4506 205.8393\n22 193.4506 205.8393\n\n\n$`4`\n$`4`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 194.0074 194.0119 194.0147 194.0164 194.0174 194.0180 194.0184 194.0186\n [9] 194.0187 194.0188\n\n$`4`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 169.7973 156.9812\n14 165.6741 150.6730\n15 164.2944 148.5614\n16 163.8005 147.8051\n17 163.6201 147.5288\n18 163.5539 147.4272\n19 163.5296 147.3898\n20 163.5207 147.3761\n21 163.5175 147.3711\n22 163.5163 147.3692\n\n$`4`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 218.2176 231.0336\n14 222.3497 237.3509\n15 223.7350 239.4680\n16 224.2322 240.2276\n17 224.4146 240.5059\n18 224.4821 240.6088\n19 224.5071 240.6469\n20 224.5165 240.6611\n21 224.5200 240.6664\n22 224.5213 240.6684\n\n\n$`5`\n$`5`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 206.8929 210.7977 213.3851 215.0996 216.2356 216.9884 217.4872 217.8178\n [9] 218.0368 218.1819\n\n$`5`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 178.2600 163.1026\n14 176.4492 158.2662\n15 176.8082 157.4455\n16 177.5860 157.7275\n17 178.3181 158.2458\n18 178.8949 158.7294\n19 179.3167 159.1104\n20 179.6134 159.3893\n21 179.8176 159.5856\n22 179.9562 159.7208\n\n$`5`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 235.5258 250.6831\n14 245.1461 263.3291\n15 249.9620 269.3246\n16 252.6131 272.4716\n17 254.1531 274.2255\n18 255.0819 275.2475\n19 255.6578 275.8641\n20 256.0221 276.2462\n21 256.2559 276.4879\n22 256.4076 276.6430\n\n\n$`6`\n$`6`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 245.0709 240.0400 240.0400 240.0400 240.0400 240.0400 240.0400 240.0400\n [9] 240.0400 240.0400\n\n$`6`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 212.6687 195.5160\n14 196.9893 174.1996\n15 196.9893 174.1996\n16 196.9893 174.1996\n17 196.9893 174.1996\n18 196.9893 174.1996\n19 196.9893 174.1996\n20 196.9893 174.1996\n21 196.9893 174.1996\n22 196.9893 174.1996\n\n$`6`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 277.4731 294.6259\n14 283.0907 305.8803\n15 283.0907 305.8803\n16 283.0907 305.8803\n17 283.0907 305.8803\n18 283.0907 305.8803\n19 283.0907 305.8803\n20 283.0907 305.8803\n21 283.0907 305.8803\n22 283.0907 305.8803\n\n\n$`7`\n$`7`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 278.0001 278.0001 278.0002 278.0002 278.0002 278.0002 278.0002 278.0002\n [9] 278.0002 278.0002\n\n$`7`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 236.8903 215.1282\n14 228.5879 202.4307\n15 225.3145 197.4243\n16 223.9224 195.2953\n17 223.3147 194.3659\n18 223.0466 193.9559\n19 222.9278 193.7742\n20 222.8751 193.6936\n21 222.8516 193.6577\n22 222.8412 193.6418\n\n$`7`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 319.1098 340.8720\n14 327.4123 353.5695\n15 330.6859 358.5760\n16 332.0780 360.7051\n17 332.6857 361.6345\n18 332.9538 362.0445\n19 333.0726 362.2262\n20 333.1254 362.3069\n21 333.1488 362.3427\n22 333.1592 362.3587\n\n\n$`8`\n$`8`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 349.0540 373.2678 369.7906 348.0549 325.4487 315.1915 319.8599 332.7645\n [9] 344.2812 348.1670\n\n$`8`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 315.6225 297.9249\n14 322.1404 295.0752\n15 314.7795 285.6584\n16 292.8344 263.6024\n17 266.5768 235.4118\n18 252.9822 220.0505\n19 257.0954 223.8699\n20 269.7958 236.4622\n21 280.1875 246.2583\n22 283.2781 248.9280\n\n$`8`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 382.4855 400.1831\n14 424.3952 451.4604\n15 424.8018 453.9229\n16 403.2754 432.5074\n17 384.3206 415.4855\n18 377.4009 410.3325\n19 382.6243 415.8498\n20 395.7332 429.0668\n21 408.3750 442.3042\n22 413.0559 447.4061\n\n\n$`9`\n$`9`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 378.9729 406.5723 408.7509 392.6048 372.9147 361.5778 362.0569 370.2398\n [9] 379.1516 383.6927\n\n$`9`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 336.2126 313.5766\n14 342.0963 307.9648\n15 339.3660 302.6358\n16 323.1265 286.3469\n17 300.2319 261.7560\n18 285.7363 245.5882\n19 285.5516 245.0521\n20 293.6654 253.1294\n21 301.8675 260.9558\n22 305.8147 264.5885\n\n$`9`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 421.7333 444.3692\n14 471.0482 505.1797\n15 478.1359 514.8660\n16 462.0831 498.8627\n17 445.5975 484.0734\n18 437.4193 477.5674\n19 438.5622 479.0617\n20 446.8142 487.3503\n21 456.4356 497.3473\n22 461.5707 502.7968\n\n\n$`10`\n$`10`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 391.9249 381.5489 381.5489 381.5489 381.5489 381.5489 381.5489 381.5489\n [9] 381.5489 381.5489\n\n$`10`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 331.8921 300.1126\n14 304.6704 263.9734\n15 304.6704 263.9734\n16 304.6704 263.9734\n17 304.6704 263.9734\n18 304.6704 263.9734\n19 304.6704 263.9734\n20 304.6704 263.9734\n21 304.6704 263.9734\n22 304.6704 263.9734\n\n$`10`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 451.9577 483.7372\n14 458.4274 499.1244\n15 458.4274 499.1244\n16 458.4274 499.1244\n17 458.4274 499.1244\n18 458.4274 499.1244\n19 458.4274 499.1244\n20 458.4274 499.1244\n21 458.4274 499.1244\n22 458.4274 499.1244\n\n\n$`11`\n$`11`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 408.4203 410.7762 412.3990 413.5168 414.2868 414.8171 415.1824 415.4340\n [9] 415.6074 415.7268\n\n$`11`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 342.2241 307.1820\n14 330.3960 287.8452\n15 326.1006 280.4170\n16 324.5481 277.4509\n17 324.0788 276.3255\n18 324.0270 275.9656\n19 324.1175 275.9106\n20 324.2390 275.9632\n21 324.3506 276.0422\n22 324.4407 276.1168\n\n$`11`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 474.6165 509.6586\n14 491.1565 533.7072\n15 498.6974 544.3810\n16 502.4855 549.5827\n17 504.4948 552.2480\n18 505.6072 553.6686\n19 506.2474 554.4543\n20 506.6291 554.9049\n21 506.8641 555.1726\n22 507.0128 555.3367\n\n\n$`12`\n$`12`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 502.9998 476.0531 476.0531 476.0531 476.0531 476.0531 476.0531 476.0531\n [9] 476.0531 476.0531\n\n$`12`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 437.2687 402.4728\n14 387.1722 340.1214\n15 387.1722 340.1214\n16 387.1722 340.1214\n17 387.1722 340.1214\n18 387.1722 340.1214\n19 387.1722 340.1214\n20 387.1722 340.1214\n21 387.1722 340.1214\n22 387.1722 340.1214\n\n$`12`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 568.7308 603.5267\n14 564.9341 611.9848\n15 564.9341 611.9848\n16 564.9341 611.9848\n17 564.9341 611.9848\n18 564.9341 611.9848\n19 564.9341 611.9848\n20 564.9341 611.9848\n21 564.9341 611.9848\n22 564.9341 611.9848\n\ndev.off()\n\nnull device \n          1 \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-03/index.html",
    "href": "posts/rtip-2023-03-03/index.html",
    "title": "Simple examples of pmap() from {purrr}",
    "section": "",
    "text": "Introduction\nThe pmap() function in R is part of the purrr library, which is a package designed to make it easier to work with functions that operate on vectors, lists, and other types of data structures.\nThe pmap() function is used to apply a function to a list of arguments, where each element in the list contains the arguments for a single function call. The function is applied in parallel, meaning that each call is executed concurrently, which can help speed up computations when working with large datasets.\nHere is the basic syntax of the pmap() function:\n\npmap(.l, .f, ...)\n\nwhere:\n\n.l - is a list of arguments, where each element of the list contains the arguments for a single function call.\n.f - is the function to apply to the arguments in .l.\n... - is used to pass additional arguments to .f.\n\nThe pmap() function returns a list, where each element of the list contains the output of a single function call.\nLet’s define a function for an example.\n\n\nFunction\n\nmy_function <- function(a, b, c) {\n  # do something with a, b, and c\n  return(a + b + c)\n}\n\nA very simple function that just adds up the elements passed.\nNow let’s go over a couple simple examples.\n\n\nExample\n\nlibrary(purrr)\nlibrary(TidyDensity)\n\n\n# create a list of vectors with your arguments\nmy_args <- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(7, 8, 9)\n)\n\n# apply your function to each combination of arguments in parallel\nresults <- pmap(my_args, my_function)\n\n# print the results\nprint(results)\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 18\n\n\nNow lets see a couple more examples.\n\nargsl <- list(\n  c(100, 100, 100, 100), # this is .n\n  c(0,1,2,3),            # this is .mean\n  c(4,3,2,1),            # this is .sd\n  c(10,10,10,10)         # this is .num_sims\n)\n\npmap(argsl, tidy_normal)\n\n[[1]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx        dy     p      q\n   <fct>      <int>  <dbl> <dbl>     <dbl> <dbl>  <dbl>\n 1 1              1  3.56  -15.0 0.0000353 0.814  3.56 \n 2 1              2 -0.433 -14.6 0.0000679 0.457 -0.433\n 3 1              3 -1.93  -14.3 0.000125  0.315 -1.93 \n 4 1              4  1.68  -14.0 0.000219  0.663  1.68 \n 5 1              5  4.18  -13.7 0.000369  0.852  4.18 \n 6 1              6  0.805 -13.4 0.000596  0.580  0.805\n 7 1              7  7.99  -13.1 0.000922  0.977  7.99 \n 8 1              8 -1.61  -12.8 0.00137   0.344 -1.61 \n 9 1              9  1.83  -12.5 0.00195   0.676  1.83 \n10 1             10  6.66  -12.1 0.00267   0.952  6.66 \n# … with 990 more rows\n\n[[2]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx        dy      p      q\n   <fct>      <int>  <dbl> <dbl>     <dbl>  <dbl>  <dbl>\n 1 1              1 -0.335 -9.02 0.0000814 0.328  -0.335\n 2 1              2  2.00  -8.82 0.000162  0.630   2.00 \n 3 1              3 -0.238 -8.62 0.000304  0.340  -0.238\n 4 1              4  1.17  -8.41 0.000544  0.523   1.17 \n 5 1              5  1.50  -8.21 0.000921  0.567   1.50 \n 6 1              6  4.68  -8.01 0.00148   0.890   4.68 \n 7 1              7  4.59  -7.81 0.00227   0.884   4.59 \n 8 1              8 -1.18  -7.61 0.00331   0.233  -1.18 \n 9 1              9  2.35  -7.40 0.00460   0.673   2.35 \n10 1             10 -3.73  -7.20 0.00610   0.0574 -3.73 \n# … with 990 more rows\n\n[[3]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx       dy     p      q\n   <fct>      <int>  <dbl> <dbl>    <dbl> <dbl>  <dbl>\n 1 1              1  4.42  -3.98 0.000118 0.886  4.42 \n 2 1              2  2.24  -3.86 0.000211 0.547  2.24 \n 3 1              3 -0.207 -3.73 0.000369 0.135 -0.207\n 4 1              4  3.32  -3.61 0.000622 0.745  3.32 \n 5 1              5  0.999 -3.48 0.00101  0.308  0.999\n 6 1              6  4.08  -3.36 0.00160  0.851  4.08 \n 7 1              7  5.81  -3.23 0.00244  0.972  5.81 \n 8 1              8  6.11  -3.11 0.00362  0.980  6.11 \n 9 1              9  2.30  -2.98 0.00518  0.560  2.30 \n10 1             10  0.231 -2.86 0.00718  0.188  0.231\n# … with 990 more rows\n\n[[4]]\n# A tibble: 1,000 × 7\n   sim_number     x     y      dx       dy       p     q\n   <fct>      <int> <dbl>   <dbl>    <dbl>   <dbl> <dbl>\n 1 1              1 3.41  -0.635  0.000128 0.658   3.41 \n 2 1              2 0.415 -0.557  0.000243 0.00487 0.415\n 3 1              3 3.24  -0.479  0.000440 0.593   3.24 \n 4 1              4 3.73  -0.401  0.000758 0.768   3.73 \n 5 1              5 4.22  -0.324  0.00124  0.889   4.22 \n 6 1              6 3.70  -0.246  0.00193  0.757   3.70 \n 7 1              7 4.35  -0.168  0.00288  0.911   4.35 \n 8 1              8 1.50  -0.0899 0.00408  0.0672  1.50 \n 9 1              9 2.58  -0.0120 0.00551  0.336   2.58 \n10 1             10 3.41   0.0658 0.00713  0.661   3.41 \n# … with 990 more rows\n\npmap(argsl, tidy_normal) |>\n  map(tidy_autoplot)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-06/index.html",
    "href": "posts/rtip-2023-03-06/index.html",
    "title": "Simple examples of imap() from {purrr}",
    "section": "",
    "text": "Introduction\nThe imap() function is a powerful tool for iterating over a list or a vector while also keeping track of the index or names of the elements. This function applies a given function to each element of a list, along with the name or index of that element, and returns a new list with the results.\nThe imap() function takes two main arguments: x and .f. x is the list or vector to iterate over, and .f is the function to apply to each element. The .f function takes two arguments: x and i, where x is the value of the element and i is the index or name of the element.\n\n\nFunction\nHere is the imap() function.\n\nimap(.x, .f, ...)\n\nHere is the documentation from the function page:\n\n.x - A list or atomic vector.\n.f - A function, specified in one of the following ways:\n\nA named function, e.g. paste.\nAn anonymous function, e.g. (x, idx) x + idx or function(x, idx) x + idx.\nA formula, e.g. ~ .x + .y. You must use .x to refer to the current element and .y to refer to the current index. Only recommended if you require backward compatibility with older versions of R.\n\n... - Additional arguments passed on to the mapped function. We now generally recommend against using … to pass additional (constant) arguments to .f. Instead use a shorthand anonymous function:\n\n\n# Instead of\nx |> map(f, 1, 2, collapse = \",\")\n# do:\nx |> map(\\(x) f(x, 1, 2, collapse = \",\"))\n\nThis makes it easier to understand which arguments belong to which function and will tend to yield better error messages.\n\n\nExample\nHere’s an example of using imap() with a simple list of integers:\n\nlibrary(purrr)\n\n# create a list of integers\nmy_list <- list(1, 2, 3, 4, 5)\n\n# define a function to apply to each element of the list\nmy_function <- function(x, i) {\n  paste(\"The element at index\", i, \"is\", x)\n}\n\n# apply the function to each element of the list using imap()\nresult <- imap(my_list, my_function)\n\n# print the result\nprint(result)\n\n[[1]]\n[1] \"The element at index 1 is 1\"\n\n[[2]]\n[1] \"The element at index 2 is 2\"\n\n[[3]]\n[1] \"The element at index 3 is 3\"\n\n[[4]]\n[1] \"The element at index 4 is 4\"\n\n[[5]]\n[1] \"The element at index 5 is 5\"\n\n\nIn this example, we create a list of integers called my_list. We define a function called my_function that takes two arguments: x, which is the value of each element in the list, and i, which is the index of that element. We then use imap() to apply my_function to each element of my_list, passing both the value and the index of the element as arguments. The result is a new list where each element contains the output of my_function applied to the corresponding element of my_list.\nNow let’s take a look at a slightly more complex example. In this case, we will use imap() to iterate over a list of data frames, apply a function to each data frame that subsets the data to include only certain columns, and return a new list of data frames with the subsetted data.\n\n# create a list of data frames\nmy_list <- list(\n  data.frame(x = 1:5, y = c(\"a\", \"b\", \"c\", \"d\", \"e\")),\n  data.frame(x = 6:10, y = c(\"f\", \"g\", \"h\", \"i\", \"j\")),\n  data.frame(x = 11:15, y = c(\"k\", \"l\", \"m\", \"n\", \"o\"))\n)\n\n# define a function to apply to each element of the list\nmy_function <- function(df, i) {\n  # subset the data to include only the x column\n  df_subset <- df[, \"x\", drop = FALSE]\n  # rename the column to include the index of the element\n  colnames(df_subset) <- paste(\"x_\", i, sep = \"\")\n  # return the subsetted data frame\n  return(df_subset)\n}\n\n# apply the function to each element of the list using imap\nresult <- imap(my_list, my_function)\n\n# print the result\nprint(result)\n\n[[1]]\n  x_1\n1   1\n2   2\n3   3\n4   4\n5   5\n\n[[2]]\n  x_2\n1   6\n2   7\n3   8\n4   9\n5  10\n\n[[3]]\n  x_3\n1  11\n2  12\n3  13\n4  14\n5  15\n\n\nIn this example, we create a list of three data frames called my_list. We define a function called my_function that takes two arguments: df, which is the value of each element in the list (a data frame), and i, which is the index of that element. The function subsets the data frame to include only the x column, renames the column to include the index of the element, and returns the subsetted data frame.\nWe use imap() to apply my_function to each element of my_list, passing both the data frame and the index of the element as arguments. The result is a new list of data frames, where each data frame contains only the x column from the original data frame, with a new name that includes the index of the element.\nAs you can see, the output is a list of three data frames, each containing only the x column from the corresponding original data frame, with a new name that includes the index of the element.\nIn summary, the imap() function from the R library purrr is a useful tool for iterating over a list or a vector while also keeping track of the index or names of the elements. The function takes a list or a vector as its first argument, and a function as its second argument, which takes two arguments: the value of each element, and the index or name of that element. The function returns a new list or vector with the results of applying the function to each element of the original list or vector. This function is particularly useful for complex data structures, where the index or name of each element is important for further data analysis or processing.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-07/index.html",
    "href": "posts/rtip-2023-03-07/index.html",
    "title": "tidy_bernoulli() with {data.table}",
    "section": "",
    "text": "Introduction\nSo I was challanged by Adrian Antico to learn data.table, so yesterday I started with a single function from my package {TidyDensity} called tidy_bernoulli().\nSo let’s see how I did (hint, works but needs a lot of improvement, so I’ll learn it.)\n\n\nFunction\nLet’s see the function in data.table\n\nlibrary(data.table)\nlibrary(tidyr)\nlibrary(stats)\nlibrary(purrr)\n\nnew_func <- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data <- data.table(sim_number = factor(seq(1, num_sims, 1)))\n  \n  # Group the data by sim_number and add columns for x and y\n  sim_data[, `:=` (\n    x = list(1:n),\n    y = list(stats::rbinom(n = n, size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, `:=` (\n    d = list(density(unlist(y), n = n)[c(\"x\", \"y\")] |>\n      set_names(\"dx\", \"dy\") |>\n      as_tibble())\n  ), by = sim_number]\n  \n  # Compute the p-values for the y values and add a column for p\n  sim_data[, `:=` (\n    p = list(stats::pbinom(unlist(y), size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, `:=` (\n    q = list(stats::qbinom(unlist(p), size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Unnest the columns for x, y, d, p, and q\n  sim_data <- sim_data[, \n                       unnest(\n                         .SD, \n                         cols = c(\"x\", \"y\", \"d\", \"p\", \"q\")\n                         ), \n                       by = sim_number]\n  \n  # Remove the grouping\n  sim_data[, sim_number := as.factor(sim_number)]\n  \n  return(sim_data)\n}\n\n\n\nExample\nNow, let’s see the output of the original function tidy_bernoulli() and new_func().\n\nlibrary(TidyDensity)\nn <- 50\npr <- 0.1\nsims <- 5\n\nset.seed(123)\ntb <- tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n\nset.seed(123)\nnf <- new_func(n = n, num_sims = sims, pr = pr)\n\nprint(tb)\n\n# A tibble: 250 × 7\n   sim_number     x     y      dx     dy     p     q\n   <fct>      <int> <int>   <dbl>  <dbl> <dbl> <dbl>\n 1 1              1     0 -0.405  0.0292   0.9     0\n 2 1              2     0 -0.368  0.0637   0.9     0\n 3 1              3     0 -0.331  0.129    0.9     0\n 4 1              4     0 -0.294  0.243    0.9     0\n 5 1              5     1 -0.258  0.424    1       1\n 6 1              6     0 -0.221  0.688    0.9     0\n 7 1              7     0 -0.184  1.03     0.9     0\n 8 1              8     0 -0.147  1.44     0.9     0\n 9 1              9     0 -0.110  1.87     0.9     0\n10 1             10     0 -0.0727 2.25     0.9     0\n# … with 240 more rows\n\nprint(nf)\n\n     sim_number  x y         dx          dy   p q\n  1:          1  1 0 -0.4053113 0.029196114 0.9 0\n  2:          1  2 0 -0.3683598 0.063683226 0.9 0\n  3:          1  3 0 -0.3314083 0.129227066 0.9 0\n  4:          1  4 0 -0.2944568 0.242967496 0.9 0\n  5:          1  5 1 -0.2575054 0.424395426 1.0 1\n ---                                             \n246:          5 46 0  1.2575054 0.057872104 0.9 0\n247:          5 47 0  1.2944568 0.033131931 0.9 0\n248:          5 48 1  1.3314083 0.017621873 1.0 1\n249:          5 49 1  1.3683598 0.008684076 1.0 1\n250:          5 50 0  1.4053113 0.003981288 0.9 0\n\n\nOk so at least the output is identical which is a good sign. Now let’s benchmark the two solutions.\n\nlibrary(rbenchmark)\nlibrary(dplyr)\n\nbenchmark(\n  \"original\" = {\n    tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n  },\n  \"data.table\" = {\n    new_func(n = n, pr = pr, num_sims = sims)\n  },\n  replications = 100,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) |>\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1   original          100    3.29     1.00      2.51     0.08\n2 data.table          100    4.64     1.41      3.34     0.04\n\n\nYeah, needs some work but it’s a start."
  },
  {
    "objectID": "posts/rtip-2023-03-08/index.html",
    "href": "posts/rtip-2023-03-08/index.html",
    "title": "Getting NYS Home Heating Oil Prices with {rvest}",
    "section": "",
    "text": "Introduction\nIf you live in New York and rely on heating oil to keep your home warm during the colder months, you know how important it is to keep track of heating oil prices. Fortunately, with a bit of R code, you can easily access the latest heating oil prices in New York.\nThe code uses the {dplyr} package to clean and manipulate the data, as well as the {timetk} package to plot the time series. Here’s a breakdown of what the code does:\n\nFirst, it loads the necessary packages and sets the URL for the data source.\nNext, it reads the HTML from the URL using the read_html function from the xml2 package.\nIt then uses the html_node function from the rvest package to extract the HTML node that contains the data table.\n\nThe resulting data table is then cleaned and transformed using dplyr functions such as html_table, as_tibble, set_names, select, mutate, and arrange.\nFinally, the resulting time series data is plotted using plot_time_series from the timetk package.\nTo run this code, you will need to have these packages installed on your machine. You can install them using the install.packages function in R. Here’s how you can install the packages:\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"xml2\")\ninstall.packages(\"rvest\")\ninstall.packages(\"tibble\")\ninstall.packages(\"purrr\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"timetk\")\n\nOnce you have installed the packages, you can copy and paste the code into your R console or RStudio and run it to get the latest heating oil prices in New York.\nIn conclusion, the code above provides a simple and efficient way to access and visualize heating oil prices in New York using R. By keeping track of these prices, you can make informed decisions about when to buy heating oil and how much to purchase, ultimately saving you money on your heating bills.\n\n\nExample\nNow let’s run it!\n\nurl  <- \"https://www.eia.gov/opendata/qb.php?sdid=PET.W_EPD2F_PRS_SNY_DPG.W\"\npage <- xml2::read_html(url)\nnode <- rvest::html_node(\n    x = page\n    , xpath = \"/html/body/div[1]/section/div/div/div[2]/div[1]/table\"\n)\nny_tbl <- node |>\n    rvest::html_table() |>\n    tibble::as_tibble() |>\n    purrr::set_names('series_name','period','frequency','value','units') |>\n    dplyr::select(period, frequency, value, units, series_name) |>\n    dplyr::mutate(period = lubridate::ymd(period)) |>\n    dplyr::arrange(period)\n\nny_tbl |>\n    timetk::plot_time_series(.date_var = period, .value = value)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-09/index.html",
    "href": "posts/rtip-2023-03-09/index.html",
    "title": "Multiple Solutions to speedup tidy_bernoulli() with {data.table}",
    "section": "",
    "text": "Introduction\nI had just recently posted on making an attempt to speedup computations with my package {TidyDensity} using a purely data.table solution, yes of course I can use {dtplyr} or {tidytable} but that not the challenge put to me.\nMy original attempt was worse than the original solution of tidy_bernoulli(). After I posted on Mastadon, LinkedIn and Reddit, I recieved potential solutions from each site by users. Let’s check them out below.\n\n\nFunction\nFirst let’s load in the necessary libraries.\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(rbenchmark)\nlibrary(TidyDensity)\n\nNow let’s look at the different solutions.\n\n# My original new function\nnew_func <- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data <- data.table(sim_number = factor(seq(1, num_sims, 1)))\n\n  # Group the data by sim_number and add columns for x and y\n  sim_data[, `:=` (\n    x = list(1:n),\n    y = list(stats::rbinom(n = n, size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, `:=` (\n    d = list(density(unlist(y), n = n)[c(\"x\", \"y\")] |>\n               set_names(\"dx\", \"dy\") |>\n               as_tibble())\n  ), by = sim_number]\n\n  # Compute the p-values for the y values and add a column for p\n  sim_data[, `:=` (\n    p = list(stats::pbinom(unlist(y), size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, `:=` (\n    q = list(stats::qbinom(unlist(p), size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Unnest the columns for x, y, d, p, and q\n  sim_data <- sim_data[,\n                       unnest(\n                         .SD,\n                         cols = c(\"x\", \"y\", \"d\", \"p\", \"q\")\n                       ),\n                       by = sim_number]\n\n  # Remove the grouping\n  sim_data[, sim_number := as.factor(sim_number)]\n\n  return(sim_data)\n}\n\nreddit_func <- function(num_sims, n, pr) {\n  sim_dat <- data.table(sim_number = rep(1:num_sims,each=n),\n                        x          = rep(1:n,num_sims))\n\n  sim_dat[, y := stats::rbinom(n = n, size = 1, prob = pr), by=sim_number]\n  sim_dat[, c(\"dx\",\"dy\") := density(y,n=n)[c(\"x\",\"y\")]    , by=sim_number]\n  sim_dat[, p := stats::pbinom(y, size = 1, prob = pr)    , by=sim_number]\n  sim_dat[, q := stats::qbinom(p, size = 1, prob = pr)    , by=sim_number]\n  \n  return(sim_dat)\n}\n\nmastadon_func <- function(num_sims, n, pr){\n  sim_data <- data.table(sim_number = 1:num_sims\n  )[, `:=`( x = .(1:n), y= .(rbinom(n = n, size = 1, prob = pr))), sim_number\n  ][, `:=`( d = .(density(unlist(y), n = n)[c('x','y')] |> \n                    as.data.table() |> \n                    setnames(c('dx','dy'))\n                  )\n            ), sim_number\n  ][, `:=`( p = .(pbinom(unlist(y), size = 1, prob = pr))), sim_number\n  ][, `:=`( q = .(qbinom(unlist(p), size = 1, prob = pr))), sim_number]\n\n    cbind(\n      sim_data[, lapply(.SD, unlist), by = sim_number, .SDcol = c('x','y','p','q')],\n      rbindlist(sim_data$d)\n    ) |>\n    setcolorder(c('sim_number','x','y','dx','dy'))\n    \n    return(sim_data)\n}\n\nlinkedin_func <- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data <- CJ(sim_number = factor(1:num_sims), x = 1:n)\n\n  # Group the data by sim_number and add columns for x and y\n  sim_data[, y := stats::rbinom(n = .N, size = 1, prob = pr)]\n\n\n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, c(\"dx\", \"dy\") := density(y, n = n)[c(\"x\", \"y\")], by = sim_number]\n\n  # Compute the p-values for the y values and add a column for p\n  sim_data[, p := stats::pbinom(y, size = 1, prob = pr)]\n\n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, q := stats::qbinom(p, size = 1, prob = pr)]\n  setkey(sim_data, NULL) # needed only to compare with new_func\n  return(sim_data)\n}\n\nAll of the functions work in the same set of three arguments as input: * num_sims: an integer value that specifies the number of simulations to run * n: an integer value that specifies the sample size * pr: a numeric value that specifies the probability of success\nThe functions use the data.table package to create a data table named sim_dat/sim_data. The data table has two columns: sim_number and x. The sim_number column represents the simulation number, and x column represents the observation number.\nThe functions then generate random binary data using the rbinom function from the stats package. The function generates n binary data points for each simulation number (sim_number) using the input parameter pr as the probability of success. The resulting binary data points are stored in the y column of sim_dat/data.\nNext, the function calculates the density of y using the density function from the stats package. The function calculates the density separately for each simulation number (sim_number) and stores the resulting values in the dx and dy columns of sim_dat/data.\nThe functions then calculate the cumulative probability (p) of each binary data point using the pbinom function from the stats package. The function calculates the cumulative probability separately for each simulation number (sim_number) and stores the resulting values in the p column of sim_dat.\nFinally, the functions calculate the inverse of the cumulative probability (q) using the qbinom function from the stats package. The function calculates the inverse of the cumulative probability separately for each simulation number (sim_number) and stores the resulting values in the q column of sim_dat.\nThe functions then return the data table containing the results of the simulations.\n\n\nExample\nHow do they stack up to each other? Lets see!\n\nn <- 50\npr <- 0.1\nnum_sims <- sims <- 5\n\nbenchmark(\n  \"tidy_bernoulli()\" = {\n    tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n  },\n  \"my.first.attempt\" = {\n    new_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"linkedin.attempt\" = {\n    linkedin_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"mastadon.attempt\" = {\n    mastadon_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"reddit.attempt\" = {\n    reddit_func(n = n, pr = pr, num_sims = sims)\n  },\n  replications = 200,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) |>\n  arrange(relative)\n\n              test replications elapsed relative user.self sys.self\n1 linkedin.attempt          200    0.84    1.000      0.64     0.00\n2   reddit.attempt          200    0.86    1.024      0.69     0.03\n3 mastadon.attempt          200    1.57    1.869      1.17     0.08\n4 tidy_bernoulli()          200    6.47    7.702      4.68     0.11\n5 my.first.attempt          200    8.82   10.500      6.67     0.01\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-10/index.html",
    "href": "posts/rtip-2023-03-10/index.html",
    "title": "Use of the apply family of functions",
    "section": "",
    "text": "In this post I will talk about the use of the R functions apply(), lapply(), sapply(), tapply(), and vapply() with examples.\nThese functions are all designed to help users apply a function to a set of data in R, but they differ in their input and output types, as well as in the way they handle missing values and other complexities. By using the right function for your particular problem, you can make your code more efficient and easier to read.\nLet’s start with the basics.\n\n\nBefore we dive into the details of each function, let’s define some terms:\n\nA vector is a one-dimensional array of data, like a list of numbers or strings.\nA matrix is a two-dimensional array of data, like a table of numbers.\nA data frame is a two-dimensional object that can hold different types of data, like a spreadsheet.\nA list is a collection of objects, which can be of different types, like a shopping bag full of different items.\n\nEach of the five functions we’ll discuss here takes a list as input (although some can also take vectors or matrices). Let’s create a list object to use in our examples:\n\nmy_list <- list(\n  a = c(1, 2, 3),\n  b = matrix(1:6, nrow = 2),\n  c = data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\")),\n  d = c(4, NA, 6),\n  e = list(\"foo\", \"bar\", \"baz\")\n)\n\nThis list contains five elements:\n\nA vector of numbers (a)\nA matrix of numbers (b)\nA data frame with two columns (c)\nA vector of numbers with a missing value (d)\nA list of character strings (e)\n\nNow that we have our data, let’s look at each of the functions in turn.\n\n\n\napply()\nThe apply() function applies a function to the rows or columns of a matrix or array. It is most commonly used with matrices, but can also be used with higher-dimensional arrays. The function takes three arguments:\n\nThe matrix or array to apply the function to\nThe margin (1 for rows, 2 for columns, or a vector of dimensions)\nThe function to apply\n\nLet’s apply the mean() function to the columns of our matrix in my_list$b:\n\napply(my_list$b, 2, mean)\n\n[1] 1.5 3.5 5.5\n\n\nThis will return a vector of means for each column of the matrix\nlapply()\nThe lapply() function applies a function to each element of a list and returns a list of the results. It takes two arguments:\n\nThe list to apply the function to\nThe function to apply\n\nLet’s apply the class() function to each element of our list:\n\nlapply(my_list, class)\n\n$a\n[1] \"numeric\"\n\n$b\n[1] \"matrix\" \"array\" \n\n$c\n[1] \"data.frame\"\n\n$d\n[1] \"numeric\"\n\n$e\n[1] \"list\"\n\n\nThis will return a list of the classes of each element.\nsapply()\nThe sapply() function is similar to lapply(), but it simplifies the output to a vector or matrix if possible. It takes the same two arguments as lapply():\n\nThe list to apply the function\nThe function to apply\n\nLet’s apply the length() function to each element of our list using sapply():\n\nsapply(my_list, length)\n\na b c d e \n3 6 2 3 3 \n\n\nThis will return a vector of lengths for each element.\ntapply()\nThe tapply() function applies a function to subsets of a vector or data frame, grouped by one or more factors. It takes three arguments:\n\nThe vector or data frame to apply the function to\nThe factor(s) to group the data by\nThe function to apply\n\nLet’s apply the mean() function to the elements of our vector my_list$d, grouped by whether they are missing or not:\n\ntapply(my_list$d, !is.na(my_list$d), mean)\n\nFALSE  TRUE \n   NA     5 \n\n\nThis will return a vector of means for each group where they are NOT NA.\nvapply()\nThe vapply() function is similar to sapply(), but allows the user to specify the output type and length, making it more efficient and less prone to errors. It takes four arguments:\n\nThe list to apply the function to\nThe function to apply\nThe output type of the function\nThe length of the output vector or matrix\n\nLet’s apply the length() function to each element of our list, specifying that the output type is an integer and the length is 1:\n\nvapply(my_list, length, integer(1))\n\na b c d e \n3 6 2 3 3 \n\n\nThis will return a matrix of lengths for each element, with 1 row:"
  },
  {
    "objectID": "posts/rtip-2023-03-17/index.html",
    "href": "posts/rtip-2023-03-17/index.html",
    "title": "Use of the apply family of functions",
    "section": "",
    "text": "I am thrilled to announce that the R universe of packages {healthyverse} has surpassed 60,000 downloads! Thank you to everyone who has downloaded and used these packages, your support is greatly appreciated.\nFor those who are not familiar, the {healthyverse} package is a collection of R packages focused on data science and analysis with an emphasis on healthcare. These packages are ever evolving and for the most part still in an experimental stage, but are maturing. These packages are:\n\n{healthyR}\n{healthyR.ts}\n{healthyR.ai}\n{healthyR.data}\n{TidyDensity}\n{tidyAML}\n\nI am continuously working on updates and improvements to the {healthyverse} package, and I hope to release them soon. Some of the updates include bug fixes, new functionality, and enhancements to existing functions.\nAdditionally, I want to encourage users who are interested in contributing to the {healthyverse} package to submit pull requests. Contributions can be in the form of bug fixes, new functions, or enhancements to existing ones. I am always open to feedback and suggestions on how to improve these packages.\nOnce again, thank you to everyone who has downloaded and used the {healthyverse} package. Your support motivates me to continue working on this project and making it the best it can be.\n\n\n\n60k"
  },
  {
    "objectID": "posts/rtip-2023-03-21/index.html",
    "href": "posts/rtip-2023-03-21/index.html",
    "title": "Getting the CCI30 Index Current Makeup",
    "section": "",
    "text": "Introduction\nThe CCI30 Crypto Index is a cryptocurrency index that tracks the performance of the top 30 cryptocurrencies by market capitalization. It was created in 2017 by a team of researchers and analysts from the CryptoCompare and MVIS indices.\nThe CCI30 Crypto Index is designed to provide a broad-based and representative measure of the cryptocurrency market’s overall performance. It includes a diverse range of cryptocurrencies, such as Bitcoin, Ethereum, Litecoin, Ripple, and many others. The index is weighted by market capitalization, with each cryptocurrency’s weight determined by its market capitalization relative to the total market capitalization of all 30 cryptocurrencies.\nThe CCI30 Crypto Index has become a popular benchmark for the cryptocurrency market, as it offers a comprehensive view of the market’s performance, rather than just focusing on one particular cryptocurrency. It is often used by investors, traders, and researchers to analyze trends and make investment decisions.\nOne notable feature of the CCI30 Crypto Index is that it is rebalanced every quarter. This means that the composition of the index is adjusted to reflect changes in the market capitalization of the constituent cryptocurrencies. This helps to ensure that the index remains representative of the overall cryptocurrency market.\nOverall, the CCI30 Crypto Index provides a useful tool for tracking the performance of the cryptocurrency market. It is a valuable resource for investors, traders, and researchers who are interested in this exciting and rapidly evolving field.\n\n\nCode Explanation\nLet’s break it down step by step:\n\nThe first line of the code loads the “dplyr” package, which provides a set of functions for data manipulation.\nThe second line of the code reads the HTML code from the website “https://cci30.com/” using the “read_html” function from the “xml2” package.\nThe next two blocks of code extract two tables from the HTML document using the “html_node” function from the “rvest” package. The tables are located at two different XPaths in the HTML document.\nThe extracted tables are then converted into tibbles using the “as_tibble” function from the “tibble” package. The tibbles are further transformed by selecting only the columns from the second to the fifth column using the “select” function from the “dplyr” package.\nThe column names of the tibbles are then set using the “set_names” function from the “purrr” package.\nFinally, the two tibbles are combined using the “union” function from the “dplyr” package, and the resulting tibble is printed to the console.\n\nIn summary, the code is extracting two tables from a website, transforming them into tibbles, selecting a subset of columns, renaming the columns, and combining them into a single tibble.\n\n\nExample\n\ncci30 <- xml2::read_html(\"https://cci30.com/\")\n\ntbl1 <- cci30 |>\n    rvest::html_node(xpath = \"/html/body/div[2]/div/div/div/div[2]/div[1]/table\") |>\n    rvest::html_table(header = 1) |>\n    tibble::as_tibble() |>\n    dplyr::select(2:5) |>\n    purrr::set_names(\n        \"Coin\",\"Price\",\"Mkt Cap\",\"Daily Change\"\n    )\n\ntbl2 <- cci30 |>\n    rvest::html_node(xpath = \"/html/body/div[2]/div/div/div/div[2]/div[2]/table\") |>\n    rvest::html_table(header = 1) |>\n    tibble::as_tibble() |>\n    dplyr::select(2:5) |>\n    purrr::set_names(\n        \"Coin\",\"Price\",\"Mkt Cap\",\"Daily Change\"\n    )\n\ntbl <- tbl1 |>\n  dplyr::union(tbl2) |>\n  knitr::kable()\n\ntbl\n\n\n\n\nCoin\nPrice\nMkt Cap\nDaily Change\n\n\n\n\nBitcoin\n$27,767.24\n$536,553,055,078\n0.17%\n\n\nEthereum\n$1,735.32\n$212,357,972,798\n0.04%\n\n\nBNB\n$332.92\n$52,565,516,823\n0.13%\n\n\nXRP\n$0.37\n$19,087,613,742\n0.13%\n\n\nCardano\n$0.33\n$11,547,419,916\n0.05%\n\n\nPolygon\n$1.10\n$9,643,536,324\n0.17%\n\n\nDogecoin\n$0.07\n$9,484,198,878\n0.34%\n\n\nSolana\n$22.18\n$8,507,167,040\n0.12%\n\n\nPolkadot\n$6.10\n$7,119,808,610\n0.08%\n\n\nShiba Inu\n$0.00\n$6,169,390,592\n0.24%\n\n\nTRON\n$0.07\n$5,936,468,687\n0.14%\n\n\nLitecoin\n$78.42\n$5,685,409,727\n0.40%\n\n\nAvalanche\n$16.64\n$5,418,625,875\n0.06%\n\n\nUniswap\n$6.19\n$4,716,487,304\n0.19%\n\n\nChainlink\n$7.06\n$3,649,558,739\n0.06%\n\n\nCosmos\n$11.56\n$3,309,216,299\n0.03%\n\n\nUNUS SED LEO\n$3.35\n$3,195,413,769\n-0.55%\n\n\nToncoin\n$2.38\n$2,907,590,168\n-0.62%\n\n\nMonero\n$151.58\n$2,767,118,876\n-0.01%\n\n\nEthereum Classic\n$19.58\n$2,741,016,944\n-6.84%\n\n\nOKB\n$44.34\n$2,660,455,327\n0.56%\n\n\nBitcoin Cash\n$130.60\n$2,526,173,508\n-0.48%\n\n\nStellar\n$0.09\n$2,293,156,708\n-0.04%\n\n\nCronos\n$0.07\n$1,787,408,658\n1.05%\n\n\nNEAR Protocol\n$2.00\n$1,728,135,015\n0.26%\n\n\nVeChain\n$0.02\n$1,665,251,562\n0.33%\n\n\nQuant\n$126.33\n$1,525,183,149\n0.31%\n\n\nInternet Computer\n$5.11\n$1,516,076,264\n0.19%\n\n\nAlgorand\n$0.21\n$1,498,361,340\n0.41%\n\n\nApeCoin\n$4.06\n$1,496,070,125\n0.12%"
  },
  {
    "objectID": "posts/rtip-2023-03-22/index.html",
    "href": "posts/rtip-2023-03-22/index.html",
    "title": "Some Examples of Cumulative Mean with {TidyDensity}",
    "section": "",
    "text": "Introduction\nCumulative mean is a statistical measure that calculates the mean of a set of numbers up to a certain point in time or after a certain number of observations. It is also known as a running average or moving average.\nCumulative mean can be useful in a variety of contexts. For example:\n\nTracking progress: Cumulative mean can be used to track progress over time. For instance, a teacher might use it to track the average test scores of her students throughout the school year.\nAnalyzing trends: Cumulative mean can help identify trends in data. For example, a business might use it to track the average revenue generated by a new product over the course of several months.\nSmoothing data: Cumulative mean can be used to smooth out fluctuations in data. For instance, a meteorologist might use it to calculate the average temperature over the course of a year, which would help to smooth out the effects of daily temperature fluctuations.\n\nIn summary, cumulative mean is a useful statistical measure that can help track progress, analyze trends, and smooth out fluctuations in data.\n\n\nFunction\nThe function we will review is cmean() from the {TidyDensity} R package. Let’s take a look at it.\n\ncmean()\n\nThe only argument is .x which is a numeric vector as this is a vectorized function. Let’s see it in use.\n\n\nExample\nFirst let’s load in TidyDensity\n\nlibrary(TidyDensity)\n\nOk now let’s make some data. For this we are going to use the simple rnorm() function.\n\nx <- rnorm(100)\n\nhead(x)\n\n[1] -0.8293250 -1.2983499  2.2782337 -0.1521549  0.6859169  0.3809020\n\n\nOk, now that we have our vector, let’s run it through the function and see what it outputs and then we will graph it.\n\ncmx <- cmean(x)\nhead(cmx)\n\n[1] -0.8293249774 -1.0638374319  0.0501862766 -0.0003990095  0.1368641726\n[6]  0.1775371452\n\n\nNow let’s graph it.\n\nplot(cmx, type = \"l\")\n\n\n\n\nOk nice, so can we do this on grouped data or lists of data? Of course! First let’s use a for loop to generate a list of rnorm() values.\n\n# Initialize an empty list to store the generated values\nmy_list <- list()\n\n# Generate values using rnorm(5) in a for loop and store them in the list\nfor (i in 1:5) {\n  my_list[[i]] <- rnorm(100)\n}\n\n# Print the generated list\npurrr::map(my_list, head)\n\n[[1]]\n[1] -0.8054353 -0.4596541 -0.2362475  1.1486398 -0.7242154  0.5184610\n\n[[2]]\n[1]  0.3243327  0.7170802 -0.5963424 -1.0307104  0.3388504  0.5717486\n\n[[3]]\n[1]  1.7360816 -1.0359467 -0.3206138 -1.2157684 -0.8841356  0.1856481\n\n[[4]]\n[1] -1.1401642 -0.4437817 -0.2555245 -0.1809040 -0.2131763 -0.1251750\n\n[[5]]\n[1]  0.08835903 -1.79153379 -2.15010900  0.67344844  1.06125849  0.99848796\n\n\nNow that we have our list object let’s go ahead and plot the values out after we pass the data through cmean().\n\nlibrary(purrr)\n\nmy_list |>\n  map(\\(x) x |> cmean() |> plot(type = \"l\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n\nFrom here I think it is easy to see how one could do this on gruoped data as well with dplyr’s group_by()."
  },
  {
    "objectID": "posts/rtip-2023-03-24/index.html",
    "href": "posts/rtip-2023-03-24/index.html",
    "title": "How fast do the files read in?",
    "section": "",
    "text": "I will demonstrate how to generate a 1,000 row and column matrix with random numbers in R, and then save it in different file formats. I will also show how to get the file size of each saved object and benchmark how long it takes to read in each file using different functions.\n\n\nTo generate a 1,000 row and column matrix with random numbers, we can use the matrix() function and the runif() function in R. Here’s the code to generate the matrix:\n\n# set seed for reproducibility\nset.seed(123)\n\n# number of rows/columns in matrix\nn <- 1000\n\n# generate matrix with random normal values\nmat <- matrix(runif(n^2), nrow = n) \n\nThis code sets the random number generator seed to ensure that the same random numbers are generated every time the code is run. It then generates a vector of 1,000^2 random numbers using the runif() function, and creates a matrix with 1,000 columns using the matrix() function.\n\n\n\nWe can save the generated matrix in different file formats using different functions in R. Here are the functions we will use for each file format:\n\nCSV: write.csv()\nRDS: saveRDS()\nFST: write_fst()\nArrow: write_feather()\n\nHere’s the code to save the matrix in each file format:\n\nlibrary(fst)\nlibrary(arrow)\n\n# Save matrix in different file formats\nwrite.csv(mat, \"matrix.csv\", row.names=FALSE)\nsaveRDS(mat, \"matrix.rds\")\nwrite_fst(as.data.frame(mat), \"matrix.fst\")\nwrite_feather(as_arrow_table(as.data.frame(mat)), \"matrix.arrow\")\n\nThis code saves the matrix in each file format using the corresponding function, with the file name specified as the second argument. Getting the file size of each saved object\nTo get the file size of each saved object, we can use the file.size() function in R. Here’s the code to get the file size of each saved object:\n\n# Get file size of each saved object\ncsv_size <- file.size(\"matrix.csv\")  / (1024^2)\nrds_size <- file.size(\"matrix.rds\") / (1024^2)\nfst_size <- file.size(\"matrix.fst\") / (1024^2)\narrow_size <- file.size(\"matrix.arrow\") / (1024^2)\n\n# Print file size in human-readable format\nprint(paste(\"CSV file size in MB:\", format(csv_size, units=\"auto\")))\n\n[1] \"CSV file size in MB: 17.17339\"\n\nprint(paste(\"RDS file size in MB:\", format(rds_size, units=\"auto\")))\n\n[1] \"RDS file size in MB: 5.079627\"\n\nprint(paste(\"FST file size in MB:\", format(fst_size, units=\"auto\")))\n\n[1] \"FST file size in MB: 7.700841\"\n\nprint(paste(\"Arrow file size in MB:\", format(arrow_size, units=\"auto\")))\n\n[1] \"Arrow file size in MB: 6.705355\"\n\n\nThis code uses the file.size() function to get the file size of each object, and stores the file size of each object in a separate variable.\nFinally, it prints the file size of each object in a human-readable format using the format() function with the units=“auto” argument. The units=“auto” argument automatically chooses the most appropriate unit (e.g., KB, MB, GB) based on the file size.\n\n\n\nTo benchmark how long it takes to read in each file, we can use the {rbenchmark} package in R. In this example, we will compare the read times for the CSV file using four different functions: read.csv(), read_csv() from the {readr} package, fread() from the {data.table} package, and vroom() from the {vroom} package. We will also benchmark the read times for the RDS file using readRDS(), the FST file using read_fst(), and the Arrow file using read_feather().\nHere’s the code to benchmark the read times:\n\n# Load rbenchmark package\nlibrary(rbenchmark)\nlibrary(readr)\nlibrary(data.table)\nlibrary(vroom)\nlibrary(dplyr)\n\nn = 30\n\n# Benchmark read times for CSV file\nbenchmark(\n  # CSV File\n  \"read.csv\" = {\n    a <- read.csv(\"matrix.csv\")\n  },\n  \"read_csv\" = {\n    b <- read_csv(\"matrix.csv\")\n  },\n  \"fread\" = {\n    c <- fread(\"matrix.csv\")\n  },\n  \"vroom alltrep false\" = {\n    d <- vroom(\"matrix.csv\")\n  },\n  \"vroom alltrep true\" = {\n    dd <- vroom(\"matrix.csv\", altrep = TRUE)\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               fread           30    1.35    1.000      0.90     0.20\n2  vroom alltrep true           30    6.59    4.881      3.58     1.71\n3 vroom alltrep false           30    6.62    4.904      3.43     1.62\n4            read.csv           30   33.86   25.081     26.15     0.22\n5            read_csv           30   82.39   61.030     20.39     3.47\n\n# RDS File\nbenchmark(\n  # RDS File\n  \"readRDS\" = {\n    e <- readRDS(\"matrix.rds\")\n  },\n  \"read_rds\" = {\n    f <- read_rds(\"matrix.rds\")\n  },\n  \n  # Repications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n      test replications elapsed relative user.self sys.self\n1 read_rds           30    0.95    1.000      0.74     0.01\n2  readRDS           30    0.97    1.021      0.74     0.02\n\n# FST / Arrow\nbenchmark(\n  # FST\n  \"read_fst\" = {\n    g <- read_fst(\"matrix.fst\")\n  },\n  \n  # Arrow\n  \"arrow\" = {\n    h <- read_feather(\"matrix.arrow\")\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n      test replications elapsed relative user.self sys.self\n1 read_fst           30    0.21    1.000      0.05     0.12\n2    arrow           30    3.00   14.286      1.60     0.11\n\n\nThis code loads the {rbenchmark} package, and uses the benchmark() function to compare the read times for each file format. We specify the function to use for each file format, and set the number of replications to 10. Conclusion\nIn this blog post, we demonstrated how to generate a large matrix with random numbers in R, and how to save it in different file formats. We also showed how to get the file size of each saved object, and benchmarked the read times for each file format using different functions.\nThis example demonstrates the importance of choosing the appropriate file format and read function for your data. Depending on the size of your data and the requirements of your analysis, some file formats and functions may be more efficient than others."
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html",
    "href": "posts/rtip-2023-03-27/index.html",
    "title": "How fast does a compressed file in?",
    "section": "",
    "text": "I received an email over the weekend in regards to my last post not containing the reading in of gz compressed file(s) for the benchmarking. While this was not an over site per-se it was a good reminder that people would probably be interested in seeing this as well.\nBenchmarking is the process of measuring and comparing the performance of different programs, tools, or configurations in order to identify which one is the most efficient for a specific task. It is a critical step in software development that can help developers identify performance bottlenecks and improve the overall performance of their applications.\nIn this post I create a square matrix and then convert it to a data.frame (2,000 rows by 2,000 columns) and then saved it as a gz compressed csv file. The benchmark compares different R packages and functions, including base R, data.table, vroom, and readr, and measures their relative speeds based on the time it takes to read in the .csv.gz file.\nHere are some pro’s of trying things different ways and properly benchmarking them:\n\nIdentify the most efficient solution: Benchmarking can help you identify the most efficient solution for a specific task. By measuring the relative speeds of different programs or tools, you can determine which one is the fastest and use it to improve the performance of your application.\nOptimize resource utilization: Benchmarking can help you optimize resource utilization by identifying programs or tools that consume more resources than others. By choosing the most resource-efficient solution, you can reduce the cost of running your application and improve its scalability.\nAvoid premature optimization: Benchmarking can help you avoid premature optimization by measuring the performance of different programs or tools before you start optimizing them. By identifying the slowest parts of your application, you can focus your optimization efforts on the most critical areas and avoid wasting time optimizing code that doesn’t need it.\nKeep up with technology: Benchmarking can help you keep up with technology by comparing the performance of different tools and libraries. By staying up to date with the latest technologies, you can improve the performance of your application and stay ahead of your competitors.\nImprove code quality: Benchmarking can help you improve the quality of your code by identifying performance bottlenecks and areas for optimization. By optimizing your code, you can improve its maintainability, reliability, and readability.\n\nIn conclusion, benchmarking is an essential tool for software developers that can help them identify the most efficient solutions for their applications. By measuring the relative speeds of different programs or tools, developers can optimize resource utilization, avoid premature optimization, keep up with technology, and improve the quality of their code."
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#base-r",
    "href": "posts/rtip-2023-03-27/index.html#base-r",
    "title": "How fast does a compressed file in?",
    "section": "Base R",
    "text": "Base R\n\nread.csv()\nread.table()"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#data.table",
    "href": "posts/rtip-2023-03-27/index.html#data.table",
    "title": "How fast does a compressed file in?",
    "section": "data.table",
    "text": "data.table\n\nfread"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#vroom",
    "href": "posts/rtip-2023-03-27/index.html#vroom",
    "title": "How fast does a compressed file in?",
    "section": "vroom",
    "text": "vroom\n\nvroom() with altrep = FALSE\nvroom() with altrep = TRUE"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#readr",
    "href": "posts/rtip-2023-03-27/index.html#readr",
    "title": "How fast does a compressed file in?",
    "section": "readr",
    "text": "readr\n\nread_csv()"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#benchmarking",
    "href": "posts/rtip-2023-03-27/index.html#benchmarking",
    "title": "How fast does a compressed file in?",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nlibrary(rbenchmark)\nlibrary(data.table)\nlibrary(readr)\nlibrary(vroom)\nlibrary(dplyr)\n\nn <- 30\n\nbenchmark(\n  # Base R\n  \"read.table\" = {\n    a <- read.table(\"matrix.csv.gz\", sep = \",\")\n  },\n  \"read.csv\" = {\n    b <- read.csv(\"matrix.csv.gz\", sep = \",\")\n  },\n  \n  # data.table\n  \"fread\" = {\n    c <- fread(\"matrix.csv.gz\", sep = \",\")\n  },\n  \n  # vroom\n  \"vroom alltrep false\" = {\n    d <- vroom(\"matrix.csv.gz\", delim = \",\")\n  },\n  \"vroom alltrep true\" = {\n    e <- vroom(\"matrix.csv.gz\", delim = \",\", altrep = TRUE)\n  },\n  \n  # readr\n  \"readr\" = {\n    f <- read_csv(\"matrix.csv.gz\")\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               fread           30   19.44    1.000     13.56     1.59\n2  vroom alltrep true           30   22.06    1.135     10.54     2.63\n3 vroom alltrep false           30   24.75    1.273     10.22     2.84\n4          read.table           30   94.34    4.853     79.02     0.64\n5            read.csv           30  143.28    7.370    115.64     0.74\n6               readr           30  177.61    9.136     50.37    10.05\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html",
    "href": "posts/rtip-2023-03-28/index.html",
    "title": "How fast does a compressed file in Part 2",
    "section": "",
    "text": "Yesterday I posted on performing a benchmark on reading in a compressed .csv.gz file of a 2,000 by 2,000 data.frame. It was brought to my attention by someone on Mastadon (@mariviere@fediscience.org - https://fediscience.org/@mariviere) that I should also use {duckdb} and {arrow} so I will perform the same analysis as yesterday but I will also add in the two aforementioned packages."
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html#make-the-data",
    "href": "posts/rtip-2023-03-28/index.html#make-the-data",
    "title": "How fast does a compressed file in Part 2",
    "section": "Make the Data",
    "text": "Make the Data\nLet’s make that dataset again:\n\nlibrary(R.utils)\n\n# create a 1000 x 1000 matrix of random numbers\ndf <- matrix(rnorm(2000000), nrow = 2000, ncol = 2000) |>\n  as.data.frame()\n\n# Make and save gzipped file\nwrite.csv(df, \"df.csv\")\ngzip(\n  filename = \"df.csv\", \n  destname = \"df.csv.gz\",\n  overwrite = FALSE, \n  remove = TRUE\n)"
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html#benchmarking",
    "href": "posts/rtip-2023-03-28/index.html#benchmarking",
    "title": "How fast does a compressed file in Part 2",
    "section": "Benchmarking",
    "text": "Benchmarking\nTime to benchmark\n\nlibrary(rbenchmark)\nlibrary(data.table)\nlibrary(readr)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(vroom)\nlibrary(dplyr)\nlibrary(DBI)\n\nn <- 30\n\nbenchmark(\n  # Base R\n  \"read.table\" = {\n    a <- read.table(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \"read.csv\" = {\n    b <- read.csv(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \n  # data.table\n  \"fread\" = {\n    c <- fread(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \n  # vroom\n  \"vroom alltrep false\" = {\n    d <- vroom(\"df.csv.gz\", delim = \",\", col_types = \"d\")\n  },\n  \"vroom alltrep true\" = {\n    e <- vroom(\"df.csv.gz\", delim = \",\", altrep = TRUE, col_types = \"d\")\n  },\n  \n  # readr\n  \"readr\" = {\n    f <- read_csv(\"df.csv.gz\", col_types = \"d\")\n  },\n  \n  # Arrow\n  \"arrow\" = {\n    g <- open_csv_dataset(\"df.csv.gz\")\n  },\n  \n  # DuckDB\n  \"duckdb\" = {\n    con <- dbConnect(duckdb())\n    h <- duckdb_read_csv(\n      conn = con,\n      name = \"df\",\n      files = \"C:\\\\Users\\\\ssanders\\\\Documents\\\\GitHub\\\\steveondata\\\\posts\\\\rtip-2023-03-28\\\\df.csv.gz\"\n    )\n    dbDisconnect(con)\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               arrow           30    3.01    1.000      5.04     0.25\n2               fread           30   28.28    9.395     19.56     4.30\n3 vroom alltrep false           30   31.89   10.595     26.25    10.75\n4  vroom alltrep true           30   33.72   11.203     25.75    10.67\n5              duckdb           30   94.09   31.259     90.70     2.77\n6               readr           30   98.28   32.651    113.05    45.12\n7          read.table           30  109.97   36.535    107.78     1.24\n8            read.csv           30  153.79   51.093    152.44     0.56\n\n\nImportant note is the session info on the pc I am using to write this:\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] dplyr_1.1.1       vroom_1.6.1       arrow_11.0.0.3    duckdb_0.7.1-1   \n [5] DBI_1.1.3         readr_2.1.4       data.table_1.14.8 rbenchmark_1.0.0 \n [9] R.utils_2.12.2    R.oo_1.25.0       R.methodsS3_1.8.2\n\nloaded via a namespace (and not attached):\n [1] pillar_1.9.0      compiler_4.2.3    tools_4.2.3       digest_0.6.31    \n [5] bit_4.0.5         jsonlite_1.8.4    evaluate_0.20     lifecycle_1.0.3  \n [9] tibble_3.2.1      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] rstudioapi_0.14   parallel_4.2.3    yaml_2.3.7        xfun_0.38        \n[17] fastmap_1.1.1     knitr_1.42        generics_0.1.3    vctrs_0.6.1      \n[21] htmlwidgets_1.6.2 hms_1.1.3         bit64_4.0.5       tidyselect_1.2.0 \n[25] glue_1.6.2        R6_2.5.1          fansi_1.0.4       rmarkdown_2.21   \n[29] tzdb_0.3.0        purrr_1.0.1       magrittr_2.0.3    htmltools_0.5.5  \n[33] assertthat_0.2.1  utf8_1.2.3        crayon_1.5.2     \n\n Sys.info() |> \n   as.data.frame() |> \n   tibble::rownames_to_column() |> \n   as_tibble() |> \n   slice(1,2,3,5)\n\n# A tibble: 4 × 2\n  rowname `Sys.info()`\n  <chr>   <chr>       \n1 sysname Windows     \n2 release 10 x64      \n3 version build 19045 \n4 machine x86-64      \n\n memory.profile() |>\n   as.data.frame()\n\n            memory.profile()\nNULL                       1\nsymbol                 24303\npairlist              642504\nclosure                11189\nenvironment             4009\npromise                22963\nlanguage              189766\nspecial                   47\nbuiltin                  701\nchar                 2039073\nlogical                18866\ninteger               108132\ndouble                 20060\ncomplex                    5\ncharacter             160381\n...                       21\nany                        0\nlist                   58500\nexpression                 5\nbytecode               41555\nexternalptr            12382\nweakref                13860\nraw                    10113\nS4                      1362\n\n gc()\n\n           used  (Mb) gc trigger  (Mb) max used  (Mb)\nNcells  3363479 179.7    5830931 311.5  5830931 311.5\nVcells 32950395 251.4   81254422 620.0 81254324 620.0"
  },
  {
    "objectID": "posts/rtip-2023-03-29/index.html",
    "href": "posts/rtip-2023-03-29/index.html",
    "title": "A Bootstrapped Time Series Model with auto.arima() from {forecast}",
    "section": "",
    "text": "Introduction\nTime series analysis is a powerful tool for understanding and predicting patterns in data that vary over time. In this tutorial, we will use the AirPassengers dataset to create a bootstrapped timeseries model in R.\nThe AirPassengers dataset The AirPassengers dataset contains data on the number of passengers traveling on international flights per month from 1949 to 1960. To begin, we will load the dataset into R and plot it to get an idea of the data’s structure and any underlying patterns.\n\nlibrary(forecast)\n\ndata(AirPassengers)\nplot(AirPassengers, main = \"International Airline Passengers 1949-1960\")\n\n\n\n\nFrom the plot, we can see that there is a clear upward trend in the data, as well as some seasonality.\n\n\nCreating a bootstrapped timeseries model\nNow that we have an idea of the structure of the data, we can create a bootstrapped timeseries model using the auto.arima() function from the {forecast} package. The auto.arima() function uses an automated algorithm to determine the best model for a given timeseries.\n\nset.seed(123)\nn <- length(AirPassengers)\nn_boot <- 1000\n\n# create bootstrap sample indices\nboot_indices <- replicate(n_boot, sample(1:n, replace = TRUE))\n\n# create list to store models\nmodels <- list()\n\n# create bootstrapped models\nfor(i in 1:n_boot) {\n  boot_data <- AirPassengers[boot_indices[, i]]\n  models[[i]] <- auto.arima(boot_data)\n}\n\nmodels[[1]]\n\nSeries: boot_data \nARIMA(0,0,0) with non-zero mean \n\nCoefficients:\n          mean\n      275.5347\ns.e.    9.5443\n\nsigma^2 = 13209:  log likelihood = -887.01\nAIC=1778.02   AICc=1778.1   BIC=1783.96\n\n\nIn the code above, we first set a seed to ensure reproducibility of our results. We then specify the length of the timeseries and the number of bootstrap iterations we want to run. We create a list to store the models and a set of bootstrap sample indices.\nWe then loop through each bootstrap iteration, creating a new dataset from the original timeseries by sampling with replacement using the boot_indices. We use the auto.arima() function to create a timeseries model for each bootstrap sample and store it in our models list.\n\n\nSummarizing and plotting residuals\nNow that we have created our bootstrapped timeseries models, we can summarize and plot the residuals of each model to get an idea of how well our models fit the data.\n\n# create list to store residuals\nresiduals <- list()\n\n# create residuals for each model\nfor(i in 1:n_boot) {\n  boot_data <- AirPassengers[boot_indices[, i]]\n  residuals[[i]] <- residuals(models[[i]])\n}\n\n# summarize residuals\nresidual_means <- sapply(residuals, mean)\nresidual_sd <- sapply(residuals, sd)\n\n# plot residuals\npar(mfrow = c(2, 1))\n\nhist(\n  residual_means, \n  main = \"Bootstrapped Model Residuals\", \n  xlab = \"Mean Residuals\"\n  )\nhist(\n  residual_sd, \n  col = \"red\", \n  main = \"\", \n  xlab = \"SD Residuals\"\n  )\n\n\n\npar(mfrow = c(1,1))\n\nIn the code above, we create a list to store the residuals for each model, loop through each model to create residuals using the residuals() function, and summarize the residuals by taking the mean and standard deviation of each set of residuals.\nWe then plot the mean residuals and standard deviations for each model using the plot() function and add a legend to indicate the meaning of the two lines.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html",
    "href": "posts/rtip-2023-04-03/index.html",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "",
    "text": "In this blog post, we will be discussing how to create a Shiny application in R that will download and extract data from a zip file and allow users to choose which data they would like to see presented to them in the app from a selection drop-down menu. We will be using the current_hosp_data() function to obtain and read in the data. This function is in the upcoming release for the {healthyR.data} package."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#install",
    "href": "posts/rtip-2023-04-03/index.html#install",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Install",
    "text": "Install\n\ninstall.packages(c(\"shiny\",\"shinythemes\"))"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#current-hospital-data",
    "href": "posts/rtip-2023-04-03/index.html#current-hospital-data",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Current Hospital Data",
    "text": "Current Hospital Data\nHere is the current_hospital_data() function:\n\ncurrent_hosp_data <- function() {\n\n  # URL for file\n  url <- \"https://data.cms.gov/provider-data/sites/default/files/archive/Hospitals/current/hospitals_current_data.zip\"\n\n  # Create a temporary directory to process the zip file\n  tmp_dir <- tempdir()\n  download_location <- file.path(tmp_dir, \"download.zip\")\n  extract_location <- file.path(tmp_dir, \"extract\")\n\n  # Download the zip file to the temporary location\n  utils::download.file(\n    url = url,\n    destfile = download_location\n  )\n\n  # Unzip the file\n  utils::unzip(download_location, exdir = extract_location)\n\n  # Read the csv files into a list\n  csv_file_list <- list.files(\n    path = extract_location,\n    pattern = \"\\\\.csv$\",\n    full.names = TRUE\n  )\n\n  # make named list\n  csv_names <-\n    stats::setNames(\n      object = csv_file_list,\n      nm =\n        csv_file_list |>\n        basename() |>\n        gsub(pattern = \"\\\\.csv$\", replacement = \"\") |>\n        janitor::make_clean_names()\n    )\n\n  # Process CSV Files\n  parse_csv_file <- function(file) {\n    # Normalize the path to use C:/path/to/file structure\n    normalizePath(file, \"/\") |>\n      # read in the csv file and use check.names = FALSE because some of\n      # the names are very long\n      utils::read.csv(check.names = FALSE) |>\n      dplyr::as_tibble() |>\n      # clean the field names\n      janitor::clean_names()\n  }\n\n  list_of_tables <- lapply(csv_names, parse_csv_file)\n\n  unlink(tmp_dir, recursive = TRUE)\n\n  # Return the tibbles\n  # Add and attribute and a class type to the object\n  attr(list_of_tables, \".list_type\") <- \"current_hosp_data\"\n  class(list_of_tables) <- c(\"current_hosp_data\", class(list_of_tables))\n\n  list_of_tables\n}"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#app-file",
    "href": "posts/rtip-2023-04-03/index.html#app-file",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "App File",
    "text": "App File\nNext, let’s create a new file called app.R. In this file, we will create the Shiny app. The app will have a user interface (UI) and a server.\nThe UI is responsible for creating the layout of the app, while the server is responsible for processing the data and responding to user input.\nFirst, let’s create the UI. The UI will consist of a drop-down menu that will allow users to choose which data they would like to see presented to them in the app.\n\nlibrary(shiny)\nlibrary(shinythemes)\n\nhosp_data <- current_hosp_data()\n\nui <- fluidPage(theme = shinytheme(\"cerulean\"),\n                \n                # Set up the dropdown menu\n                selectInput(inputId = \"table\", \n                            label = \"Select a table:\", \n                            choices = names(hosp_data), \n                            selected = NULL),\n                \n                # Set up the table output\n                tableOutput(outputId = \"table_output\")\n)\n\nThe fluidPage() function creates a new Shiny app page. We also specify the theme using the {shinythemes} package. The selectInput() function creates the drop-down menu, which allows users to select which data they would like to see presented to them in the app. The choices argument is set to the names of the tables in the current_hosp_data() object. The tableOutput() function creates the output for the selected table."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#server",
    "href": "posts/rtip-2023-04-03/index.html#server",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Server",
    "text": "Server\nNext, let’s create the server. The server will be responsible for processing the data and generating the output based on user input.\n\nserver <- function(input, output) {\n    \n    # Load the data into a reactive object\n    data <- reactive(hosp_data)\n    \n    # Set up the table output\n    output$table_output <- renderTable({\n        # Get the selected table\n        table_selected <- input$table\n        \n        # Get the table from the data object\n        table_data <- data()[[table_selected]]\n        \n        # Return the table data\n        table_data\n    })\n}\n\nThe reactive() function is used to create a reactive object that will load the data when the app starts. The renderTable() function generates the output for the selected table. It does this by getting the selected table from the drop-down menu, getting the table data from the reactive data object, and returning the table data."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#shiny-app",
    "href": "posts/rtip-2023-04-03/index.html#shiny-app",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Shiny App",
    "text": "Shiny App\nFinally, we need to run the appl using the shinyApp() function:\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#pros-and-cons",
    "href": "posts/rtip-2023-04-03/index.html#pros-and-cons",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nPros:\n\nThe app is easy to use, and users can quickly select which data they would like to see presented to them in the app.\nThe current_hosp_data() function is only called once when the app starts, which can save time and resources if the function is time-consuming or resource-intensive.\n\nCons:\n\nThe app will not update if the data in the zip file changes. Users will need to restart the app to see the updated data.\nThe app loads all the data into memory when it starts, which can be an issue if the data is large and memory-intensive."
  },
  {
    "objectID": "posts/rtip-2023-04-04/index.html",
    "href": "posts/rtip-2023-04-04/index.html",
    "title": "A sample Shiny App to view Forecasts on the AirPassengers Data",
    "section": "",
    "text": "Hello! In this code, we are making a program that will help us predict the number of air passengers in the future. Let me explain what each part of the code does, step by step.\nFirst, we need to load some tools that will help us create the program. These tools are called “packages.” We use the library() function to load them. The packages we need are called shiny, forecast, and ggplot2.\n\n\n\nlibrary(shiny)\nlibrary(forecast)\nlibrary(ggplot2)\n\n\n\n\nNext, we need some data to work with. We will use a dataset of the number of air passengers each month from 1949 to 1960. We load this dataset using the data() function.\n\ndata(AirPassengers)\n\n\n\n\nNow, we need to create the user interface, or UI. This is what the user will see and interact with. In this case, we will create a simple app with a title, a dropdown menu to choose a forecasting model, and a plot and table to display the forecast results. We use the fluidPage() function to create the UI, and we define the UI elements inside it.\n\nui <- fluidPage(\n  titlePanel(\"AirPassengers Forecast\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"model\", label = \"Choose a model:\",\n                  choices = c(\"auto.arima\", \"ets\", \"holtwinters\"))\n    ),\n    mainPanel(\n      plotOutput(outputId = \"forecast_plot\"),\n      tableOutput(outputId = \"forecast_table\")\n    )\n  )\n)\n\n\n\n\nNow, we need to define the server. The server is where the program does the calculations and generates the output based on what the user selects in the UI. We define the server inside the function(input, output) argument.\n\nserver <- function(input, output) {\n\nInside the server, we need to create a reactive expression that generates the forecast based on the model the user selects. We use an if statement to check which model the user selected, and then we use the corresponding function to generate the forecast.\n\n\n\nforecast_data <- reactive({\n    if (input$model == \"auto.arima\") {\n      fit <- auto.arima(AirPassengers)\n      forecast(fit)\n    } else if (input$model == \"ets\") {\n      fit <- ets(AirPassengers)\n      forecast(fit)\n    } else {\n      fit <- hw(AirPassengers)\n      forecast(fit)\n    }\n  })\n\n\n\n\nThe renderPlot() function tells the program to create a plot based on the reactive expression we defined earlier. We use the plotOutput() function in the UI to display the plot.\n\noutput$forecast_plot <- renderPlot({\n    plot(forecast_data())\n  })\n\nSimilarly, the renderTable() function tells the program to create a table based on the reactive expression we defined earlier. We use the tableOutput() function in the UI to display the table.\n\noutput$forecast_table <- renderTable({\n    forecast_data()$mean\n  })\n\nFinally, we run the app using the shinyApp() function, with the UI and server arguments.\n\nshinyApp(ui = ui, server = server)\n\nAnd that’s it! This program allows the user to choose a forecasting model, and then generates a plot and table with the predicted number of air passengers based on that model.\nHere is the Full code block”\n\n# Load required packages\nlibrary(shiny)\nlibrary(forecast)\nlibrary(ggplot2)\n\n# Load AirPassengers dataset\ndata(AirPassengers)\n\n# Define UI\nui <- fluidPage(\n  \n  # Title of the app\n  titlePanel(\"AirPassengers Forecast\"),\n  \n  # Sidebar with input controls\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"model\", label = \"Choose a model:\",\n                  choices = c(\"auto.arima\", \"ets\", \"holtwinters\"))\n    ),\n    \n    # Output plot and table\n    mainPanel(\n      plotOutput(outputId = \"forecast_plot\"),\n      tableOutput(outputId = \"forecast_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Reactive expression to create forecast based on selected model\n  forecast_data <- reactive({\n    if (input$model == \"auto.arima\") {\n      fit <- auto.arima(AirPassengers)\n      forecast(fit)\n    } else if (input$model == \"ets\") {\n      fit <- ets(AirPassengers)\n      forecast(fit)\n    } else {\n      fit <- hw(AirPassengers)\n      forecast(fit)\n    }\n  })\n  \n  # Output plot\n  output$forecast_plot <- renderPlot({\n    plot(forecast_data())\n    #checkresiduals(forecast_data())\n  })\n  \n  # Output table\n  output$forecast_table <- renderTable({\n    forecast_data()$mean\n  })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-05/index.html",
    "href": "posts/rtip-2023-04-05/index.html",
    "title": "Looking at Daily Log Returns with tidyquant, TidyDensity, and Shiny",
    "section": "",
    "text": "In this blog post, we’ll walk through how to create a shiny application that allows users to analyze the weekly returns of FAANG stocks (AAPL, AMZN, FB, GOOGL, and NFLX) using the {tidyquant} and {TidyDensity} packages in R.\n\n\nThe first section of the code sets up the necessary R packages and creates the UI for the shiny app. The packages we’ll be using are:\n\nshiny: for creating interactive web applications in R\ntidyquant: for easily getting and analyzing financial data in R\nTidyDensity: for computing and visualizing probability distributions in a tidy way\ndplyr: for manipulating data in a tidy way\nDT: for creating interactive and scrollable data tables\n\nAnalysts assemble your packages!\n\nlibrary(shiny)\nlibrary(tidyquant)\nlibrary(TidyDensity)\nlibrary(dplyr)\nlibrary(DT)\n\nThe UI consists of a title panel, a sidebar panel, and a main panel. The sidebar panel contains a select input that allows users to choose which FAANG stock to analyze, as well as a numeric input for the number of simulations to run. The main panel contains two sections: one for the tidy_autoplot() output (a plot of the stock returns), and one for the tidy_empirical() output (a table of the log returns).\n\n\n\nThe second section of the code defines the server function for the shiny app. The server function takes the input values from the UI (i.e. the selected stock and number of simulations) and uses them to get and analyze the stock data.\nTo get the stock data, we use the tq_get() function from the tidyquant package to retrieve the adjusted stock prices for the selected security from January 1, 2010 to the present. We then use the tq_transmute() function to compute the weekly log returns of the stock and rename the resulting column to “log_return”.\nThe tidy_empirical() function from the TidyDensity package is used to compute the empirical distribution of the log returns. The resulting table is displayed using the renderDT() function from the DT package, which creates a scrollable data table that can be sorted and filtered.\nThe tidy_autoplot() function is used to create a plot of the log returns, which is displayed using the renderPlot() function.\n\n\n\nThe final section of the code runs the shiny app using the ui and server functions.\nOverall, this shiny app provides a simple and interactive way for users to analyze the weekly returns of FAANG stocks using tidyquant and TidyDensity in R. By allowing users to choose which stock to analyze and how many simulations to run, the app provides a customizable way to explore the empirical distributions of the log returns."
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html",
    "href": "posts/rtip-2023-04-06/index.html",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "",
    "text": "BRVM"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_ticker_desc-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_ticker_desc-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_ticker_desc() function",
    "text": "The BRVM_ticker_desc() function\nIt receives no argument and returns BRVM tickers information such as its full name, sector and country.\n\n# Display tickers of BRVM\ntickers <- BRVM_ticker_desc()\ntickers\n\n\n\nWarning: package 'kableExtra' was built under R version 4.2.3\n\n\n\n\n \n  \n    Ticker \n    Company name \n    Sector \n    Country \n  \n \n\n  \n    ABJC \n    SERVAIR ABIDJAN  COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    BICC \n    BICI COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    BNBC \n    BERNABE COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    BOAB \n    BANK OF AFRICA BENIN \n    FINANCE \n    BENIN \n  \n  \n    BOABF \n    BANK OF AFRICA BURKINA FASO \n    FINANCE \n    BURKINA FASO \n  \n  \n    BOAC \n    BANK OF AFRICA COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    BOAM \n    BANK OF AFRICA MALI \n    FINANCE \n    MALI \n  \n  \n    BOAN \n    BANK OF AFRICA NIGER \n    FINANCE \n    NIGER \n  \n  \n    BOAS \n    BANK OF AFRICA SENEGAL \n    FINANCE \n    SENEGAL \n  \n  \n    CABC \n    SICABLE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    CBIBF \n    CORIS BANK INTERNATIONAL BURKINA FASO \n    FINANCE \n    BURKINA FASO \n  \n  \n    CFAC \n    CFAO MOTORS COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    CIEC \n    CIE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    ECOC \n    ECOBANK COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    ETIT \n    Ecobank Transnational Incorporated TOGO \n    FINANCE \n    TOGO \n  \n  \n    FTSC \n    FILTISAC COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    NEIC \n    NEI-CEDA COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    NSBC \n    NSIA BANQUE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    NTLC \n    NESTLE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    ONTBF \n    ONATEL BURKINA FASO \n    PUBLIC SERVICE \n    BURKINA FASO \n  \n  \n    ORAC \n    ORANGE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    ORGT \n    ORAGROUP TOGO \n    FINANCE \n    TOGO \n  \n  \n    PALC \n    PALM COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    PRSC \n    TRACTAFRIC MOTORS COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    SAFC \n    SAFCA COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SCRC \n    SUCRIVOIRE COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SDCC \n    SODE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    SDSC \n    BOLLORE TRANSPORT & LOGISTICS COTE D'IVOIRE \n    TRANSPORT \n    IVORY COAST \n  \n  \n    SEMC \n    CROWN SIEM COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SGBC \n    SOCIETE GENERALE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SHEC \n    VIVO ENERGY COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    SIBC \n    SOCIETE IVOIRIENNE DE BANQUE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SICC \n    SICOR COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SIVC \n    AIR LIQUIDE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SLBC \n    SOLIBRA COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SMBC \n    SMB COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SNTS \n    SONATEL SENEGAL \n    PUBLIC SERVICE \n    SENEGAL \n  \n  \n    SOGC \n    SOGB COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SPHC \n    SAPH COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    STAC \n    SETAO COTE D'IVOIRE \n    OTHER \n    IVORY COAST \n  \n  \n    STBC \n    SITAB COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SVOC \n    MOVIS COTE D'IVOIRE \n    TRANSPORT \n    IVORY COAST \n  \n  \n    TTLC \n    TOTAL COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    TTLS \n    TOTAL SENEGAL \n    DISTRIBUTION \n    SENEGAL \n  \n  \n    TTRC \n    TRITURAF Ste en Liquid \n    INDUSTRY \n    IVORY COAST \n  \n  \n    UNLC \n    UNILEVER COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    UNXC \n    UNIWAX COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_index-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_index-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_index() function :",
    "text": "The BRVM_index() function :\nIt receives no argument and returns a table of updated data (with as table header: indexes, previous closing, closing, change (%), Year to Date Change) on all the indices available on the BRVM exchange.\n\n\n\n\n \n  \n    Indexes \n    Previous closing \n    Closing \n    Change (%) \n    Year to Date Change \n  \n \n\n  \n    BRVM-30 \n    99.71 \n    99.75 \n    0.04 \n    0.00 \n  \n  \n    BRVM - AGRICULTURE \n    281.76 \n    281.25 \n    -0.18 \n    -0.66 \n  \n  \n    BRVM - OTHER SECTOR \n    1295.58 \n    1357.27 \n    4.76 \n    -7.32 \n  \n  \n    BRVM - COMPOSITE \n    199.37 \n    199.46 \n    0.05 \n    0.85 \n  \n  \n    BRVM - DISTRIBUTION \n    346.02 \n    345.33 \n    -0.20 \n    0.69 \n  \n  \n    BRVM - FINANCE \n    74.53 \n    75.03 \n    0.67 \n    -0.66 \n  \n  \n    BRVM - INDUSTRY \n    98.33 \n    98.10 \n    -0.23 \n    0.92 \n  \n  \n    BRVM - PRESTIGE \n    102.61 \n    102.56 \n    -0.05 \n    0.00 \n  \n  \n    BRVM - PRINCIPAL \n    94.56 \n    94.62 \n    0.06 \n    0.00 \n  \n  \n    BRVM - PUBLIC SERVICES \n    480.97 \n    479.60 \n    -0.28 \n    2.23 \n  \n  \n    BRVM - TRANSPORT \n    345.28 \n    341.70 \n    -1.04 \n    0.35"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_get.symbol-.from-.to-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_get.symbol-.from-.to-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_get(“.symbol”, “.from”, “.to”) function",
    "text": "The BRVM_get(“.symbol”, “.from”, “.to”) function\nThis function will get the data of the companies listed on the BVRM stock exchange in Rich Bourse website. The function takes a single parameter, .symbol (which represents the “Ticker”). The function will automatically format tickers you enter in uppercase using toupper() and then ensure that the passed ticker is in a Google spreadsheet of allowed tickers.\n\n.symbol : A vector of symbols, like: c(“BICC”,“XOM”,“SlbC”) ;\n.from : A quoted start date, ie. “2020-01-01” or “2020/01/01”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD” ;\n.to : A quoted end date, ie. “2022-01-31” or “2022/01/31”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”.\n\n\n#' Displaying data of SONATEL Senegal stock\nBRVM_get(.symbol = \"snts\")\n\n[1] \"SNTS\"\n\n\n# A tibble: 251 × 6\n   Date        Open  High   Low Close Volume\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1 2022-04-06 15800 15895 15750 15800   7436\n 2 2022-04-07 15800 15900 15750 15900   1265\n 3 2022-04-08 15900 15995 15800 15900   1164\n 4 2022-04-11 15895 15900 15800 15800   4252\n 5 2022-04-12 15800 15800 15780 15800   6561\n 6 2022-04-13 15800 15865 15795 15850   5409\n 7 2022-04-14 15855 15900 15850 15900  16957\n 8 2022-04-15 15995 15995 15900 15900    791\n 9 2022-04-19 15900 15995 15895 15900  31217\n10 2022-04-20 15900 15995 15895 15990  32322\n# ℹ 241 more rows\n\nsymbols <- c(\"BiCc\",\"XOM\",\"SlbC\")   # We use here three tickers\ndata_tbl <- BRVM_get(.symbol = symbols, .from = \"2020-01-01\", .to = Sys.Date() - 1)\n\n[1] \"BICC\" \"SLBC\"\n\n# Display the first twenty observations of the table\nhead(data_tbl, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2020-01-10  6500  6500  6500  6500     24 BICC  \n 2 2020-01-13  6370  6500  6370  6500     29 BICC  \n 3 2020-01-14  6495  6495  6495  6495     10 BICC  \n 4 2020-01-29  6010  6010  6010  6010     24 BICC  \n 5 2020-01-30  6000  6000  6000  6000     50 BICC  \n 6 2020-02-04  5800  5800  5800  5800     12 BICC  \n 7 2020-02-07  5650  5650  5650  5650      5 BICC  \n 8 2020-02-10  5500  5500  5500  5500      5 BICC  \n 9 2020-02-14  5300  5300  5300  5300      9 BICC  \n10 2020-02-17  4910  4910  4910  4910    210 BICC  \n11 2020-02-18  4910  4910  4910  4910     50 BICC  \n12 2020-02-20  4895  4895  4895  4895      5 BICC  \n13 2020-02-21  4895  4895  4890  4890     13 BICC  \n14 2020-02-25  4525  4525  4525  4525     16 BICC  \n15 2020-02-26  4435  4435  4430  4430     21 BICC  \n16 2020-02-27  4345  4760  4335  4760   1809 BICC  \n17 2020-03-03  4745  4750  4745  4750     11 BICC  \n18 2020-03-05  4700  4700  4700  4700      5 BICC  \n19 2020-03-06  4695  4695  4695  4695      6 BICC  \n20 2020-03-11  4345  4450  4345  4450    135 BICC  \n\n# Display the last twenty elements of the table\ntail(data_tbl, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2023-02-15 80000 80000 79000 79000      2 SLBC  \n 2 2023-02-17 78000 78000 78000 78000      5 SLBC  \n 3 2023-02-21 80000 80000 80000 80000      5 SLBC  \n 4 2023-02-23 80000 80000 80000 80000     18 SLBC  \n 5 2023-02-24 80000 80000 80000 80000      6 SLBC  \n 6 2023-02-27 80000 80000 80000 80000     98 SLBC  \n 7 2023-02-28 80000 80000 80000 80000     11 SLBC  \n 8 2023-03-02 80000 80000 80000 80000     11 SLBC  \n 9 2023-03-08 80000 80000 80000 80000      2 SLBC  \n10 2023-03-09 80000 80000 80000 80000      2 SLBC  \n11 2023-03-13 80005 80005 80000 80000     12 SLBC  \n12 2023-03-14 80000 80000 80000 80000      1 SLBC  \n13 2023-03-20 80000 80000 80000 80000      3 SLBC  \n14 2023-03-21 80000 80000 80000 80000      4 SLBC  \n15 2023-03-27 78000 80000 78000 80000    169 SLBC  \n16 2023-03-28 80000 80000 80000 80000    435 SLBC  \n17 2023-03-30 80000 80000 80000 80000      3 SLBC  \n18 2023-03-31 80000 80000 80000 80000      1 SLBC  \n19 2023-04-04 80000 86000 80000 86000      3 SLBC  \n20 2023-04-05 85950 86000 85950 86000      6 SLBC"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_get1ticker-period-from-to-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_get1ticker-period-from-to-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_get1(“ticker”, “Period”, “from”, “to”) function",
    "text": "The BRVM_get1(“ticker”, “Period”, “from”, “to”) function\nThis function will get data of the companies listed on the BVRM stock exchange through the sikafinance site. The function takes in a single parameter of ticker and will auto-format the tickers you input into all upper case by using toupper()\n\nticker : A vector of ticker, like: c(“BICC”,“XOM”,“SlbC”, “BRvm10”);\nPeriod : Numeric number indicating time period. Valid entries are 0, 1, 5, 30, 91, and 365 representing respectively ‘daily’, ‘one year’, ‘weekly’, ‘monthly’, ‘quarterly’ and ‘yearly’;\nfrom : A quoted start date, ie. “2020-01-01” or “2020/01/01”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”;\nto : A quoted end date, ie. “2022-01-31” or “2022/01/31”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”\n\n** NB : There is a small difference between the BRVM_get and BRVM_get1 functions. * With BRVM_get it is only possible to download tickers’ daily data. * But with BRVM_get1, you can download daily, weekly, monthly, annual tickers’ data, indices and even market capitalization.\n\n#' Displaying data of SONATEL Senegal stock\nBRVM_get1(\"snts\")\n\n[1] \"Make sure you have an active internet connection\"\n\n# Get daily data of all indexes\nall_ind <- BRVM_get1(\"ALL INDEXES\", Period = 0, from = \"2020-01-04\", to = \"2023-03-24\") \n\n[1] \"We obtained BRVM10 data from 2019-12-26 to 2023-01-04\"\n[1] \"We obtained BRVMAG data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMC data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMAS data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMDI data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMFI data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMIN data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMSP data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMTR data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMPR data from 2023-01-01 to 2023-03-24\"\n[1] \"We obtained BRVMPA data from 2023-01-04 to 2023-03-24\"\n[1] \"We obtained BRVM30 data from 2023-01-01 to 2023-03-24\"\n[1] \"We obtained CAPIB data from 2020-01-02 to 2023-03-24\"\n\n# display the first two tens elements of the table\nhead(all_ind, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2022-12-26  169.  169.  169.  169.      0 BRVM10\n 2 2022-12-27  169.  169.  169.  169.      0 BRVM10\n 3 2022-12-28  167.  167.  167.  167.      0 BRVM10\n 4 2022-12-29  167.  167.  167.  167.      0 BRVM10\n 5 2022-12-30  166.  166.  166.  166.      0 BRVM10\n 6 2023-01-02  166.  166.  166.  166.      0 BRVM10\n 7 2023-01-03  166.  166.  166.  166.      0 BRVM10\n 8 2023-01-04  166.  166.  166.  166.      0 BRVM10\n 9 2022-09-26  163.  163.  163.  163.      0 BRVM10\n10 2022-09-27  162.  162.  162.  162.      0 BRVM10\n11 2022-09-28  162.  162.  162.  162.      0 BRVM10\n12 2022-09-29  163.  163.  163.  163.      0 BRVM10\n13 2022-09-30  164.  164.  164.  164.      0 BRVM10\n14 2022-10-03  162.  162.  162.  162.      0 BRVM10\n15 2022-10-04  162.  162.  162.  162.      0 BRVM10\n16 2022-10-05  161.  161.  161.  161.      0 BRVM10\n17 2022-10-06  161.  161.  161.  161.      0 BRVM10\n18 2022-10-07  161.  161.  161.  161.      0 BRVM10\n19 2022-10-10  160.  160.  160.  160.      0 BRVM10\n20 2022-10-11  160.  160.  160.  160.      0 BRVM10\n\n# display the two tens of the last elements of the table\ntail(all_ind, 20)\n\n# A tibble: 20 × 7\n   Date          Open    High     Low   Close Volume Ticker\n   <date>       <dbl>   <dbl>   <dbl>   <dbl>  <dbl> <chr> \n 1 2020-02-26 4281311 4281311 4281311 4281311      0 CAPIB \n 2 2020-02-27 4314933 4314933 4314933 4314933      0 CAPIB \n 3 2020-02-28 4346515 4346515 4346515 4346515      0 CAPIB \n 4 2020-03-02 4424073 4424073 4424073 4424073      0 CAPIB \n 5 2020-03-03 4379647 4379647 4379647 4379647      0 CAPIB \n 6 2020-03-04 4369550 4369550 4369550 4369550      0 CAPIB \n 7 2020-03-05 4342229 4342229 4342229 4342229      0 CAPIB \n 8 2020-03-06 4359879 4359879 4359879 4359879      0 CAPIB \n 9 2020-03-09 4338293 4338293 4338293 4338293      0 CAPIB \n10 2020-03-10 4357221 4357221 4357221 4357221      0 CAPIB \n11 2020-03-11 4332656 4332656 4332656 4332656      0 CAPIB \n12 2020-03-12 4318096 4318096 4318096 4318096      0 CAPIB \n13 2020-03-13 4318112 4318112 4318112 4318112      0 CAPIB \n14 2020-03-16 4285184 4285184 4285184 4285184      0 CAPIB \n15 2020-03-17 4301727 4301727 4301727 4301727      0 CAPIB \n16 2020-03-18 4288582 4288582 4288582 4288582      0 CAPIB \n17 2020-03-19 4207231 4207231 4207231 4207231      0 CAPIB \n18 2020-03-20 4209788 4209788 4209788 4209788      0 CAPIB \n19 2020-03-23 4154445 4154445 4154445 4154445      0 CAPIB \n20 2020-03-24 4144325 4144325 4144325 4144325      0 CAPIB \n\n# To get yearly data\nyearly_data <- BRVM_get1(c(\"brvmtr\", \"BiCc\", \"BOAS\"), Period = 365 ) \n# display the first two tens elements of the table\nhead(yearly_data, 20) \n\n# A tibble: 20 × 6\n   Date         Open   High    Low  Close Ticker\n   <date>      <dbl>  <dbl>  <dbl>  <dbl> <chr> \n 1 2003-04-11   74.0   88.6   73.6   88.6 BRVMTR\n 2 2004-01-02   88.6   89.2   72.9   89.2 BRVMTR\n 3 2005-01-03   89.2  107.    70.7  104.  BRVMTR\n 4 2006-01-02  104.   158.   104.   153.  BRVMTR\n 5 2007-01-02  153.   275.   149.   249.  BRVMTR\n 6 2008-01-02  249.   386.   226.   296.  BRVMTR\n 7 2009-01-02  275.   296.   227.   236.  BRVMTR\n 8 2010-01-04  236.   259.   224.   238.  BRVMTR\n 9 2011-01-03  238.   249.   204.   239   BRVMTR\n10 2012-01-02  239    349.   201.   349.  BRVMTR\n11 2013-01-02  349.   794.   339.   789.  BRVMTR\n12 2014-01-02  789.  1213.   601.  1213.  BRVMTR\n13 2015-01-02 1213.  1525.   653.  1525.  BRVMTR\n14 2016-01-04 1525.  1525.  1216.  1432.  BRVMTR\n15 2017-01-02 1432.  1433.   764.  1203.  BRVMTR\n16 2018-01-02 1114.  1193.   966.   966.  BRVMTR\n17 2019-06-03  403.   429.   311.   367.  BRVMTR\n18 2020-01-01  367.   475.   292.   379.  BRVMTR\n19 2021-01-04  376.   622.   325    622.  BRVMTR\n20 2022-01-03  667.   667.   295.   342.  BRVMTR\n\n# display the two tens of the last elements of the table\ntail(yearly_data, 20) \n\n# A tibble: 20 × 6\n   Date        Open  High   Low Close Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl> <chr> \n 1 2014-01-02  5650  7848  5650  7800 BICC  \n 2 2015-01-02  8385 10750  7800 10100 BICC  \n 3 2016-01-04 10000 10700  8566  9890 BICC  \n 4 2017-01-05  9750 10000  6440  8490 BICC  \n 5 2018-01-02  8700  8750  3795  7900 BICC  \n 6 2019-01-04  7550  7550  3710  6800 BICC  \n 7 2020-01-01  6800  6890  2855  6680 BICC  \n 8 2021-01-04  6680  7525  4280  7400 BICC  \n 9 2022-01-03  7250  7250  5550  6850 BICC  \n10 2023-01-02  6500  6850  5785  6275 BICC  \n11 2014-12-10  1613  3225  1613  3225 BOAS  \n12 2015-01-02  3370  4300  2900  3950 BOAS  \n13 2016-01-04  3700  4101  2000  2350 BOAS  \n14 2017-01-02  2325  3875  2035  2500 BOAS  \n15 2018-01-02  2400  3250  1700  2020 BOAS  \n16 2019-01-02  1900  2000  1500  1545 BOAS  \n17 2020-01-01  1550  1700  1295  1495 BOAS  \n18 2021-01-04  1480  2750  1340  2350 BOAS  \n19 2022-01-03  2350  2780  2200  2450 BOAS  \n20 2023-01-02  2580  2585  2175  2265 BOAS"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm.index-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm.index-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM.index() function :",
    "text": "The BRVM.index() function :\nIt receives no argument and returns the name of all indexes available on BRVM Stock Exchange.\n\nBRVM.index()\n\n [1] \"BRVMAG\" \"BRVMC\"  \"BRVMAS\" \"BRVMDI\" \"BRVMFI\" \"BRVMIN\" \"BRVMSP\" \"BRVMTR\"\n [9] \"BRVMPR\" \"BRVMPA\" \"BRVM30\"\n\n\nAuthors : \n\nKoffi Frederic Sessie (koffisessie@gmail.com),\nAbdoul Oudouss Diakité (abdouloudoussdiakite@gmail.com),\nSanderson Steven(spsanderson@gmail.com)\n\nCreator : Koffi Frederic Sessie \ncph (Copyright Holder) : Koffi Frederic Sessie \nLicense : MIT 2023, BRVM authors. All rights reserved."
  },
  {
    "objectID": "posts/rtip-2023-04-07/index.html",
    "href": "posts/rtip-2023-04-07/index.html",
    "title": "Reading in Multiple Excel Sheets with lapply and {readxl}",
    "section": "",
    "text": "Intruduction\nReading in an Excel file with multiple sheets can be a daunting task, especially for users who are not familiar with the process. In this blog post, we will walk through a sample function that can be used to read in an Excel file with multiple sheets using the R programming language.\n\n\nFunction\nThe function we will be using is called excel_sheet_reader(). This function takes one argument: filename, which is the name of the Excel file we want to read in. This function, since it is using the {readxl} package will automatically read that data to a tibble.\n\n\nExample\nHere is the function:\n\nexcel_sheet_reader <- function(filename) {\n  sheets <- excel_sheets(filename)\n  x <- lapply(sheets, function(X) read_excel(filename, sheet = X))\n  names(x) <- sheets\n  x\n}\n\nThe first thing the excel_sheet_reader() function does is to determine the names of all the sheets in the Excel file using the excel_sheets function from the readxl package. This function returns a character vector containing the names of all the sheets in the Excel file.\n\nsheets <- excel_sheets(filename)\n\nNext, the function uses the lapply function to loop through all the sheet names and read in each sheet using the read_excel() function, also from the readxl package. This function takes two arguments: filename, which is the name of the Excel file, and sheet, which is the name of the sheet we want to read in. The lapply function returns a list containing all the sheets.\n\nx <- lapply(sheets, function(X) read_excel(filename, sheet = X))\n\nFinally, the function uses the names function to assign the sheet names to the list of sheets and returns the list.\n\nnames(x) <- sheets\nx\n\nNow that we have explained the excel_sheet_reader() function, let’s use it to read in the iris and mtcars datasets.\n\nlibrary(healthyR)\nlibrary(dplyr)\nlibrary(writexl)\nlibrary(readxl)\n\niris |>\n  named_item_list(Species) |>\n  write_xlsx(path = \"iris.xlsx\")\n\nmtcars |>\n  named_item_list(cyl) |>\n  write_xlsx(path = \"mtcars.xlsx\")\n\niris_sheets <- excel_sheet_reader(\"iris.xlsx\")\nmtcars_sheets <- excel_sheet_reader(\"mtcars.xlsx\")\n\nNow lets see the structure of each file.\n\niris_sheets\n\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <chr>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <chr>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# ℹ 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <chr>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# ℹ 40 more rows\n\n\nNow mtcars_sheets\n\nmtcars_sheets\n\n$`4`\n# A tibble: 11 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  22.8     4 108      93  3.85  2.32  18.6     1     1     4     1\n 2  24.4     4 147.     62  3.69  3.19  20       1     0     4     2\n 3  22.8     4 141.     95  3.92  3.15  22.9     1     0     4     2\n 4  32.4     4  78.7    66  4.08  2.2   19.5     1     1     4     1\n 5  30.4     4  75.7    52  4.93  1.62  18.5     1     1     4     2\n 6  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n 7  21.5     4 120.     97  3.7   2.46  20.0     1     0     3     1\n 8  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1\n 9  26       4 120.     91  4.43  2.14  16.7     0     1     5     2\n10  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n11  21.4     4 121     109  4.11  2.78  18.6     1     1     4     2\n\n$`6`\n# A tibble: 7 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n3  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n4  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n5  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n6  17.8     6  168.   123  3.92  3.44  18.9     1     0     4     4\n7  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6\n\n$`8`\n# A tibble: 14 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 2  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 3  16.4     8  276.   180  3.07  4.07  17.4     0     0     3     3\n 4  17.3     8  276.   180  3.07  3.73  17.6     0     0     3     3\n 5  15.2     8  276.   180  3.07  3.78  18       0     0     3     3\n 6  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n 7  10.4     8  460    215  3     5.42  17.8     0     0     3     4\n 8  14.7     8  440    230  3.23  5.34  17.4     0     0     3     4\n 9  15.5     8  318    150  2.76  3.52  16.9     0     0     3     2\n10  15.2     8  304    150  3.15  3.44  17.3     0     0     3     2\n11  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4\n12  19.2     8  400    175  3.08  3.84  17.0     0     0     3     2\n13  15.8     8  351    264  4.22  3.17  14.5     0     1     5     4\n14  15       8  301    335  3.54  3.57  14.6     0     1     5     8\n\n\nAnd that’s it! Hope this has been helpful!"
  },
  {
    "objectID": "posts/rtip-2023-04-10/index.html",
    "href": "posts/rtip-2023-04-10/index.html",
    "title": "Styling Tables for Excel with {styledTables}",
    "section": "",
    "text": "Introduction\nIn the analytics realm whether some like it or not, Excel is huge and maybe King. This is due to the fact of the shear volume of people using it. Microsoft has positioned Excel well in this situation, but, that does not mean we cannot extend Excel with R. In fact we can do just that. I will be focusing new posts on this topic as I gear up to collaborate on a new project focusing on this issue.\nFor this post we are going to discuss the {styledTable} R package that can be installed from GitHub. Here are a few ways in which the styledTable package can help.\n\nCreating visually appealing tables: Excel is a powerful tool for data analysis and visualization, but it can be limited in terms of formatting options. With the ‘styledtable’ package, users can create tables with a wide range of formatting options, such as bold text, colored cells, and borders. This can make the tables more visually appealing and easier to read, which can be helpful when presenting data to others.\nAutomating data analysis: The ‘styledtable’ package can be used in combination with other R packages to automate data analysis tasks. For example, users can use R to clean and transform data, and then use the ‘styledtable’ package to create formatted tables for reporting or sharing with others. This can save time and reduce errors associated with manual data entry and formatting.\nIntegrating with other R packages: R has a large ecosystem of packages for data analysis, visualization, and reporting. The ‘styledtable’ package can be used in conjunction with other R packages to extend the functionality of Excel. For example, users can use R to perform statistical analysis on data, and then use the ‘styledtable’ package to create formatted tables for reporting the results in Excel.\nFacilitating collaboration: Sharing Excel files can be challenging when working with multiple users or teams. With the ‘styledtable’ package, users can export styled tables to Excel format, which can be shared with others. This can facilitate collaboration and streamline the process of sharing data and analysis results.\n\nThe styledtable package in R, which allows users to create styled tables in R Markdown documents. The package can help to create tables with various formatting options such as bold text, colored cells, and borders. It also has functionality on how to port these to Excel itself.\nThe package offers a simple syntax that allows users to specify formatting options using HTML and CSS. The resulting table can be customized by changing the CSS file or by using the ‘styler’ function to apply custom styles to individual cells or rows.\nOverall, the styledtable package provides a useful tool for creating visually appealing tables in R Markdown documents, and the ability to export these tables to Excel format makes it easier to share and analyze data with others.\n\n\nFunctions\n\n\nExamples\n\n# Install development version from GitHub\ndevtools::install_github('R-package/styledTables', build_vignettes = TRUE)"
  },
  {
    "objectID": "posts/rtip-2023-04-11/index.html",
    "href": "posts/rtip-2023-04-11/index.html",
    "title": "Styling Tables for Excel with {styledTables}",
    "section": "",
    "text": "Introduction\nIn the analytics realm whether some like it or not, Excel is huge and maybe King. This is due to the fact of the shear volume of people using it. Microsoft has positioned Excel well in this situation, but, that does not mean we cannot extend Excel with R. In fact we can do just that. I will be focusing new posts on this topic as I gear up to collaborate on a new project focusing on this issue.\nFor this post we are going to discuss the {styledTable} R package that can be installed from GitHub. Here are a few ways in which the styledTable package can help.\n\nCreating visually appealing tables: Excel is a powerful tool for data analysis and visualization, but it can be limited in terms of formatting options. With the ‘styledtable’ package, users can create tables with a wide range of formatting options, such as bold text, colored cells, and borders. This can make the tables more visually appealing and easier to read, which can be helpful when presenting data to others.\nAutomating data analysis: The ‘styledtable’ package can be used in combination with other R packages to automate data analysis tasks. For example, users can use R to clean and transform data, and then use the ‘styledtable’ package to create formatted tables for reporting or sharing with others. This can save time and reduce errors associated with manual data entry and formatting.\nIntegrating with other R packages: R has a large ecosystem of packages for data analysis, visualization, and reporting. The ‘styledtable’ package can be used in conjunction with other R packages to extend the functionality of Excel. For example, users can use R to perform statistical analysis on data, and then use the ‘styledtable’ package to create formatted tables for reporting the results in Excel.\nFacilitating collaboration: Sharing Excel files can be challenging when working with multiple users or teams. With the ‘styledtable’ package, users can export styled tables to Excel format, which can be shared with others. This can facilitate collaboration and streamline the process of sharing data and analysis results.\n\nThe styledtable package in R, which allows users to create styled tables in R Markdown documents. The package can help to create tables with various formatting options such as bold text, colored cells, and borders. It also has functionality on how to port these to Excel itself.\nThe package offers a simple syntax that allows users to specify formatting options using HTML and CSS. The resulting table can be customized by changing the CSS file or by using the ‘styler’ function to apply custom styles to individual cells or rows.\nOverall, the styledtable package provides a useful tool for creating visually appealing tables in R Markdown documents, and the ability to export these tables to Excel format makes it easier to share and analyze data with others.\n\n\nExamples\n\n# Install development version from GitHub\ndevtools::install_github('R-package/styledTables', build_vignettes = TRUE)\n\n\nlibrary(styledTables)\nlibrary(dplyr)\nlibrary(xlsx)\n\ndf <- mtcars |>\n  select(mpg, cyl, am)\n\ndf\n\n                     mpg cyl am\nMazda RX4           21.0   6  1\nMazda RX4 Wag       21.0   6  1\nDatsun 710          22.8   4  1\nHornet 4 Drive      21.4   6  0\nHornet Sportabout   18.7   8  0\nValiant             18.1   6  0\nDuster 360          14.3   8  0\nMerc 240D           24.4   4  0\nMerc 230            22.8   4  0\nMerc 280            19.2   6  0\nMerc 280C           17.8   6  0\nMerc 450SE          16.4   8  0\nMerc 450SL          17.3   8  0\nMerc 450SLC         15.2   8  0\nCadillac Fleetwood  10.4   8  0\nLincoln Continental 10.4   8  0\nChrysler Imperial   14.7   8  0\nFiat 128            32.4   4  1\nHonda Civic         30.4   4  1\nToyota Corolla      33.9   4  1\nToyota Corona       21.5   4  0\nDodge Challenger    15.5   8  0\nAMC Javelin         15.2   8  0\nCamaro Z28          13.3   8  0\nPontiac Firebird    19.2   8  0\nFiat X1-9           27.3   4  1\nPorsche 914-2       26.0   4  1\nLotus Europa        30.4   4  1\nFord Pantera L      15.8   8  1\nFerrari Dino        19.7   6  1\nMaserati Bora       15.0   8  1\nVolvo 142E          21.4   4  1\n\n\nOk, now we have our data that we are going to work with, so let’s check out some features.\nFirst we will just apply the styled_table() function and inspect the output.\n\nstl_df <- df |>\n  styled_table(keep_header = TRUE)\n\nclass(stl_df)\n\n[1] \"StyledTable\"\nattr(,\"package\")\n[1] \"styledTables\"\n\n\nNow let’s apply some simple formatting.\n\nstl_df <- stl_df |>\n  set_border_position(\"all\", row_id = 1) |>\n  set_bold(row_id = 1) |>\n  set_fill_color(\"#00FF00\", col_id = 2, condition = X == \"6\")\n\nWrite out to excel.\n\nwb <- createWorkbook()\nsheet <- createSheet(wb, \"mtcars_tbl\")\n\n# Insert table\nwrite_excel(sheet, stl_df)\n\n# Save workbook\nsaveWorkbook(wb, \"test.xlsx\")\n\nHere is the test output:\n\n\n\nTest Output"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html",
    "href": "posts/rtip-2023-04-18/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "",
    "text": "Shiny is an R package that allows you to build interactive web applications using R code. TidyDensity is an R package that provides a tidyverse-style interface for working with probability density functions. In this tutorial, we’ll use these two packages to build a Shiny app that allows users to interact with TidyDensity functions."
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#required-packages",
    "href": "posts/rtip-2023-04-18/index.html#required-packages",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "Required Packages",
    "text": "Required Packages\nBefore we dive into the code, let’s go over the packages that we’ll be using in this app:\n\nShiny: As mentioned earlier, Shiny is an R package for building interactive web applications. It provides a variety of input controls and output elements that allow you to create user interfaces for your R code.\nTidyDensity: TidyDensity is an R package that provides a tidyverse-style interface for working with probability density functions. It provides a set of functions for generating density functions, as well as a tidy_autoplot() function for creating visualizations.\ntidyverse: Tidyverse is a collection of R packages designed for data science. It includes many popular packages such as ggplot2, dplyr, and tidyr. We’ll be using some functions from the tidyverse packages in our Shiny app.\nDT: DT is an R package for creating interactive tables in RMarkdown documents, Shiny apps, and RStudio. We’ll be using the DT::datatable() function to create a table of output data in our app.\n\nLoad them up!\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(tidyverse)\nlibrary(TidyDensity)"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#the-ui-object",
    "href": "posts/rtip-2023-04-18/index.html#the-ui-object",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "The UI Object",
    "text": "The UI Object\nThe UI object is the first argument of the shinyApp() function, and it defines the layout and appearance of the app. In our TidyDensity Shiny app, we’ll use a sidebar layout with two input controls and two output elements:\n\nSelect Function Input: A selectInput() control that allows users to select one of four TidyDensity functions: tidy_normal(), tidy_bernoulli(), tidy_beta(), and tidy_gamma().\nNumber of Simulations Input: A numericInput() control that allows users to specify the number of simulations to use in the TidyDensity function.\nSample Size Input: A numericInput() control that allows users to specify the sample size to use in the TidyDensity function.\nDensity Plot Output: A plotOutput() element that displays the density plot generated by tidy_autoplot().\nData Table Output: A dataTableOutput() element that displays the output data from the TidyDensity function.\n\nHere’s the code for the UI object:\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"function\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                    )\n                  ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200)\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      dataTableOutput(\"data_table\")\n    )\n  )\n)"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#the-server-object",
    "href": "posts/rtip-2023-04-18/index.html#the-server-object",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "The Server Object",
    "text": "The Server Object\nThe server object is the second argument of the shinyApp() function, and it defines the behavior and output of the app. In our TidyDensity Shiny app, the server object consists of two reactive expressions that generate the output elements based on the user inputs:\n\nData Reactive Expression: A reactive expression that generates the output data for the selected TidyDensity function based on the user inputs. We use match.fun() to convert the selected function name into an R function, and we pass the num_sims and n arguments from the input controls.\nDensity Plot Reactive Expression: A reactive expression that generates the density plot using tidy_autoplot() and the output data from the data reactive expression.\nData Table Output: We use DT::renderDataTable() to generate the data table output element based on the output data from the data reactive expression.\n\nHere’s the code for the server object:\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$function)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() %>%\n      tidy_autoplot()\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n}"
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html",
    "href": "posts/rtip-2023-04-19/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "",
    "text": "Shiny is an R package that allows you to create interactive web applications from R code. In this blog post, we’ll explore the different components of a Shiny application and show how they work together to create an interactive data visualization app. This is a part 2 with a small enhancement."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-ui",
    "href": "posts/rtip-2023-04-19/index.html#the-ui",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The UI",
    "text": "The UI\nThe user interface (UI) is the visual part of the app that the user interacts with. In our app, the UI is defined using the fluidPage() function from the shiny package. It consists of a title panel, a sidebar layout, and a main panel.\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      )\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\nThe title panel displays the app name, while the sidebar layout contains the input controls for the user. In this case, we have four input elements:\n\nselectInput() allows the user to choose a statistical distribution to generate data from.\nnumericInput() allows the user to set the number of simulations.\nAnother numericInput() allows the user to set the sample size.\nselectInput() allows the user to choose the type of plot to display.\n\nThe main panel contains the output elements for the app, in this case a plot and a table."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-server",
    "href": "posts/rtip-2023-04-19/index.html#the-server",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The Server",
    "text": "The Server\nThe server is the backend of the app that handles the logic and generates the output based on user input. In our app, the server is defined using the server() function from the shiny package.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n}\n\nThe server() function takes two arguments, input and output. These arguments allow the server to interact with the user interface.\nFirst, we create a reactive data object data, which takes in the user’s input for the function, number of simulations, and sample size, and passes it to the appropriate function using match.fun().\nNext, we create the density_plot output. We use the renderPlot() function to create a reactive plot of the data using the tidy_autoplot() function from the {TidyDensity} package. The tidy_autoplot() function allows the user to choose from several plot types, including density, quantile, probability, qq, and mcmc. We then print the plot using the print() function.\nFinally, we create the data_table output using the DT::renderDataTable() function. This output displays the reactive data as a table using the DT::datatable() function."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-shiny-app",
    "href": "posts/rtip-2023-04-19/index.html#the-shiny-app",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The Shiny App",
    "text": "The Shiny App\nFinally, we run the Shiny app using the shinyApp() function, which takes the ui and server functions as arguments:\n\nshinyApp(ui = ui, server = server)\n\nThis launches the app and displays the user interface. The user can interact with the app by selecting a function, specifying the number of simulations and sample size, and viewing the resulting density plot and data table. The app provides a simple and interactive way to explore the TidyDensity package and its functionality."
  },
  {
    "objectID": "posts/rtip-2023-04-20/index.html",
    "href": "posts/rtip-2023-04-20/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 3",
    "section": "",
    "text": "Introduction\nIn the previous post we allowed users to choose a distribution and a plot type. Now, we want to allow users to download a .csv file of the data that is generated.\nIn the UI, we added a downloadButton with outputId = \"download_data\" and label = \"Download Data\". In the server, we added a downloadHandler that takes a filename and content function. The filename function returns the name of the file to be downloaded (in this case, we used the selected function name as the file name with “.csv” extension). The content function writes the reactive data to a CSV file using the write.csv function. The downloadHandler returns the file to be downloaded when the button is clicked.\nSee here: \n\n\nUI Section\nHere is the update to the UI Section\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      # Download the data\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n\n\nServer Section\nHere is the update to the Server section.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      paste0(input$functions, \".csv\")\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\n\n\nConclusion\nWith these changes, the user can now export the data to a .csv file by clicking the “Export Data” button and selecting where to save the file.\nI hope this update to the TidyDensity app will make it more useful for your data analysis needs. If you have any questions or feedback, please feel free to let me know, and as usual…Steal this Code!! Modify for yourself and see what you come up with.\nHere is the entire script:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      paste0(input$functions, \".csv\")\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-21/index.html",
    "href": "posts/rtip-2023-04-21/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 4",
    "section": "",
    "text": "Introduction\nIf you’re new to data science or statistics, you may have heard about probability distributions. Probability distributions are mathematical functions that help us understand the probability of a random variable taking on a certain value. For example, if we’re rolling a fair six-sided die, we know that each number has an equal chance of being rolled (1/6 or about 17% chance). We can represent this using a probability distribution, specifically a discrete uniform distribution.\nHowever, not all probability distributions are as simple as a uniform distribution. Many real-world phenomena, such as the heights of people, the number of cars passing through a toll booth in a day, or the amount of rainfall in a particular area, are continuous and can’t be represented using a discrete distribution. Instead, we use continuous probability distributions, which describe the probability of a continuous variable taking on a range of values.\nThere are many different types of continuous probability distributions, each with their own properties and use cases. For example, the normal distribution, also known as the bell curve, is commonly used to model many natural phenomena, such as human heights and weights. The beta distribution is used to model proportions or percentages, such as the proportion of voters who support a particular candidate. The gamma distribution is used to model the time between events in a Poisson process, such as the time between customers arriving at a store.\nThe sample TidyDensity App is a tool that helps us explore and visualize these different types of probability distributions. It’s a web application built using the R programming language and the Shiny framework, which allows us to create interactive web applications with R.\nLet’s break down the different components of the TidyDensity App.\n\n\nUser Interface\nThe user interface, or UI for short, is what the user sees and interacts with when they use the app. It’s built using HTML, CSS, and JavaScript, and it’s the first thing the user sees when they open the app.\nThe TidyDensity App has a simple UI that allows the user to select from four different probability distributions: normal, Bernoulli, beta, and gamma. Each of these distributions has its own properties and use cases, and the user can select which one they want to explore using a dropdown menu.\nIn addition, the user can specify the number of simulations they want to run, which determines how many times the probability distribution is sampled to generate data. They can also specify the sample size, which determines how many data points are generated in each simulation.\nFinally, the user can select which type of plot they want to see, such as a density plot, a quantile plot, a probability plot, or a QQ plot. Each of these plots shows a different aspect of the data generated from the probability distribution, and the user can choose which one to explore.\nHere is the code:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n\n\n\nHere is the new addition to the UI\n\n\n\n\nServer\nThe server is the back-end of the TidyDensity App. It’s responsible for generating the data based on the user’s inputs, and for creating the plots and tables that the user sees on the UI.\nThe server is written in R, and it uses several R packages to generate the data and create the plots. For example, the TidyDensity package is used to generate data from the selected probability distribution, and the ggplot2 package is used to create the plots.\nThe server is also responsible for handling user inputs, such as which probability distribution to use, how many simulations to run, and which plot type to show. It then generates the appropriate data and plot based on these inputs and sends them back to the UI for display.\nThe first thing we do is create a reactive variable data that will store the output of the match.fun() function, which is called with the arguments .num_sims and .n obtained from the user interface. We use the reactive variable because it will update automatically whenever the inputs are changed.\nThe output$density_plot object is created with renderPlot(), which takes the reactive variable data() and passes it to tidy_autoplot() with the plot type selected by the user in the input$plot_type object. The resulting plot is then printed to the user interface.\nThe output$data_table object is created with DT::renderDataTable(), which takes the reactive variable data() and returns a table to the user interface using the DT::datatable() function.\nFinally, the output$download_data object is created using downloadHandler(), which creates a download button for the user to download a .csv file of the data. The filename argument specifies the name of the file, and the content argument writes the data to a .csv file.\nHere is the code:\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n\n\nData Table\nThe data table is a table that shows the data generated from the probability distribution. It’s displayed on.\nOverall, this app is designed to allow users to generate various types of probability density plots and accompanying data tables based on user input. By allowing users to select different functions, sample sizes, and plot types, this app provides a flexible and customizable tool for exploring and visualizing probability distributions.\n\n\nFull Shiny App\nHere is the full script:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-24/index.html",
    "href": "posts/rtip-2023-04-24/index.html",
    "title": "Exploring Distributions with {shiny}, {TidyDensity} and {plotly} Part 5",
    "section": "",
    "text": "Introduction\nI have been writing about using the {TidyDensity} package with shiny for the last few posts, and this one is the last. This post will go over the app and discuss how to change the output of the graph from a ggplot2 object into a plotly object. So we will end up with something like this in the menu panel:\n\n\n\nPlotly Output\n\n\nAnd here is the difference between the plots, first the ggplot2 plot: \nAnd the plotly_plot: \nFirst, the required libraries are loaded: shiny, TidyDensity, tidyverse, DT, and plotly.\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(plotly)\n\n\n\nUI\nThe user interface (UI) is defined using the fluidPage() function from the shiny package. The UI consists of a title panel, a sidebar panel, and a main panel. The title panel simply displays the title of the app, while the sidebar panel contains user input elements such as radio buttons, text inputs, and numeric inputs. The main panel displays the plot, data table, and download button.\n\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      # user input elements\n    ),\n    mainPanel(\n      # plot, data table, and download button\n    )\n  )\n)\n\nNext, the server is defined using the server() function from the shiny package. The server is responsible for generating the output based on the user inputs. The first step is to create reactive data using the reactive() function. The reactive data is created based on the user inputs for the distribution function or the entered data. The match.fun() function is used to match the selected function with the corresponding function in the TidyDensity package. The tidy_empirical() function is used if the user entered their own data.\n\n\nServer\n\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n\nAfter the reactive data is created, the output is generated. The output consists of the density plot, data table, and download button. The renderPlot() and renderPlotly() functions are used to generate the plot output. The renderDataTable() function is used to generate the data table output. The downloadHandler() function is used to generate the download button.\n\n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n\nNext, we define the server function, which contains the code that will run in response to user input. We start by creating a reactive data object called data. This object will store the data that will be used to generate the plots and tables in the app.\nThe data that data stores depends on the user’s input. If the user selects “Enter Data” in the sidebar, then data will be set to a tidy_empirical() object generated from the user-entered data. Otherwise, if the user selects “Select Function”, then data will be set to a tidy_ function object generated using the user’s choices for number of simulations and sample size.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  ...\n}\n\nThe tidy_empirical() function is used to generate a density plot of the empirical distribution of the user-entered data. This function takes the user-entered data as input and returns a tidy data frame that can be used to create a density plot.\nThe tidy_ functions are used to simulate data from various distributions and generate plots based on that data. These functions take the number of simulations and sample size as input and return a tidy data frame that can be used to create various types of plots.\nNext, we define the code for generating the density plot. This code uses the data object that was created earlier to generate a plot. The tidy_autoplot() function is used to generate the plot based on the user’s selected plot type. If the user selects the “Use Plotly” option, then the plot is generated using the ggplotly() function from the plotly package.\n\n # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n\nThe ggplotly() function is used to generate an interactive version of the plot that can be zoomed in and out of and hovered over to see details about specific data points.\nNext, we define the code for generating the data table. This code simply displays the data object as a table using the datatable() function from the DT package.\n\n# Create data table\noutput$data_table <- DT::renderDataTable({\n  # Return reactive data as a data table\n  if (!is.null(data())) {\n    DT::datatable(data())\n  }\n})\n\nFinally, we define the code for downloading the data as a CSV file. This code uses the downloadHandler() function to generate a file download link that, when clicked, will download the data as a CSV file. The name of the CSV file depends on the user’s input.\n\n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n\nFinally, here is the script in it’s entirety, steal it and see what you can come up with!!\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(plotly)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      selectInput(inputId = \"plotly_option\",\n                  label = \"Use Plotly\",\n                  choices = c(\"TRUE\", \"FALSE\"),\n                  selected = \"FALSE\"\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      conditionalPanel(\n        condition = \"input.plotly_option == 'TRUE'\",\n        plotlyOutput(\"density_plotly\")\n      ),\n      conditionalPanel(\n        condition = \"input.plotly_option == 'FALSE'\",\n        plotOutput(\"density_plot\")\n      ),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-25/index.html",
    "href": "posts/rtip-2023-04-25/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 1",
    "section": "",
    "text": "Introduction\nWelcome to the {tidyAML} Model Builder, a Shiny web application that allows you to build predictive models using the tidyAML and Parsnip packages in R.\nLet’s dive into the code to understand how it works!\n\n\nLoad Libraries\nFirst, we load the necessary packages:\n\nshiny\ntidyAML\nrecipes\nDT\nglmnet.\n\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\n\n\n\nUI\nNext, we define the user interface (UI) of the Shiny app using the fluidPage() function from the shiny package. The UI consists of a title panel, a sidebar panel, and a main panel.\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\n        \"dataset\", \n        \"Choose a built-in dataset:\", \n        choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\n        \"predictor_col\", \n        \"Select the predictor column:\", \n        choices = NULL\n      ),\n      selectInput(\n        \"model_type\", \n        \"Select a model type:\", \n        choices = c(\"regression\", \"classification\")\n      ),\n      selectInput(\n        \"model_fn\", \n        \"Select a model function:\", \n         choices = c(\"lm\", \"glm\", \"glmnet\")\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\")\n    )\n  )\n)\n\nThe sidebarPanel() contains several input elements that allow the user to specify the dataset, the predictor column, the type of model, and the model function. There is also an input element that allows the user to upload their own data file. The actionButton() is used to trigger the model building process. Finally, the verbatimTextOutput() element is used to display the output of the model building process.\nThe mainPanel() contains a single verbatimTextOutput() element that displays the output of the model building process.\nNext, we define the server function, which is responsible for handling the user inputs and building the predictive models. The server function takes three arguments:input, output, and session.\n\nserver <- function(input, output, session){\n  ...\n}\n\nWe start by defining a reactive expression called data. This expression reads in the user-specified dataset or data file and updates the predictor_col select input with the names of the columns of the dataset.\n\n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n\nThe first reactive expression, data, reads in the data file uploaded by the user or selects a built-in dataset, depending on which option the user chooses. If the user uploads a file, the read.csv() function is used to read the data file into a data frame. If the user selects a built-in dataset, the get() function is used to retrieve the data frame associated with that dataset. In both cases, the column names of the data frame are used to update the choices in the predictor_col select input, so that the user can select which column to use as the predictor variable.\nThe next reactive expression, recipe_obj, creates a recipe object based on thepredictor_col selected by the user and the data frame returned by data(). The as.formula() function is used to create a formula that specifies the predictor column as the response variable and all other columns as the predictors. The resulting formula is passed to the recipe() function, along with the data frame. The step_normalize() function is then used to standardize all numeric predictors (except for the outcome variable) to have a mean of 0 and a standard deviation of 1. The resulting recipe object is returned by the reactive expression.\n\n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    \n    return(rec)\n  })\n\nThe model_fn reactive expression uses a switch() statement to determine which model function to use based on the model_fn select input. The available options are \"lm\" (for linear regression), \"glm\" (for generalized linear models), and \"glmnet\" (for regularized linear models).\n\n  model_fn <- reactive({\n    switch(\n      input$model_fn,\n      \"lm\" = \"lm\",\n      \"glm\" = \"glm\",\n      \"glmnet\" = \"glmnet\"\n    )\n  })\n\nThe last reactive expression, model, uses the fast_regression() or fast_classification() functions from the tidyAML package to build a regression or classification model based on the data, recipe, and model function selected by the user. The resulting model object is returned by the reactive expression.\n\n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(\n        .data = data(),\n        .rec_obj = recipe_obj(),\n        .parsnip_eng = model_fn()\n      )\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(\n        .data = data(),\n        .rec_obj = recipe_obj(),\n        .parsnip_eng = model_fn()\n      )\n    }\n    return(mod)\n  })\n\nFinally we output the summary of the recipe_obj and print the resulting tibble of model(s) to the screen.\n\n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n\nAnd of course, we cannot serve our app until we run the following line:\n\nshinyApp(ui = ui, server = server)\n\nI hope you have enjoyed this post. Please steal this code and see what you can do with it. I am trying to figure out how to print the tibble using the DT package so maybe in another post.\n\n\nFull Shiny App\nHere are some pictures \n\n\n\nMaking a recipe change\n\n\n\n\n\nSingle Model Output\n\n\n\n\n\nTwo Model Output with one successful failure\n\n\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n                  ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n                  ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_fn\", \"Select a model function:\", \n                  choices = c(\"lm\", \"glm\", \"glmnet\")\n                  ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n                  ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_fn <- reactive({\n    switch(input$model_fn,\n           \"lm\" = \"lm\",\n           \"glm\" = \"glm\",\n           \"glmnet\" = \"glmnet\")\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_fn())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_fn())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-26/index.html",
    "href": "posts/rtip-2023-04-26/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 2",
    "section": "",
    "text": "Introduction\nYesterday I spoke about building tidymodels models using my package {tidyAML} and {shiny}. I have made an update to it, and will continue to make updates to it this week.\nI have added all of the supported engines for regression problems only, NOT classification yet, that will be tomorrow’s work. I will then add a drop down for users to pick which backend function they want to use from {parsnp} like linear_reg().\nHere are some pictures of the udpates.\n\n\n\nNew Drop Down Additions\n\n\n\n\n\nreactable Error, not sure on how to fix yet\n\n\n\n\n\nreactable output\n\n\nHere is the full application, please steal this code and modify for yourself, you never know what you might come up with!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n                  ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n                  ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_fn\", \"Select a model function:\", \n                  choices = c(\"all\",\"lm\",\"brulee\",\"gee\",\"glm\",\n                              \"glmer\",\"glmnet\",\"gls\",\"lme\",\n                              \"lmer\",\"stan\",\"stan_glmer\",\n                              \"Cubist\",\"hurdle\",\"zeroinfl\",\"earth\",\n                              \"rpart\",\"dbarts\",\"xgboost\",\"lightgbm\",\n                              \"partykit\",\"mgcv\",\"nnet\",\"kknn\",\"ranger\",\n                              \"randomForest\",\"xrf\",\"LiblineaR\",\"kernlab\"\n                            )\n                  ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n                  ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_fn <- reactive({\n    switch(input$model_fn,\n           \"all\" = \"all\",\n           \"lm\" = \"lm\",\n           \"brulee\" = \"brulee\",\n           \"gee\" = \"gee\",\n           \"glm\" = \"glm\",\n           \"glmer\" = \"glmer\",\n           \"glmnet\" = \"glmnet\",\n           \"gls\" = \"gls\",\n           \"lme\" = \"lme\",\n           \"lmer\" = \"lmer\",\n           \"stan\" = \"stan\",\n           \"stan_glmer\" = \"stan_glmer\",\n           \"Cubist\" = \"Cubist\",\n           \"hurdle\" = \"hurdle\",\n           \"zeroinfl\" = \"zeroinfl\",\n           \"earth\" = \"earth\",\n           \"rpart\" = \"rpart\",\n           \"dbarts\" = \"dbarts\",\n           \"xgboost\" = \"xgboost\"          ,\n           \"lightgbm\" = \"lightgbm\",\n           \"partykit\" = \"partykit\",\n           \"mgcv\" = \"mgcv\",\n           \"nnet\" = \"nnet\",\n           \"kknn\" = \"kknn\",\n           \"ranger\" = \"ranger\",\n           \"randomForest\" = \"randomForest\",\n           \"xrf\" = \"xrf\",\n           \"LiblineaR\" = \"LiblineaR\",\n           \"kernlab = kernlab\")\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_fn())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_fn())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/rtip-2023-04-27/index.html",
    "href": "posts/rtip-2023-04-27/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 3",
    "section": "",
    "text": "Introduction\nAs data science continues to be a sought-after field, creating a reliable and accurate model is essential. While there are various machine learning algorithms available, the process of selecting the correct algorithm can be complex. The {tidyAML} package, part of the tidymodels suite, offers an easy-to-use, consistent interface for building machine learning models. In this post, we will explore a Shiny application that utilizes tidyAML to build a machine learning model.\nToday I have updated the tidyAML shiny app to include the ability to set the parameter of the fast_regression() function .parsnip_fns and this is things like linear_reg.\nHere is a full list of what is available:\n\nlibrary(tidyAML)\nlibrary(dplyr)\n\nc(\"all\",\n  make_regression_base_tbl() |> \n    pull(.parsnip_fns) |> \n    unique()\n  )\n\n [1] \"all\"              \"linear_reg\"       \"cubist_rules\"     \"poisson_reg\"     \n [5] \"bag_mars\"         \"bag_tree\"         \"bart\"             \"boost_tree\"      \n [9] \"decision_tree\"    \"gen_additive_mod\" \"mars\"             \"mlp\"             \n[13] \"nearest_neighbor\" \"rand_forest\"      \"rule_fit\"         \"svm_linear\"      \n[17] \"svm_poly\"         \"svm_rbf\"         \n\n\nI have updated the UI to reflect using that method as well. Here is the UI changes:\n\n      selectInput(\"model_engine\", \"Select a model engine:\", \n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_engine) |> \n                                unique()\n                  )\n      ),\n      selectInput(\"model_fns\", \"Select a model function:\",\n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_fns) |> \n                                unique()\n                              )\n\nHere are some pictures showing the changes:\n\n\n\nUI Change\n\n\n\n\n\nUI Change 2\n\n\n\n\n\nOutput\n\n\nSo what this means is that we can just pick a function like parsnip::linear_reg() and leave the engine set to \"all\" and it will build models for all engines supported that work with linear_reg().\n\n\nThe Shiny Application\nThe Shiny application is a graphical user interface (GUI) that allows users to select a dataset, predictor column, model type, and engine, and then build a machine learning model. The user can upload a CSV or TXT file or choose one of two built-in datasets: “mtcars” or “iris”. The user can select the predictor column, which is the variable used to predict the outcome, and then choose the model type, either “regression” or “classification”. Next, the user can select a model engine and a model function to use in building the model. Once the user has made all the selections, they can click the “Build Model” button to create the model.\nThe code for the Shiny application can be broken down into two parts, the User Interface (UI) and the Server. Let’s take a closer look at each of these parts.\n\n\nThe UI\nThe UI is created using the fluidPage() function from the shiny package. The titlePanel() function creates the title of the application. The sidebarLayout() function creates the sidebar and main panel. The sidebar contains input controls such as file input, select input, and an action button. The main panel displays the outputs generated by the model.\nThe fileInput() function creates a widget that allows the user to upload a data file. The selectInput() function creates dropdown menus for the user to select the dataset, predictor column, model type, model engine, and model function. The actionButton() function creates a button that the user clicks to build the model. The verbatimTextOutput() function and reactableOutput() function display the output generated by the model.\n\n\nThe Server\nThe Server is where the input data is processed, the model is built, and the output is generated. The Server is created using the server() function from the shiny package.\nThe reactive() function is used to create a reactive object called data that reads in the data file or built-in dataset selected by the user. The eventReactive() function is used to create a reactive object called recipe_obj that creates a recipe for preprocessing the data. The recipe includes steps to normalize the numeric variables and remove the outcome variable from the recipe.\nTwo other reactive objects, model_engine and model_fns, are created using the switch() function. These objects contain a list of available engines and model functions for the user to choose from.\nFinally, the eventReactive() function is used to create a reactive object called model that builds the machine learning model. The fast_regression() and fast_classification() functions from the tidyAML package are used to build the regression and classification models, respectively.\n\n\nConclusion\nIn this post, we explored a Shiny application that uses tidyAML to build a machine learning model. The application allows users to select a dataset, predictor column, model type, engine, and function to build a machine learning model. The Shiny application is an excellent tool for those who are new to machine learning or those who want to streamline the rapid prototyping process.\n\n\nFull Application\nThis is a work in progress, and I want you to steal this code and see what you can come up with!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_engine\", \"Select a model engine:\", \n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_engine) |> \n                                unique()\n                  )\n      ),\n      selectInput(\"model_fns\", \"Select a model function:\",\n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_fns) |> \n                                unique()\n                              )\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n      )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n    ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_engine <- reactive({\n    switch(input$model_engine,\n           \"all\" = \"all\",\n           \"lm\" = \"lm\",\n           \"brulee\" = \"brulee\",\n           \"gee\" = \"gee\",\n           \"glm\" = \"glm\",\n           \"glmer\" = \"glmer\",\n           \"glmnet\" = \"glmnet\",\n           \"gls\" = \"gls\",\n           \"lme\" = \"lme\",\n           \"lmer\" = \"lmer\",\n           \"stan\" = \"stan\",\n           \"stan_glmer\" = \"stan_glmer\",\n           \"Cubist\" = \"Cubist\",\n           \"hurdle\" = \"hurdle\",\n           \"zeroinfl\" = \"zeroinfl\",\n           \"earth\" = \"earth\",\n           \"rpart\" = \"rpart\",\n           \"dbarts\" = \"dbarts\",\n           \"xgboost\" = \"xgboost\"          ,\n           \"lightgbm\" = \"lightgbm\",\n           \"partykit\" = \"partykit\",\n           \"mgcv\" = \"mgcv\",\n           \"nnet\" = \"nnet\",\n           \"kknn\" = \"kknn\",\n           \"ranger\" = \"ranger\",\n           \"randomForest\" = \"randomForest\",\n           \"xrf\" = \"xrf\",\n           \"LiblineaR\" = \"LiblineaR\",\n           \"kernlab = kernlab\")\n  })\n  \n  model_fns <- reactive({\n    switch(input$model_fns,\n           \"all\" = \"all\",\n           \"linear_reg\" = \"linear_reg\",\n           \"cubist_rules\" = \"cubist_rules\",\n           \"poisson_reg\" = \"poisson_reg\",\n           \"bag_mars\" = \"bag_mars\",\n           \"bag_tree\" = \"bag_tree\",\n           \"bart\" = \"bart\",\n           \"boost_tree\" = \"boost_tree\",\n           \"decision_tree\" = \"decision_tree\",\n           \"gen_additive_mod\" = \"gen_additive_mod\",\n           \"mars\" = \"mars\",\n           \"mlp\" = \"mlp\",\n           \"nearest_neighbor\" = \"nearest_neighbor\",\n           \"rand_forest\" = \"rand_forest\",\n           \"rule_fit\" = \"rule_fit\",\n           \"svm_linear\" = \"svm_linear\",\n           \"svm_poly\" = \"svm_poly\",\n           \"svm_rbf\" = \"svm_rbf\"\n    )\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_engine(),\n                             .parsnip_fns = model_fns())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_engine(),\n                                 .parsnip_fns = model_fns())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-29/index.html",
    "href": "posts/rtip-2023-04-29/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 4",
    "section": "",
    "text": "Introduction\nThis is a Shiny app for building models using the {tidyAML} which is based on the tidymodels package in R. The app allows you to upload your own data or choose from one of two built-in datasets (mtcars or iris) and select the type of model you want to build (regression or classification).\nLet’s take a closer look at the code.\nFirst, the necessary packages are loaded:\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\nThe tidymodels_prefer() function is called to set some default options for the tidymodels package, and load_deps() from tidyAML is called to make sure all the necessary packages are loaded, you can also separately run install_deps() to make sure they all get installed.\n\ntidymodels_prefer()\nload_deps()\n\nNext, the user interface (UI) is defined using the fluidPage() function. The UI consists of a title panel and a sidebar layout with various input elements, such as file input and select input. There are also two conditional panels that are shown depending on the selected model type (regression or classification). The UI also includes an action button and some output elements, such as verbatimTextOutput and reactableOutput.\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\",\n                  \"Select the predictor column:\",\n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")\n      ),\n      conditionalPanel(\n        condition = \"input.model_type == 'regression'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique()))\n      ),\n      \n      conditionalPanel(\n        condition = \"input.model_type == 'classification'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique())),\n        checkboxInput(\"predictor_factor\",\n                      \"Convert predictor column to factor?\",\n                      value = TRUE)\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nAfter defining the UI, the server function is defined. The server function handles the reactive behavior of the app.\nThe first reactive element is data, which reads in the data file if one is uploaded or loads the selected built-in dataset if one is chosen. It also converts the predictor column to a factor if the classification model type is selected.\nIn the server function, we first define a reactive expression data() that will read the data file uploaded by the user or one of the built-in datasets (mtcars or iris). If the user has uploaded a file, the function read.csv is used to read the data, and if it’s a classification problem, the predictor column is converted to a factor variable. The updateSelectInput function is then called to update the predictor_col select input with the names of the columns in the data. If the user has chosen one of the built-in datasets, it is loaded using the get function, and the same preprocessing is performed.\nNext, we define an event reactive recipe_obj() that creates a recipes object based on the selected predictor column and normalizes the numeric variables in the data. The step_normalize function standardizes all numeric variables (except the outcome variable) to have mean 0 and standard deviation 1. This is a common preprocessing step in machine learning pipelines that can improve model performance.\nTwo reactive expressions, model_engine() and model_fns(), are then defined to generate the available model engines and functions based on the selected model type. For regression models, the make_regression_base_tbl functions are used, and for classification models, the make_classification_base_tbl functions are used. These functions return a table with information about the available model engines and functions for a given problem type. The pull function is used to extract the relevant columns from the table, and unique is used to remove duplicate values. The c function is used to concatenate the “all” choice with the available model engines or functions.\nFinally, an event reactive model() is defined that builds the model based on the selected parameters. If the model type is regression, the fast_regression function from the tidyAML package is used, and if the model type is classification, the fast_classification function is used. These functions take as inputs the data, the recipes object, the selected model engine and function, and any additional model parameters.\nThere are three output functions defined in the server: output$recipe_output, output$model_table, and output$model_reactable. The first output function output$recipe_output renders a summary of the recipes object created by recipe_obj() if the predictor_col input is not null. The second output function output$model_table prints the model object returned by model() if the build_model button has been clicked. The third output function output$model_reactable renders a reactive table using the reactable function from the reactable package if the build_model button has been clicked. This table displays the tidyaml_model_tbl.\nOverall, this code creates a Shiny web application that allows users to build machine learning models using the tidymodels framework via {tidyAML}. Users can upload their own data or use one of the built-in datasets, select a predictor column, choose a model type, select a model engine and function, and build the model. The output is displayed in a table that provides insights into the model’s performance and coefficients. This code is useful for data scientists and analysts who want to quickly build and evaluate machine learning models without having to write code from scratch.\n\n\nFull Application\nAs usual, steal this code and make it your own! See what you can do too!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\",\n                  \"Select the predictor column:\",\n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")\n      ),\n      conditionalPanel(\n        condition = \"input.model_type == 'regression'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique()))\n      ),\n      \n      conditionalPanel(\n        condition = \"input.model_type == 'classification'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique())),\n        checkboxInput(\"predictor_factor\",\n                      \"Convert predictor column to factor?\",\n                      value = TRUE)\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n      )\n      if (input$model_type == \"classification\") {\n        df[[input$predictor_col]] <- as.factor(df[[input$predictor_col]])\n      }\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      if (input$model_type == \"classification\") {\n        df[[input$predictor_col]] <- as.factor(df[[input$predictor_col]])\n      }\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n    ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_engine <- reactive({\n    if (input$model_type == \"regression\") {\n      c(\"all\", \n        make_regression_base_tbl() %>% \n          pull(.parsnip_engine) %>% \n          unique())\n    } else if (input$model_type == \"classification\") {\n      c(\"all\", \n        make_classification_base_tbl() %>% \n          pull(.parsnip_engine) %>% \n          unique())\n    }\n  })\n  \n  model_fns <- reactive({\n    if (input$model_type == \"regression\") {\n      c(\"all\", \n        make_regression_base_tbl() %>% \n          pull(.parsnip_fns) %>% \n          unique())\n    } else if (input$model_type == \"classification\") {\n      c(\"all\", \n        make_classification_base_tbl() %>% \n          pull(.parsnip_fns) %>% \n          unique())\n    }\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_engine(),\n                             .parsnip_fns = model_fns())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_engine(),\n                                 .parsnip_fns = model_fns())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-01/index.html",
    "href": "posts/rtip-2023-05-01/index.html",
    "title": "Extracting a model call from a fitted workflow in {tidymodels}",
    "section": "",
    "text": "Introduction\nIn this post, we are using a package called tidymodels, which provides a suite of tools for modeling and machine learning.\nNow, let’s take a closer look at the code itself and how we extract a model call from a fitted workflow object.\n\nlibrary(tidymodels)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nrec_obj\n\nThe first line loads the tidymodels package. Then, we create a “recipe” object called rec_obj using the recipe() function. A recipe is a set of instructions for preparing data for modeling. In this case, we are telling the recipe to use the mpg variable as the outcome or dependent variable, and all other variables in the mtcars dataset as the predictors or independent variables.\n\nmodel_spec <- linear_reg(mode = \"regression\", engine = \"lm\")\nmodel_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNext, we create a “model specification” object called model_spec using the linear_reg() function. This specifies the type of model we want to use, which is a linear regression model in this case. We also specify that the model is a regression (i.e., we are predicting a continuous outcome variable) and that the model engine is “lm”, which stands for “linear model”.\n\nwflw <- workflow() |>\n  add_recipe(rec_obj) |>\n  add_model(model_spec)\nwflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nIn the next section of code, we create a “workflow” object called wflw using the workflow() function. A workflow is a way of organizing the steps involved in building a machine learning model. In this case, we are using a “pipe” (|>) to sequentially add the recipe and model specification to the workflow. This means that we first add the recipe to the workflow using the add_recipe() function, and then add the model specification using the add_model() function.\n\nwflw_fit <- fit(wflw, data = mtcars)\nwflw_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   12.30337     -0.11144      0.01334     -0.02148      0.78711     -3.71530  \n       qsec           vs           am         gear         carb  \n    0.82104      0.31776      2.52023      0.65541     -0.19942  \n\n\nFinally, we fit the workflow to the data using the fit() function, which takes the workflow object (wflw) and the data (mtcars) as input. This creates a new object called wflw_fit, which is the fitted model object. This object contains various pieces of information about the fitted model, such as the model coefficients and the R-squared value.\n\nwflw_fit$fit$fit$fit$call\n\nstats::lm(formula = ..y ~ ., data = data)\n\n\nThe last line of code extracts the actual function call that was used to fit the model. This can be useful for reproducing the analysis later on.\nOverall, the code you shared shows how to build a simple linear regression model using the tidymodels package in R. We start by creating a recipe that specifies the outcome variable and predictor variables, then create a model specification for a linear regression model, and finally combine these into a workflow and fit the model to the data."
  },
  {
    "objectID": "posts/rtip-2023-05-03/index.html",
    "href": "posts/rtip-2023-05-03/index.html",
    "title": "How to Download a File from the Internet using download.file()",
    "section": "",
    "text": "Introduction\nThe download.file() function in R is used to download files from the internet and save them onto your computer. Here’s a simple explanation of how to use it:\n\nSpecify the URL of the file you want to download.\nSpecify the file name and the location where you want to save the file on your computer.\nCall the download.file() function, passing in the URL and file name/location as arguments.\n\nHere’s an example:\n\n# Specify the URL of the file you want to download\nurl <- \"https://example.com/data.csv\"\n\n# Specify the file name and location where you want to save the file on your computer\nfile_name <- \"my_data.csv\"\nfile_path <- \"/path/to/save/folder/\"\n\n# Call the download.file() function, passing in the URL and file name/location as arguments\ndownload.file(url, paste(file_path, file_name, sep = \"\"), mode = \"wb\")\n\nIn this example, we’re downloading a CSV file from “https://example.com/data.csv”, and saving it as “my_data.csv” in the “/path/to/save/folder/” directory on our computer.\nThe mode = “wb” argument specifies that we want to download the file in binary mode.\nOnce you run this code, the file will be downloaded from the URL and saved to your specified file location.\nLet’s try a working example.\n\n\nExample\nWe are going to download the Measure Dates file from the following location: {https://data.cms.gov/provider-data/dataset/4j6d-yzce}\n\nurl <- \"https://data.cms.gov/provider-data/sites/default/files/resources/49244993de5a948bcb0d69bf5cc778bd_1681445112/Measure_Dates.csv\"\n\nfile_name <- \"measure_dates.csv\"\nfile_path <- \"C:\\\\Downloads\\\\\"\n\ndownload.file(url = url, destfile = paste0(file_path, file_name, sep = \"\"))\n\nNow let’s read in the file in order to make sure it actually downloaded.\n\nmeasure_dates_df <- read.csv(file = paste0(file_path, file_name))\n\ndplyr::glimpse(measure_dates_df)\n\nRows: 170\nColumns: 6\n$ Measure.ID            <chr> \"ASC_11\", \"ASC_12\", \"ASC_13\", \"ASC_14\", \"ASC_17\"…\n$ Measure.Name          <chr> \"Percentage of patients who had cataract surgery…\n$ Measure.Start.Quarter <chr> \"1Q2021\", \"1Q2019\", \"1Q2021\", \"1Q2021\", \"3Q2020\"…\n$ Start.Date            <chr> \"01/01/2021\", \"01/01/2019\", \"01/01/2021\", \"01/01…\n$ Measure.End.Quarter   <chr> \"4Q2021\", \"4Q2021\", \"4Q2021\", \"4Q2021\", \"4Q2021\"…\n$ End.Date              <chr> \"12/31/2021\", \"12/31/2021\", \"12/31/2021\", \"12/31…\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-11/index.html",
    "href": "posts/rtip-2023-01-11/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2022, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2023!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2022\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nfp <- \"linkedin_content.xlsx\"\n\nengagement_tbl <- read_excel(fp, sheet = \"ENGAGEMENT\") %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ntop_posts_tbl <- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %>%\n  clean_names()\n\nfollowers_tbl <- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ndemographics_tbl <- read_excel(fp, sheet = \"DEMOGRAPHICS\") %>%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 362\nColumns: 4\n$ date              <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 202…\n$ impressions       <dbl> 3088, 3911, 3303, 3134, 1118, 799, 3068, 1954, 2663,…\n$ engagements       <dbl> 31, 56, 51, 42, 8, 4, 43, 20, 33, 43, 14, 41, 5, 17,…\n$ `Engagement Rate` <dbl> 1.0038860, 1.4318589, 1.5440509, 1.3401404, 0.715563…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 5\n$ post_url_1  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ engagements <dbl> 241, 136, 123, 117, 117, 115, 107, 106, 104, 104, 95, 81, …\n$ x3          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_4  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ impressions <dbl> 52300, 33903, 30752, 29887, 25953, 24139, 23769, 18522, 18…\n\nglimpse(followers_tbl)\n\nRows: 362\nColumns: 2\n$ date          <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 2022-01…\n$ new_followers <dbl> 10, 10, 12, 5, 12, 13, 9, 8, 11, 4, 9, 6, 7, 9, 10, 11, …\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics <chr> \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            <chr> \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       <chr> \"0.054587073624134064\", \"0.035217467695474625\", \"0.02…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %>%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\n\nYou will notice that I placed a blue line where I started my telegram channel @steveondata and a red line where I started this blog. So far, not bad, it looks like the telegram channel helped a little bit but writing on the blog seems to maybe been helping the most.\nLet’s look at a cumulative view of things.\n\nengagement_tbl %>%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %>%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %>%\n  slice(1:12) %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %>%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %>%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %>%\n  slice(1:12) %>%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html",
    "href": "posts/rtip-2023-05-04/index.html",
    "title": "Maps with {shiny}",
    "section": "",
    "text": "The code is used to create a Shiny app that allows the user to search for a type of amenity (such as a pharmacy) in a particular city, state, and country, and then display the results on a map. Here is a step-by-step explanation of how the code works."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#concatenating-the-address",
    "href": "posts/rtip-2023-05-04/index.html#concatenating-the-address",
    "title": "Maps with {shiny}",
    "section": "Concatenating the Address",
    "text": "Concatenating the Address\nThe first thing that the observeEvent function does is concatenate the user inputs for city, state, and country into a single string. This is done using the paste function. The sep argument specifies that the words should be separated by a comma and space. The resulting string is the address that will be used to search for the specified amenity."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#obtaining-the-bounding-box",
    "href": "posts/rtip-2023-05-04/index.html#obtaining-the-bounding-box",
    "title": "Maps with {shiny}",
    "section": "Obtaining the Bounding Box",
    "text": "Obtaining the Bounding Box\nNext, the code uses the getbb function from the osmdata library to obtain the bounding box for the specified address. A bounding box is a rectangle that contains the entire area of interest (in this case, the specified city, state, and country). The bounding box is necessary to limit the search for the specified amenity to only the specified area."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#creating-the-query",
    "href": "posts/rtip-2023-05-04/index.html#creating-the-query",
    "title": "Maps with {shiny}",
    "section": "Creating the Query",
    "text": "Creating the Query\nThe code then creates a query object using the opq function from the osmdata library. The bbox argument specifies the bounding box that was obtained in the previous step. The add_osm_feature function is then used to specify the amenity that the user is searching for. The key argument specifies that we are searching for an “amenity”, and the value argument specifies the specific type of amenity that the user entered (e.g., pharmacy)."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#obtaining-the-results",
    "href": "posts/rtip-2023-05-04/index.html#obtaining-the-results",
    "title": "Maps with {shiny}",
    "section": "Obtaining the Results",
    "text": "Obtaining the Results\nThe osmdata_sf function is used to retrieve the results of the query. This function returns a sf object that contains the spatial data for the points that match the specified amenity. The resulting sf object is then passed to the mapview function from the mapview library, which creates an interactive map of the results."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#displaying-the-map",
    "href": "posts/rtip-2023-05-04/index.html#displaying-the-map",
    "title": "Maps with {shiny}",
    "section": "Displaying the Map",
    "text": "Displaying the Map\nFinally, the renderLeaflet function is used to display the map in the UI. The m@map argument specifies that we want to display the map that was created by the mapview function. The resulting map is displayed in the leafletOutput that was defined in the UI."
  },
  {
    "objectID": "posts/rtip-2023-05-05/index.html",
    "href": "posts/rtip-2023-05-05/index.html",
    "title": "Maps with {shiny} Pt 2",
    "section": "",
    "text": "Introduction\nThe code provided at the end of this post is an example of how to create a simple Shiny app in R that utilizes the OpenStreetMap (OSM) API to create a map of amenities in a specific location.\nThe app has two main parts: the user interface (UI) and the server.\nThe UI section is defined using the fluidPage function from the Shiny library, which creates a responsive, fluid layout for the app. It includes a title panel, a sidebar panel with text input fields for the city, state, country, and amenity type, and a submit button. The main panel of the UI includes a leafletOutput object, which will display the map of amenities.\nThe server section is defined using the server function from the Shiny library. This function is responsible for processing the inputs from the UI, performing any necessary calculations, and rendering the output.\nThe observeEvent function is used to capture the click event of the submit button. When the button is clicked, the function getbb from the osmdata library is used to retrieve the bounding box (bbox) for the specified location.\nNext, the opq function from the osmdata library is used to create a query object that searches for amenities of the specified type (input$amenity) within the retrieved bbox.\nThe assign function is used to set a variable has_internet_via_proxy to TRUE in the curl environment. This is necessary to ensure that the osmdata_sf function, which downloads the OSM data, works properly.\nThe osmdata_sf function is then called with the created query object as its argument. This function downloads the OSM data and converts it to an sf object. The resulting sf object contains a data frame with information about the amenities found in the specified location.\nA mapview object is then created using the osm_points part of the sf object. This object is assigned to the variable m.\nFinally, the renderLeaflet function is used to display the resulting map. The mapview object m is accessed and its @map attribute is used as the input to the renderLeaflet function. This displays the map of amenities in the specified location.\nThere is also some commented out code in the server section that provides an alternative way to display the map using the leaflet library instead of the mapview library. This code creates a leaflet object, adds tiles to the map, and then adds circle markers to represent the amenities found in the specified location. The popup argument specifies what information is displayed in the popups that appear when the user clicks on a marker.\nOverall, this code demonstrates how to use the Shiny library to create an interactive web application that utilizes the OSM API to display maps of amenities in specific locations.\n\n\nFull Application\nAs usual, here is the full code. Please take it and see what you can do with it.\n\nlibrary(shiny)\nlibrary(osmdata)\nlibrary(mapview)\nlibrary(leaflet)\nlibrary(htmltools)\n\nui <- fluidPage(\n  titlePanel(\"Mapping with Shiny\"),\n  sidebarLayout(\n    sidebarPanel(\n      textInput(\"city\", \"City\", placeholder = \"e.g. Queens\"),\n      textInput(\"state\", \"State\", placeholder = \"e.g. New York\"),\n      textInput(\"country\", \"Country\", placeholder = \"e.g. USA\"),\n      textInput(\"amenity\", \"Amenity Type\", placeholder = \"e.g. pharmacy\"),\n      actionButton(\"submit\", \"Submit\")\n    ),\n    mainPanel(\n      leafletOutput(\"map\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  observeEvent(input$submit, {\n    # Concatenate city, state, and country inputs into a single string\n    address <- paste(input$city, input$state, input$country, sep = \", \")\n    \n    bbox <- getbb(address)\n    \n    query <- opq(bbox = bbox) |>\n      add_osm_feature(key = \"amenity\", value = input$amenity)\n    \n    assign(\"has_internet_via_proxy\", TRUE, environment(curl::has_internet))\n    sf_obj <- osmdata_sf(query)\n    \n    m <- mapview(sf_obj$osm_points)\n    output$map <- renderLeaflet({\n      m@map\n    })\n    \n    # output$map <- renderLeaflet({\n    #   leaflet(sf_obj$osm_points) |>\n    #     addTiles() |>\n    #     addCircleMarkers(\n    #       radius = 3, \n    #       popup = ~as.character(\n    #         paste(\n    #           \"Name: \", name, \"<br/>\",\n    #           \"OSM ID: \", osm_id, \"<br/>\"\n    #         )\n    #       ),\n    #       opacity = 0.3\n    #     )\n    # })\n  })\n}\n\nshinyApp(ui, server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-08/index.html",
    "href": "posts/rtip-2023-05-08/index.html",
    "title": "Updates to {healthyR.data}",
    "section": "",
    "text": "Introduction\nIntroducing the Updated {healthyR.data} Package: Your Ultimate Health Data Companion\nIf you’re a healthcare professional or a data enthusiast, you’re probably familiar with the healthyR.data package. This R package has been an invaluable resource for accessing and analyzing public health data. With its latest release, version 1.0.3, the package has undergone some significant changes, including the addition of several new functions and a requirement for R version 3.4.0. In this post, we’ll take a closer look at the updates and how they can help you work with health data more efficiently.\n\n\nBreaking Changes\nIn keeping with tidyverse practices, healthyR.data now requires R version 3.4.0. This change may affect some users who haven’t updated their R version recently, but it’s an important step to keep the package up-to-date and compatible with other tidyverse packages.\n\n\nNew Functions\nOne of the main highlights of the new version is the addition of several new functions. Let’s take a look at each one and how it can help you work with health data:\n\ndl_hosp_data_dict(): This function downloads the data dictionary for the Hospital Compare dataset. This information can be crucial when working with health data, as it provides a clear understanding of the variables and their definitions.\ncurrent_hosp_data(): This function retrieves the most recent Hospital Compare dataset, which includes information on hospital quality, patient experience, and more.\ncurrent_asc_data(): This function retrieves the most recent Ambulatory Surgical Center (ASC) dataset, which includes information on ASC quality measures.\ncurrent_asc_oas_cahps_data(): This function retrieves the most recent ASC Outpatient and Ambulatory Surgery Consumer Assessment of Healthcare Providers and Systems (OAS CAHPS) dataset, which includes patient experience measures for ASCs.\ncurrent_comp_death_data(): This function retrieves the most recent data on hospital mortality rates for conditions such as heart attack, pneumonia, and stroke.\ncurrent_hai_data(): This function retrieves the most recent Healthcare-Associated Infections (HAI) dataset, which includes information on infections acquired during hospitalization.\ncurrent_hcahps_data(): This function retrieves the most recent Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS) dataset, which includes patient experience measures for hospitals.\ncurrent_hvbp_data(): This function retrieves the most recent Hospital Value-Based Purchasing (HVBP) dataset, which includes information on hospital quality and payment incentives.\ncurrent_ipfqr_data(): This function retrieves the most recent Inpatient Psychiatric Facility Quality Reporting (IPFQR) dataset, which includes information on psychiatric facility quality measures.\ncurrent_maternal_data(): This function retrieves the most recent Maternal and Infant Health Care Quality dataset, which includes information on maternal and infant health outcomes.\ncurrent_medicare_hospital_spending_data(): This function retrieves the most recent Medicare Hospital Spending by Claim dataset, which includes information on Medicare payments for hospital services.\ncurrent_opqr_data(): This function retrieves the most recent Outpatient Prospective Payment System (OPPS) Quality Reporting (OPQR) dataset, which includes information on outpatient facility quality measures.\ncurrent_imaging_efficiency_data(): This function retrieves the most recent Radiology Imaging Efficiency (RIE) dataset, which includes information on the appropriateness of imaging studies.\ncurrent_unplanned_hospital_visits_data(): This function retrieves the most recent Unplanned Hospital Visits dataset, which includes information on hospital readmissions and emergency department visits.\ncurrent_payments_data(): This function retrieves the most recent Provider-Level Payments dataset, which includes information on payments to healthcare providers.\ncurrent_pch_hcahps_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) dataset.\ncurrent_pch_hai_hospital_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Healthcare-Associated Infections (HAI) Hospital dataset, which includes information on healthcare-associated infections in PCMH hospitals.\ncurrent_pch_oncology_measures_hospital_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Oncology Measures Hospital dataset, which includes information on oncology measures in PCMH hospitals.\ncurrent_pch_outcomes_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Outcomes dataset, which includes information on outcomes for PCMH practices.\ncurrent_timely_and_effective_care_data(): This function retrieves the most recent Timely and Effective Care dataset, which includes information on hospital performance on timely and effective care measures.\ncurrent_va_data(): This function retrieves the most recent Veterans Affairs (VA) dataset, which includes information on VA hospital quality measures.\n\nAll of these functions provide valuable access to important health data, allowing users to perform detailed analyses and gain insights into various aspects of healthcare quality and outcomes.\n\n\nOther Improvements\nIn addition to the new functions, healthyR.data version 1.0.3 also includes several bug fixes and improvements. For example, the logic in the current_hosp_data() function has been confirmed by user feedback.\n\n\nConclusion\nThe healthyR.data package has long been a valuable resource for anyone working with health data. With the latest release, version 1.0.3, the package has become even more powerful and versatile, thanks to the addition of many new functions and improvements. If you’re a healthcare professional, researcher, or data enthusiast, healthyR.data is a must-have tool in your arsenal. Give it a try and see how it can help you gain new insights into the world of healthcare quality and outcomes."
  },
  {
    "objectID": "posts/rtip-2023-05-09/index.html",
    "href": "posts/rtip-2023-05-09/index.html",
    "title": "VBA to R and Back Again: Running R from VBA",
    "section": "",
    "text": "Introduction\nToday I am going to briefly go over an extremely simple example of running some R code via Excel VBA.\nLet’s start by discussing each line of code one by one:\nSub CallRnorm()\nThis line defines a subroutine called “CallRnorm”. A subroutine is a block of code that can be executed repeatedly from any part of the code, and it starts with the “Sub” keyword followed by the subroutine name and any arguments in parentheses.\nDim R As Variant\nDim result As Variant\nThese two lines declare two variables named “R” and “result” as “Variant” data type. “Variant” is a data type that can store any type of data.\nColumns(\"A\").Delete\nThis line deletes the entire column A from the active worksheet.\nR = \"library(stats);rnorm(10) |&gt; as.data.frame()\"\nThis line assigns a string of R code to the variable “R”. The code will load the “stats” package and generate 10 random numbers from a normal distribution using the “rnorm()” function, and then convert the result to a data frame using the pipe operator “|&gt;” and the “as.data.frame()” function.\nresult = VBA.CreateObject(\"WScript.Shell\").Exec(\"C:\\Program Files\\R\\R-4.1.2\\bin\\x64\\Rscript.exe -e \"\"\" & R & \"\"\"\").StdOut.ReadAll\nThis line uses the “CreateObject” method to create a new object of the “WScript.Shell” class, which allows us to execute commands in the Windows command shell. It then uses the “Exec” method to execute the R code stored in the “R” variable using the “Rscript.exe” command-line tool, which runs R scripts from the command line. The result of the command is stored in the “result” variable by reading the output of the command using the “StdOut” property of the “Exec” object and the “ReadAll” method.\nresult = Split(result, vbCrLf)\nFor i = 0 To UBound(result)\n    ActiveSheet.Range(\"A1\").Offset(i, 0).Value = result(i)\nNext i\nThese two lines split the result of the R code execution into an array of strings using the “Split” function and the newline character (vbCrLf) as the delimiter. It then loops through the array using a “For” loop and assigns each element to a cell in the active worksheet, starting from cell A1 and offsetting each cell by one row using the “Offset” method.\nSo, in summary, this VBA code creates a subroutine that deletes column A from the active worksheet, executes a block of R code that generates 10 random numbers from a normal distribution and converts the result to a data frame, captures the output of the R code execution, splits the output into an array of strings, and pastes the result into column A of the active worksheet.\n\n\n\nVBA to R and Back Again"
  },
  {
    "objectID": "posts/rtip-2023-05-10/index.html",
    "href": "posts/rtip-2023-05-10/index.html",
    "title": "VBA to R and Back Again: Running R from VBA Pt 2",
    "section": "",
    "text": "Introduction\nYesterday I posted on using VBA to execute R code that is written inside of the VBA script. So today, I will go over a simple example on executing an R script from VBA. So let’s get into the code and what it does.\nFirst, let’s look at the Function called “Run_R_Script”. This function takes four arguments, where the first two are mandatory, and the last two are optional.\n\nsRApplicationPath - This is the path to the R application that you want to use to run your script. It is a required argument, and you need to provide the full path to the Rscript.exe file on your machine.\nsRFilePath - This is the path to the R script file that you want to execute. It is also a required argument, and you need to provide the full path to your R script file.\niStyle - This is an optional argument that specifies how the script will be executed. By default, it is set to 1, which means that the script will run in a minimized window.\nbWaitTillComplete - This is another optional argument that specifies whether the function should wait until the script has finished running before returning control to the caller. By default, it is set to True, which means that the function will not return control until the script has completed execution.\n\nThe first line inside the Function defines two variables: sPath and shell.\n\nsPath - This variable will hold the path to the Rscript.exe file and the path to the R script file, which will be used later to run the script.\nshell - This variable is used to create an instance of the WScript.Shell object.\n\nNext, we wrap the R path with double quotations to avoid any issues with spaces in the path.\nAfter that, the script deletes Column A.\nThen, instead of using the “shell.Run” function, the code uses the “shell.Exec” function to execute the R script. This function returns an object that has a “StdOut” property, which contains the output of the script.\nThe output is then read using the “ReadAll” method, and the resulting string is split into an array using the “Split” function. The array is then iterated using a “For” loop, and each element of the array is written to Column A, starting at cell A1.\nFinally, the Function returns an Integer value, which is the result of the “shell.Run” function.\nThe Subroutine called “Demo” just demonstrates how to use the “Run_R_Script” function by calling it with the appropriate parameters.\n\n\nFull Code\nHere is the R Script\n\ndata.frame(\n    x = 1:10,\n    y = rnorm(10)\n)\n\nlist(\n    data.frame(\n        x = 1:10,\n        y = rnorm(10)\n    ),\n    data.frame(\n        x = 1:10,\n        y = rnorm(10)\n    )\n)\n\nFull VBA\nFunction Run_R_Script(sRApplicationPath As String, _\n                        sRFilePath As String, _\n                        Optional iStyle As Integer = 1, _\n                        Optional bWaitTillComplete As Boolean = True) As Integer\n\n    Dim sPath As String\n    Dim shell As Object\n\n    'Define shell object\n    Set shell = VBA.CreateObject(\"WScript.Shell\")\n\n    'Wrap the R path with double quotations\n    sPath = \"\"\"\" & sRApplicationPath & \"\"\"\"\n    sPath = sPath & \" \"\n    sPath = sPath & sRFilePath\n\n    'Delete Coumn A\n    Columns(\"A\").Delete\n    \n    'Get Result\n    result = shell.Exec(sPath).StdOut.ReadAll\n    result = Split(result, vbCrLf)\n    For i = 0 To UBound(result)\n        ActiveSheet.Range(\"A1\").Offset(i, 0).Value = result(i)\n    Next i\n    \nEnd Function\n\nSub Demo()\n    Dim iEerrorCode As Integer\n    iEerrorCode = Run_R_Script(\"C:\\Program Files\\R\\R-4.1.2\\bin\\x64\\Rscript.exe\", \"C:\\Users\\ssanders\\Desktop\\test.R\")\nEnd Sub\n\n\nPicture\n\n\n\nExample Output, VBA, and R\n\n\n\n\nReference\nhttps://stackoverflow.com/a/54816881"
  },
  {
    "objectID": "posts/rtip-2023-05-12/index.html",
    "href": "posts/rtip-2023-05-12/index.html",
    "title": "Working with Dates and Times Pt 1",
    "section": "",
    "text": "Introduction\nIn this post, we will cover the basics of handling dates and times in R using the as.Date, as.POSIXct, and as.POSIXlt functions. We will use the example code below to explain each line in simple terms. Let’s get started!\nHere is the script we are going to look at:\n\n# the date object\nSteve_online &lt;- as.Date(\"1981-02-25\")\n\nstr(Steve_online) #Date[1:1], format: \"1981-02-25\n\n Date[1:1], format: \"1981-02-25\"\n\nclass(Steve_online) #Date\n\n[1] \"Date\"\n\nas.numeric(Steve_online) # stored as number of days since 1970-01-01\n\n[1] 4073\n\nas.numeric(as.Date(\"1970-01-01\")) # equals zero\n\n[1] 0\n\nas.Date(as.Date(\"1970-01-01\") + 4073) # produces 1981-02-25 -- our original date\n\n[1] \"1981-02-25\"\n\n# vectors can contain multiple dates\nSteve_online &lt;- as.Date(c(\"1981-02-25\", \"1997-01-12\"))\nstr(Steve_online)\n\n Date[1:2], format: \"1981-02-25\" \"1997-01-12\"\n\nSteve_online[2]\n\n[1] \"1997-01-12\"\n\n# what about POSIX?\n# POSIXct stores date time as integer == # seconds since 1970-01-01 UTC\nSteve_online &lt;- as.POSIXct(\"1981-02-25 02:25:00\", tz = \"US/Mountain\")\nas.integer(`Steve_online`)\n\n[1] 351941100\n\n# POSIXlt stores date time as list:sec, min, hour, mday, mon, year, wday, yday, isdst, zone, gmtoff\nSteve_online &lt;- as.POSIXlt(\"1981-02-25 02:25:00\", tz = \"US/Mountain\")\nas.integer(Steve_online) # no longer an integer\n\nWarning: NAs introduced by coercion\n\n\n [1]  0 25  2 25  1 81  3 55  0 NA NA\n\nunclass(Steve_online) # this shows the components of the list\n\n$sec\n[1] 0\n\n$min\n[1] 25\n\n$hour\n[1] 2\n\n$mday\n[1] 25\n\n$mon\n[1] 1\n\n$year\n[1] 81\n\n$wday\n[1] 3\n\n$yday\n[1] 55\n\n$isdst\n[1] 0\n\n$zone\n[1] \"MST\"\n\n$gmtoff\n[1] NA\n\nattr(,\"tzone\")\n[1] \"US/Mountain\"\n\nmonth.name[Steve_online$mon + 1] # equals February\n\n[1] \"February\"\n\n\nThe first line of code creates a date object called Steve_online with the value of February 25, 1981, using the as.Date function. This function is used to convert a character string to a date object. The str function is then used to show the structure of the Steve_online object, which is of class Date.\nThe as.numeric function is used to convert the Steve_online object to the number of days since January 1, 1970 (known as the Unix epoch). This is a common way of representing dates in programming languages, and is useful for calculations involving dates. We also demonstrate that as.numeric(as.Date(\"1970-01-01\")) returns zero, since this is the starting point of the Unix epoch.\nWe then show how to add or subtract days from a date object by adding or subtracting the desired number of days (as an integer) to the as.Date function with the reference date of January 1, 1970. In this case, we add 4073 days to January 1, 1970, resulting in the date of February 25, 1981 (our original date).\nNext, we demonstrate how to create a vector of date objects by passing a character vector of dates to the as.Date function. The str function is used again to show the structure of the Steve_online object, which is now a vector of two date objects. We then show how to access the second element of the vector using indexing (Steve_online[2]).\nMoving on to POSIX objects, we introduce the as.POSIXct function, which creates a POSIXct object that stores date time as an integer equal to the number of seconds since January 1,"
  },
  {
    "objectID": "posts/rtip-2023-05-15/index.html",
    "href": "posts/rtip-2023-05-15/index.html",
    "title": "Working with Dates and Times Pt 2: Finding the Next Mothers Day with Simplicity",
    "section": "",
    "text": "Introduction\nMother’s Day is a special occasion to honor and appreciate the incredible women in our lives. As programmers, we can use our coding skills to make our lives easier when it comes to important dates like Mother’s Day. In this blog post, we’ll walk through a simple and engaging R code that helps us find the next Mother’s Day. So grab your coding hats, and let’s get started!\n\n# if you aren't using times, use the Date class; it's simpler\nNextMothersDay &lt;- as.Date(\n  c(\n    startMothersDay = \"2024-05-14\", \n    endMothersDay =\"2024-05-14\"\n    )\n  )\n\nNextMothersDay\n\nstartMothersDay   endMothersDay \n   \"2024-05-14\"    \"2024-05-14\" \n\n\nIn the first part of our code, we use the as.Date() function to find the next Mother’s Day. Since we don’t need to consider specific times, we can simply use the Date class, which simplifies the process. We create a vector with two elements: startMothersDay and endMothersDay, both set to “2024-05-14”. This represents the range of Mother’s Day for the year 2024. Finally, we store the result in the variable NextMothersDay and print it to the console. Voilà! We have the next Mother’s Day date.\n\n# if you have times, then use POSIX.\nNextMothersDay_ct &lt;- as.POSIXct(\n  c(\n    startMothersDay = \"2024-05-15 10:00\", # Let Mommy Sleep!\n    endMothersDay =\"2024-05-15 23:59\"\n    ),\n  tz = \"GMT\"\n  )\n\nNextMothersDay_ct\n\n          startMothersDay             endMothersDay \n\"2024-05-15 10:00:00 GMT\" \"2024-05-15 23:59:00 GMT\" \n\n\nNow, let’s say we want to consider specific times for Mother’s Day celebrations. We can use the as.POSIXct() function to handle dates and times together. We create another vector with two elements: startMothersDay and endMothersDay, but this time with specific times. The start time is set to “2024-05-15 10:00” (because let’s let Mommy sleep in!) and the end time is set to “2024-05-15 23:59”. We also specify the time zone as “GMT” using the tz argument. The result is stored in the variable NextMothersDay_ct, and when we print it, we get the range of Mother’s Day with times included.\n\n# converting from one POSIX to another is easy\nNextMothersDay_lt &lt;- as.POSIXlt(NextMothersDay_ct)\nunclass(NextMothersDay_lt)\n\n$sec\n[1] 0 0\n\n$min\n[1]  0 59\n\n$hour\n[1] 10 23\n\n$mday\n[1] 15 15\n\n$mon\n[1] 4 4\n\n$year\nstartMothersDay   endMothersDay \n            124             124 \n\n$wday\n[1] 3 3\n\n$yday\n[1] 135 135\n\n$isdst\n[1] 0 0\n\nattr(,\"tzone\")\n[1] \"GMT\"\n\n\nNow, let’s explore how to convert a POSIXct object to a POSIXlt object. We use the as.POSIXlt() function to convert NextMothersDay_ct into a POSIXlt object. This conversion allows us to access more detailed components of the date and time, such as the day of the week, hour, minute, and second. Finally, we use the unclass() function to remove the class attributes from the object and print the result to the console.\n\n\nConclusion\nWith just a few lines of code, we have learned how to find the next Mother’s Day using R. Whether you need a simple date or a specific time range, R provides us with convenient functions to handle both scenarios. So the next time you want to plan a special surprise for your mom, you can rely on your coding skills to\n\n\nFull Script\n\n# if you aren't using times, use the Date class it's simpler\nNextMothersDay &lt;- as.Date(\n  c(\n    startMothersDay = \"2024-05-14\", \n    endMothersDay =\"2024-05-14\"\n    )\n  )\n\nNextMothersDay\n\n# if you have times, then use POSIX.\nNextMothersDay_ct &lt;- as.POSIXct(\n  c(\n    startMothersDay = \"2024-05-15 10:00\", # Let Mommy Sleep!\n    endMothersDay =\"2024-05-15 23:59\"\n    ),\n  tz = \"GMT\"\n  )\n\nNextMothersDay_ct\n\n# converting from one POSIX to another is easy\nNextMothersDay_lt &lt;- as.POSIXlt(NextMothersDay_ct)\nunclass(NextMothersDay_lt)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-16/index.html",
    "href": "posts/rtip-2023-05-16/index.html",
    "title": "Working with Dates and Times Pt 3",
    "section": "",
    "text": "Introduction\nDates and times are essential components in many programming tasks, and R provides various functions and packages to handle them effectively. In this post, we’ll explore some common operations using both the base R functions and the lubridate package, comparing their simplicity and ease of understanding.\nLet’s dive right in!\n\n# What class does as.Date() produce?\nclass(as.Date(\"1881/10/25\"))\n\n[1] \"Date\"\n\n# be sure lubridate \n# install.packages(\"lubridate\")\nlibrary(lubridate)\n\n# Which do you find easier to understand? base or lubridate?\ntoday() # today() = Sys.Date()\n\n[1] \"2023-05-16\"\n\nnow() # now() = Sys.time()\n\n[1] \"2023-05-16 08:27:52 EDT\"\n\n# as_date and as.Date produce the same class\nclass(as_date(\"1881/10/25\")) # lubridate\n\n[1] \"Date\"\n\nclass(as.Date(\"1881/10/25\")) # base\n\n[1] \"Date\"\n\n# simpler strptime\nstrptime(\"2014-07-13 16:00:00 -0300\", \"%Y-%m-%d %H:%M:%S %z\") # time zone is messed up\n\n[1] \"2014-07-13 15:00:00\"\n\nparse_date_time(\"2014-07-13 16:00:00 -0300\", \"ymd HMS z\") # time zone works\n\n[1] \"2014-07-13 19:00:00 UTC\"\n\n# lubridate takes it one step further\nymd(\"2014-07-13 16:00:00 -0300\")\n\n[1] NA\n\nymd_hms(\"2014-07-13 16:00:00 -0300\")\n\n[1] \"2014-07-13 19:00:00 UTC\"\n\nmdy_hm(\"July 13, 2014 4:00 pm\")\n\n[1] \"2014-07-13 16:00:00 UTC\"\n\n\n1️⃣ Determining the Class of a Date: The first line of code checks the class produced by the as.Date() function when given the input “1881/10/25.” By using the class() function, we can identify that the output is of class “Date.” This means that the as.Date() function converts the input into a date format.\n2️⃣ Base R vs. lubridate: Before we proceed further, we need to ensure that the lubridate package is installed. If not, the code installs it using the install.packages() function. We then load the package using the library() function.\nNext, we compare the ease of use between base R and lubridate for working with dates and times.\n\nToday’s Date and Current Time: The today() function, equivalent to Sys.Date(), gives you the current date. Similarly, now() returns the current date and time using Sys.time(). These functions make it straightforward to obtain the current date or date and time in R.\nClass Comparison: We compare the classes of dates produced by as_date() from lubridate and as.Date() from base R. Using the class() function on each result, we observe that both functions produce the same “Date” class output. Hence, both methods are equivalent in this regard.\n\n3️⃣ Simplifying Date and Time Parsing: Parsing date and time strings can sometimes be tricky, especially when dealing with time zones. However, lubridate provides simplified functions to handle such scenarios.\n\nBase R’s strptime(): The strptime() function is a base R function that parses a date and time string based on a given format. In this case, we try to parse “2014-07-13 16:00:00 -0300” with the format “%Y-%m-%d %H:%M:%S %z.” However, we encounter a problem with the time zone, as it does not parse correctly.\nlubridate’s parse_date_time(): To overcome the time zone issue, lubridate offers the parse_date_time() function. We provide the same date and time string along with the format “ymd HMS z.” This time, the time zone is parsed correctly, resulting in a valid date and time object.\n\n4️⃣ Going the Extra Mile with lubridate: lubridate takes date and time manipulation a step further with its intuitive functions.\n\nymd(): The ymd() function converts a character string of the form “2014-07-13 16:00:00 -0300” into a date object. It handles various date formats and automatically infers the year, month, and day information.\nymd_hms(): Similar to ymd(), the ymd_hms() function converts a character string into a date-time object, considering the year, month, day, hour, minute, and second components.\nmdy_hm(): The mdy_hm() function allows us to parse a character string like “July 13, 2014 4:00 pm” into\n\na date-time object. It handles different date formats and automatically extracts the month, day, year, hour, and minute information.\nBy leveraging these functions, lubridate simplifies the process of working with dates and times, offering a more intuitive and concise syntax compared to base R.\nIn conclusion, understanding how to handle dates and times in R is crucial for many programming tasks. While base R provides essential functions, the lubridate package offers additional capabilities and a more straightforward syntax, making it an attractive choice for working with dates and times in R."
  },
  {
    "objectID": "posts/rtip-2023-05-17/index.html",
    "href": "posts/rtip-2023-05-17/index.html",
    "title": "Working with Dates and Times Pt 4",
    "section": "",
    "text": "Introduction\nFormatting dates is an essential task in data analysis and programming. In R, there are various ways to manipulate and present dates according to specific requirements. In this blog post, we will explore the world of date formatting in R, uncovering the power of the strftime() function. We will walk through practical examples using the provided code snippet, demonstrating how to format dates in a clear and concise manner. So, let’s dive in and uncover the secrets of date formatting in R!\n#Understanding the strftime() Function:\nIn R, the strftime() function allows us to format dates and times based on a set of predefined modifiers. These modifiers act as placeholders for different components of the date and time. By using these modifiers, we can customize the output format to suit our needs.\nLet’s analyze the code snippet provided to gain a better understanding of the strftime() function and its capabilities.\n\n# all of the modifiers\nfor (formatter in sort(c(letters, LETTERS))) {\n  modifier &lt;- paste0(\"%\", formatter)\n  print(\n    paste0(\n      modifier, \n      \" used on: \",\n      RightNow,\n      \" will give: \",\n      strftime(RightNow, modifier)\n    )\n  )\n}\n\nThe code snippet above iterates through a set of modifiers, both lowercase and uppercase letters, and applies each modifier to the RightNow variable. It then prints the modifier, the original RightNow value, and the formatted output. This allows us to see the effect of each modifier on the date and time representation.\n\n\nModifier Showcase:\nLet’s explore some commonly used modifiers and their corresponding output formats:\n%a - Abbreviated weekday name (e.g., \"Mon\").\n%A - Full weekday name (e.g., \"Monday\").\n%b - Abbreviated month name (e.g., \"Jan\").\n%B - Full month name (e.g., \"January\").\n%d - Day of the month (01-31).\n%H - Hour in 24-hour format (00-23).\n%I - Hour in 12-hour format (01-12).\n%m - Month (01-12).\n%M - Minute (00-59).\n%p - AM/PM indicator.\n%S - Second (00-59).\n%Y - Year with century (e.g., \"2023\").\n%y - Year without century (e.g., \"23\").\nFeel free to experiment with different modifiers and observe the changes in the output format.\nHere is a full example\n\nRightNow &lt;- Sys.time()\n\n# all of the modifiers\nfor (formatter in sort(c(letters, LETTERS))) {\n  modifier &lt;- paste0(\"%\", formatter)\n  print(\n    paste0(\n      modifier, \n      \" used on: \",\n      RightNow,\n      \" will give: \",\n      strftime(RightNow, modifier)\n    )\n  )\n}\n\n[1] \"%a used on: 2023-05-17 09:34:47 will give: Wed\"\n[1] \"%A used on: 2023-05-17 09:34:47 will give: Wednesday\"\n[1] \"%b used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%B used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%c used on: 2023-05-17 09:34:47 will give: Wed May 17 09:34:47 2023\"\n[1] \"%C used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%d used on: 2023-05-17 09:34:47 will give: 17\"\n[1] \"%D used on: 2023-05-17 09:34:47 will give: 05/17/23\"\n[1] \"%e used on: 2023-05-17 09:34:47 will give: 17\"\n[1] \"%E used on: 2023-05-17 09:34:47 will give: E\"\n[1] \"%f used on: 2023-05-17 09:34:47 will give: f\"\n[1] \"%F used on: 2023-05-17 09:34:47 will give: 2023-05-17\"\n[1] \"%g used on: 2023-05-17 09:34:47 will give: 23\"\n[1] \"%G used on: 2023-05-17 09:34:47 will give: 2023\"\n[1] \"%h used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%H used on: 2023-05-17 09:34:47 will give: 09\"\n[1] \"%i used on: 2023-05-17 09:34:47 will give: i\"\n[1] \"%I used on: 2023-05-17 09:34:47 will give: 09\"\n[1] \"%j used on: 2023-05-17 09:34:47 will give: 137\"\n[1] \"%J used on: 2023-05-17 09:34:47 will give: J\"\n[1] \"%k used on: 2023-05-17 09:34:47 will give:  9\"\n[1] \"%K used on: 2023-05-17 09:34:47 will give: K\"\n[1] \"%l used on: 2023-05-17 09:34:47 will give:  9\"\n[1] \"%L used on: 2023-05-17 09:34:47 will give: L\"\n[1] \"%m used on: 2023-05-17 09:34:47 will give: 05\"\n[1] \"%M used on: 2023-05-17 09:34:47 will give: 34\"\n[1] \"%n used on: 2023-05-17 09:34:47 will give: \\n\"\n[1] \"%N used on: 2023-05-17 09:34:47 will give: N\"\n[1] \"%o used on: 2023-05-17 09:34:47 will give: o\"\n[1] \"%O used on: 2023-05-17 09:34:47 will give: O\"\n[1] \"%p used on: 2023-05-17 09:34:47 will give: AM\"\n[1] \"%P used on: 2023-05-17 09:34:47 will give: am\"\n[1] \"%q used on: 2023-05-17 09:34:47 will give: q\"\n[1] \"%Q used on: 2023-05-17 09:34:47 will give: Q\"\n[1] \"%r used on: 2023-05-17 09:34:47 will give: 09:34:47 AM\"\n[1] \"%R used on: 2023-05-17 09:34:47 will give: 09:34\"\n[1] \"%s used on: 2023-05-17 09:34:47 will give: 1684330487\"\n[1] \"%S used on: 2023-05-17 09:34:47 will give: 47\"\n[1] \"%t used on: 2023-05-17 09:34:47 will give: \\t\"\n[1] \"%T used on: 2023-05-17 09:34:47 will give: 09:34:47\"\n[1] \"%u used on: 2023-05-17 09:34:47 will give: 3\"\n[1] \"%U used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%v used on: 2023-05-17 09:34:47 will give: 17-May-2023\"\n[1] \"%V used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%w used on: 2023-05-17 09:34:47 will give: 3\"\n[1] \"%W used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%x used on: 2023-05-17 09:34:47 will give: 5/17/2023\"\n[1] \"%X used on: 2023-05-17 09:34:47 will give: 9:34:47 AM\"\n[1] \"%y used on: 2023-05-17 09:34:47 will give: 23\"\n[1] \"%Y used on: 2023-05-17 09:34:47 will give: 2023\"\n[1] \"%z used on: 2023-05-17 09:34:47 will give: -0400\"\n[1] \"%Z used on: 2023-05-17 09:34:47 will give: EDT\"\n\n\n\n\nConclusion\nIn this blog post, we explored the strftime() function in R, which provides powerful capabilities for formatting dates. By using the various modifiers available, we can easily customize the representation of dates and times to meet our specific requirements. Understanding date formatting is crucial for effective data analysis, visualization, and reporting.\nRemember to refer to the R documentation for strftime() to discover additional modifiers and advanced formatting options. With the knowledge gained from this blog post, you are now equipped to master date formatting in R and handle dates with confidence in your programming endeavors.\nHappy coding with R and may your dates always be formatted to perfection!"
  },
  {
    "objectID": "posts/rtip-2023-05-18/index.html",
    "href": "posts/rtip-2023-05-18/index.html",
    "title": "The which() Function in R",
    "section": "",
    "text": "Introduction:\nAs a programmer, one of the most important tasks is to extract valuable insights from data. To make this process efficient, it is crucial to have a reliable tool at your disposal. Enter the which() function in R. This versatile function allows you to locate specific elements within a vector or a data frame, helping you filter and analyze data with ease. In this blog post, we’ll explore the ins and outs of the which() function, discussing its syntax, common use cases, and providing practical examples to solidify your understanding.\n\n\nUnderstanding the Syntax:\nBefore diving into real-world examples, let’s grasp the basic syntax of the which() function. The general structure of the function is as follows:\n\nwhich(logical_vector, arr.ind = FALSE)\n\nThe logical_vector parameter represents the condition or logical expression you want to evaluate. It can be any expression that returns a logical vector, such as a comparison or logical operation. The optional arr.ind parameter, when set to TRUE, returns the result in array indices instead of a vector, that is to say that the which() function will return a vector of integers that correspond to the positions of the elements in the vector that satisfy the condition.\n\n\nExample 1: Locating Elements in a Numeric Vector\nSuppose we have a numeric vector called scores, representing test scores of students. We want to find the indices of scores greater than or equal to 90. Here’s how we can accomplish that using the which() function:\n\nscores &lt;- c(85, 92, 88, 94, 79, 91, 87, 98, 84, 90)\nindices &lt;- which(scores &gt;= 90)\nindices\n\n[1]  2  4  6  8 10\n\n\nIn this example, the which() function evaluates the logical expression scores &gt;= 90 and returns the indices of the elements satisfying the condition. The resulting indices vector will contain [2, 4, 6, 8, 10], indicating the positions of the scores that meet the criteria.\n\n\nExample 2: Filtering Data Frames\nData frames are widely used in data analysis. The which() function can be incredibly useful when working with data frames to filter rows based on specific conditions. Consider the following example:\n\ndata &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Charlie\", \"Dave\"),\n                   Age = c(25, 32, 28, 30),\n                   City = c(\"New York\", \"London\", \"Paris\", \"Sydney\"))\n\nselected_rows &lt;- which(data$Age &gt;= 30)\nfiltered_data &lt;- data[selected_rows, ]\nfiltered_data\n\n  Name Age   City\n2  Bob  32 London\n4 Dave  30 Sydney\n\n\nIn this case, we use the which() function to find the rows where the Age column is greater than or equal to 30. The selected_rows vector will hold the indices [2, 4], which we subsequently use to filter the original data frame. The resulting filtered_data will contain the rows corresponding to the selected indices, in this case, rows for Bob and Dave.\n\n\nExample 3: Using the arr.ind Parameter\nThe arr.ind parameter of the which() function comes in handy when working with multi-dimensional arrays. It allows you to obtain the indices as an array instead of a vector. Let’s illustrate this with an example:\n\nmatrix_data &lt;- matrix(1:12, nrow = 3, ncol = 4)\nselected_indices &lt;- which(matrix_data %% 3 == 0, arr.ind = TRUE)\nselected_indices\n\n     row col\n[1,]   3   1\n[2,]   3   2\n[3,]   3   3\n[4,]   3   4\n\n\nIn this example, we create a matrix called matrix_data and use the which() function to find the indices where the matrix elements are divisible by 3. By setting arr.ind = TRUE, we obtain a matrix of indices, where each row represents the position of an element satisfying the condition.\n\n\nConclusion:\nThe which() function in R proves to be an invaluable tool for data exploration and filtering. By allowing you to locate specific elements in vectors or data frames, it simplifies the process of extracting relevant information from your data. Throughout this blog post, we explored the syntax and various practical examples of using the which() function. Armed with this knowledge, you can now confidently apply the which() function to your own data analysis tasks in R, boosting your productivity and uncovering hidden insights with ease."
  },
  {
    "objectID": "posts/2023-05-19/index.html",
    "href": "posts/2023-05-19/index.html",
    "title": "Mastering File Manipulation with R’s list.files() Function",
    "section": "",
    "text": "Introduction\nWhen it comes to working with files in R, having a powerful tool at your disposal can make a world of difference. Enter the list.files() function, a versatile and handy utility that allows you to effortlessly navigate through directories, retrieve file names, and perform various file-related operations. In this blog post, we will delve into the intricacies of list.files() and explore real-world examples to help you harness its full potential.\n\nlist.files(\n  path, \n  all.files = FALSE, \n  full.names = FALSE, \n  recursive = FALSE, \n  pattern = NULL\n)\n\n\npath is a character vector specifying the directory to list. If no path is specified, the current working directory is used.\nall.files is a logical value specifying whether all files should be listed, including hidden files. The default value is FALSE, which only lists visible files.\nfull.names is a logical value specifying whether the full paths to the files should be returned. The default value is FALSE, which only returns the file names.\nrecursive is a logical value specifying whether subdirectories should be searched. The default value is FALSE, which only lists files in the specified directory.\npattern is a regular expression that can be used to filter the files that are listed. If no pattern is specified, all files are listed.\n\n\n\nUnderstanding the Basics\nBefore diving into the practical examples, let’s familiarize ourselves with the fundamental aspects of the list.files() function. In its simplest form, list.files() retrieves a character vector containing the names of files and directories within a specified directory. It takes in several optional arguments that provide flexibility and control over the file selection process.\n\n\nExample 1: Listing Files in a Directory\n\n# List all files in the current working directory\nfile_names &lt;- list.files()\nprint(file_names)\n\n[1] \"blank.txt\"       \"index.qmd\"       \"index.rmarkdown\"\n\n\nIn this example, the list.files() function is called without any arguments, resulting in the retrieval of all file names within the current working directory. The file_names variable will store the obtained character vector, which can then be printed or further processed.\n\n\nExample 2: Specifying a Directory\n\n# List all files in a specific directory\ndirectory &lt;- \"../rtip-2022-10-24/\"\nfile_names &lt;- list.files(path = directory)\nprint(file_names)\n\n[1] \"index.qmd\"\n\n\nHere, by setting the path argument to the desired directory, you can obtain the list of file names within that particular location. Remember to provide the appropriate path to the directory you wish to explore.\n\n\nExample 3: Selecting Files with a Pattern\n\n# List only files with a specific extension\npattern &lt;- \"\\\\.txt$\"\nfile_names &lt;- list.files(pattern = pattern)\nprint(file_names)\n\n[1] \"blank.txt\"\n\n\nIn this case, the pattern argument is used to filter the file names based on a regular expression. The example showcases the retrieval of only those files with a “.txt” extension. Customize the pattern as per your requirements, utilizing the power of regular expressions.\n\n\nExample 4: Recursive File Listing\n\n# List files recursively within a directory and its subdirectories\ndirectory &lt;- \"../rtip-2023-02-14/R/box/\"\nfile_names &lt;- list.files(path = directory, recursive = TRUE)\nprint(file_names)\n\n[1] \"global_options/global_options.R\" \"io/exports.R\"                   \n[3] \"io/imports.R\"                    \"mod/mod.R\"                      \n\n\nBy setting the recursive argument to TRUE, you can instruct list.files() to search for files not only in the specified directory but also in its subdirectories. This feature is particularly useful when dealing with nested file structures.\n\n\nExample 5: Excluding Directories\n\n# List only files and exclude directories\ndirectory &lt;- \"../rtip-2023-02-14/R/box/\"\nfile_names &lt;- list.files(path = directory, include.dirs = FALSE)\nprint(file_names)\n\n[1] \"global_options\" \"io\"             \"mod\"           \n\n\nIn scenarios where you only want to retrieve files and exclude directories, set the include.dirs argument to FALSE. This ensures that only the file names are included in the result, omitting any directory names.\nHere are some more examples:\n\n# List all files in the current working directory\nlist.files()\n\n# List all files in the current working directory, including hidden files\nlist.files(all.files = TRUE)\n\n# List all files in the current working directory with the .csv extension\nlist.files(pattern = \"\\\\.csv$\")\n\n# List all files in the /data directory\nlist.files(\"/data\")\n\n# List all files in the /data directory, including subdirectories\nlist.files(\"/data\", recursive = TRUE)\n\n\n\nConclusion\nThe list.files() function in R is an invaluable tool for file manipulation, enabling you to effortlessly retrieve file names, filter based on patterns, explore nested directories, and more. By mastering this function, you gain greater control over your file-handling tasks and can efficiently process and analyze data stored in files.\nRemember to consult R’s documentation for additional details on the various optional arguments and explore the wide range of possibilities offered by list.files(). With practice and experimentation, you’ll become a proficient file explorer in no time!\nHappy coding!"
  },
  {
    "objectID": "posts/2023-05-22/index.html",
    "href": "posts/2023-05-22/index.html",
    "title": "Update to {TidyDensity}",
    "section": "",
    "text": "Introduction\nTo effectively extract insights and communicate findings, you need powerful tools that simplify the process and present data in an engaging manner. If you’re a programmer with a penchant for data analysis, you’re in luck! The latest version of {TidyDensity}, the popular R package, has just been released, bringing you exciting new features and enhancements. In this blog post, we’ll explore the highlights of TidyDensity 1.2.5 and why you should download it today.\n\n\nNew Feature: Introducing util_burr_param_estimate()\nTidyDensity 1.2.5 introduces a new function: util_burr_param_estimate(). This function enables you to estimate parameters using the Burr distribution, expanding the possibilities of your data analysis. Whether you’re working with survival analysis, reliability modeling, or extreme value theory, util_burr_param_estimate() equips you with a powerful tool to tackle complex scenarios with ease. Say goodbye to manual calculations and embrace the simplicity and accuracy of TidyDensity.\n\n\nMinor Fixes and Improvements for Enhanced Workflow\nIn addition to the groundbreaking new feature, TidyDensity 1.2.5 addresses user feedback and provides several minor fixes and improvements. Let’s take a look at a couple of them:\n\nImproved Parameter Rounding: With the new version, you now have more control over the rounding of parameter estimates. The updated function tidy_distribution_comparison() includes a parameter called .round_to_place, allowing you to precisely control the rounding behavior of the parameter estimates passed to their corresponding distribution parameters. This enhancement ensures that your analysis remains accurate and aligned with your specific requirements.\n\n\n\nWhy Upgrade to TidyDensity 1.2.5?\n\nStay Ahead of the Curve: The world of data analysis is constantly evolving, and staying up to date with the latest tools and features is crucial to remain competitive. TidyDensity 1.2.5 empowers you with advanced capabilities, enabling you to analyze and visualize data more effectively than ever before.\nSimplify Complex Analysis: With the new util_burr_param_estimate() function, TidyDensity 1.2.5 simplifies complex data analysis tasks. Whether you’re a seasoned data scientist or a beginner, this feature allows you to explore a wider range of statistical distributions and unlock deeper insights from your data.\nFine-Tuned Precision: The improved parameter rounding in tidy_distribution_comparison() ensures that your analysis is not only powerful but also precise. This level of control over rounding provides you with the flexibility to align your analysis with your specific requirements.\n\n\n\nConclusion\nTidyDensity 1.2.5 is a significant update that brings you exciting new features and enhancements. From the introduction of util_burr_param_estimate() to the fine-tuned parameter rounding and polished visuals, this version is designed to empower you in your data analysis journey. By downloading TidyDensity1.2.5, you can stay at the forefront of data analysis, simplify complex tasks, and elevate the precision and user experience of your projects. Upgrade to TidyDensity 1.2.5 today!."
  },
  {
    "objectID": "posts/2023-05-23/index.html",
    "href": "posts/2023-05-23/index.html",
    "title": "What is the sink() function? Capturing Output to External Files",
    "section": "",
    "text": "Introduction\nThe sink() function in R is used to divert R output to an external connection. This can be useful for a variety of purposes, such as exporting data to a file, logging R output, or debugging R code.\nIn this blog post, we will explore the inner workings of the sink() function, understand its purpose, and provide practical examples using the popular datasets mtcars and iris.\nThe sink() function takes four arguments:\n\nfile: The name of the file to which R output will be diverted. If file is NULL, then R output will be diverted to the console.\nappend: A logical value indicating whether R output should be appended to the file (TRUE) or overwritten (FALSE). The default value is FALSE.\ntype: A character string. Either the output stream or the messages stream. The name will be partially match so can be abbreviated.\nsplit: logical: if TRUE, output will be sent to the new sink and the current output stream, like the Unix program tee.\n\n\n\nExamples\nHere are some examples of how to use the sink() function. To export the mtcars dataset to a file called “mtcars.csv”, you would use the following code:\n\nsink(\"mtcars.csv\")\nprint(mtcars)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nsink()\n\nTo log R output to a file called “r_output.log”, you would use the following code:\n\nsink(\"r_output.log\")\n# Your R code goes here\nsink()\n\nTo debug R code, you can use the sink() function to divert R output to a file. This can be helpful for tracking down errors in your code. For example, if you are trying to debug a function called my_function(), you could use the following code:\n\nsink(\"my_function.log\")\nmy_function()\nsink()\n\n\n\nCapturing Summary Statistics of mtcars Dataset\n\nsink(\"summary_output.txt\")  # Redirect output to the file\n\nsummary(mtcars)  # Generate summary statistics\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\nsink()  # Turn off redirection\n\nIn this example, the output of the summary(mtcars) command will be saved in the “summary_output.txt” file. We can later open the file to review the summary statistics of the mtcars dataset.\n\n\nSaving Regression Results of iris Dataset\n\nsink(\"regression_results.txt\")  # Redirect output to the file\n\nfit &lt;- lm(Sepal.Length ~ Sepal.Width, data = iris)  # Perform linear regression\n\nsummary(fit)  # Display regression summary\n\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5561 -0.6333 -0.1120  0.5579  2.2226 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.5262     0.4789   13.63   &lt;2e-16 ***\nSepal.Width  -0.2234     0.1551   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8251 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\nsink()  # Turn off redirection\n\nIn this example, the output of the summary(fit) command will be saved in the “regression_results.txt” file. By redirecting the output, we can analyze the regression results in detail without cluttering the console.\n\n\nAppending Output to a File\nBy default, calling sink() with a file name will overwrite any existing content in the file. However, if we want to append output to an existing file, we can pass the append = TRUE argument to sink().\n\nsink(\"output.txt\", append = TRUE)  # Append output to the existing file\n\ncat(\"Additional text\\n\")  # Append custom text\n\nAdditional text\n\nsink()  # Turn off redirection\n\nIn this example, the string “Additional text” will be appended to the “output.txt” file. This feature is useful when we want to continuously update a log file or add multiple output sections to a single file.\n\n\nConclusion\nThe sink() function is a handy tool in R that allows us to redirect output to external files. By using this function, we can save and review the output generated during data analysis, statistical modeling, or any other R programming tasks. In this blog post, we explored the basic usage of sink() and provided practical examples using the mtcars and iris datasets. By mastering sink(), you can efficiently manage your R output and ensure a more organized workflow."
  },
  {
    "objectID": "posts/2023-05-24/index.html",
    "href": "posts/2023-05-24/index.html",
    "title": "Exploring Data with TidyDensity: A Guide to Using tidy_empirical() and tidy_four_autoplot() in R",
    "section": "",
    "text": "Introduction\nYesterday I had the need to see data that had a grouping column in it. I wanted to use the tidy_four_autoplot() function on it from the {TidyDensity} library on it. This post will explain how I did it. The data in my session was called df_tbl. In this blog post, we will explore the steps involved in using the tidy_empirical() and tidy_four_autoplot() functions from the R library TidyDensity. These functions are incredibly useful when working with data, as they allow us to analyze and visualize empirical distributions efficiently. We will walk through a code snippet that demonstrates how to use these functions within a map() function, enabling us to analyze multiple subsets of data simultaneously.\n#Prerequisites\nTo follow along with this tutorial, it is assumed that you have a basic understanding of the R programming language, as well as familiarity with the dplyr, purrr, and TidyDensity libraries. Make sure you have these packages installed and loaded before proceeding.\nHere is the code that I used, the explanation will follow:\n\nlibrary(dplyr) # to use group_split()\nlibrary(purrr) # to use map()\nlibrary(TidyDensity) # to use tidy_empirical() and tidy_four_plot()\n\ndf_tbl |&gt;\n  group_split(SP_NAME) |&gt;\n  map(\\(run_time) pull(run_time) |&gt;\n        tidy_empirical() |&gt;\n        tidy_four_autoplot()\n      )\n\n\n\nCode Explanation\nLet’s break down the code step by step:\nImporting Required Libraries:\n\nTo access the necessary functions, we need to load the required libraries. In this case, we use library(dplyr) to utilize the group_split() function from the dplyr package, library(purrr) to use the map() function from the purrr package, and library(TidyDensity) to access the tidy_empirical() and tidy_four_autoplot() functions from the TidyDensity package.\n\nGrouping and Splitting the Data:\n\nThe first line of the code snippet takes a dataframe named df_tbl and uses the group_split() function from the dplyr library to split it into multiple subsets based on a variable called SP_NAME. This creates a list of dataframes, each representing a unique group based on SP_NAME.\n\nApplying Functions to Each Subset using map():\n\nThe second line of code utilizes the map() function from the purrr library to iterate over each subset of data created in the previous step. The map() function takes two arguments: the object to iterate over (in this case, the list of dataframes) and a function to apply to each element.\n\nAnonymous Function Inside map():\n\nWithin the map() function, an anonymous function (denoted by (run_time)) is defined. This function takes a single argument named run_time, representing each individual subset of data. The purpose of this anonymous function is to perform the necessary computations and visualizations on each subset of data.\n\nData Manipulation and Visualization:\n\nInside the anonymous function, the pull(run_time) function is used to extract the run_time column from each subset of data. This column is then passed to the tidy_empirical() function from the TidyDensity library, which calculates the empirical distribution of the data. The result is a tidy dataframe that contains information about the empirical distribution.\n\nTidy Four Autoplot:\n\nThe output of tidy_empirical() is then piped (|&gt;) into the tidy_four_autoplot() function from the TidyDensity library. This function generates a visualization called a “Tidy Four Plot,” which consists of four individual plots: empirical density, empirical cumulative density, QQ plot, and histogram.\n\nFinal Output:\n\nThe result of the tidy_four_autoplot() function is the final output of the anonymous function within map(). This output represents the visualization of the empirical distribution for each subset of data.\n\nHappy Coding!"
  },
  {
    "objectID": "posts/2023-05-25/index.html",
    "href": "posts/2023-05-25/index.html",
    "title": "Comparing R Packages for Writing Excel Files: An Analysis of writexl, openxlsx, and xlsx in R",
    "section": "",
    "text": "Introduction\nIn the realm of data analysis and manipulation, R has become a popular programming language due to its extensive collection of packages and libraries. One common task is exporting data to Excel files, which allows for easy sharing and presentation of results. In this blog post, we will explore three popular R packages for writing Excel files: writexl, openxlsx, and xlsx. We will compare their performance using the benchmarking package and analyze the results. So let’s dive in!\n\n\nSetting up the Environment\nBefore we proceed, make sure you have the necessary packages installed. We will be using the rbenchmark, nycflights13, and dplyr packages. The nycflights13 package provides a dataset named “flights,” which we will use for our benchmarking tests.\n\nlibrary(rbenchmark)\nlibrary(nycflights13)\nlibrary(dplyr)\n\n#Defining the Number of Replications\nTo ensure reliable performance measurements, we will repeat each test multiple times. The variable n represents the number of replications, and you can adjust its value depending on your requirements.\n\nn &lt;- 5\n\n\n\nBenchmarking the Packages\nNow, let’s move on to the actual benchmarking process. We will use the benchmark() function from the rbenchmark package to compare the performance of writexl, openxlsx, and xlsx.\n\nbenchmark(\n  \"writexl\" = {\n    writexl::write_xlsx(flights, tempfile())\n  },\n  \"openxlsx\" = {\n    openxlsx::write.xlsx(flights, tempfile())\n  },\n  \"xlsx\" = {\n    xlsx::write.xlsx(flights, paste0(tempfile(),\".xlsx\"))\n  },\n  replications = n,\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n)\n\nIn the code snippet above, we define three tests, each representing one package. We provide the code to execute for each test. For example, in the “writexl” test, we use the write_xlsx() function from the writexl package to write the “flights” dataset to a temporary Excel file.\nThe replications parameter specifies the number of times each test should be repeated. In our case, we set it to n, which we defined earlier as 5.\nThe columns parameter defines the columns to include in the benchmarking results. We specify “test” for the test name, “replications” for the number of replications, “elapsed” for the total time taken, “relative” for the relative performance compared to the fastest test, “user.self” for the CPU time used in user code, and “sys.self” for the CPU time used in system code.\n\n\nPrettifying the Results\nTo make the results more readable, we can use the arrange() function from the dplyr package to sort the results by the “relative” column in ascending order.\n\narrange(relative)\n\nThis will arrange the benchmarking results in ascending order of relative performance, allowing us to easily identify the most efficient package.\n\n\nBenchmark Output\n\ntest replications elapsed relative user.self sys.self\n1 writexl       5   0.034   1.000000   0.024   0.010\n2 openxlsx       5   0.055   1.617647   0.044   0.011\n3 xlsx          5   0.101   2.941176   0.078   0.023\n\n\n\nInterpretation of the Results\nThe results of the benchmark show that writexl is the fastest package for writing to Excel, followed by openxlsx and xlsx. The difference in performance between the three packages is not significant, but writexl is consistently faster than the other two packages.\n\n\nConclusion\nIn this blog post, we compared the performance of three R packages, writexl, openxlsx, and xlsx, for writing Excel files. We used the rbenchmark package to benchmark the packages, considering the number of replications, elapsed time, relative performance, user CPU time, and system CPU time. By arranging the results using the dplyr package, we obtained a sorted view of the relative performance. This analysis can help you choose the most suitable package for your specific needs, considering both performance and functionality.\nRemember, benchmarking can vary depending on the dataset and system specifications. So, it’s always a good idea to run your own benchmarks and evaluate the results in your specific context. Happy coding!"
  },
  {
    "objectID": "posts/2023-05-26/index.html",
    "href": "posts/2023-05-26/index.html",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "",
    "text": "When working with data, it is important to be aware of the file size of the data you are working with. This is especially true when you are working with large datasets, as the file size can have a significant impact on the performance of your code.\nIn R, there are a number of different ways to write data to files. Each method has its own advantages and disadvantages, and the file size of the output can vary depending on the method you use.\nIn this blog post, we will discuss why it is a good idea to check the file size output for different methods. We will also provide three examples of how to check the file size output using the R libraries writexl, openxlsx, and xlsx."
  },
  {
    "objectID": "posts/2023-05-26/index.html#writexl",
    "href": "posts/2023-05-26/index.html#writexl",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "writexl",
    "text": "writexl\nTo check the file size output of the writexl::write_xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the\n\nlibrary(writexl)\n\nwrite_xlsx(iris, tmp1 &lt;- tempfile())\n\nfile.info(tmp1)$size\n\n[1] 8497"
  },
  {
    "objectID": "posts/2023-05-26/index.html#openxlsx",
    "href": "posts/2023-05-26/index.html#openxlsx",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "openxlsx",
    "text": "openxlsx\nTo check the file size output of the openxlsx::write.xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the output:\n\nlibrary(openxlsx)\n\nwrite.xlsx(iris, tmp2 &lt;- tempfile())\n\nfile.info(tmp2)$size\n\n[1] 9631"
  },
  {
    "objectID": "posts/2023-05-26/index.html#xlsx",
    "href": "posts/2023-05-26/index.html#xlsx",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "xlsx",
    "text": "xlsx\nTo check the file size output of the xlsx::write.xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the output:\n\nlibrary(xlsx)\n\nwrite.xlsx(iris, tmp3 &lt;- paste0(tempfile(), \".xlsx\"))\n\nfile.info(tmp3)$size\n\n[1] 7905"
  },
  {
    "objectID": "posts/2023-05-30/index.html",
    "href": "posts/2023-05-30/index.html",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "",
    "text": "Programming is often about making decisions based on certain conditions. In the world of R, there are numerous functions that can help us simplify our code and make it more efficient. One such function is any(). In this blog post, we’ll explore the any() function and learn how it can be used to streamline our logical operations. Whether you’re a beginner or an experienced programmer, this post aims to make the concept accessible to everyone. So, let’s dive in!"
  },
  {
    "objectID": "posts/2023-05-30/index.html#basic-examples",
    "href": "posts/2023-05-30/index.html#basic-examples",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Basic Examples",
    "text": "Basic Examples\nNow, let’s see some basic examples on how to use the any() function.\n\nx &lt;- c(1, 2, 3, 4, 5)\n\nany(x &gt; 10)\n\n[1] FALSE\n\n\n\nx &lt;- c(1, 2, NA, 4, 5)\n\nany(x &gt; 10)\n\n[1] NA\n\nany(x == 5)\n\n[1] TRUE\n\n\nNow, let’s explore some examples to see how any() can be utilized in various scenarios:"
  },
  {
    "objectID": "posts/2023-05-30/index.html#checking-for-the-presence-of-a-specific-value",
    "href": "posts/2023-05-30/index.html#checking-for-the-presence-of-a-specific-value",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Checking for the Presence of a Specific Value:",
    "text": "Checking for the Presence of a Specific Value:\nSuppose we have a vector of numbers, and we want to check if any of them are divisible by 5. We can use the any() function to accomplish this as follows:\n\nnumbers &lt;- c(2, 7, 12, 15, 21)\nis_divisible_by_5 &lt;- any(numbers %% 5 == 0)\n\nif (is_divisible_by_5) {\n  print(\"At least one number is divisible by 5.\")\n} else {\n  print(\"None of the numbers are divisible by 5.\")\n}\n\n[1] \"At least one number is divisible by 5.\"\n\n\nIn this example, we use the modulus operator (%%) to check if each number in the vector has a remainder of 0 when divided by 5. The any() function then returns TRUE if any such element is found, indicating the presence of at least one number divisible by 5."
  },
  {
    "objectID": "posts/2023-05-30/index.html#validating-user-input",
    "href": "posts/2023-05-30/index.html#validating-user-input",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Validating User Input:",
    "text": "Validating User Input:\nLet’s say we are building a program that requires the user to input a positive number. We can use the any() function to validate the input as shown below:\n\nuser_input &lt;- as.numeric(readline(prompt = \"Enter a positive number: \"))\n\nEnter a positive number: \n\n# Dummy input\nuser_input &lt;- 5\nis_positive &lt;- any(user_input &gt; 0)\n\nif (is_positive) {\n  print(\"Input is a positive number.\")\n} else {\n  print(\"Input is not a positive number.\")\n}\n\n[1] \"Input is a positive number.\"\n\n\nHere, we convert the user input to a numeric value using as.numeric() and then check if it is greater than zero. The any() function returns TRUE if any element satisfies this condition, confirming that the input is indeed a positive number."
  },
  {
    "objectID": "posts/2023-05-31/index.html",
    "href": "posts/2023-05-31/index.html",
    "title": "Demystifying Regular Expressions: A Programmer’s Guide for Beginners",
    "section": "",
    "text": "Introduction\nRegular expressions, often abbreviated as regex, are powerful tools used in programming to match and manipulate text patterns. While they might seem intimidating at first, regular expressions are incredibly useful for tasks like data validation, text parsing, and pattern matching. In this blog post, we’ll explore regular expressions in the context of R programming, breaking down the concepts step by step and providing practical examples along the way. By the end, you’ll have a solid understanding of regular expressions and be ready to apply them to your own projects.\n\n\nWhat are Regular Expressions?\nAt its core, a regular expression is a sequence of characters that define a search pattern. It allows you to search, extract, and manipulate text based on specific patterns of characters. Regular expressions are supported in many programming languages, including R, and they provide a concise and flexible way to work with text.\n\n\nHow do regular expressions work?\nRegular expressions work by matching patterns of characters in text. The basic syntax of a regular expression is a sequence of characters enclosed in delimiters, such as slashes (/). The characters in the regular expression can be literal characters, special characters, or character classes.\nLiteral characters are characters that match themselves. For example, the regular expression /a/ matches the letter a.\nSpecial characters are characters that have special meaning in regular expressions. For example, the special character . matches any character.\nCharacter classes are a way to specify a set of characters. For example, the character class [a-z] matches any lowercase letter.\n\n\nHow to use regular expressions in R\nRegular expressions can be used in R to search for, extract, and replace text. To use regular expressions in R, you can use the grep(), grepl(), sub(), and gsub() functions.\nThe grep() function is used to search for text that matches a regular expression. The grepl() function is similar to grep(), but it returns a logical vector indicating whether each element of a vector matches the regular expression. The sub() function is used to replace text that matches a regular expression. The gsub() function is similar to sub(), but it replaces all occurrences of the text that matches the regular expression.\n\n\nBasic Characters\n\n. | Matches any single character except a newline character.\n[] | Matches any character within the brackets. For example, [a-z] matches any lowercase letter.\n* | Matches zero or more occurrences of the preceding character. For example, a* matches any number of a characters, including zero.\n+ | Matches one or more occurrences of the preceding character. For example, a+ matches one or more a characters.\n? | Matches zero or one occurrences of the preceding character. For example, a? matches either one or zero a characters.\n^ | Matches the beginning of the string.\n$ | Matches the end of the string.\n\n\n\nSpecial Characters\nThe following are the special characters used in regular expressions:\n\n\\d | Matches a digit.\n\\s | Matches a whitespace character.\n\\w | Matches a word character (alphanumeric character or underscore).\n\\W | Matches a non-word character.\n\\n | Matches a newline character.\n\\r | Matches a carriage return character.\n\\t | Matches a tab character.\n\n\n\nExamples of regular expressions in R\nHere are some examples of regular expressions in R:\n\nTo search for all occurrences of the word “hello” in a string, you would use the following code:\n\n\ngrep(\"hello\", \"This is a string that contains the word 'hello'\")\n\n[1] 1\n\n\n\nTo extract all of the email addresses from a string, you would use the following code:\n\ngrepl(\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}\"), “This is a string that contains some email addresses”)\n\nTo replace all of the spaces in a string with underscores, you would use the following code:\n\n\nsub(\" \", \"_\", \"This is a string with some spaces\")\n\n[1] \"This_is a string with some spaces\"\n\n\n\nTo replace all of the occurrences of the word “hello” with the word “goodbye” in a string, you would use the following code:\n\n\ngsub(\"hello\", \"goodbye\", \"This is a string that contains the word 'hello'\")\n\n[1] \"This is a string that contains the word 'goodbye'\"\n\n\n\n\nMatching a Simple Pattern\nLet’s start with a simple example in R. Suppose we have a character vector called fruits that contains various fruit names:\n\nfruits &lt;- c(\"apple\", \"banana\", \"orange\", \"kiwi\", \"mango\")\n\nWe can use a regular expression to find all the fruits that start with the letter “a”. In R, the grep() function allows us to perform pattern matching. Here’s how we can achieve this:\n\npattern &lt;- \"^a\"  # ^ denotes the start of the line\nmatching_fruits &lt;- grep(pattern, fruits, value = TRUE)\nprint(matching_fruits)\n\n[1] \"apple\"\n\n\nThe output will be “apple”.\nIn this example, the pattern “^a” specifies that we want to match any fruit that starts with the letter “a”. The grep() function returns the matching fruit names, and we set value = TRUE to obtain the matched values instead of their indices.\n\n\nExtracting Digits from a String\nRegular expressions can be used to extract specific information from a string. Suppose we have a character vector called sentences containing sentences with numbers:\n\nsentences &lt;- c(\"I have 10 apples.\", \"The recipe calls for 2 cups of sugar.\", \"You are the 3rd winner.\")\n\nTo extract the digits from each sentence, we can use the gsub() function, which replaces specific patterns within a string:\n\npattern &lt;- \"\\\\D\"  # \\\\D matches any non-digit character\ndigits &lt;- gsub(pattern, \"\", sentences)\nprint(digits)\n\n[1] \"10\" \"2\"  \"3\" \n\n\nThe output will be “10” “2” “3”\nIn this example, the pattern “\\D” matches any non-digit character. By replacing these characters with an empty string, we effectively extract the digits from each sentence.\n\n\nConclusion\nRegular expressions are an invaluable tool for working with text patterns in programming. While they may seem daunting at first, breaking down the concepts and understanding their building blocks can help demystify them. In this blog post, we explored the basics of regular expressions in R, showcasing practical examples along the way. Armed with this knowledge, you can now confidently incorporate regular expressions into your programming projects, allowing you to manipulate and extract information from text efficiently.\nRemember, practice makes perfect when it comes to regular expressions. Experiment with different patterns, explore the rich set of metacharacters and operators available, and refer to the R documentation for more in-depth information. Regular expressions open up a whole new world of possibilities in text manipulation, so embrace their power and have fun exploring the endless patterns you can match!"
  },
  {
    "objectID": "posts/2023-06-01/index.html",
    "href": "posts/2023-06-01/index.html",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "",
    "text": "As a programmer, you’re always on the lookout for tools that can enhance your productivity and make your code more efficient. In the world of R programming, the do.call() function is one such gem. This often-overlooked function is a powerful tool that allows you to dynamically call other functions, opening up a world of possibilities for code organization, reusability, and flexibility. In this blog post, we will demystify the do.call() function in simple terms and provide you with practical examples that showcase its versatility."
  },
  {
    "objectID": "posts/2023-06-01/index.html#example-1-combining-multiple-vectors-with-rbind",
    "href": "posts/2023-06-01/index.html#example-1-combining-multiple-vectors-with-rbind",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "Example 1: Combining Multiple Vectors with rbind()",
    "text": "Example 1: Combining Multiple Vectors with rbind()\nLet’s say you have a list of vectors, and you want to combine them into a single matrix using the rbind() function. Instead of manually specifying the vectors one by one, you can leverage do.call() to dynamically generate the function call:\n\nvectors &lt;- list(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9))\ncombined_matrix &lt;- do.call(rbind, vectors)\ncombined_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nIn this example, do.call() dynamically constructs the function call rbind(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9)), resulting in a matrix that combines the vectors."
  },
  {
    "objectID": "posts/2023-06-01/index.html#example-2-applying-a-function-to-multiple-data-frames-with-lapply",
    "href": "posts/2023-06-01/index.html#example-2-applying-a-function-to-multiple-data-frames-with-lapply",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "Example 2: Applying a Function to Multiple Data Frames with lapply()",
    "text": "Example 2: Applying a Function to Multiple Data Frames with lapply()\nSuppose you have a list of data frames, and you want to apply a specific function to each of them, such as summarizing the mean of a column. Instead of writing repetitive code, you can use do.call() to apply the desired function dynamically:\n\ndata_frames &lt;- list(\n  data.frame(a = 1:3), \n  data.frame(a = 4:6), \n  data.frame(a = 7:9)\n  )\nmean_results &lt;- do.call(\n  rbind, \n  lapply(data_frames, function(df) mean(df$a))\n  )\nmean_results\n\n     [,1]\n[1,]    2\n[2,]    5\n[3,]    8\n\n\nIn this example, do.call() combines the results of applying the mean function to each data frame’s ‘a’ column into a single matrix."
  },
  {
    "objectID": "posts/2023-06-02/index.html",
    "href": "posts/2023-06-02/index.html",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "",
    "text": "In the realm of data analysis and programming, organizing and sorting data efficiently is crucial. In R, a programming language renowned for its data manipulation capabilities, we have three powerful functions at our disposal: order(), sort(), and rank(). In this blog post, we will delve into the intricacies of these functions, explore their applications, and understand their parameters. These R functions are all used to sort data, however, they each have different purposes and use different methods to sort the data."
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-order",
    "href": "posts/2023-06-02/index.html#parameters-of-order",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of order():",
    "text": "Parameters of order():\n\n... - Specify the vectors to be sorted."
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-sort",
    "href": "posts/2023-06-02/index.html#parameters-of-sort",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of sort():",
    "text": "Parameters of sort():\n\nx - The vector or matrix to be sorted.\ndecreasing - A logical value indicating whether the sorting should be in descending order. (Default is FALSE)"
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-rank",
    "href": "posts/2023-06-02/index.html#parameters-of-rank",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of rank():",
    "text": "Parameters of rank():\n\nx - The vector to be ranked.\nties.method - A string specifying the method to handle ties in ranking. (Options: “average”, “first”, “last”, “random”, “max”, “min”) (Default is “average”)"
  },
  {
    "objectID": "posts/2023-06-06/index.html",
    "href": "posts/2023-06-06/index.html",
    "title": "Simplifying Data Transformation with pivot_longer() in R’s tidyr Library",
    "section": "",
    "text": "Introduction\nIn the world of data analysis and manipulation, tidying and reshaping data is often an essential step. R’s tidyr library provides powerful tools to efficiently transform and reshape data. One such function is pivot_longer(). In this blog post, we’ll explore how pivot_longer() works and demonstrate its usage through several examples. By the end, you’ll have a solid understanding of how to use this function to make your data more manageable and insightful.\nThe tidyr library holds the function, so we are going to have to load it first.\n\nlibrary(tidyr)\n\n\n\nUnderstanding pivot_longer()\nThe pivot_longer() function is designed to reshape data from a wider format to a longer format. It takes columns that represent different variables and consolidates them into key-value pairs, making it easier to analyze and visualize the data.\nSyntax: The basic syntax of pivot_longer() is as follows:\npivot_longer(data, cols, names_to, values_to)\n\ndata: The data frame or tibble to be reshaped.\ncols: The columns to be transformed.\nnames_to: The name of the new column that will hold the variable names.\nvalues_to: The name of the new column that will hold the corresponding values.\n\n\n\nExample 1: Reshaping Wide Data to Long Data\nLet’s start with a simple example to demonstrate the usage of pivot_longer(). Suppose we have a data frame called students with columns representing subjects and their respective scores:\n\nstudents &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  math = c(90, 85, 92),\n  science = c(95, 88, 91),\n  history = c(87, 92, 78)\n)\n\nTo reshape this data from a wider format to a longer format, we can use pivot_longer() as follows:\n\nstudents_long &lt;- pivot_longer(\n  students, \n  cols = -name, \n  names_to = \"subject\", \n  values_to = \"score\"\n  )\n\nstudents_long\n\n# A tibble: 9 × 3\n  name    subject score\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 Alice   math       90\n2 Alice   science    95\n3 Alice   history    87\n4 Bob     math       85\n5 Bob     science    88\n6 Bob     history    92\n7 Charlie math       92\n8 Charlie science    91\n9 Charlie history    78\n\n\nThe resulting students_long data frame will have three columns: name, subject, and score, where each row represents a student’s score in a specific subject.\nExample 2: Handling Multiple Variables In many cases, data frames contain multiple variables that need to be pivoted simultaneously. Consider a data frame called sales with columns representing sales figures for different products in different regions:\n\nsales &lt;- data.frame(\n  region = c(\"North\", \"South\", \"East\"),\n  product_A = c(100, 120, 150),\n  product_B = c(80, 90, 110),\n  product_C = c(60, 70, 80)\n)\n\nTo reshape this data, we can specify multiple columns to pivot using pivot_longer():\n\nsales_long &lt;- pivot_longer(\n  sales, \n  cols = starts_with(\"product\"), \n  names_to = \"product\", \n  values_to = \"sales\"\n  )\n\nsales_long\n\n# A tibble: 9 × 3\n  region product   sales\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 North  product_A   100\n2 North  product_B    80\n3 North  product_C    60\n4 South  product_A   120\n5 South  product_B    90\n6 South  product_C    70\n7 East   product_A   150\n8 East   product_B   110\n9 East   product_C    80\n\n\nThe resulting sales_long data frame will have three columns: region, product, and sales, where each row represents the sales figure of a specific product in a particular region.\n\n\nExample 3: Handling Irregular Data\nSometimes, data frames contain irregular structures, such as missing values or uneven numbers of columns. pivot_longer() can handle such scenarios gracefully. Consider a data frame called measurements with columns representing different measurement types and their respective values:\n\nmeasurements &lt;- data.frame(\n  timestamp = c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"),\n  temperature = c(25.3, 27.1, 24.8),\n  humidity = c(65.2, NA, 68.5),\n  pressure = c(1013, 1012, NA)\n)\n\nTo reshape this data, we can use pivot_longer() and handle the missing values:\n\nmeasurements_long &lt;- pivot_longer(\n  measurements, \n  cols = -timestamp, \n  names_to = \"measurement\", \n  values_to = \"value\", \n  values_drop_na = TRUE\n  )\n\nmeasurements_long\n\n# A tibble: 7 × 3\n  timestamp  measurement  value\n  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n1 2022-01-01 temperature   25.3\n2 2022-01-01 humidity      65.2\n3 2022-01-01 pressure    1013  \n4 2022-01-02 temperature   27.1\n5 2022-01-02 pressure    1012  \n6 2022-01-03 temperature   24.8\n7 2022-01-03 humidity      68.5\n\n\nThe resulting measurements_long data frame will have three columns: timestamp, measurement, and value, where each row represents a specific measurement at a particular timestamp. The values_drop_na argument ensures that rows with missing values are dropped.\n\n\nConclusion\nIn this blog post, we explored the pivot_longer() function from the tidyr library, which allows us to reshape data from a wider format to a longer format. We covered the syntax and provided several examples to illustrate its usage. By mastering pivot_longer(), you’ll be equipped to tidy your data and unleash its true potential for analysis and visualization."
  },
  {
    "objectID": "posts/2023-06-08/index.html",
    "href": "posts/2023-06-08/index.html",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "",
    "text": "In R, the file.info() function is a useful tool for retrieving file information, such as file attributes and metadata. It allows programmers to gather details about files, including their size, permissions, and timestamps. In this post, we will explore the file.info() function and demonstrate how it can be used to list files by date.\n\n\nThe file.info() function returns a data frame with file information as its columns. Each row corresponds to a file, and the columns contain attributes such as the file size, permissions, and timestamps. This function accepts one or more file paths as its argument, providing flexibility in examining multiple files simultaneously. The following columns are returned in the data.frame that results from file.info():\n\nname: The name of the file.\nsize: The size of the file in bytes.\nmode: The mode of the file, which can be used to determine the file’s permissions.\nmtime: The modification time of the file.\nctime: The creation time of the file.\natime: The last access time of the file.\n\nIn order to get some data to work with, we will save the iris dataset as an excel file four times in a for loop, waiting 10 seconds between each save.\nlibrary(writexl)\n\n# Generate file names\nfile_prefix &lt;- \"iris\"\nfile_extension &lt;- \".xlsx\"\nnum_files &lt;- 4\n\n# Save iris dataset as Excel files\nfor (i in 1:num_files) {\n  file_name &lt;- paste0(file_prefix, \"_\", i, file_extension)\n  write_xlsx(iris, file_name)\n  cat(\"File\", file_name, \"saved successfully.\\n\")\n  Sys.sleep(10) # Sleep for 10 seconds then go again\n}"
  },
  {
    "objectID": "posts/2023-06-08/index.html#example-1-retrieving-file-information",
    "href": "posts/2023-06-08/index.html#example-1-retrieving-file-information",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "Example 1: Retrieving File Information",
    "text": "Example 1: Retrieving File Information\nLet’s begin by retrieving information about a single file. we have a file named “iris_1.xlsx” located in our working directory. We can use the file.info() function to obtain its attributes:\n\nfile_info &lt;- file.info(\"iris_1.xlsx\")\nprint(file_info)\n\n            size isdir mode               mtime               ctime\niris_1.xlsx 8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\n                          atime exe\niris_1.xlsx 2023-06-08 07:58:19  no\n\n\nThe output will display a data frame with the attributes of the “iris_1.xlsx” file, including the file size, permissions, and timestamps. This information can be valuable for tasks such as file management and quality control."
  },
  {
    "objectID": "posts/2023-06-08/index.html#example-2-listing-files-by-date",
    "href": "posts/2023-06-08/index.html#example-2-listing-files-by-date",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "Example 2: Listing Files by Date",
    "text": "Example 2: Listing Files by Date\nNow, let’s dive into listing files based on their dates. To achieve this, we will combine the file.info() function with other functions to extract and manipulate the timestamp information.\n\n# Obtain file information for all files in a directory\nfiles &lt;- list.files(full.names = TRUE, pattern = \"*.xlsx$\")\nfile_info &lt;- file.info(files)\nfile_info$file_name &lt;- rownames(file_info)\n\n# Sort files by modification date in ascending order\nsorted_files &lt;- files[order(file_info$mtime)]\n\n# Display the sorted file list\nprint(sorted_files)\n\n[1] \"./iris_1.xlsx\" \"./iris_2.xlsx\" \"./iris_3.xlsx\" \"./iris_4.xlsx\"\n\nfile_info[order(file_info$mtime), ]\n\n              size isdir mode               mtime               ctime\n./iris_1.xlsx 8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\n./iris_2.xlsx 8497 FALSE  666 2023-06-08 07:34:41 2023-06-08 07:34:41\n./iris_3.xlsx 8497 FALSE  666 2023-06-08 07:34:52 2023-06-08 07:34:52\n./iris_4.xlsx 8497 FALSE  666 2023-06-08 07:35:05 2023-06-08 07:35:05\n                            atime exe     file_name\n./iris_1.xlsx 2023-06-08 07:59:30  no ./iris_1.xlsx\n./iris_2.xlsx 2023-06-08 07:58:19  no ./iris_2.xlsx\n./iris_3.xlsx 2023-06-08 07:58:19  no ./iris_3.xlsx\n./iris_4.xlsx 2023-06-08 07:58:19  no ./iris_4.xlsx\n\n\nIn this example, we first specify the directory path where our target files are located. By using list.files(), we obtain a vector of file names within that directory. Setting full.names = TRUE ensures that the file paths include the directory path. We also used the pattern parameter to ensure that we only grab the Excel files.\nNext, we use file.info() on the vector of file names to retrieve the file information for all files in the directory. The resulting data frame, file_info, contains details about each file, including the modification timestamp (mtime).\nTo list the files by date, we sort the file names vector based on the modification timestamp, using order(file_info$mtime). The resulting sorted_files vector contains the file paths sorted in ascending order based on the modification date.\nFinally, we print the sorted file list to the console, providing an easy way to visualize the files listed by their modification date.\nLet’s go over some more examples. How about you want to see the files that were created in the last 24 hours, well, you could then do the following:\n\nfiles &lt;- file.info(list.files(), full.names = TRUE)\nfiles &lt;- files[files$mtime &gt;= Sys.time() - 24 * 60 * 60, ]\nprint(files)\n\n                size isdir mode               mtime               ctime\nindex.qmd       5161 FALSE  666 2023-06-08 07:59:28 2023-06-07 08:14:49\nindex.rmarkdown 5281 FALSE  666 2023-06-08 07:59:30 2023-06-08 07:59:30\niris_1.xlsx     8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\niris_2.xlsx     8497 FALSE  666 2023-06-08 07:34:41 2023-06-08 07:34:41\niris_3.xlsx     8497 FALSE  666 2023-06-08 07:34:52 2023-06-08 07:34:52\niris_4.xlsx     8497 FALSE  666 2023-06-08 07:35:05 2023-06-08 07:35:05\nNA                NA    NA &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n                              atime  exe\nindex.qmd       2023-06-08 07:59:29   no\nindex.rmarkdown 2023-06-08 07:59:30   no\niris_1.xlsx     2023-06-08 07:59:30   no\niris_2.xlsx     2023-06-08 07:59:30   no\niris_3.xlsx     2023-06-08 07:59:30   no\niris_4.xlsx     2023-06-08 07:59:30   no\nNA                             &lt;NA&gt; &lt;NA&gt;\n\n\nThe file.infor() function can also be used to filter files by other criteria such as size. Lets say we want to find all files that are larger than 100MB, well we could do the following:\n\nfiles &lt;- file.info(list.files(), full.name = TRUE)\nfiles &lt;- files[files$size &gt; 100 * 1024^2, ]\nprint(files)\n\n   size isdir mode mtime ctime atime  exe\nNA   NA    NA &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n\n\nWe can see that we had no files greater than 100MB in the current directory."
  },
  {
    "objectID": "posts/2023-06-13/index.html",
    "href": "posts/2023-06-13/index.html",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "",
    "text": "As a programmer, you may come across various scenarios where you need to create complex model formulas in R. However, constructing these formulas can often be challenging and time-consuming. This is where the ‘reformulate()’ function comes to the rescue! In this blog post, we will explore the purpose and usage of the reformulate() function in R, and provide you with simple examples to help you grasp its power."
  },
  {
    "objectID": "posts/2023-06-13/index.html#example-1-linear-regression",
    "href": "posts/2023-06-13/index.html#example-1-linear-regression",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "Example 1: Linear Regression",
    "text": "Example 1: Linear Regression\nLet’s say we want to use the mtcars dataset containing information about cars, including their hp and number of cylinders. We want to perform a linear regression to predict the mpg of the car based upon hp and cyl. Here’s how we can use ‘reformulate()’ for this purpose:\n\nlibrary(stats)\n\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"hp\", \"cyl\"), response = \"mpg\")\n\n# Fitting a linear regression model\nmodel &lt;- lm(formula, data = mtcars)\n\nformula\n\nmpg ~ hp + cyl\n\nmodel\n\n\nCall:\nlm(formula = formula, data = mtcars)\n\nCoefficients:\n(Intercept)           hp          cyl  \n   36.90833     -0.01912     -2.26469  \n\n\nIn this example, the ‘reformulate()’ function creates a formula object that specifies the relationship between the response variable “mpg” and the predictor variables “hp” and “cyl”. This formula is then passed to the ‘lm()’ function for fitting a linear regression model."
  },
  {
    "objectID": "posts/2023-06-13/index.html#example-2-logistic-regression",
    "href": "posts/2023-06-13/index.html#example-2-logistic-regression",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "Example 2: Logistic Regression",
    "text": "Example 2: Logistic Regression\nConsider a scenario where we use the mtcars dataset. We use the mpg, hp, and disp variables, and whether the car is an automatic or manual. We want to perform a logistic regression to predict the probability of passing based on the mpg, hp, and disp. Here’s how ‘reformulate()’ can help us:\n\nlibrary(stats)\n\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"mpg\", \"hp\", \"disp\"), response = \"am\")\n\n# Fitting a logistic regression model\nmodel &lt;- glm(formula, data = mtcars, family = \"binomial\")\n\nformula\n\nam ~ mpg + hp + disp\n\nmodel\n\n\nCall:  glm(formula = formula, family = \"binomial\", data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           hp         disp  \n  -33.81283      1.28498      0.14936     -0.06545  \n\nDegrees of Freedom: 31 Total (i.e. Null);  28 Residual\nNull Deviance:      43.23 \nResidual Deviance: 10.15    AIC: 18.15\n\n\nIn this example, the ‘reformulate()’ function constructs a formula that defines the relationship between the response variable “am” and the predictor variables “mpg”, “hp”, and “disp”. The resulting formula is then passed to the glm() function for fitting a logistic regression model."
  },
  {
    "objectID": "posts/2023-06-14/index.html",
    "href": "posts/2023-06-14/index.html",
    "title": "Pulling a formula from a recipe object",
    "section": "",
    "text": "Introduction\nThe formula() function in R is a generic function that is used to create and manipulate formulas. Formulas are used to specify the relationship between variables in statistical models. The basic syntax for a formula is:\nresponse ~ predictors\nThe response is the variable that you are trying to predict, and the predictors are the variables that you are using to predict the response. You can use multiple predictors by separating them with + signs. For example, the following formula predicts the mpg (miles per gallon) of a car based on the wt (weight) and hp (horsepower) of the car:\nmpg ~ wt + hp\nThe formula() function can be used to create formulas from scratch, or it can be used to extract formulas from existing objects. For example, the following code creates a formula object called my_formula that predicts the mpg of a car based on the wt and hp of the car:\n\nmy_formula &lt;- formula(mpg ~ wt + hp)\nmy_formula\n\nmpg ~ wt + hp\n\n\nThe formula() function can also be used to manipulate formulas. For example, the following code adds a new predictor called drat (drive ratio) to the my_formula formula:\n\nmy_formula &lt;- update(my_formula, mpg ~ wt + hp + drat)\nmy_formula\n\nmpg ~ wt + hp + drat\n\n\nThe formula() function is a powerful tool that can be used to create, manipulate, and analyze formulas in R.\nHere are some additional things to know about the formula() function:\n\nFormulas are objects in R, and they have a number of methods that can be used to manipulate them. For example, you can use the summary() method to get a summary of a formula, or you can use the plot() method to plot a formula.\nFormulas can be used with a variety of statistical functions in R. For example, you can use the lm() function to fit a linear model to a formula, or you can use the glm() function to fit a generalized linear model to a formula.\nFormulas are a powerful tool for statistical analysis, and they can be used to solve a wide variety of problems. If you are working with data in R, it is important to understand how to use formulas.\n\nNow that we have a decent understanding of the function, I want to shift focus a little bit and show how we can use the generics function formula() in order to extract a formula from a recipe object.\nHere is the full code that we are going to look at:\n\nlibrary(recipes)\n\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\n\nsummary(rec_obj)\n\n# A tibble: 11 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 cyl      &lt;chr [2]&gt; predictor original\n 2 disp     &lt;chr [2]&gt; predictor original\n 3 hp       &lt;chr [2]&gt; predictor original\n 4 drat     &lt;chr [2]&gt; predictor original\n 5 wt       &lt;chr [2]&gt; predictor original\n 6 qsec     &lt;chr [2]&gt; predictor original\n 7 vs       &lt;chr [2]&gt; predictor original\n 8 am       &lt;chr [2]&gt; predictor original\n 9 gear     &lt;chr [2]&gt; predictor original\n10 carb     &lt;chr [2]&gt; predictor original\n11 mpg      &lt;chr [2]&gt; outcome   original\n\n# Get formula\nrec_obj |&gt; prep() |&gt; formula()\n\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n&lt;environment: 0x0000013e3255d2f0&gt;\n\n\nLet’s break down each line and understand what it does:\nlibrary(recipes)\nThe first line imports the recipes package, which is a powerful tool for preparing and preprocessing data in a structured and reproducible manner.\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\nHere, we create a recipe object named rec_obj. This object represents a set of instructions for data transformation. In this case, we specify the formula mpg ~ ., which means we want to predict the miles per gallon (mpg) using all other variables in the mtcars dataset.\nrec_obj |&gt; prep() |&gt; formula()\nThe next line leverages the magrittr pipe operator (|&gt;) to chain multiple operations. Let’s break it down:\n\nrec_obj is passed to the prep() function. This function performs data preparation steps specified in the recipe object, such as handling missing values, feature scaling, or encoding categorical variables.\nThe output of prep() is then piped to the formula() function, which extracts the formula representation from the preprocessed recipe object. The resulting formula can be used in subsequent modeling steps.\n\nThat’s it! With just a few lines of code, we have defined a recipe, prepared the data accordingly, and obtained the formula representation for further modeling.\nNow, let’s dive into a couple more examples to showcase the versatility of the recipes package:\n\nrec_obj &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_normalize(all_predictors())\n\nrec_obj |&gt; prep() |&gt; formula()\n\nSpecies ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\n&lt;environment: 0x0000013e2e8346f0&gt;\n\n\nIn this example, we create a recipe to predict the species (Species) using all other variables in the iris dataset. We then use the step_normalize() function to standardize all predictor variables in the recipe. This step ensures that variables are on a similar scale, which can be beneficial for certain machine learning algorithms.\nrec_obj &lt;- recipe(SalePrice ~ ., data = train_data) |&gt;\n  step_dummy(all_nominal(), -all_outcomes())\nHere, we define a recipe to predict the sale price (SalePrice) using all other variables in the train_data dataset. The step_dummy() function is used to convert all nominal variables in the recipe into dummy variables. The all_nominal() argument specifies that all variables should be considered, while the -all_outcomes() argument ensures that the outcome variable (SalePrice) is not transformed.\nThese examples provide a glimpse into the power and flexibility of the recipes package for data preprocessing in R. It enables you to define a clear and reproducible data transformation pipeline that can greatly simplify your machine learning workflows.\nHappy coding! 🚀"
  },
  {
    "objectID": "posts/2023-06-15/index.html",
    "href": "posts/2023-06-15/index.html",
    "title": "Introduction to Linear Regression in R: Analyzing the mtcars Dataset with lm()",
    "section": "",
    "text": "Introduction\nThe lm() function in R is used for fitting linear regression models. It stands for “linear model,” and it allows you to analyze the relationship between variables and make predictions based on the data.\nLet’s dive into the parameters of the lm() function:\n\nformula: This is the most important parameter, as it specifies the relationship between the variables. It follows a pattern: y ~ x1 + x2 + ..., where y is the response variable, and x1, x2, etc., are the predictor variables. For example, in the mtcars dataset, we can use the formula mpg ~ wt to predict the miles per gallon (mpg) based on the weight (wt) of the cars.\ndata: This parameter refers to the dataset you want to use for the analysis. In our case, we’ll use the mtcars dataset that comes with R.\n\nNow, let’s see some examples using the mtcars dataset\n\n\nExamples\nExample 1: Simple Linear Regression\n\n# Fit a linear regression model to predict mpg based on weight\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nExample 2: Multiple Linear Regression\n\n# Fit a linear regression model to predict mpg based on weight and horsepower\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nExample 3: Include Interaction Term\n\n# Fit a linear regression model to predict mpg based on weight, horsepower, and their interaction\nmodel &lt;- lm(mpg ~ wt + hp + wt:hp, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp + wt:hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0632 -1.6491 -0.7362  1.4211  4.5513 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.80842    3.60516  13.816 5.01e-14 ***\nwt          -8.21662    1.26971  -6.471 5.20e-07 ***\nhp          -0.12010    0.02470  -4.863 4.04e-05 ***\nwt:hp        0.02785    0.00742   3.753 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.153 on 28 degrees of freedom\nMultiple R-squared:  0.8848,    Adjusted R-squared:  0.8724 \nF-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13\n\n\nThese examples demonstrate how to use the lm() function with different sets of predictor variables. After fitting the model, you can use the summary() function to get detailed information about the regression results, including coefficients, p-values, and R-squared values.\nI encourage you to try running these examples and explore different variables in the mtcars dataset. Feel free to modify the formulas and experiment with additional parameters to deepen your understanding of linear regression modeling in R!"
  },
  {
    "objectID": "posts/2023-06-08/index.html#explaining-the-file.info-function",
    "href": "posts/2023-06-08/index.html#explaining-the-file.info-function",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "",
    "text": "The file.info() function returns a data frame with file information as its columns. Each row corresponds to a file, and the columns contain attributes such as the file size, permissions, and timestamps. This function accepts one or more file paths as its argument, providing flexibility in examining multiple files simultaneously. The following columns are returned in the data.frame that results from file.info():\n\nname: The name of the file.\nsize: The size of the file in bytes.\nmode: The mode of the file, which can be used to determine the file’s permissions.\nmtime: The modification time of the file.\nctime: The creation time of the file.\natime: The last access time of the file.\n\nIn order to get some data to work with, we will save the iris dataset as an excel file four times in a for loop, waiting 10 seconds between each save.\nlibrary(writexl)\n\n# Generate file names\nfile_prefix &lt;- \"iris\"\nfile_extension &lt;- \".xlsx\"\nnum_files &lt;- 4\n\n# Save iris dataset as Excel files\nfor (i in 1:num_files) {\n  file_name &lt;- paste0(file_prefix, \"_\", i, file_extension)\n  write_xlsx(iris, file_name)\n  cat(\"File\", file_name, \"saved successfully.\\n\")\n  Sys.sleep(10) # Sleep for 10 seconds then go again\n}"
  },
  {
    "objectID": "posts/2023-06-16/index.html",
    "href": "posts/2023-06-16/index.html",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, it’s crucial to have a deep understanding of the tools at your disposal. In the realm of data analysis and manipulation, R stands as a powerhouse. One function that proves to be invaluable in many scenarios is diff(). In this blog post, we will explore the ins and outs of the diff() function, showcasing its functionality and providing you with practical examples to enhance your programming skills."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-1-simple-vector",
    "href": "posts/2023-06-16/index.html#example-1-simple-vector",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 1: Simple Vector",
    "text": "Example 1: Simple Vector\nLet’s start with a straightforward example using a numeric vector:\n\n# Create a vector\nmy_vector &lt;- c(2, 5, 9, 12, 18)\n\n# Compute differences\ndiff_vector &lt;- diff(my_vector)\n\n# Display the result\ndiff_vector\n\n[1] 3 4 3 6\n\n\nIn this example, the diff() function calculates the differences between consecutive elements in my_vector. The resulting vector, diff_vector, shows the differences [3, 4, 3, 6]."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-2-time-series-data",
    "href": "posts/2023-06-16/index.html#example-2-time-series-data",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 2: Time Series Data",
    "text": "Example 2: Time Series Data\nThe diff() function is particularly handy when working with time series data. Let’s consider a time series dataset representing monthly sales:\n\n# Create a time series\nmonthly_sales &lt;- c(150, 200, 180, 250, 300, 270, 350)\n\n# Compute month-to-month differences\nmonthly_diff &lt;- diff(monthly_sales)\n\n# Display the result\nmonthly_diff\n\n[1]  50 -20  70  50 -30  80\n\n\nHere, the diff() function calculates the changes in sales between consecutive months. The resulting vector, monthly_diff, displays the differences [50, -20, 70, 50, -30, 80]."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-3-advanced-applications",
    "href": "posts/2023-06-16/index.html#example-3-advanced-applications",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 3: Advanced Applications",
    "text": "Example 3: Advanced Applications\nBeyond simple differences, the diff() function can be combined with other R functions to solve more complex problems. Let’s say we have a vector representing the daily closing prices of a stock:\n\n# Create a vector of stock prices\nstock_prices &lt;- c(105.2, 103.9, 105.8, 107.5, 109.1)\n\n# Compute daily price changes as percentages\ndaily_returns &lt;- diff(stock_prices) / stock_prices[-length(stock_prices)] * 100\n\n# Display the result\ndaily_returns\n\n[1] -1.235741  1.828681  1.606805  1.488372\n\n\nIn this example, we calculate the daily returns as a percentage by taking the differences between consecutive closing prices and dividing them by the previous day’s closing price. The resulting vector, daily_returns, represents the daily percentage changes."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-4-miscellaneous-examples",
    "href": "posts/2023-06-16/index.html#example-4-miscellaneous-examples",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 4: Miscellaneous Examples",
    "text": "Example 4: Miscellaneous Examples\n\nx &lt;- rnorm(10)\n\n# Calculate the first-order difference of a vector\ndiff(x)\n\n[1] -0.5814577  0.5824454  1.0677214 -0.7505515  0.9924554 -2.0034078  0.5492343\n[8] -1.8906742  1.1942760\n\n# Calculate the second-order difference of a vector\ndiff(x, differences=2)\n\n[1]  1.163903  0.485276 -1.818273  1.743007 -2.995863  2.552642 -2.439908\n[8]  3.084950\n\n# Calculate the first-order difference of a matrix\ndiff(x, lag=1, differences=1)\n\n[1] -0.5814577  0.5824454  1.0677214 -0.7505515  0.9924554 -2.0034078  0.5492343\n[8] -1.8906742  1.1942760\n\n# Calculate the second-order difference of a matrix\ndiff(x, lag=1, differences=2)\n\n[1]  1.163903  0.485276 -1.818273  1.743007 -2.995863  2.552642 -2.439908\n[8]  3.084950"
  },
  {
    "objectID": "posts/2023-06-20/index.html",
    "href": "posts/2023-06-20/index.html",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "",
    "text": "As a programmer, you’re constantly faced with the task of organizing and analyzing data. One powerful tool in your R arsenal is the xtabs() function. In this blog post, we’ll explore the versatility and simplicity of xtabs() for aggregating data. We’ll use the mtcars dataset and the healthyR.data::healthyR_data dataset to illustrate its functionality. Get ready to dive into the world of data aggregation with xtabs()!"
  },
  {
    "objectID": "posts/2023-06-20/index.html#example-1-analyzing-car-performance-with-mtcars-dataset",
    "href": "posts/2023-06-20/index.html#example-1-analyzing-car-performance-with-mtcars-dataset",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "Example 1: Analyzing Car Performance with mtcars Dataset",
    "text": "Example 1: Analyzing Car Performance with mtcars Dataset\nLet’s start with the mtcars dataset, which contains information about various car models. Suppose we want to understand the distribution of cars based on the number of cylinders and the transmission type. We can use xtabs() to accomplish this:\n\n# Create a contingency table using xtabs()\ntable_cars &lt;- xtabs(~ cyl + am, data = mtcars)\n\n# View the resulting table\ntable_cars\n\n   am\ncyl  0  1\n  4  3  8\n  6  4  3\n  8 12  2\n\n\nIn this example, the formula ~ cyl + am specifies that we want to cross-tabulate the “cyl” (number of cylinders) variable with the “am” (transmission type) variable. The resulting table provides a clear breakdown of car counts based on these two factors.\nThe xtabs() function also allows you to specify the order of the variables in the formula. For example, the following formula would create the same contingency table as the previous formula, but the rows of the table would be ordered by the number of cylinders in the car:\n\nxtabs(~am + cyl, data = mtcars)\n\n   cyl\nam   4  6  8\n  0  3  4 12\n  1  8  3  2"
  },
  {
    "objectID": "posts/2023-06-20/index.html#example-2-analyzing-health-data-with-healthyr.data",
    "href": "posts/2023-06-20/index.html#example-2-analyzing-health-data-with-healthyr.data",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "Example 2: Analyzing Health Data with healthyR.data",
    "text": "Example 2: Analyzing Health Data with healthyR.data\nLet’s now explore the healthyR.data::healthyR_data dataset, which is a simulated administrative dataset. Suppose we’re interested in analyzing the distribution of patients’ insurance type based on their type of stay. Here’s how we can use xtabs() for this analysis:\n\n# Load the dataset\nlibrary(healthyR.data)\n\n# Create a contingency table using xtabs()\ntable_health &lt;- xtabs(~ payer_grouping + ip_op_flag, data = healthyR_data)\n\n# View the resulting table\ntable_health\n\n                ip_op_flag\npayer_grouping       I     O\n  ?                  1     0\n  Blue Cross     10797 13560\n  Commercial      3328  3239\n  Compensation     787  1715\n  Exchange Plans  1206  1194\n  HMO             8113  9331\n  Medicaid        7131  1646\n  Medicaid HMO   15466 10018\n  Medicare A     52621     1\n  Medicare B       293 22270\n  Medicare HMO   13572  5425\n  No Fault        1713   645\n  Self Pay        2089  1560\n\n\nIn this example, the formula ~ payer_grouping + ip_op_flag specifies that we want to cross-tabulate the “payer_grouping” variable with the “ip_op_flag” variable. By using xtabs(), we obtain a comprehensive summary of patients’ insurance type and their stay type."
  },
  {
    "objectID": "posts/2023-06-21/index.html",
    "href": "posts/2023-06-21/index.html",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "",
    "text": "Sampling is a fundamental technique in data analysis and statistical modeling. It allows us to draw meaningful insights and make inferences about a larger population based on a representative subset. In the world of R programming, the sample() function stands as a versatile tool that enables us to create random samples efficiently. In this post, we will explore the sample() function and its various applications through a series of plain English examples.\nFirst, let’s take a look at the syntax:\nsample(x, size, replace = FALSE, prob = NULL)\nwhere:\n\nx is the dataset or vector from which to take the sample\nsize is the number of elements to include in the sample\nreplace is a logical value that indicates whether or not to allow sampling with replacement (the default is FALSE)\nprob is a vector of probabilities that can be used to weight the sample (the default is NULL)"
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-1-simple-random-sampling",
    "href": "posts/2023-06-21/index.html#example-1-simple-random-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 1: Simple Random Sampling",
    "text": "Example 1: Simple Random Sampling\nLet’s say we have a dataset containing the ages of 100 people. To create a random sample of 10 individuals, we can use the sample() function as follows:\n\nages &lt;- 1:100\nrandom_sample &lt;- sample(ages, size = 10)\nrandom_sample\n\n [1] 53 13 84 50 55  9 12 38 79 15\n\n\nThe sample() function randomly selects 10 values from the ages vector, without replacement, resulting in a new vector named random_sample. This technique represents simple random sampling, where each individual in the population has an equal chance of being included in the sample."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-2-sampling-with-replacement",
    "href": "posts/2023-06-21/index.html#example-2-sampling-with-replacement",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 2: Sampling with Replacement",
    "text": "Example 2: Sampling with Replacement\nIn some scenarios, we might want to allow repeated selections from the population. Let’s say we have a bag with colored balls, and we want to simulate drawing 5 balls with replacement. Here’s how we can achieve it:\n\ncolors &lt;- c(\"red\", \"blue\", \"green\", \"yellow\")\nsample_with_replacement &lt;- sample(colors, size = 5, replace = TRUE)\nsample_with_replacement\n\n[1] \"yellow\" \"yellow\" \"green\"  \"green\"  \"red\"   \n\n\nThe sample() function, with the replace = TRUE argument, enables us to randomly select 5 colors from the colors vector, allowing duplicates. This approach represents sampling with replacement, where each selection is independent of the previous ones."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-3-weighted-sampling",
    "href": "posts/2023-06-21/index.html#example-3-weighted-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 3: Weighted Sampling",
    "text": "Example 3: Weighted Sampling\nIn certain situations, we may want to assign different probabilities to elements in the population. Let’s assume we have a list of items and corresponding weights denoting their probabilities of being selected. We can use the sample() function with the prob parameter to achieve weighted sampling. Consider the following example:\n\nlibrary(dplyr)\n\nitems &lt;- c(\"apple\", \"banana\", \"orange\")\nweights &lt;- c(0.4, 0.2, 0.4)\nweighted_sample &lt;- sample(items, size = 1, prob = weights)\nweighted_sample\n\n[1] \"apple\"\n\ntibble(x = 1:10) |&gt; \n  group_by(x) |&gt; \n  mutate(rs = sample(items, size = 1, prob = weights)) |&gt;\n  ungroup()\n\n# A tibble: 10 × 2\n       x rs    \n   &lt;int&gt; &lt;chr&gt; \n 1     1 orange\n 2     2 apple \n 3     3 apple \n 4     4 apple \n 5     5 apple \n 6     6 orange\n 7     7 orange\n 8     8 orange\n 9     9 apple \n10    10 orange\n\n\nBy specifying the prob argument with the corresponding weights, the sample() function randomly selects a single item from the items vector. The probability of each item being chosen is proportional to its weight. In this case, “apple” and “orange” have a higher chance (40% each) of being selected compared to “banana” (20%)."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-4-stratified-sampling",
    "href": "posts/2023-06-21/index.html#example-4-stratified-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 4: Stratified Sampling",
    "text": "Example 4: Stratified Sampling\nStratified sampling involves dividing the population into subgroups or strata and then sampling from each stratum proportionally. Let’s assume we have a dataset of students’ grades in different subjects, and we want to select a sample that maintains the proportion of students from each subject. We can achieve this using the sample() function along with additional parameters. Consider the following example:\n\nsubjects &lt;- c(\"Math\", \"Science\", \"English\", \"History\")\ngrades &lt;- c(80, 90, 85, 70, 75, 95, 60, 92, 88, 83, 78, 91)\nstrata &lt;- factor(subjects)\nstratified_sample &lt;- unlist(\n  by(\n    grades, \n    rep(strata, 3), \n    FUN = function(x) sample(x, size = 2)\n    )\n  )\nstratified_sample\n\nEnglish1 English2 History1 History2    Math1    Math2 Science1 Science2 \n      78       60       92       91       80       75       90       95 \n\n\nIn this example, we use the by() function to group the grades by subject (strata). Then, we apply the sample() function to each subgroup (subject) using the FUN argument. The result is a stratified sample of two grades from each subject, maintaining the relative proportions of students in the final sample."
  },
  {
    "objectID": "posts/2023-06-22/index.html",
    "href": "posts/2023-06-22/index.html",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, you’re constantly faced with the need to repeat tasks efficiently. Repetition is a fundamental concept in programming, and R provides a powerful tool to accomplish this: the rep() function. In this blog post, we will explore the syntax of the rep() function and delve into several examples to showcase its versatility and practical applications. Whether you’re working with data manipulation, generating sequences, or creating repeated patterns, rep() will become your go-to function for mastering repetition in R."
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-1-repeating-a-single-value",
    "href": "posts/2023-06-22/index.html#example-1-repeating-a-single-value",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 1: Repeating a Single Value",
    "text": "Example 1: Repeating a Single Value\nLet’s start with a simple example. Suppose we want to repeat the value 5 three times. We can achieve this using the following code:\n\nresult &lt;- rep(5, times = 3)\nprint(result)\n\n[1] 5 5 5"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-2-replicating-a-vector",
    "href": "posts/2023-06-22/index.html#example-2-replicating-a-vector",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 2: Replicating a Vector",
    "text": "Example 2: Replicating a Vector\nThe rep() function can also replicate entire vectors. Consider the following example where we replicate the vector c(1, 2, 3) four times:\n\nvector &lt;- c(1, 2, 3)\nresult &lt;- rep(vector, times = 4)\nprint(result)\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-3-repeating-elements-using-each",
    "href": "posts/2023-06-22/index.html#example-3-repeating-elements-using-each",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 3: Repeating Elements Using ‘each’",
    "text": "Example 3: Repeating Elements Using ‘each’\nThe each argument allows us to repeat each element of a vector a specific number of times. Let’s illustrate this with the following example:\n\nvector &lt;- c(1, 2, 3)\nresult &lt;- rep(vector, times = 2, each = 2)\nprint(result)\n\n [1] 1 1 2 2 3 3 1 1 2 2 3 3"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-4-creating-repeated-patterns",
    "href": "posts/2023-06-22/index.html#example-4-creating-repeated-patterns",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 4: Creating Repeated Patterns",
    "text": "Example 4: Creating Repeated Patterns\nOne interesting use case of the rep() function is to create repeated patterns. Consider this example, where we want to generate a pattern of “ABABAB” ten times:\n\npattern &lt;- rep(c(\"A\", \"B\"), times = 10)\nresult &lt;- paste(pattern, collapse = \"\")\nprint(result)\n\n[1] \"ABABABABABABABABABAB\""
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-5-expanding-factors-or-categories",
    "href": "posts/2023-06-22/index.html#example-5-expanding-factors-or-categories",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 5: Expanding Factors or Categories",
    "text": "Example 5: Expanding Factors or Categories\nThe rep() function is useful for expanding factors or categories. Let’s say we have a factor with three levels, and we want to replicate each level four times:\n\nfactor &lt;- factor(c(\"low\", \"medium\", \"high\"))\nresult &lt;- rep(factor, times = 4)\nprint(result)\n\n [1] low    medium high   low    medium high   low    medium high   low   \n[11] medium high  \nLevels: high low medium"
  },
  {
    "objectID": "posts/2023-06-23/index.html",
    "href": "posts/2023-06-23/index.html",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "",
    "text": "Bootstrap resampling is a powerful technique used in statistics and data analysis to estimate the uncertainty of a statistic by repeatedly sampling from the original data. In R, we can easily implement a bootstrap function using the lapply, rep, and sample functions. In this blog post, we will explore how to write a bootstrap function in R and provide an example using the “mpg” column from the popular “mtcars” dataset."
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-1-load-the-required-dataset",
    "href": "posts/2023-06-23/index.html#step-1-load-the-required-dataset",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 1: Load the required dataset",
    "text": "Step 1: Load the required dataset\nLet’s begin by loading the “mtcars” dataset, which is included in the base R package:\n\ndata(mtcars)"
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-2-define-the-bootstrap-function",
    "href": "posts/2023-06-23/index.html#step-2-define-the-bootstrap-function",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 2: Define the bootstrap function",
    "text": "Step 2: Define the bootstrap function\nWe’ll define a function called bootstrap() that takes two arguments: data (the input data vector) and n (the number of bootstrap iterations).\n\nbootstrap &lt;- function(data, n) {\n  resampled_data &lt;- lapply(1:n, function(i) {\n    resample &lt;- sample(data, replace = TRUE)\n    # Perform desired operations on the resampled data, e.g., compute a statistic\n    # and return the result\n  })\n  return(resampled_data)\n}\n\nbootstrapped_samples &lt;- bootstrap(mtcars$mpg, 5)\nbootstrapped_samples\n\n[[1]]\n [1] 21.0 18.1 33.9 21.4 17.3 19.2 19.2 15.8 16.4 30.4 18.1 14.3 32.4 10.4 15.0\n[16] 16.4 30.4 17.8 21.4 19.2 17.3 22.8 14.3 22.8 30.4 18.7 13.3 13.3 15.2 10.4\n[31] 15.0 13.3\n\n[[2]]\n [1] 18.7 32.4 21.0 10.4 15.0 14.7 24.4 10.4 32.4 10.4 21.0 19.7 21.4 10.4 30.4\n[16] 17.3 10.4 22.8 15.2 15.2 21.4 15.8 21.4 33.9 24.4 15.2 18.1 19.2 21.0 24.4\n[31] 15.5 21.0\n\n[[3]]\n [1] 15.5 30.4 21.0 22.8 27.3 18.1 21.0 13.3 15.2 17.3 15.8 21.0 18.1 14.3 17.8\n[16] 15.8 21.0 18.1 19.2 24.4 19.2 22.8 18.7 14.3 26.0 21.4 22.8 32.4 14.7 15.2\n[31] 15.2 14.3\n\n[[4]]\n [1] 13.3 21.0 13.3 15.0 19.2 18.1 18.1 19.2 22.8 18.7 26.0 21.4 14.7 14.3 17.8\n[16] 22.8 19.7 21.4 30.4 30.4 18.7 17.3 16.4 21.5 18.1 21.0 17.8 21.4 14.3 19.7\n[31] 32.4 18.7\n\n[[5]]\n [1] 15.0 21.4 21.5 26.0 17.3 30.4 18.1 17.8 17.3 30.4 24.4 32.4 21.0 17.8 33.9\n[16] 32.4 19.2 22.8 19.7 16.4 17.8 22.8 14.3 33.9 21.5 10.4 21.4 26.0 33.9 14.7\n[31] 21.5 18.1\n\n\nIn the above code, we use lapply to generate a list of n resampled datasets. Inside the lapply function, we use the sample function to randomly sample from the original data with replacement (replace = TRUE). This ensures that each resampled dataset has the same length as the original dataset."
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-3-perform-desired-operations-on-resampled-data",
    "href": "posts/2023-06-23/index.html#step-3-perform-desired-operations-on-resampled-data",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 3: Perform desired operations on resampled data",
    "text": "Step 3: Perform desired operations on resampled data\nWithin the lapply function, you can perform any desired operations on the resampled data. This could involve calculating statistics, fitting models, or conducting hypothesis tests. Customize the code within the lapply function to suit your specific needs.\nExample: Bootstrapping the “mpg” column in mtcars: Let’s illustrate the usage of our bootstrap function by resampling the “mpg” column from the “mtcars” dataset. We will calculate the mean of the resampled datasets.\n\n# Step 1: Load the dataset\ndata(mtcars)\n\n# Step 2: Define the bootstrap function\nbootstrap &lt;- function(data, n) {\n  resampled_data &lt;- lapply(1:n, function(i) {\n    resample &lt;- sample(data, replace = TRUE)\n    mean(resample)  # Calculate the mean of each resampled dataset\n  })\n  return(resampled_data)\n}\n\n# Step 3: Perform the bootstrap resampling\nbootstrapped_means &lt;- bootstrap(mtcars$mpg, n = 1000)\n\n# Display the first few resampled means\nhead(bootstrapped_means)\n\n[[1]]\n[1] 20.21562\n\n[[2]]\n[1] 20.09375\n\n[[3]]\n[1] 19.59375\n\n[[4]]\n[1] 20.13437\n\n[[5]]\n[1] 21.17813\n\n[[6]]\n[1] 21.5375\n\n\nIn the above example, we resample the “mpg” column of the “mtcars” dataset 1000 times. The bootstrap() function calculates the mean of each resampled dataset and returns a list of resampled means. The head() function is then used to display the first few resampled means.\nOf course we do not have to specify a statistic function in the bootstrap, we can choose to just return bootstrap samples and then perform some sort of statistic on it. Look at the following example using the above bootstrapped_samples data.\n\nquantile(unlist(bootstrapped_samples), \n         probs = c(0.025, 0.25, 0.5, 0.75, 0.975))\n\n  2.5%    25%    50%    75%  97.5% \n10.400 15.725 19.200 22.800 33.900 \n\nmean(unlist(bootstrapped_samples))\n\n[1] 20.06625\n\nsd(unlist(bootstrapped_samples))\n\n[1] 5.827239"
  },
  {
    "objectID": "posts/2023-06-26/index.html",
    "href": "posts/2023-06-26/index.html",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "",
    "text": "Welcome to the world of data visualization in R! In this blog post, we will explore the abline() function, a versatile tool that allows you to add straight lines to your plots effortlessly. Whether you’re a beginner or an experienced R programmer, mastering abline() will empower you to create more informative and visually appealing graphs. So, let’s dive in!"
  },
  {
    "objectID": "posts/2023-06-26/index.html#example-1.-simple-linear-regression-line",
    "href": "posts/2023-06-26/index.html#example-1.-simple-linear-regression-line",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Example 1. Simple Linear Regression Line:",
    "text": "Example 1. Simple Linear Regression Line:\nLet’s start with a classic example of drawing a linear regression line on a scatter plot. Consider the following data:\n\nx &lt;- 1:10\ny &lt;- c(2, 3, 5, 7, 9, 10, 13, 15, 17, 19)\n\nTo visualize the relationship between x and y, we can plot the points and add a regression line using abline():\n\nplot(x, y, main = \"Linear Regression Example\", xlab = \"x\", ylab = \"y\")\nabline(lm(y ~ x), col = \"red\")"
  },
  {
    "objectID": "posts/2023-06-26/index.html#examle-2.-custom-slope-and-intercept",
    "href": "posts/2023-06-26/index.html#examle-2.-custom-slope-and-intercept",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Examle 2. Custom Slope and Intercept",
    "text": "Examle 2. Custom Slope and Intercept\nThe abline() function allows you to specify custom slope and intercept values. Suppose you have a dataset where y increases by 3 for every unit increase in x. We can draw a line with a slope of 3 and an intercept of 0 using the following code:\n\nplot(x, y, main = \"Custom Slope and Intercept\", xlab = \"x\", ylab = \"y\")\nabline(a = 0, b = 3, col = \"blue\")"
  },
  {
    "objectID": "posts/2023-06-26/index.html#example-3.-vertical-and-horizontal-lines",
    "href": "posts/2023-06-26/index.html#example-3.-vertical-and-horizontal-lines",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Example 3. Vertical and Horizontal Lines:",
    "text": "Example 3. Vertical and Horizontal Lines:\nabline() isn’t limited to just diagonal lines; you can also draw vertical and horizontal lines. For instance, let’s draw a vertical line at x = 5 and a horizontal line at y = 12:\n\nplot(x, y, main = \"Vertical and Horizontal Lines\", xlab = \"x\", ylab = \"y\")\nabline(v = 5, col = \"green\") # Vertical line\nabline(h = 12, col = \"orange\") # Horizontal line"
  },
  {
    "objectID": "posts/2023-06-26/index.html#encouragement-to-try-it-yourself",
    "href": "posts/2023-06-26/index.html#encouragement-to-try-it-yourself",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Encouragement to Try It Yourself",
    "text": "Encouragement to Try It Yourself\nNow that you’ve seen a few examples of what the abline() function can do, I encourage you to unleash your creativity and explore its full potential. Experiment with different datasets, slopes, intercepts, and line styles. The more you practice, the more comfortable you will become with this powerful visualization tool."
  },
  {
    "objectID": "posts/2023-06-26/index.html#conclusion",
    "href": "posts/2023-06-26/index.html#conclusion",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we delved into the abline() function in R, exploring its capabilities for adding straight lines to plots. We covered simple linear regression lines, custom slopes and intercepts, as well as vertical and horizontal lines. Armed with this knowledge, you can enhance your data visualizations, making them more informative and engaging. So, go ahead, give abline() a try, and unlock a whole new world of possibilities in R programming! Happy coding!"
  },
  {
    "objectID": "posts/2023-06-27/index.html",
    "href": "posts/2023-06-27/index.html",
    "title": "The ave() Function in R",
    "section": "",
    "text": "In the world of data analysis and statistics, grouping data based on certain criteria is a common task. Whether you’re working with large datasets or analyzing trends within smaller subsets, having a reliable and efficient tool for data grouping can make your life as a programmer much easier. In this blog post, we’ll dive into the R function ave() and explore how it can help you achieve seamless data grouping and computation."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-1-computing-average-sales-by-region",
    "href": "posts/2023-06-27/index.html#example-1-computing-average-sales-by-region",
    "title": "The ave() Function in R",
    "section": "Example 1: Computing Average Sales by Region",
    "text": "Example 1: Computing Average Sales by Region\nLet’s consider a dataset containing sales data for different regions. We’ll use ave() to calculate the average sales for each region.\n\nsales &lt;- data.frame(\n  region = c(\"North\", \"South\", \"North\", \"East\", \"South\", \"East\"),\n  sales = c(500, 700, 600, 450, 800, 550)\n)\n\nsales$avg_sales &lt;- ave(sales$sales, sales$region)\nsales[order(sales$region),]\n\n  region sales avg_sales\n4   East   450       500\n6   East   550       500\n1  North   500       550\n3  North   600       550\n2  South   700       750\n5  South   800       750\n\n\nIn this example, we create a new column called avg_sales and assign the output of ave() to it. The resulting dataset will include the average sales for each region, as computed by ave()."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-2-calculating-median-age-by-gender",
    "href": "posts/2023-06-27/index.html#example-2-calculating-median-age-by-gender",
    "title": "The ave() Function in R",
    "section": "Example 2: Calculating Median Age by Gender",
    "text": "Example 2: Calculating Median Age by Gender\nLet’s explore another scenario where we have a dataset containing information about individuals’ ages and genders. We’ll use ave() to calculate the median age for each gender category.\n\npeople &lt;- data.frame(\n  age = c(32, 28, 35, 40, 26, 30),\n  gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\")\n)\n\npeople$median_age &lt;- ave(people$age, people$gender, FUN = median)\npeople[order(people$gender),]\n\n  age gender median_age\n2  28 Female         30\n4  40 Female         30\n6  30 Female         30\n1  32   Male         32\n3  35   Male         32\n5  26   Male         32\n\n\nIn this example, we introduce the FUN argument to specify the median() function. ave() will compute the median age for each gender category and assign the values to the new column median_age."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-3-finding-maximum-temperature-by-month",
    "href": "posts/2023-06-27/index.html#example-3-finding-maximum-temperature-by-month",
    "title": "The ave() Function in R",
    "section": "Example 3: Finding Maximum Temperature by Month",
    "text": "Example 3: Finding Maximum Temperature by Month\nLet’s say we have a weather dataset containing temperature readings for different months. We can use ave() to calculate the maximum temperature recorded for each month.\n\nweather &lt;- data.frame(\n  month = rep(c(\"Jan\", \"Feb\", \"Mar\"), each = 4),\n  temperature = c(15, 18, 20, 14, 16, 22, 25, 23, 19, 21, 24, 20)\n)\n\nweather$max_temp &lt;- ave(weather$temperature, weather$month, FUN = max)\nweather\n\n   month temperature max_temp\n1    Jan          15       20\n2    Jan          18       20\n3    Jan          20       20\n4    Jan          14       20\n5    Feb          16       25\n6    Feb          22       25\n7    Feb          25       25\n8    Feb          23       25\n9    Mar          19       24\n10   Mar          21       24\n11   Mar          24       24\n12   Mar          20       24\n\n\nIn this example, we use ave() to compute the maximum temperature for each month, and the resulting values are assigned to the new column max_temp."
  },
  {
    "objectID": "posts/2023-06-28/index.html",
    "href": "posts/2023-06-28/index.html",
    "title": "Exploring Rolling Correlation with the rollapply Function: A Powerful Tool for Analyzing Time-Series Data",
    "section": "",
    "text": "Introduction\nIn the world of data analysis, time-series data is a common sight. Whether it’s stock prices, weather patterns, or website traffic, understanding the relationship between variables over time is crucial. One valuable technique in this domain is calculating rolling correlation, which allows us to examine the evolving correlation between two variables as our data moves through time. In this blog post, we will delve into the rollapply function and its capabilities, exploring its applications through a series of practical examples. So, let’s get started!\n\n\nUnderstanding Rolling Correlation\nBefore we jump into the technical details, let’s quickly recap what correlation means. In simple terms, correlation measures the strength and direction of the linear relationship between two variables. It ranges between -1 and 1, where -1 represents a perfect negative correlation, 1 represents a perfect positive correlation, and 0 represents no correlation.\nRolling correlation takes this concept further by calculating correlation values over a moving window of observations. By doing so, we can observe how the correlation between two variables changes over time, gaining insights into trends, seasonality, or other patterns in our data.\n\n\nIntroducing the rollapply Function\nIn R programming, the rollapply function, available in the zoo package, is a powerful tool for calculating rolling correlation. It enables us to apply a function, such as correlation, to a rolling window of our data. The general syntax for using rollapply is as follows:\nrollapply(data, width, FUN, ...)\nHere’s what each parameter represents: - data: The time-series data we want to analyze. - width: The size of the rolling window, indicating how many observations should be included in each correlation calculation. - FUN: The function we want to apply to each rolling window. In this case, we will use the cor function to calculate correlation. - ...: Additional arguments that can be passed to the correlation function or any other function used with rollapply.\nNow, let’s dive into some practical examples to see the rollapply function in action.\n\n\nExample\nImagine we have a dataset containing daily stock prices for two companies, A and B. Our goal is to explore the rolling correlation between the returns of these two stocks over a 30-day window.\n\nlibrary(zoo)\nlibrary(tidyquant)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndf &lt;- FANG |&gt; \n  filter(symbol %in% c(\"FB\", \"AMZN\")) |&gt; \n  select(symbol, adjusted) |&gt; \n  pivot_wider(values_from = adjusted, names_from = symbol) |&gt;\n  unnest()\n\nfb_rets &lt;- diff(log(df$FB))\namzn_rets &lt;- diff(log(df$AMZN))\ndf_rets &lt;- cbind(fb_rets, amzn_rets)\ncorrelation &lt;- rollapply(\n  df_rets, \n  width = 5, \n  function(x) cor(x[,1], x[,2]), \n  by.column = FALSE\n  )\n\nplot(correlation, type=\"l\")\n\n\n\n\nIn this example, we calculate the logarithmic returns of FB and AMZN using the diff function. Then, we apply the cor function to the rolling window of returns, with a width of 5. The by.column = FALSE parameter ensures that the correlation is computed across rows instead of columns, and the fill = NA parameter fills any incomplete windows with NA values.\n\n\nConclusion\nIn this blog post, we explored the concept of rolling correlation and its significance in analyzing time-series data. We learned how to harness the power of the rollapply function from the zoo package to calculate rolling correlation effortlessly. By utilizing rollapply, we can observe the dynamic nature of correlation, uncover trends, and gain valuable insights from our time-dependent datasets.\nRemember, rolling correlation is just one of the many applications of the rollapply function. Its versatility empowers us to explore various other statistics, such as moving averages, standard deviations, and more. So, dive into the world of time-series analysis with rollapply and unlock the hidden patterns in your data!\nHappy coding and happy analyzing!"
  },
  {
    "objectID": "posts/2023-06-29/index.html",
    "href": "posts/2023-06-29/index.html",
    "title": "How to Use a Windows .bat File to Execute an R Script",
    "section": "",
    "text": "Introduction\nUsing a Windows .bat file to execute an R script can be a convenient way to automate tasks and streamline your workflow. In this blog post, we will explain each line of a sample .bat file and its corresponding R script, along with a simple explanation of what each section does.\n\nThe .bat File:\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\nNow, let’s break down each line:\n\n@echo off: This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem Set the path to the Rscript executable: The rem keyword denotes a comment in a batch file. This line sets the path to the Rscript executable, which is the command-line interface for executing R scripts.\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\": This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nrem Set the path to the R script to execute: This line is another comment, specifying that the next line sets the path to the R script that will be executed.\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\": Here, the path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE%: This line executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nrem Pause so the user can see the output: This comment suggests that the script should pause after execution so that the user can view the output before the command prompt window closes.\nexit: This command exits the batch file and closes the command prompt window.\n\n\n\nThe R Script:\nThe R script contains several sections. Here is the full script and then I will give an explanation of each section:\n# Library Load\nlibrary(DBI)\nlibrary(odbc)\nlibrary(dplyr)\nlibrary(writexl)\nlibrary(stringr)\nlibrary(Microsoft365R)\nlibrary(glue)\nlibrary(blastula)\n\n# Source SSMS Connection Functions \nsource(\"C:/Path/to/SQL_Connection_Functions.r\")\n\n# Connect to SSMS\ndbc &lt;- db_connect()\n\n# Query SSMS\nquery &lt;- DBI::dbGetQuery(\n  conn = dbc,\n  statement = paste0(\n    \"\n    select encounter,\n        pt_no \n    from dbo.c_xfer_fac_tbl \n    where encounter in \n        (\n        select distinct encounter\n        from DBO.c_xfer_fac_tbl \n        group by encounter, file_name \n        having Count(Distinct pt_no) &gt; 1\n        ) \n        and INSERT_DATETIME = \n        (\n        select Max(INSERT_DATETIME) \n        from dbo.c_xfer_fac_tbl\n        ) \n    group by encounter, pt_no \n    order by encounter\n    \"\n  )\n)\n\ndb_disconnect(dbc)\n\n# Save file to disk\npath &lt;- \"C:/Path/to/files/encounter_duplicates/\"\nf_name &lt;- \"Encounter_Duplicates_\"\nf_date &lt;- Sys.time() |&gt; \n  str_replace_all(pattern = \"[-|:]\",\"\") |&gt;\n  str_replace(pattern = \"[ ]\", \"_\")\nfull_file_name &lt;- paste0(f_name, f_date, \".xlsx\")\nfpn &lt;- paste0(path, full_file_name)\n\nwrite_xlsx(\n  x = query,\n  path = fpn\n)\n\n# Compose Email ----\n# Open Outlook\nOutlook &lt;- get_business_outlook()\n\nemail_body &lt;- md(glue(\n\"\n  ## Important!\n  \n  Please see attached file {full_file_name}\n  \n  The file attached contains a list of accounts from Hospital B\n  that have two or more Hospital A account numbers associated with them. We therefore\n  cannot process these accounts.\n  \n  Thank you,\n\n  The Team\n  \"\n))\n\nemail_template &lt;- compose_email(\n  body = email_body,\n  footer = md(\"sent via Microsoft365R and The Team\")\n)\n\n# Create Email\nOutlook$create_email(email_template)$\n  #set_body(email_body, content_type=\"html\")$\n  set_recipients(to=c(\"email1@email.com\", \"email2@email.com\"))$\n  set_subject(\"Encounter Duplicates\")$\n  add_attachment(fpn)$\n  send()\n\n# Archive File after it has been sent\narchive_path &lt;- \"C:/Path/to/Encounter_Duplicate_Files/Sent/\"\nmove_to_path &lt;- paste0(archive_path, full_file_name)\nfile.rename(\n  from = fpn,\n  to = move_to_path\n)\n\n# Clear the Session\nrm(list = ls())\n\nLibrary Load: This section loads various R libraries needed for the script’s functionality, such as database connections, data manipulation, and email composition.\nSource SSMS Connection Functions: Here, a separate R script file (SQL_Connection_Functions.r) is sourced. This file likely contains custom functions related to connecting to and querying a SQL Server Management System (SSMS) database.\nConnect to SSMS: This line establishes a connection to the SSMS database using the db_connect() function.\nQuery SSMS: The script executes a SQL query against the SSMS database using the dbGetQuery() function. The result of the query is assigned to the query variable.\nSave file to disk: The script saves the query result (query) to an Excel file on the local disk using the write_xlsx() function.\nCompose Email: This section composes an email using the blastula package, preparing the email body and setting the recipients, subject, and\n\nattachments.\n\nCreate Email: The composed email is created using the create_email() function from the Microsoft365R package. The body, recipients, subject, and attachment are set.\nSend Email: The email is sent using the send() function, which relies on a connection to Microsoft Outlook. The email body, recipients, subject, and attachment are all included in the email.\nArchive File after it has been sent: The script moves the Excel file to an archive folder after sending the email, using the file.rename() function.\nClear the Session: The rm() function is used to clear the current R session, removing any remaining objects from memory.\n\n\n\n\nConclusion\nUsing a Windows .bat file to execute an R script allows for easy automation and integration of R scripts into your workflow. By understanding each line of the .bat file and the corresponding R script sections, you can customize and adapt the process to suit your specific needs."
  },
  {
    "objectID": "posts/2023-06-30/index.html",
    "href": "posts/2023-06-30/index.html",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "",
    "text": "Managing files is an essential task for any programmer, and when working with R, the file.rename() function can become your best friend. In this blog post, we’ll explore the ins and outs of file.rename(), discuss its syntax, provide real-life examples, and share some best practices to empower you in your file management endeavors. So grab a cup of coffee and let’s dive into the world of file.rename()!"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-1-renaming-a-single-fil",
    "href": "posts/2023-06-30/index.html#example-1-renaming-a-single-fil",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 1: Renaming a Single Fil",
    "text": "Example 1: Renaming a Single Fil\nSuppose you have a file named “old_file.txt,” and you want to rename it to “new_file.txt”. Here’s how you can accomplish this with file.rename():\nfile.rename(from = \"old_file.txt\", to = \"new_file.txt\")"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-2-renaming-multiple-file",
    "href": "posts/2023-06-30/index.html#example-2-renaming-multiple-file",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 2: Renaming Multiple File",
    "text": "Example 2: Renaming Multiple File\nImagine you have a folder with several files that need to be renamed simultaneously. Let’s say you want to change the file extensions from “.doc” to “.docx”. Here’s how you can achieve this using file.rename():\nfiles &lt;- list.files(path = \"path/to/folder\", pattern = \"*.doc\", full.names = TRUE)\nnew_names &lt;- sub(pattern = \".doc$\", replacement = \".docx\", x = files)\nfile.rename(from = files, to = new_names)"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-1-renaming-a-single-file",
    "href": "posts/2023-06-30/index.html#example-1-renaming-a-single-file",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 1: Renaming a Single File",
    "text": "Example 1: Renaming a Single File\nSuppose you have a file named “old_file.txt,” and you want to rename it to “new_file.txt”. Here’s how you can accomplish this with file.rename():\nfile.rename(from = \"old_file.txt\", to = \"new_file.txt\")"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-2-renaming-multiple-files",
    "href": "posts/2023-06-30/index.html#example-2-renaming-multiple-files",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 2: Renaming Multiple Files",
    "text": "Example 2: Renaming Multiple Files\nImagine you have a folder with several files that need to be renamed simultaneously. Let’s say you want to change the file extensions from “.doc” to “.docx”. Here’s how you can achieve this using file.rename():\nfiles &lt;- list.files(path = \"path/to/folder\", pattern = \"*.doc\", full.names = TRUE)\nnew_names &lt;- sub(pattern = \".doc$\", replacement = \".docx\", x = files)\nfile.rename(from = files, to = new_names)"
  },
  {
    "objectID": "posts/2023-07-11/index.html",
    "href": "posts/2023-07-11/index.html",
    "title": "A Closer Look at the R Function identical()",
    "section": "",
    "text": "Introduction\nIn the realm of programming, R is a widely-used language for statistical computing and data analysis. Within R, there exists a powerful function called identical() that allows programmers to compare objects for exact equality. In this blog post, we will delve into the syntax and usage of the identical() function, providing clear explanations and practical examples along the way.\n\n\nSyntax of identical()\nThe identical() function in R has the following simple syntax:\nidentical(x, y)\nHere, x and y are the objects that we want to compare. The function returns a logical value of either TRUE or FALSE, indicating whether x and y are exactly identical.\n\n\nExamples\n\nComparing Numeric Values: Let’s start with a simple example comparing two numeric values:\n\n\na &lt;- 5\nb &lt;- 5\nidentical(a, b)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE since both a and b have the same numeric value of 5.\n\nComparing Character Strings: Now, let’s consider an example with character strings:\n\n\nname1 &lt;- \"John\"\nname2 &lt;- \"John\"\nidentical(name1, name2)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE as both name1 and name2 contain the same string “John”.\n\nComparing Vectors: The identical() function can also compare vectors. Let’s see an example:\n\n\nvec1 &lt;- c(1, 2, 3)\nvec2 &lt;- c(1, 2, 3)\nidentical(vec1, vec2)\n\n[1] TRUE\n\n\nHere, the identical() function will return TRUE since vec1 and vec2 have the same values in the same order.\n\nComparing Data Frames: Data frames are a fundamental data structure in R. Let’s compare two data frames using identical():\n\n\ndf1 &lt;- data.frame(a = 1:3, b = c(\"A\", \"B\", \"C\"))\ndf2 &lt;- data.frame(a = 1:3, b = c(\"A\", \"B\", \"C\"))\nidentical(df1, df2)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE as both df1 and df2 have the same column names, column types, and corresponding values.\n\nHandling Inexact Equality: The identical() function is particularly useful when we want to ensure that two objects are precisely the same. However, it does not handle cases where inexact equality is expected. For example:\n\n\nx &lt;- sqrt(2) * sqrt(2)\ny &lt;- 2\nidentical(x, y)\n\n[1] FALSE\n\n\nSurprisingly, the identical() function will return FALSE in this case. This occurs because sqrt(2) introduces a slight rounding error, resulting in x and y being slightly different despite representing the same mathematical value.\n\n\nConclusion\nIn this blog post, we explored the syntax and various use cases of the identical() function in R. By leveraging this function, you can determine whether two objects are exactly identical, whether they are numbers, strings, vectors, or even complex data structures like data frames. Remember that identical() is designed for exact equality, so if you require inexact comparisons, you may need to explore alternative approaches. Happy coding with R!"
  },
  {
    "objectID": "posts/2023-07-12/index.html",
    "href": "posts/2023-07-12/index.html",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, working with data is a crucial aspect of our work. In R, there are numerous functions available that simplify data analysis tasks. One such function is colMeans(), which allows us to calculate the mean of columns in a matrix or data frame. In this blog post, we will delve into the colMeans() function, understand its usage, and explore various examples to see how it can help us gain valuable insights from our data."
  },
  {
    "objectID": "posts/2023-07-12/index.html#example-1-calculating-column-means-in-a-matri",
    "href": "posts/2023-07-12/index.html#example-1-calculating-column-means-in-a-matri",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "Example 1: Calculating column means in a matri",
    "text": "Example 1: Calculating column means in a matri\n\n# Create a matrix\nmy_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\n\n# Calculate column means\ncol_means &lt;- colMeans(my_matrix)\n\n# Print the result\nprint(col_means)\n\n[1] 1.5 3.5 5.5\n\n\nIn this example, we created a 2x3 matrix called ‘my_matrix’ and used colMeans() to calculate the means for each column. The resulting vector ‘col_means’ contains the mean values of columns [1 3 5], [2 3 6], which are [1.5, 3.5, 5.5] respectively."
  },
  {
    "objectID": "posts/2023-07-12/index.html#example-2-handling-missing-values",
    "href": "posts/2023-07-12/index.html#example-2-handling-missing-values",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "Example 2: Handling missing values",
    "text": "Example 2: Handling missing values\n\n# Create a matrix with missing values\nmy_matrix &lt;- matrix(c(1, 2, NA, 4, 5, 6), nrow = 2, ncol = 3)\n\n# Calculate column means with missing values removed\ncol_means &lt;- colMeans(my_matrix, na.rm = TRUE)\n\n# Print the result\nprint(col_means)\n\n[1] 1.5 4.0 5.5\n\n\nIn this example, our matrix ‘my_matrix’ contains a missing value (NA). By setting the ‘na.rm’ argument to TRUE, colMeans() excludes the missing value while calculating the means. As a result, we obtain the column means [1.5 4.0 5.5]"
  },
  {
    "objectID": "posts/2023-07-13/index.html",
    "href": "posts/2023-07-13/index.html",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "",
    "text": "As a programmer, you’ll often come across situations where you need to check whether a file exists before performing any operations on it. Thankfully, the R programming language provides a handy function called file.exists() that allows you to easily determine the existence of a file. In this blog post, we’ll explore the syntax and usage of file.exists() and provide you with practical examples to encourage you to try it out for yourself."
  },
  {
    "objectID": "posts/2023-07-13/index.html#example-1-checking-the-existence-of-a-file",
    "href": "posts/2023-07-13/index.html#example-1-checking-the-existence-of-a-file",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "Example 1: Checking the Existence of a File",
    "text": "Example 1: Checking the Existence of a File\nSuppose you want to check whether a file named “data.csv” exists in the current working directory. You can use the following code:\n\nfile_path &lt;- \"data.csv\"\nif (file.exists(file_path)) {\n  print(\"The file exists!\")\n} else {\n  print(\"The file does not exist.\")\n}\n\n[1] \"The file does not exist.\"\n\n\nIn this example, we assign the file path to the variable file_path and then use file.exists() to check if the file exists. If the condition is met, it will print “The file exists!” Otherwise, it will print “The file does not exist.”"
  },
  {
    "objectID": "posts/2023-07-13/index.html#example-2-conditional-operations-with-file.exists",
    "href": "posts/2023-07-13/index.html#example-2-conditional-operations-with-file.exists",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "Example 2: Conditional Operations with file.exists()",
    "text": "Example 2: Conditional Operations with file.exists()\nLet’s imagine you want to perform different actions based on the existence of multiple files. Consider the following code snippet:\n\nfile1 &lt;- \"data1.csv\"\nfile2 &lt;- \"data2.csv\"\n\nif (file.exists(file1)) {\n  # Perform an operation if file1 exists\n  print(\"Performing operation on file1...\")\n} else {\n  # Perform a different operation if file1 doesn't exist\n  print(\"File1 does not exist.\")\n}\n\n[1] \"File1 does not exist.\"\n\nif (file.exists(file2)) {\n  # Perform an operation if file2 exists\n  print(\"Performing operation on file2...\")\n} else {\n  # Perform a different operation if file2 doesn't exist\n  print(\"File2 does not exist.\")\n}\n\n[1] \"File2 does not exist.\"\n\n\nIn this example, we check the existence of two files, data1.csv and data2.csv, and perform different actions based on their availability. You can modify the code according to your specific needs and perform any desired operations."
  },
  {
    "objectID": "posts/2023-07-14/index.html",
    "href": "posts/2023-07-14/index.html",
    "title": "Covariance in R with the cov() Function",
    "section": "",
    "text": "In the world of data analysis, understanding the relationship between variables is crucial. One powerful tool for measuring this relationship is the covariance. Today, we’ll explore the cov() function in R and delve into the fascinating world of covariance. Whether you’re a beginner or an experienced programmer, this blog post will equip you with the knowledge to harness the potential of cov() in your data analysis projects."
  },
  {
    "objectID": "posts/2023-07-14/index.html#example-1-calculating-covariance-between-two-variables",
    "href": "posts/2023-07-14/index.html#example-1-calculating-covariance-between-two-variables",
    "title": "Covariance in R with the cov() Function",
    "section": "Example 1: Calculating Covariance between Two Variables",
    "text": "Example 1: Calculating Covariance between Two Variables\nSuppose we have two vectors, x and y, representing the number of hours studied and the corresponding test scores, respectively, for a group of students. We want to measure the covariance between these two variables.\n\n# Create example vectors\nx &lt;- c(5, 7, 3, 6, 8)\ny &lt;- c(65, 80, 50, 70, 90)\n\n# Calculate covariance\ncovariance &lt;- cov(x, y)\n\ncovariance\n\n[1] 29\n\n\nIn this example, the cov() function takes the vectors x and y as inputs and returns the covariance between the two variables. The resulting covariance value will help us understand the relationship between the hours studied and the corresponding test scores. What this is particular example is saying is that for every unit increase in x there is a 29 unit increase in y."
  },
  {
    "objectID": "posts/2023-07-14/index.html#example-2-calculating-covariance-matrix",
    "href": "posts/2023-07-14/index.html#example-2-calculating-covariance-matrix",
    "title": "Covariance in R with the cov() Function",
    "section": "Example 2: Calculating Covariance Matrix",
    "text": "Example 2: Calculating Covariance Matrix\nNow let’s consider a scenario where we have multiple variables, and we want to calculate the covariance matrix to gain insights into their relationships.\n\n# Create example vectors\nx &lt;- c(5, 7, 3, 6, 8)\ny &lt;- c(65, 80, 50, 70, 90)\nz &lt;- c(150, 200, 100, 180, 220)\n\n# Combine vectors into a matrix\ndata &lt;- cbind(x, y, z)\n\n# Calculate covariance matrix\ncov_matrix &lt;- cov(data)\ncov_matrix\n\n     x   y    z\nx  3.7  29   90\ny 29.0 230  700\nz 90.0 700 2200\n\n\nIn this example, we have three variables, x, y, and z, representing hours studied, test scores, and total marks, respectively. We use the cbind() function to combine the vectors into a matrix called data. By applying the cov() function to this matrix, we obtain a covariance matrix that reveals the relationships between all the variables."
  },
  {
    "objectID": "posts/2023-07-17/index.html",
    "href": "posts/2023-07-17/index.html",
    "title": "Finding Duplicate Values in a Data Frame in R: A Guide Using Base R and dplyr",
    "section": "",
    "text": "Introduction\nIn data analysis and programming, it’s common to encounter situations where you need to identify duplicate values within a dataset. Whether you’re a beginner or an experienced programmer, knowing how to find duplicate values is a fundamental skill. In this blog post, we will explore two different approaches to accomplish this task using base R functions and the dplyr package in R. By the end, you’ll have a clear understanding of how to detect and manage duplicate values in your own datasets.\n\n\nUsing Base R Functions\nR provides a variety of functions for data manipulation and analysis, including those specifically designed for identifying duplicate values. Let’s consider a simple data frame to demonstrate this approach:\n\n# Creating a sample data frame\ndf &lt;- data.frame(\n  ID = c(1, 2, 3, 3, 4, 5),\n  Name = c(\"John\", \"Jane\", \"Mark\", \"Mark\", \"Luke\", \"Kate\"),\n  Age = c(25, 30, 35, 35, 40, 45)\n)\n\nTo find duplicate values in this data frame using base R functions, we can utilize the duplicated() and table() functions:\n\n# Using base R functions to find duplicate values\nduplicates &lt;- df[duplicated(df), ]\nduplicate_counts &lt;- table(df[duplicated(df), ])\n\nduplicates\n\n  ID Name Age\n4  3 Mark  35\n\nduplicate_counts\n\n, , Age = 35\n\n   Name\nID  Mark\n  3    1\n\n\nThe duplicated() function identifies the duplicate rows in the data frame, while the table() function creates a frequency table of the duplicate values. By combining these two functions, we can detect and examine the duplicate entries in the data frame.\n\n\nUsing dplyr\nThe dplyr package provides a powerful set of tools for data manipulation and analysis. Let’s see how we can accomplish the same task of finding duplicate values using dplyr functions:\n\n# loading the dplyr package\nlibrary(dplyr)\n\n# Using dplyr to find duplicate values\nduplicates &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\nduplicate_counts &lt;- df |&gt;\n  add_count(ID, Name, Age) |&gt;\n  filter(n &gt; 1) |&gt;\n  distinct()\n\nduplicates\n\n# A tibble: 2 × 3\n     ID Name    Age\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     3 Mark     35\n2     3 Mark     35\n\nduplicate_counts\n\n  ID Name Age n\n1  3 Mark  35 2\n\n\nLet’s break the first one down step by step:\nduplicates &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\ndf refers to a data frame in R.\ngroup_by_all() groups the data frame by all columns. This means that the subsequent operations will consider duplicate values across all columns.\nfilter(n() &gt; 1) filters the grouped data frame to only keep rows where the count (n()) of observations is greater than 1. In other words, it keeps only the rows that have duplicates.\nungroup() removes the grouping, ensuring that the resulting data frame is not grouped anymore.\nThe resulting data frame with duplicate rows is assigned to the variable duplicates.\n\nNow, let’s move on to the second part:\nduplicate_counts &lt;- df |&gt;\n  add_count(ID, Name, Age) |&gt;\n  filter(n &gt; 1) |&gt;\n  distinct()\n\nadd_count(ID, Name, Age) adds a new column called “n” to the data frame, which represents the count of observations for each combination of ID, Name, and Age.\nfilter(n &gt; 1) keeps only the rows where the count (“n”) is greater than 1. This retains only the rows that have duplicates based on the specified columns.\ndistinct() removes any duplicate rows that may still exist after the previous steps, keeping only unique rows.\nThe resulting data frame with duplicate counts and unique rows is assigned to the variable duplicate_counts.\n\nIn simple terms, the code first identifies and extracts the duplicate rows from the original data frame (df) and assigns them to duplicates. Then, it calculates the counts of duplicates based on specific columns (ID, Name, and Age) and stores the results, along with unique rows, in duplicate_counts.\nThese operations allow you to conveniently find duplicate rows and examine their counts within a data frame using both base R functions and some simple dplyr code.\n\n\nConclusion\nDetecting and managing duplicate values is an essential task in data analysis and programming. In this blog post, we explored two different approaches to find duplicate values in a data frame using base R functions and the dplyr package. By leveraging these techniques, you can efficiently identify and handle duplicate entries in your own datasets.\nI encourage you to practice using these methods on your own datasets. Familiarize yourself with the functions, experiment with different data frames, and explore various scenarios. This hands-on experience will deepen your understanding and improve your data analysis skills.\nRemember, the ability to identify and manage duplicate values is crucial for ensuring data integrity and obtaining accurate results in your data analysis projects. So go ahead, give it a try, and unlock the power of duplicate value detection in R!"
  },
  {
    "objectID": "posts/2023-07-18/index.html",
    "href": "posts/2023-07-18/index.html",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "",
    "text": "In data analysis and manipulation tasks, it’s common to encounter situations where we need to identify and handle duplicate rows in a dataset. In this blog post, we will explore three different approaches to finding duplicate rows in R: the base R method, the dplyr package, and the data.table package. We’ll compare their performance using the benchmark function and provide insights on when to use each approach. So, grab your coding gear, and let’s dive in!"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-1-base-rs-duplicated-function",
    "href": "posts/2023-07-18/index.html#approach-1-base-rs-duplicated-function",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 1: Base R’s duplicated Function",
    "text": "Approach 1: Base R’s duplicated Function\nThe simplest approach to finding duplicate rows is to use the duplicated function from base R. This function returns a logical vector indicating which rows are duplicates. We can apply it directly to our data frame df.\n\nduplicated_rows_base &lt;- duplicated(df)"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-2-dplyrs-concise-data-manipulation",
    "href": "posts/2023-07-18/index.html#approach-2-dplyrs-concise-data-manipulation",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 2: dplyr’s Concise Data Manipulation",
    "text": "Approach 2: dplyr’s Concise Data Manipulation\nThe dplyr package provides an intuitive and concise way to manipulate data frames. We can leverage its chaining syntax to filter the duplicated rows. The group_by_all function groups the data frame by all columns, and filter(n() &gt; 1) keeps only those rows with more than one occurrence within each group. Finally, ungroup removes the grouping information.\n\nduplicated_rows_dplyr &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-3-efficient-duplicate-detection-with-data.table",
    "href": "posts/2023-07-18/index.html#approach-3-efficient-duplicate-detection-with-data.table",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 3: Efficient Duplicate Detection with data.table",
    "text": "Approach 3: Efficient Duplicate Detection with data.table\nIf performance is a crucial factor, the data.table package offers highly optimized operations on large datasets. Converting our data frame to a data.table object allows us to utilize the efficient duplicated function from data.table.\n\ndtdf &lt;- data.table(df)\nduplicated_rows_datatable &lt;- duplicated(dtdf)\n\nBenchmarking and Performance Comparison: To evaluate the performance of the three approaches, we will use the benchmark function from the rbenchmark package. We’ll execute each approach ten times and collect information such as execution time (elapsed), relative performance, and CPU times (user.self and sys.self).\n\nbenchmark(\n  duplicated_rows_base = duplicated(df),\n  duplicated_rows_dplyr = df |&gt; \n    group_by_all() |&gt; \n    filter(n() &gt; 1) |&gt;\n    ungroup(),\n  duplicated_rows_datatable = duplicated(dtdf),\n  replications = 10,\n  columns = c(\"test\",\"replications\",\"elapsed\",\n              \"relative\",\"user.self\",\"sys.self\")\n) |&gt;\n  arrange(relative)\n\n                       test replications elapsed relative user.self sys.self\n1 duplicated_rows_datatable           10    0.05      1.0      0.01     0.01\n2     duplicated_rows_dplyr           10    0.29      5.8      0.27     0.02\n3      duplicated_rows_base           10    3.53     70.6      3.45     0.08"
  },
  {
    "objectID": "posts/2023-07-19/index.html",
    "href": "posts/2023-07-19/index.html",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "If you’re an aspiring data scientist or R programmer, you must be familiar with the powerful data structure called “lists.” Lists in R are collections of elements that can contain various data types such as vectors, matrices, data frames, or even other lists. They offer great flexibility and are widely used in many real-world scenarios.\nIn this blog post, we will explore one of the essential skills in working with lists: subsetting. Subsetting allows you to extract specific elements or portions of a list, helping you access and manipulate data efficiently. So, let’s dive into the world of list subsetting and learn some useful techniques along the way!\n\n\nBefore we start subsetting, let’s review how to access elements within a list. In R, you can access elements of a list using square brackets “[]” you can also use double square brackets “[[ ]]” or the dollar sign “$”. The double square brackets are used when you know the exact position of the element you want to extract, while the dollar sign is used when you know the name of the element.\n\n# Create a sample list\nmy_list &lt;- list(name = \"John\", age = 30, scores = c(85, 90, 78))\n\n# Access elements using double square brackets\nname &lt;- my_list[[1]]\nage &lt;- my_list[[2]]\nscores &lt;- my_list[[3]]\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n# Access elements using dollar sign\nname &lt;- my_list$name\nage &lt;- my_list$age\nscores &lt;- my_list$scores\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n\n\n\n\n\n\nSubsetting by position allows you to extract specific elements based on their index within the list. Remember, R uses 1-based indexing, so the first element is at position 1, the second at position 2, and so on.\n\n# Subsetting by position\nelement_1 &lt;- my_list[[1]]      # Extract the first element\nelement_2 &lt;- my_list[[2]]      # Extract the second element\nelement_last &lt;- my_list[[3]]   # Extract the last element\n\nelement_1\n\n[1] \"John\"\n\nelement_2\n\n[1] 30\n\nelement_last\n\n[1] 85 90 78\n\n# You can also use negative values to exclude elements\nexcluding_last &lt;- my_list[-3] # Exclude the last element\nexcluding_last\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\n\n\n\n\nSubsetting by name is particularly useful when you want to access elements using their names. It provides a more intuitive way to extract specific elements from a list.\n\n# Subsetting by name\nname &lt;- my_list[[\"name\"]]      # Extract the element with the name \"name\"\nscores &lt;- my_list[[\"scores\"]]  # Extract the element with the name \"scores\"\n\n# You can also use the dollar sign notation for name-based subsetting\nage &lt;- my_list$age\n\nname\n\n[1] \"John\"\n\nscores\n\n[1] 85 90 78\n\nage\n\n[1] 30\n\n\n\n\n\nYou can subset multiple elements at once using numeric or character vectors for positions or names, respectively.\n\n# Subsetting multiple elements by position\nelements_1_2 &lt;- my_list[c(1, 2)] # Extract the first and second elements\nelements_1_2\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\nelements_first_last &lt;- my_list[c(1, 3)] # Extract the first and last elements\nelements_first_last\n\n$name\n[1] \"John\"\n\n$scores\n[1] 85 90 78\n\n# Subsetting multiple elements by name\nelements_age_scores &lt;- my_list[c(\"age\", \"scores\")] # Extract elements with names \"age\" \n                                                   # and \"scores\"\nelements_age_scores\n\n$age\n[1] 30\n\n$scores\n[1] 85 90 78\n\n\n\n\n\nLists can contain other lists, creating a nested structure. To access elements within nested lists, you can use multiple “[[ ]]” or “$” operators.\n\n# Create a nested list\nnested_list &lt;- list(personal_info = my_list, hobbies = c(\"Reading\", \"Painting\"))\n\nnested_list\n\n$personal_info\n$personal_info$name\n[1] \"John\"\n\n$personal_info$age\n[1] 30\n\n$personal_info$scores\n[1] 85 90 78\n\n\n$hobbies\n[1] \"Reading\"  \"Painting\"\n\n# Access elements within nested lists\nname &lt;- nested_list[[\"personal_info\"]][[\"name\"]] # Extract the name from the nested list\nname\n\n[1] \"John\"\n\nsecond_hobby &lt;- nested_list[[\"hobbies\"]][[2]] # Extract the second \n                                              # hobby from the nested list\nsecond_hobby\n\n[1] \"Painting\"\n\n\n\n\n\n\nSubsetting lists in R is a fundamental skill that will prove invaluable in your data manipulation tasks. I encourage you to practice these techniques with your own data and explore more advanced subsetting methods, such as using logical conditions or applying functions to subsets.\nBy mastering list subsetting, you’ll unlock the true potential of R for data analysis and gain the confidence to handle complex data structures efficiently.\nSo, don’t hesitate! Dive into the world of list subsetting and enhance your R programming skills today. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-19/index.html#accessing-elements-in-a-list",
    "href": "posts/2023-07-19/index.html#accessing-elements-in-a-list",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Before we start subsetting, let’s review how to access elements within a list. In R, you can access elements of a list using square brackets “[]” you can also use double square brackets “[[ ]]” or the dollar sign “$”. The double square brackets are used when you know the exact position of the element you want to extract, while the dollar sign is used when you know the name of the element.\n\n# Create a sample list\nmy_list &lt;- list(name = \"John\", age = 30, scores = c(85, 90, 78))\n\n# Access elements using double square brackets\nname &lt;- my_list[[1]]\nage &lt;- my_list[[2]]\nscores &lt;- my_list[[3]]\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n# Access elements using dollar sign\nname &lt;- my_list$name\nage &lt;- my_list$age\nscores &lt;- my_list$scores\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78"
  },
  {
    "objectID": "posts/2023-07-19/index.html#subsetting-list-elements",
    "href": "posts/2023-07-19/index.html#subsetting-list-elements",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Subsetting by position allows you to extract specific elements based on their index within the list. Remember, R uses 1-based indexing, so the first element is at position 1, the second at position 2, and so on.\n\n# Subsetting by position\nelement_1 &lt;- my_list[[1]]      # Extract the first element\nelement_2 &lt;- my_list[[2]]      # Extract the second element\nelement_last &lt;- my_list[[3]]   # Extract the last element\n\nelement_1\n\n[1] \"John\"\n\nelement_2\n\n[1] 30\n\nelement_last\n\n[1] 85 90 78\n\n# You can also use negative values to exclude elements\nexcluding_last &lt;- my_list[-3] # Exclude the last element\nexcluding_last\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\n\n\n\n\nSubsetting by name is particularly useful when you want to access elements using their names. It provides a more intuitive way to extract specific elements from a list.\n\n# Subsetting by name\nname &lt;- my_list[[\"name\"]]      # Extract the element with the name \"name\"\nscores &lt;- my_list[[\"scores\"]]  # Extract the element with the name \"scores\"\n\n# You can also use the dollar sign notation for name-based subsetting\nage &lt;- my_list$age\n\nname\n\n[1] \"John\"\n\nscores\n\n[1] 85 90 78\n\nage\n\n[1] 30\n\n\n\n\n\nYou can subset multiple elements at once using numeric or character vectors for positions or names, respectively.\n\n# Subsetting multiple elements by position\nelements_1_2 &lt;- my_list[c(1, 2)] # Extract the first and second elements\nelements_1_2\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\nelements_first_last &lt;- my_list[c(1, 3)] # Extract the first and last elements\nelements_first_last\n\n$name\n[1] \"John\"\n\n$scores\n[1] 85 90 78\n\n# Subsetting multiple elements by name\nelements_age_scores &lt;- my_list[c(\"age\", \"scores\")] # Extract elements with names \"age\" \n                                                   # and \"scores\"\nelements_age_scores\n\n$age\n[1] 30\n\n$scores\n[1] 85 90 78\n\n\n\n\n\nLists can contain other lists, creating a nested structure. To access elements within nested lists, you can use multiple “[[ ]]” or “$” operators.\n\n# Create a nested list\nnested_list &lt;- list(personal_info = my_list, hobbies = c(\"Reading\", \"Painting\"))\n\nnested_list\n\n$personal_info\n$personal_info$name\n[1] \"John\"\n\n$personal_info$age\n[1] 30\n\n$personal_info$scores\n[1] 85 90 78\n\n\n$hobbies\n[1] \"Reading\"  \"Painting\"\n\n# Access elements within nested lists\nname &lt;- nested_list[[\"personal_info\"]][[\"name\"]] # Extract the name from the nested list\nname\n\n[1] \"John\"\n\nsecond_hobby &lt;- nested_list[[\"hobbies\"]][[2]] # Extract the second \n                                              # hobby from the nested list\nsecond_hobby\n\n[1] \"Painting\""
  },
  {
    "objectID": "posts/2023-07-19/index.html#explore-further",
    "href": "posts/2023-07-19/index.html#explore-further",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Subsetting lists in R is a fundamental skill that will prove invaluable in your data manipulation tasks. I encourage you to practice these techniques with your own data and explore more advanced subsetting methods, such as using logical conditions or applying functions to subsets.\nBy mastering list subsetting, you’ll unlock the true potential of R for data analysis and gain the confidence to handle complex data structures efficiently.\nSo, don’t hesitate! Dive into the world of list subsetting and enhance your R programming skills today. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-20/index.html",
    "href": "posts/2023-07-20/index.html",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "",
    "text": "If you’ve been working with R for some time, you might have come across situations where your code becomes cumbersome due to repetitive references to data frames or list elements. Luckily, R provides two powerful functions, with() and within(), to help you streamline your code and make it more readable. These functions offer a simple and elegant solution for manipulating data frames and lists. In this blog post, we’ll explore the syntax of these functions and provide several real-world examples to demonstrate their usefulness. So, let’s dive in and discover how with() and within() can become your new best friends in R programming!"
  },
  {
    "objectID": "posts/2023-07-20/index.html#with-syntax",
    "href": "posts/2023-07-20/index.html#with-syntax",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "with() syntax:",
    "text": "with() syntax:\nwith(data, expr)\n\ndata: The data frame or list you want to use as an environment within the expression (expr).\nexpr: The expression where you can refer to data frame/list elements directly, without prefixing them with the data name.\n\n\nwithin(): The within() function is similar to with(), but it modifies the data frame or list in place and returns the modified object."
  },
  {
    "objectID": "posts/2023-07-20/index.html#within-syntax",
    "href": "posts/2023-07-20/index.html#within-syntax",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "within() syntax:",
    "text": "within() syntax:\nwithin(data, expr)\n\ndata: The data frame or list you want to modify within the expression (expr).\nexpr: The expression where you can manipulate data frame/list elements directly, without prefixing them with the data name.\n\nNow that we know the basics, let’s explore some examples to see these functions in action."
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-1-simplifying-data-manipulation-with-with",
    "href": "posts/2023-07-20/index.html#example-1-simplifying-data-manipulation-with-with",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 1: Simplifying Data Manipulation with with()",
    "text": "Example 1: Simplifying Data Manipulation with with()\nSuppose we have a data frame containing information about employees and their salaries:\n\n# Sample data frame\nemployee_data &lt;- data.frame(\n  name = c(\"John\", \"Jane\", \"Michael\", \"Sara\"),\n  age = c(32, 28, 45, 37),\n  salary = c(50000, 60000, 75000, 62000)\n)\n\nemployee_data\n\n     name age salary\n1    John  32  50000\n2    Jane  28  60000\n3 Michael  45  75000\n4    Sara  37  62000\n\n\nWithout with(), calculating the average salary of employees would require repetitive references to the data frame:\n\n# Without with()\navg_salary &lt;- mean(employee_data$salary)\navg_salary\n\n[1] 61750\n\n\nHowever, with the with() function, we can write the same code more concisely:\n\n# With with()\navg_salary &lt;- with(employee_data, mean(salary))\navg_salary\n\n[1] 61750"
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-2-simplifying-data-transformation-with-within",
    "href": "posts/2023-07-20/index.html#example-2-simplifying-data-transformation-with-within",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 2: Simplifying Data Transformation with within()",
    "text": "Example 2: Simplifying Data Transformation with within()\nLet’s consider a scenario where we want to create a new column bonus for employees based on their age:\n\n# Without within()\nemployee_data$bonus &lt;- ifelse(employee_data$age &gt;= 35, 5000, 3000)\nemployee_data\n\n     name age salary bonus\n1    John  32  50000  3000\n2    Jane  28  60000  3000\n3 Michael  45  75000  5000\n4    Sara  37  62000  5000\n\n\nBy using within(), we can modify the data frame directly without repetitive references:\n\n# With within()\nemployee_data &lt;- within(employee_data, bonus &lt;- ifelse(age &gt;= 45, 5000, 3000))\nemployee_data\n\n     name age salary bonus\n1    John  32  50000  3000\n2    Jane  28  60000  3000\n3 Michael  45  75000  5000\n4    Sara  37  62000  3000"
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-3-simplifying-plotting-with-with",
    "href": "posts/2023-07-20/index.html#example-3-simplifying-plotting-with-with",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 3: Simplifying Plotting with with()",
    "text": "Example 3: Simplifying Plotting with with()\nWhen creating visualizations, with() can help you avoid prefixing data frame column names repeatedly. Let’s generate a scatter plot of employee age versus salary:\n\n# Without with()\nplot(\n  employee_data$salary, \n  employee_data$age, \n  xlab = \"Salary\", \n  ylab = \"Age\", \n  main = \"Age vs. Salary\"\n  )\n\n\n\n\nUsing with(), we can eliminate the repetition:\n\n# With with()\nwith(\n  employee_data, \n  plot(\n    salary, \n    age, \n    xlab = \"Salary\", \n    ylab = \"Age\", \n    main = \"Age vs. Salary\"\n    )\n  )\n\n\n\n\nHere are some additional examples of how to use the with() and within() functions. To calculate the mean of the values in the x column of the data data frame, you would use the following code:\nwith(data, mean(x))\nTo create a new data frame that contains the mean of the values in each column, you would use the following code:\nnew_data &lt;- within(data, {\n  for (column in names(data)) {\n    column_mean &lt;- mean(data[[column]])\n    data[[column]] &lt;- column_mean\n  }\n})\n\nnew_data\nTo filter the data data frame to only include rows where the value in the x column is greater than 1, you would use the following code:\nnew_data &lt;- within(data, {\n  new_data &lt;- data[data$x &gt; 1, ]\n})\n\nnew_data"
  },
  {
    "objectID": "posts/2023-07-20/index.html#conclusion",
    "href": "posts/2023-07-20/index.html#conclusion",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored two powerful R functions: with() and within(). These functions provide an elegant way to manipulate data frames and lists by reducing repetitive references and simplifying your code. By leveraging the capabilities of with() and within(), you can write more readable and efficient code. I encourage you to try out these functions in your R projects and experience the benefits firsthand. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-21/index.html",
    "href": "posts/2023-07-21/index.html",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "",
    "text": "As a programmer in R, you’ll often find yourself working with textual data and need to manipulate or display it in various ways. Two essential functions at your disposal for these tasks are paste() and cat(). These functions are powerful tools that allow you to combine and display text easily and efficiently. In this blog post, we’ll explore the syntax, similarities, and differences between these functions and provide you with practical examples to get you started. Let’s dive in!"
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-1-basic-concatenation",
    "href": "posts/2023-07-21/index.html#example-1-basic-concatenation",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 1: Basic Concatenation",
    "text": "Example 1: Basic Concatenation\n\nfruit1 &lt;- \"apple\"\nfruit2 &lt;- \"orange\"\nresult &lt;- paste(fruit1, fruit2)\nprint(result) # Output: \"apple orange\"\n\n[1] \"apple orange\""
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-2-using-different-separator",
    "href": "posts/2023-07-21/index.html#example-2-using-different-separator",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 2: Using Different Separator",
    "text": "Example 2: Using Different Separator\n\nmonths &lt;- c(\"January\", \"February\", \"March\")\nresult &lt;- paste(months, collapse = \", \")\nprint(result) # Output: \"January, February, March\"\n\n[1] \"January, February, March\""
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-1-basic-concatenation-and-display",
    "href": "posts/2023-07-21/index.html#example-1-basic-concatenation-and-display",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 1: Basic Concatenation and Display",
    "text": "Example 1: Basic Concatenation and Display\n\nfruit1 &lt;- \"apple\"\nfruit2 &lt;- \"orange\"\ncat(\"My favorite fruits are\", fruit1, \"and\", fruit2) # Output: \"My favorite fruits are apple and orange\"\n\nMy favorite fruits are apple and orange"
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-2-output-to-file",
    "href": "posts/2023-07-21/index.html#example-2-output-to-file",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 2: Output to File",
    "text": "Example 2: Output to File\nnumbers &lt;- 1:5\nfile_path &lt;- \"numbers.txt\"\ncat(numbers, file = file_path)\n# The content of \"numbers.txt\": 1 2 3 4 5"
  }
]