[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steve On Data",
    "section": "",
    "text": "From Chaos to Clarity: Mastering Weekly Data Wrangling in R with strftime()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying Dates: Finding the Day of the Week in R with lubridate\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Check if a Column is a Date in R: A Comprehensive Guide with Examples\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Check if Date is Between Two Dates in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying Date Manipulation: How to Get Week Numbers in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTaming Excel Dates in R: From Numbers to Meaningful Dates!\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAccounts Recievables Pathways in SQL\n\n\n\n\n\n\n\ncode\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nR for the Real World: Counting those Business Days like a Pro!\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Flies? Time Travels! Adding Days to Dates in R (Like a Pro)\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Time Manipulation in R: Subtracting Hours with Ease\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Extract Month from Date in R (With Examples)\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Earliest Date: A Journey Through R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Data Lengths with R’s lengths() Function\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying bootstrap_stat_plot(): Your Ticket to Insightful Data Exploration\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe new function on the block with tidyAML extract_regression_residuals()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Enhanced Features of tidyAML’s internal_make_wflw_predictions()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUsing .drop_na in Fast Classification and Regression\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTidyDensity Powers Up with Data.table: Speedier Distributions for Your Data Exploration\n\n\n\n\n\n\n\ncode\n\n\nbenchmark\n\n\ndatatable\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBenchmarking the Speed of Cumulative Functions in TidyDensity\n\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Peaks: A Dive into the Triangular Distribution in TidyDensity\n\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nNew Horizons for TidyDensity: Version 1.3.0 Release\n\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nConquering Daily Data: How to Aggregate to Months and Years Like a Pro in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Smooth Operator: Rolling Averages in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nReflecting on the Past Year: A LinkedIn Year in Review (2023)\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlinkedin\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Lowess Smoothing in R: A Step-by-Step Guide\n\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnlocking the Power of Time: Transforming Data Frames into Time Series in R\n\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Time Traveler: Plotting Time Series in R\n\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating Time Series in R with the ts() Function\n\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Variance Inflation Factor (VIF) in R: A Practical Guide\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying Odds Ratios in Logistic Regression: Your R Recipe for Loan Defaults\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDecoding the Mystery: How to Interpret Regression Output in R Like a Champ\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nConquering Unequal Variance with Weighted Least Squares in R: A Practical Guide\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring TidyAML: Simplifying Regression Analysis in R\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA Complete Guide to Stepwise Regression in R\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Spline Regression\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\ntidyAML: Now supporting gee models\n\n\n\n\n\n\n\nrtip\n\n\ntidyaml\n\n\nregression\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nNavigating Quantile Regression with R: A Comprehensive Guide\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding and Implementing Robust Regression in R\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling Power Regression: A Step-by-Step Guide in R\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nLogarithmic Regression in R: A Step-by-Step Guide with Prediction Intervals\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Exponential Regression in R: A Step-by-Step Guide\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nQuadratic Regression in R: Unveiling Non-Linear Relationships\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\n{healthyR.ts} New Features: Unlocking More Power\n\n\n\n\n\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Perform Multiple Linear Regression in R\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Predict a Single Value Using a Regression Model in R\n\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnlocking the Power of Prediction Intervals in R: A Practical Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Simulate & Plot a Bivariate Normal Distribution in R: A Hands-on Guide\n\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying Data: A Comprehensive Guide to Calculating and Plotting Cumulative Distribution Functions (CDFs) in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing TidyDensity’s New Powerhouse: The convert_to_ts() Function\n\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nFitting a Distribution to Data in R\n\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding the Triangular Distribution and Its Application in R\n\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMultinomial Distribution in R\n\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nRandomness in R: runif(), punif(), dunif(), and quinf()\n\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPlotting Log Log Plots In Base R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPlotting a Logistic Regression In Base R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWhat’s a Bland-Altman Plot? In Base R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Scree Plot in Base R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Create a Bubble Chart in R using ggplot2\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating Pareto Charts in R with the qcc Package\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Interaction Plots in R: Unveiling Hidden Relationships\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMaking Time Series Stationary Made Easy with auto_stationarize()\n\n\n\n\n\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTesting stationarity with the ts_adf_test() function in R\n\n\n\n\n\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing Time Series Growth with ts_growth_rate_vec() in healthyR.ts\n\n\n\n\n\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering the Art of Drawing Circles in Plots with R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use cex to Change the Size of Plot Elements in base R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHorizontal Legends in Base R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nResizing Legends in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Legends in R: Drawing Them Outside the Plot\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating Stacked Dot Plots in R: A Guide with Base R and ggplot2\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating Interactive Radar Charts in R with the ‘fmsb’ Library\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHorizontal Boxplots in R using the Palmer Penguins Data Set\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPlotting Decision Trees in R with rpart and rpart.plot\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Reorder Boxplots in R: A Comprehensive Guide\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nEnhancing Your Data Visualizations with Base R: Overlaying Points and Lines\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Visualization with ggplot2: A Guide to Using facet_grid()\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Visualization with Pairs Plots in Base R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating Confidence Intervals for a Linear Model in R Using Base R and the Iris Dataset\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Visualization in R: Plotting Predicted Values with the mtcars Dataset\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Data with Scatter Plots by Group in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Histogram Breaks in R: Unveiling the Power of Data Visualization\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHistograms with Two or More Variables in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Create a Histogram with Different Colors in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Plot Multiple Plots on the Same Graph in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Third Dimension with R: A Guide to the persp() Function\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPlotting SVM Decision Boundaries with e1071 in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating Population Pyramid Plots in R with ggplot2\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Visualization in R: How to Plot a Subset of Data\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Multivariate Data with Principal Component Analysis (PCA) Biplot in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWhen to use Jitter\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nKernel Density Plots in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Relationships with Correlation Heatmaps in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nEnhancing Your Histograms in R: Adding Vertical Lines for Better Insights\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Plot Multiple Histograms with Base R and ggplot2\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPlotting Multiple Lines on a Graph in R: A Step-by-Step Guide\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Data Distribution in R: A Comprehensive Guide\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling Data Distribution Patterns with stripchart() in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Box Plots with Mean Values using Base R and ggplot2\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Data Distribution with Box Plots in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Approximation with R’s approx() Function\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Power of the curve() Function in R\n\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSolving Systems of Equations in R using the solve() Function\n\n\n\n\n\n\n\nrtip\n\n\nlinearequations\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe substring() function in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\npmax() and pmin(): Finding the Parallel Maximum and Minimum in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Grouped Counting in R: A Comprehensive Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Transformation with the scale() Function in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nEnhance Your Plots with the text() Function in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring R’s Versatile str() Function: Unraveling Your Data with Ease!\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe unlist() Function in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nR Functions for Getting Objects\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe replicate() function in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe intersect() function in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnleashing the Power of Cumulative Mean in R: A Step-by-Step Guide\n\n\n\n\n\n\n\nrtip\n\n\ncumulative\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSummarizing Data in R: tapply() vs. group_by() and summarize()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnraveling Data Insights with R’s fivenum(): A Programmer’s Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Calculate Percentage by Group in R using Base R, dplyr, and data.table\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHarness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplify Your Code with R’s Powerful Functions: with() and within()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to subset list objects in R\n\n\n\n\n\n\n\nrtip\n\n\nlist\n\n\nsubset\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nEfficiently Finding Duplicate Rows in R: A Comparative Analysis\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\ndplyr\n\n\ndatatable\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nFinding Duplicate Values in a Data Frame in R: A Guide Using Base R and dplyr\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCovariance in R with the cov() Function\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying File Existence Checking in R with file.exists()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Data with colMeans() in R: A Programmer’s Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA Closer Look at the R Function identical()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying File Management in R: Introducing file.rename()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use a Windows .bat File to Execute an R Script\n\n\n\n\n\n\n\nrtip\n\n\nbatchfile\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Rolling Correlation with the rollapply Function: A Powerful Tool for Analyzing Time-Series Data\n\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe ave() Function in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nVisualization in R: Unleashing the Power of the abline() Function\n\n\n\n\n\n\n\nrtip\n\n\nabline\n\n\nviz\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrap Function in R: Resampling with the lapply and sample Functions\n\n\n\n\n\n\n\nrtip\n\n\nbootstrap\n\n\nlapply\n\n\nsample\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Repetition with R’s rep() Function: A Programmer’s Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnleashing the Power of Sampling in R: Exploring the Versatile sample() Function\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering Data Aggregation with xtabs() in R\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering the Power of R’s diff() Function: A Programmer’s Guide\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Linear Regression in R: Analyzing the mtcars Dataset with lm()\n\n\n\n\n\n\n\nrtip\n\n\nlinear\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPulling a formula from a recipe object\n\n\n\n\n\n\n\nrtip\n\n\nrecipes\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying Model Formulas with the R Function ‘reformulate()’\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding the file.info() Function in R: Listing Files by Date\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying Data Transformation with pivot_longer() in R’s tidyr Library\n\n\n\n\n\n\n\nrtip\n\n\ntidyr\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSorting, Ordering, and Ranking: Unraveling R’s Powerful Functions\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe do.call() function in R: Unlocking Efficiency and Flexibility\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying Regular Expressions: A Programmer’s Guide for Beginners\n\n\n\n\n\n\n\nrtip\n\n\nregex\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying Logical Operations with the R Function any()\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWhy Check File Size Output for Different Methods?\n\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\nopenxlsx\n\n\nxlsx\n\n\nwritexl\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nComparing R Packages for Writing Excel Files: An Analysis of writexl, openxlsx, and xlsx in R\n\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\nopenxlsx\n\n\nxlsx\n\n\nwritexl\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Data with TidyDensity: A Guide to Using tidy_empirical() and tidy_four_autoplot() in R\n\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\ndplyr\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWhat is the sink() function? Capturing Output to External Files\n\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUpdate to {TidyDensity}\n\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMastering File Manipulation with R’s list.files() Function\n\n\n\n\n\n\n\nrtip\n\n\nfiles\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe which() Function in R\n\n\n\n\n\n\n\nrtip\n\n\nwhich\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times Pt 4\n\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times Pt 3\n\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times Pt 2: Finding the Next Mothers Day with Simplicity\n\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times Pt 1\n\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nVBA to R and Back Again: Running R from VBA Pt 2\n\n\n\n\n\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nVBA to R and Back Again: Running R from VBA\n\n\n\n\n\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUpdates to {healthyR.data}\n\n\n\n\n\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMaps with {shiny} Pt 2\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nmapping\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMaps with {shiny}\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nmapping\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow to Download a File from the Internet using download.file()\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nreadxl\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExtracting a model call from a fitted workflow in {tidymodels}\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 4\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 3\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 2\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 1\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny}, {TidyDensity} and {plotly} Part 5\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 4\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 3\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 2\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity}\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nStyling Tables for Excel with {styledTables}\n\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nReading in Multiple Excel Sheets with lapply and {readxl}\n\n\n\n\n\n\n\nrtip\n\n\nreadxl\n\n\nlapply\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA New Package for the African Stock Market {BRVM}\n\n\n\n\n\n\n\nrtip\n\n\nbrvm\n\n\nmarkets\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nLooking at Daily Log Returns with tidyquant, TidyDensity, and Shiny\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\ntidyquant\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA sample Shiny App to view Forecasts on the AirPassengers Data\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ndata\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA sample Shiny App to view CMS Healthcare Data\n\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ndata\n\n\nhealthcare\n\n\ncms\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nA Bootstrapped Time Series Model with auto.arima() from {forecast}\n\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\nbootstrap\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow fast does a compressed file in Part 2\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\narrow\n\n\nduckdb\n\n\ndatatable\n\n\nreadr\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow fast does a compressed file in?\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHow fast do the files read in?\n\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSome Examples of Cumulative Mean with {TidyDensity}\n\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGetting the CCI30 Index Current Makeup\n\n\n\n\n\n\n\ncrypto\n\n\ncci30\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUse of the apply family of functions\n\n\n\n\n\n\n\nthanks\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUse of the apply family of functions\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\napply\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMultiple Solutions to speedup tidy_bernoulli() with {data.table}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\ndatatable\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGetting NYS Home Heating Oil Prices with {rvest}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrvest\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\ntidy_bernoulli() with {data.table}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndatatable\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimple examples of imap() from {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimple examples of pmap() from {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Timeseries in a list with R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nText Processing Made Easy with {healthyR}’s sql_left(), sql_mid(), and sql_right() Functions in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nsql\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nOpen a File Folder in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nshell\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nQuickly Generate Nested Time Series Models\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nautoarima\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nData Preppers with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\npreprocessor\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCalibrate and Plot a Time Series with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nConverting a {tidyAML} tibble to a {workflowsets}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\nworkflowsets\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nOfficially on CRAN {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMoving Average Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAn example of using {box}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nbox\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nOff to CRAN! {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGet the Current Hospital Data Set from CMS with {healthyR.data}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating and Predicting Fast Regression Parsnip Models with {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreating an R Project Directory\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSubsetting Named Lists in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlist\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCumulative Measurement Functions with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe Argument Matcher: A Function for Selecting the Right Arguments {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlist\n\n\ntidyaml\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDiverging Lollipop Chart: A Visual Tool for Comparing Data with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\nplots\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAttributes in R Functions: An Overview\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nmetadata\n\n\nattributes\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMedian: A Simple Way to Detect Excess Events Over Time with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\n{healthyR.ts}: The New and Improved Library for Time Series Analysis\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nService Line Grouping with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\naugment\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTransforming Your Data: A Guide to Popular Methods and How to Implement Them with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntransforms\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying List Filtering in R with purrr’s keep()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMaking Non Stationary Data Stationary\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nADF and Phillips-Perron Tests for Stationarity using lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\ntimeseries\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAnother Post on Lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBoilerplate XGBoost with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nxgboost\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGeometric Brownian Motion with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAugmenting a Brownian Motion to a Time Series with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAuto K-Means with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nkmeans\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nThe building of {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAn Update on {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\nautoml\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nReflecting on the Past Year: A LinkedIn Year in Review\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlinkedin\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nOptimal Break Points for Histograms with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\nhistograms\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nNew Release of {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBrownian Motion\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMore Randomwalks with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\nrandomwalk\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCalendar Heatmap with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nEvent Analysis with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGartner Magic Chart and its usefulness in healthcare analytics with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimulating Time Series Model Forecasts with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\ntimeseries\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nListing Functions and Parameters\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndplyr\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDistribution Statistics with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nRandom Walks with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrandomwalk\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nViewing Different Versions of the Same Statistical Distribution with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndistributions\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nModel Scedacity Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimple Moving Average Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDistribution Summaries with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMixture Distributions with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\nmixturemodels\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreate QQ Plots for Time Series Models with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreate a Faceted Historgram Plot with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhistograms\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCreate Multiple {parsnip} Model Specs with {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nparsnip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nZ-Score Scaling Step Recipe with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nNaming Items in a List with {purrr}, {dplyr}, or {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAuto KNN with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nknn\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nExtract Boilerplate Workflow Metrics with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nGenerate Random Walk Data with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDefault Metric Sets with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSummary Statistics with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\ndatatable\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nData Preprocessing Scale/Normalize with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrap Modeling with Base R\n\n\n\n\n\n\n\ncode\n\n\nbootstrap\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUpdates to {healthyverse} packages\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyverse\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrap Modeling with {purrr} and {modler}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nmodelr\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCumulative Harmonic Mean with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nAuto Prep data for XGBoost with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nxgboost\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nFind Skewed Features with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nskew\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Lag Correlation Plots\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nReading Multiple Files with {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMapping K-Means with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nkmeans\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nHyperbolic Transform with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nDiscrete Fourier Vec with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrapping and Plots with TidyDensity\n\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\nbootstrap\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCumulative Skewness\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPCA with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nControl Charts in healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nCumulative Variance\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ncumulative\n\n\nsapply\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Clustering with healthyR.ts\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nhealthyR.ai Primer\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTidyDensity Primer\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimple lapply()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To Steve On Data\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/healthyrai-20221013/index.html",
    "href": "posts/healthyrai-20221013/index.html",
    "title": "healthyR.ai Primer",
    "section": "",
    "text": "This is going to serve as a sort of primer for my r packge {healthyR.ai}. The goal of this package is to help with producing uniform machine learning/ai models either from scratch or by way of one of the boilerplate functions.\nThis particular article is going to focus on k-means clustering with umap projection and visualization.\nFirst things first, lets load in the library:\n\nlibrary(healthyR.ai)\n\n\n== Welcome to healthyR.ai ===========================================================================\nIf you find this package useful, please leave a star: \n   https://github.com/spsanderson/healthyR.ai'\n\nIf you encounter a bug or want to request an enhancement please file an issue at:\n   https://github.com/spsanderson/healthyR.ai/issues\n\nThank you for using healthyR.ai\n\n\n\nInformation\nK-Means is a partition algorithm initially designed for signal processing. The goal is to partition n observations into k clusters where each n is in k. The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters.\nThe aim of this post is to showcase the use of the healthyR.ai wrapper for the kmeans function along with the wrapper and plot for the uwot::umap projection function. We will go through the entire workflow from getting the data to getting the final UMAP plot.\n\n\nGenerate some data\n\nsuppressPackageStartupMessages(library(healthyR.data))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(broom))\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata_tbl <- healthyR_data %>%\n    filter(ip_op_flag == \"I\") %>%\n    filter(payer_grouping != \"Medicare B\") %>%\n    filter(payer_grouping != \"?\") %>%\n    select(service_line, payer_grouping) %>%\n    mutate(record = 1) %>%\n    as_tibble()\n\ndata_tbl %>%\n  glimpse()\n\nRows: 116,823\nColumns: 3\n$ service_line   <chr> \"Medical\", \"Schizophrenia\", \"Syncope\", \"Pneumonia\", \"Ch…\n$ payer_grouping <chr> \"Blue Cross\", \"Medicare A\", \"Medicare A\", \"Medicare A\",…\n$ record         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nNow that we have our data we need to generate what is called a user item table. To do this we use the function hai_kmeans_user_item_tbl which takes in just a few arguments. The purpose of the user item table is to aggregate and normalize the data between the users and the items.\nThe data that we have generated is going to look for clustering amongst the service_lines (the user) and the payer_grouping (item) columns.\nLets now create the user item table.\n\n\nUser Item Tibble\n\nuit_tbl <- hai_kmeans_user_item_tbl(\n  data_tbl, \n  service_line, \n  payer_grouping, \n  record\n)\n\nuit_tbl\n\n# A tibble: 23 × 12\n   service_line   Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷\n   <chr>            <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n 1 Alcohol Abuse   0.0941 0.0321  5.25e-4 0.0116  0.0788 0.158    0.367   0.173 \n 2 Bariatric Sur…  0.317  0.0583  0       0.0518  0.168  0.00324  0.343   0.0485\n 3 Carotid Endar…  0.0845 0.0282  0       0       0.0141 0        0.0282  0.648 \n 4 Cellulitis      0.110  0.0339  1.18e-2 0.00847 0.0805 0.0869   0.192   0.355 \n 5 Chest Pain      0.144  0.0391  2.90e-3 0.00543 0.112  0.0522   0.159   0.324 \n 6 CHF             0.0295 0.00958 5.18e-4 0.00414 0.0205 0.0197   0.0596  0.657 \n 7 COPD            0.0493 0.0228  2.28e-4 0.00548 0.0342 0.0461   0.172   0.520 \n 8 CVA             0.0647 0.0246  1.07e-3 0.0107  0.0524 0.0289   0.0764  0.555 \n 9 GI Hemorrhage   0.0542 0.0175  1.25e-3 0.00834 0.0480 0.0350   0.0855  0.588 \n10 Joint Replace…  0.139  0.0179  3.36e-2 0.00673 0.0516 0        0.0874  0.5   \n# … with 13 more rows, 3 more variables: `Medicare HMO` <dbl>,\n#   `No Fault` <dbl>, `Self Pay` <dbl>, and abbreviated variable names\n#   ¹​`Blue Cross`, ²​Commercial, ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid,\n#   ⁶​`Medicaid HMO`, ⁷​`Medicare A`\n\n\nThe table is aggregated by item for the various users to which the algorithm will be applied.\nNow that we have this data we need to find what will be out optimal k (clusters). To do this we need to generate a table of data that will have a column of k and for that k apply the k-means function to the data with that k and return the total within sum of squares.\nTo do this there is a convienent function called hai_kmeans_mapped_tbl that takes as its sole argument the output from the hai_kmeans_user_item_tbl. There is an argument .centers where the default is set to 15.\n\n\nK-Means Mapped Tibble\n\nkmm_tbl <- hai_kmeans_mapped_tbl(uit_tbl)\nkmm_tbl\n\n# A tibble: 15 × 3\n   centers k_means  glance          \n     <int> <list>   <list>          \n 1       1 <kmeans> <tibble [1 × 4]>\n 2       2 <kmeans> <tibble [1 × 4]>\n 3       3 <kmeans> <tibble [1 × 4]>\n 4       4 <kmeans> <tibble [1 × 4]>\n 5       5 <kmeans> <tibble [1 × 4]>\n 6       6 <kmeans> <tibble [1 × 4]>\n 7       7 <kmeans> <tibble [1 × 4]>\n 8       8 <kmeans> <tibble [1 × 4]>\n 9       9 <kmeans> <tibble [1 × 4]>\n10      10 <kmeans> <tibble [1 × 4]>\n11      11 <kmeans> <tibble [1 × 4]>\n12      12 <kmeans> <tibble [1 × 4]>\n13      13 <kmeans> <tibble [1 × 4]>\n14      14 <kmeans> <tibble [1 × 4]>\n15      15 <kmeans> <tibble [1 × 4]>\n\n\nAs we see there are three columns, centers, k_means and glance. The k_means column is the k_means list object and glance is the tibble returned by the broom::glance function.\n\nkmm_tbl %>%\n  tidyr::unnest(glance)\n\n# A tibble: 15 × 6\n   centers k_means  totss tot.withinss betweenss  iter\n     <int> <list>   <dbl>        <dbl>     <dbl> <int>\n 1       1 <kmeans>  1.41       1.41    1.33e-15     1\n 2       2 <kmeans>  1.41       0.592   8.17e- 1     1\n 3       3 <kmeans>  1.41       0.372   1.04e+ 0     2\n 4       4 <kmeans>  1.41       0.276   1.13e+ 0     2\n 5       5 <kmeans>  1.41       0.202   1.21e+ 0     2\n 6       6 <kmeans>  1.41       0.159   1.25e+ 0     3\n 7       7 <kmeans>  1.41       0.124   1.28e+ 0     3\n 8       8 <kmeans>  1.41       0.0884  1.32e+ 0     2\n 9       9 <kmeans>  1.41       0.0745  1.33e+ 0     3\n10      10 <kmeans>  1.41       0.0576  1.35e+ 0     2\n11      11 <kmeans>  1.41       0.0460  1.36e+ 0     2\n12      12 <kmeans>  1.41       0.0363  1.37e+ 0     3\n13      13 <kmeans>  1.41       0.0293  1.38e+ 0     3\n14      14 <kmeans>  1.41       0.0202  1.39e+ 0     2\n15      15 <kmeans>  1.41       0.0161  1.39e+ 0     2\n\n\nAs stated we use the tot.withinss to decide what will become our k, an easy way to do this is to visualize the Scree Plot, also known as the elbow plot. This is done by ploting the x-axis as the centers and the y-axis as the tot.withinss.\n\n\nScree Plot and Data\n\nhai_kmeans_scree_plt(.data = kmm_tbl)\n\n\n\n\nIf we want to see the scree plot data that creates the plot then we can use another function hai_kmeans_scree_data_tbl.\n\nhai_kmeans_scree_data_tbl(kmm_tbl)\n\n# A tibble: 15 × 2\n   centers tot.withinss\n     <int>        <dbl>\n 1       1       1.41  \n 2       2       0.592 \n 3       3       0.372 \n 4       4       0.276 \n 5       5       0.202 \n 6       6       0.159 \n 7       7       0.124 \n 8       8       0.0884\n 9       9       0.0745\n10      10       0.0576\n11      11       0.0460\n12      12       0.0363\n13      13       0.0293\n14      14       0.0202\n15      15       0.0161\n\n\nWith the above pieces of information we can decide upon a value for k, in this instance we are going to use 3. Now that we have that we can go ahead with creating the umap list object where we can take a look at a great many things associated with the data.\n\n\nUMAP List Object\nNow lets go ahead and create our UMAP list object.\n\nump_lst <- hai_umap_list(.data = uit_tbl, kmm_tbl, 3)\n\nNow that it is created, lets take a look at each item in the list. The umap_list function returns a list of 5 items.\n\numap_obj\numap_results_tbl\nkmeans_obj\nkmeans_cluster_tbl\numap_kmeans_cluster_results_tbl\n\nSince we have the list object we can now inspect the kmeans_obj, first thing we will do is use the hai_kmeans_tidy_tbl function to inspect things.\n\nkm_obj <- ump_lst$kmeans_obj\nhai_kmeans_tidy_tbl(.kmeans_obj = km_obj, .data = uit_tbl, .tidy_type = \"glance\")\n\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  <dbl>        <dbl>     <dbl> <int>\n1  1.41        0.372      1.04     2\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"augment\")\n\n# A tibble: 23 × 2\n   service_line                  cluster\n   <chr>                         <fct>  \n 1 Alcohol Abuse                 1      \n 2 Bariatric Surgery For Obesity 1      \n 3 Carotid Endarterectomy        2      \n 4 Cellulitis                    3      \n 5 Chest Pain                    3      \n 6 CHF                           2      \n 7 COPD                          2      \n 8 CVA                           2      \n 9 GI Hemorrhage                 2      \n10 Joint Replacement             2      \n# … with 13 more rows\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"tidy\")\n\n# A tibble: 3 × 14\n  Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷ Medic…⁸ No Fa…⁹\n    <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1  0.150   0.0368 3.07e-4 0.0207  0.163   0.131   0.314    0.132  0.0319 0.00136\n2  0.0784  0.0218 4.32e-3 0.00620 0.0449  0.0368  0.0800   0.563  0.152  0.00348\n3  0.117   0.0314 1.02e-2 0.0139  0.0982  0.0856  0.147    0.354  0.105  0.00707\n# … with 4 more variables: `Self Pay` <dbl>, size <int>, withinss <dbl>,\n#   cluster <fct>, and abbreviated variable names ¹​`Blue Cross`, ²​Commercial,\n#   ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid, ⁶​`Medicaid HMO`,\n#   ⁷​`Medicare A`, ⁸​`Medicare HMO`, ⁹​`No Fault`\n\n\n\n\nUMAP Plot\nNow that we have all of the above data we can visualize our clusters that are colored by their cluster number.\n\nhai_umap_plot(.data = ump_lst, .point_size = 3, TRUE)"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html",
    "href": "posts/healthyrts-20221021/index.html",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "",
    "text": "There are two components to time-series clustering with {healthyR.ts}. There is the function that will create the clustering data along with a slew of other information and then there is a plotting function that will plot out the data in a time-series fashion colored by cluster.\nThe first function as mentioned is the function ts_feature_cluster(), and the next is ts_feature_cluster_plot()\nFunction Reference:\n\nts_feature_cluster()\nts_feature_cluster_plot()`\n\nWe are going to use the built-in AirPassengers data set for this example so let’s get right to it!"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster()",
    "text": "ts_feature_cluster()\nAs mentioned there are several outputs from the ts_feature_cluster(). Those are as follows:\nData Section\n\nts_feature_tbl\nuser_item_matrix_tbl\nmapped_tbl\nscree_data_tbl\ninput_data_tbl (the original data)\n\nPlots\n\nstatic_plot\nplotly_plot\n\nNow that we have our output, let’s take a look at each individual component of the output.\nts_feature_tbl\n\noutput$data$ts_feature_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nuser_item_matrix_tbl\n\noutput$data$user_item_matrix_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nmapped_tbl\n\noutput$data$mapped_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nscree_data_tbl\n\noutput$data$scree_data_tbl |> glimpse()\n\nRows: 3\nColumns: 2\n$ centers      <int> 1, 2, 3\n$ tot.withinss <dbl> 1.8324477, 0.7364934, 0.4571258\n\n\ninput_data_tbl\n\noutput$data$input_data_tbl |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nNow the plots.\nstatic_plot\n\noutput$plots$static_plot\n\n\n\n\nplotly_plot\n\noutput$plots$plotly_plot\n\n\n\n\n\nNow that we have seen the output of the ts_feature_cluster() function, let’s take a look at the output of the ts_feature_cluster_plot() function."
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster_plot()",
    "text": "ts_feature_cluster_plot()\nThis function itself returns a list object of a multitude of data. First before we get into that lets look at the function call itself:\n\nts_feature_cluster_plot(\n  .data,\n  .date_col,\n  .value_col,\n  ...,\n  .center = 3,\n  .facet_ncol = 3,\n  .smooth = FALSE\n)\n\nThe data that comes back from this function is:\nData Section\n\noriginal_data\nkmm_data_tbl\nuser_item_tbl\ncluster_tbl\n\nPlots\n\nstatic_plot\nplotly_plot\n\nK-Means Object\n\nk-means object\n\nWe will go through the same exercise and show the output of all the sections. First we have to create the output. The static plot will automatically print out.\n\nplot_out <- ts_feature_cluster_plot(\n  .data = output,\n  .date_col = date_col,\n  .value_col = value,\n  .center = 2,\n  group_id\n)\n\nJoining, by = \"group_id\"\n\n\n\n\n\n\nThe Data Section:\noriginal_data\n\nplot_out$data$original_data |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nkmm_data_tbl\n\nplot_out$data$kmm_data_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nuser_item_data\n\nplot_out$data$user_item_data |> glimpse()\n\n NULL\n\n\ncluster_tbl\n\nplot_out$data$cluster_tbl |> glimpse()\n\nRows: 12\nColumns: 9\n$ cluster        <int> 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\n\n\nThe plot data.\nstatic_plot\n\nplot_out$plot$static_plot\n\n\n\n\nplotly_plot\n\nplot_out$plot$plotly_plot\n\n\n\n\n\n\n\nThe K-Means Object\nkmeans_object\n\nplot_out$kmeans_object\n\n[[1]]\nK-means clustering with 2 clusters of sizes 5, 7\n\nCluster means:\n  ts_x_acf1 ts_x_acf10 ts_diff1_acf1 ts_diff1_acf10 ts_diff2_acf1 ts_seas_acf1\n1 0.7456468   1.568532     0.1172685      0.4858013    -0.1799728    0.2876449\n2 0.7387865   1.528308    -0.2909349      0.3638392    -0.5916245    0.2930543\n  ts_entropy\n1  0.4918321\n2  0.6438176\n\nClustering vector:\n [1] 1 1 2 2 2 1 1 1 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 0.3704304 0.3660630\n (between_SS / total_SS =  59.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Simple lapply()",
    "section": "",
    "text": "This is a simple lapply example to start things off.\n\n# Let l be some list of lists, where all elements of lists are numbers\nl <- list(\n  a = 1:10,\n  b = 11:20,\n  c = 21:30\n)\n\nNow let’s take a look at our list l and see it’s structure.\n\nl\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$b\n [1] 11 12 13 14 15 16 17 18 19 20\n\n$c\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nNow that we see the structure, we can use the lapply function to get the sum of each list element, the mean, etc.\n\nlapply(l, sum)\n\n$a\n[1] 55\n\n$b\n[1] 155\n\n$c\n[1] 255\n\nlapply(l, mean)\n\n$a\n[1] 5.5\n\n$b\n[1] 15.5\n\n$c\n[1] 25.5\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2021-01-11/index.html",
    "href": "posts/rtip-2021-01-11/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2022, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2023!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2022\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nfp <- \"linkedin_content.xlsx\"\n\nengagement_tbl <- read_excel(fp, sheet = \"ENGAGEMENT\") %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ntop_posts_tbl <- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %>%\n  clean_names()\n\nfollowers_tbl <- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ndemographics_tbl <- read_excel(fp, sheet = \"DEMOGRAPHICS\") %>%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 362\nColumns: 4\n$ date              <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 202…\n$ impressions       <dbl> 3088, 3911, 3303, 3134, 1118, 799, 3068, 1954, 2663,…\n$ engagements       <dbl> 31, 56, 51, 42, 8, 4, 43, 20, 33, 43, 14, 41, 5, 17,…\n$ `Engagement Rate` <dbl> 1.0038860, 1.4318589, 1.5440509, 1.3401404, 0.715563…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 5\n$ post_url_1  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ engagements <dbl> 241, 136, 123, 117, 117, 115, 107, 106, 104, 104, 95, 81, …\n$ x3          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_4  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ impressions <dbl> 52300, 33903, 30752, 29887, 25953, 24139, 23769, 18522, 18…\n\nglimpse(followers_tbl)\n\nRows: 362\nColumns: 2\n$ date          <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 2022-01…\n$ new_followers <dbl> 10, 10, 12, 5, 12, 13, 9, 8, 11, 4, 9, 6, 7, 9, 10, 11, …\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics <chr> \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            <chr> \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       <chr> \"0.054587073624134064\", \"0.035217467695474625\", \"0.02…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %>%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\n\nYou will notice that I placed a blue line where I started my telegram channel @steveondata and a red line where I started this blog. So far, not bad, it looks like the telegram channel helped a little bit but writing on the blog seems to maybe been helping the most.\nLet’s look at a cumulative view of things.\n\nengagement_tbl %>%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %>%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %>%\n  slice(1:12) %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %>%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %>%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %>%\n  slice(1:12) %>%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-24/index.html",
    "href": "posts/rtip-2022-10-24/index.html",
    "title": "Cumulative Variance",
    "section": "",
    "text": "Introducton\nThis is going to be a simple example on how we can make a function in #base #r that will crate a cumulative variance function. From base R we are going to use seq_along(), stats::var(), and sapply() inside of the function we will call cvar for cumulative variance.\n\n\nGenerate Data\nThe first thing we need to do in order to showcase this function is to generate some data. Lets do that below:\n\nl <- list(\n  a = rnorm(50),\n  b = rnorm(50, 1),\n  c = rnorm(50, 2)\n)\n\nl\n\n$a\n [1] -0.96548479  0.49276394  0.14030455  1.11786377 -1.47239834 -0.06906506\n [7] -1.51133985  1.48910665  0.09444727 -0.01216806  0.35365683 -1.13562871\n[13] -1.27899694  0.10963391 -0.00708945 -1.26718573  0.92143855  0.09716551\n[19] -0.28025814 -0.18046616 -1.75919633  0.01686201 -0.10204673  0.91791398\n[25] -1.70503761  1.50856724 -1.29433294  0.42665133 -0.78176459  0.16141529\n[31]  1.42536506 -0.42168041 -0.30222269  0.05129043 -0.73717680 -1.60823604\n[37] -0.11921815  0.08357566  0.23250949  0.50846618 -0.02674088  0.12101223\n[43]  0.10390867 -1.11476987 -1.42201791 -1.35493159  0.35703193 -1.08176152\n[49] -0.08189606  0.46341303\n\n$b\n [1]  1.67636555  1.22588224  0.44445597  2.06992723  1.82473269 -0.03321279\n [7]  1.29568923 -0.29542080  1.46614555  0.51617492  2.03383464  0.13835453\n[13]  3.18982479 -0.38493278  0.67450796  1.69715532  1.19963387  1.17294403\n[19]  0.83585415  1.49308994  0.53831112  1.76345465  1.80154859  0.47358491\n[25]  1.40422472  2.50254552 -0.07376997  0.38077031  1.13606122 -0.26052567\n[31]  0.88624336  1.89232197  1.37488657  2.53211686  1.77919794  3.42367520\n[37] -0.59175356 -0.04816522  2.08963807  1.40124074 -0.73135934  0.65282741\n[43]  0.87359580  0.14540086  1.52502012  0.52190806  2.29922084  0.62462975\n[49]  2.94462210  1.06173482\n\n$c\n [1]  1.01194711  1.36267530  1.37423091  1.14980487  1.39304340  3.26911528\n [7]  1.71184232  1.88096194  2.90461886  1.39510407  1.86157191  1.14906542\n[13]  1.90072693  1.78998258  1.61307934  0.76604158  2.92366827  2.32424523\n[19]  2.94645235  2.73102591  0.87949048  3.31239943  1.05720691  1.42571354\n[25]  1.79266828  1.84627335  0.81364549  0.25976918  1.48698512  1.10254109\n[31]  1.60219278  1.84545465  1.93508206  2.13570750  2.32733075  2.53404107\n[37]  1.25864169  3.28238628  1.98998276  1.44299079  2.26296491  3.86667748\n[43]  1.84651988  3.24765507  0.18464631 -0.01404234  2.78432762 -0.05193538\n[49]  0.35160392  2.58212054\n\n\n\n\nMake Function\nNow that we have our data, lets make the function:\n\ncvar <- function(.x){\n  sapply(seq_along(.x), function(k, z) stats::var(z[1:k]), z = .x)\n}\n\nOk, now that we have our function, lets take a look at it in use.\n\n\nUse Function\n\nsapply(l, cvar)\n\n              a         b          c\n [1,]        NA        NA         NA\n [2,] 1.0632447 0.1014676 0.06150513\n [3,] 0.5789145 0.3885272 0.04239889\n [4,] 0.7633500 0.4867186 0.03075658\n [5,] 1.1294646 0.4093271 0.02873772\n [6,] 0.9043498 0.6932616 0.69685950\n [7,] 1.0277904 0.5789892 0.58271798\n [8,] 1.2918409 0.7813852 0.50862439\n [9,] 1.1344452 0.7052323 0.62156290\n[10,] 1.0088029 0.6580963 0.56764373\n[11,] 0.9242084 0.6858993 0.51210764\n[12,] 0.9418512 0.7024341 0.49623990\n[13,] 0.9661294 1.0026509 0.45782344\n[14,] 0.8992042 1.1041314 0.42295247\n[15,] 0.8371837 1.0364119 0.39358167\n[16,] 0.8556585 0.9929979 0.42396425\n[17,] 0.8822275 0.9315646 0.49164277\n[18,] 0.8344918 0.8770439 0.48215682\n[19,] 0.7888762 0.8321667 0.52875406\n[20,] 0.7473648 0.7964123 0.54171586\n[21,] 0.8305355 0.7722667 0.56162921\n[22,] 0.7940780 0.7564316 0.63535849\n[23,] 0.7587189 0.7425071 0.63686713\n[24,] 0.7802954 0.7290301 0.61692337\n[25,] 0.8409644 0.7019443 0.59130379\n[26,] 0.9248957 0.7464413 0.56765490\n[27,] 0.9359290 0.7761117 0.58464117\n[28,] 0.9159283 0.7676951 0.64765859\n[29,] 0.8952420 0.7403040 0.62681485\n[30,] 0.8690093 0.7773175 0.61856072\n[31,] 0.9251735 0.7524213 0.59834912\n[32,] 0.8976913 0.7499102 0.57961326\n[33,] 0.8702922 0.7290409 0.56296663\n[34,] 0.8452302 0.7678835 0.55094641\n[35,] 0.8301013 0.7571526 0.54480209\n[36,] 0.8638223 0.8786798 0.54627250\n[37,] 0.8400510 0.9426489 0.53823917\n[38,] 0.8195803 0.9560736 0.58478212\n[39,] 0.8028106 0.9542482 0.57032971\n[40,] 0.7943867 0.9312335 0.55895977\n[41,] 0.7750385 0.9957722 0.55033290\n[42,] 0.7581242 0.9766789 0.63799874\n[43,] 0.7417073 0.9547108 0.62281006\n[44,] 0.7453949 0.9533619 0.65240410\n[45,] 0.7629119 0.9360654 0.70195202\n[46,] 0.7747320 0.9223139 0.76179573\n[47,] 0.7652088 0.9339432 0.76550157\n[48,] 0.7645076 0.9188787 0.82293002\n[49,] 0.7490587 0.9695572 0.84800554\n[50,] 0.7434405 0.9498710 0.84419808\n\nlapply(l, cvar)\n\n$a\n [1]        NA 1.0632447 0.5789145 0.7633500 1.1294646 0.9043498 1.0277904\n [8] 1.2918409 1.1344452 1.0088029 0.9242084 0.9418512 0.9661294 0.8992042\n[15] 0.8371837 0.8556585 0.8822275 0.8344918 0.7888762 0.7473648 0.8305355\n[22] 0.7940780 0.7587189 0.7802954 0.8409644 0.9248957 0.9359290 0.9159283\n[29] 0.8952420 0.8690093 0.9251735 0.8976913 0.8702922 0.8452302 0.8301013\n[36] 0.8638223 0.8400510 0.8195803 0.8028106 0.7943867 0.7750385 0.7581242\n[43] 0.7417073 0.7453949 0.7629119 0.7747320 0.7652088 0.7645076 0.7490587\n[50] 0.7434405\n\n$b\n [1]        NA 0.1014676 0.3885272 0.4867186 0.4093271 0.6932616 0.5789892\n [8] 0.7813852 0.7052323 0.6580963 0.6858993 0.7024341 1.0026509 1.1041314\n[15] 1.0364119 0.9929979 0.9315646 0.8770439 0.8321667 0.7964123 0.7722667\n[22] 0.7564316 0.7425071 0.7290301 0.7019443 0.7464413 0.7761117 0.7676951\n[29] 0.7403040 0.7773175 0.7524213 0.7499102 0.7290409 0.7678835 0.7571526\n[36] 0.8786798 0.9426489 0.9560736 0.9542482 0.9312335 0.9957722 0.9766789\n[43] 0.9547108 0.9533619 0.9360654 0.9223139 0.9339432 0.9188787 0.9695572\n[50] 0.9498710\n\n$c\n [1]         NA 0.06150513 0.04239889 0.03075658 0.02873772 0.69685950\n [7] 0.58271798 0.50862439 0.62156290 0.56764373 0.51210764 0.49623990\n[13] 0.45782344 0.42295247 0.39358167 0.42396425 0.49164277 0.48215682\n[19] 0.52875406 0.54171586 0.56162921 0.63535849 0.63686713 0.61692337\n[25] 0.59130379 0.56765490 0.58464117 0.64765859 0.62681485 0.61856072\n[31] 0.59834912 0.57961326 0.56296663 0.55094641 0.54480209 0.54627250\n[37] 0.53823917 0.58478212 0.57032971 0.55895977 0.55033290 0.63799874\n[43] 0.62281006 0.65240410 0.70195202 0.76179573 0.76550157 0.82293002\n[49] 0.84800554 0.84419808\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-26/index.html",
    "href": "posts/rtip-2022-10-26/index.html",
    "title": "Control Charts in healthyR.ai",
    "section": "",
    "text": "Sometimes you may be working with a time series or some process data and you will want to make a control chart. This is simple to do with the {healthyR.ai} package.\nIf you do not already have it, then you can follow the simple code below to get the latest version.\n\n\nYou can install the released version of healthyR.ai from CRAN with:\n\ninstall.packages(\"healthyR.ai\")\n\nAnd the development version from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/healthyR.ai\")\n\nNow that we have the latest version installed, lets get some data and then use the function."
  },
  {
    "objectID": "posts/rtip-2022-10-31/index.html",
    "href": "posts/rtip-2022-10-31/index.html",
    "title": "Cumulative Skewness",
    "section": "",
    "text": "Function\nIn this post we will make a function cum_skewness() that will generate a vector output of the cumulative skewness of some given vector. The full function call is simply:\n\ncum_skewness(.x)\n\nIt only takes in a numeric vector, we are not going to write type checks in the function as it won’t be necessary for this post.\n\ncum_skewness <- function(.x){\n  skewness <- function(.x){\n    sqrt(length(.x)) * sum((.x - mean(.x))^3 / (sum((.x))^2)^(3/2))\n  }\n  sapply(seq_along(.x), function(k, z) skewness(z[1:k]), z = .x)\n}\n\n\n\nData\nWe are going to use the mtcars data set and use the mpg column for this example. Let’s set x equal to mtcars$mpg\n\nx <- mtcars$mpg\n\n\n\nExample\nNow let’s see the function in use.\n\ncum_skewness(x)\n\n [1]  0.000000e+00  0.000000e+00  8.249747e-06  5.049149e-06 -1.113787e-05\n [6] -8.569220e-06 -1.134377e-04 -8.440629e-05 -8.280585e-05 -5.457236e-05\n[11] -3.209937e-05 -1.758922e-05 -5.567456e-06  1.436318e-07 -6.299325e-05\n[16] -8.605705e-05 -5.869380e-05  1.594511e-04  1.675837e-04  2.221143e-04\n[21]  1.855217e-04  1.936299e-04  1.998527e-04  2.082240e-04  1.897575e-04\n[26]  1.505425e-04  1.180971e-04  9.974055e-05  1.048461e-04  9.801797e-05\n[31]  1.024713e-04  9.107160e-05\n\n\nLet’s plot it out.\n\nplot(cum_skewness(x), type = \"l\")"
  },
  {
    "objectID": "posts/rtip-2022-11-07/index.html",
    "href": "posts/rtip-2022-11-07/index.html",
    "title": "Discrete Fourier Vec with healthyR.ai",
    "section": "",
    "text": "Introduction\nSometimes in modeling you may want to get a discrete 1/0 vector of a fourier transform of some input vector. With {healthyR.ai} we can do this easily.\n\n\nFunction\nHere is the full function call:\n\nhai_fourier_discrete_vec(\n  .x,\n  .period,\n  .order,\n  .scale_type = c(\"sin\", \"cos\", \"sincos\")\n)\n\nHere are the parameters to the function and what they expect:\n\n.x - A numeric vector\n.period - The number of observations that complete a cycle\n.order - The fourier term order\n.scale_type - A character of one of the following: sin,cos,sincos\n\nThe internal caluclation is straightforward:\n\nsin = sin(2 * pi * h * x), where h = .order/.period\ncos = cos(2 * pi * h * x), where h = .order/.period\nsincos = sin(2 * pi * h * x) * cos(2 * pi * h * x) where h = .order/.period\n\n\n\nExample\nLet’s work throught a quick and simple example.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(healthyR.ai)\nlibrary(tidyr)\n\nlen_out <- 24\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n  ),\n  a = rnorm(len_out, sd = 2),\n  fv_sin = hai_fourier_discrete_vec(a, 12, 1, \"sin\"),\n  fv_cos = hai_fourier_discrete_vec(a, 12, 1, \"cos\"),\n  fv_sc  = hai_fourier_discrete_vec(a, 12, 1, \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 24 × 5\n   date_col         a fv_sin fv_cos fv_sc\n   <date>       <dbl>  <dbl>  <dbl> <dbl>\n 1 2021-01-01 -0.486       0      1     0\n 2 2021-02-01 -0.708       0      1     0\n 3 2021-03-01 -0.119       0      1     0\n 4 2021-04-01  0.0405      1      1     1\n 5 2021-05-01  1.19        1      1     1\n 6 2021-06-01  1.88        1      1     1\n 7 2021-07-01 -1.32        0      1     0\n 8 2021-08-01 -0.0214      0      1     0\n 9 2021-09-01  2.80        1      1     1\n10 2021-10-01  1.67        1      1     1\n# … with 14 more rows\n\n\n\n\nVisual\nLet’s visualize.\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-08/index.html",
    "href": "posts/rtip-2022-11-08/index.html",
    "title": "Hyperbolic Transform with healthyR.ai",
    "section": "",
    "text": "Introduction\nIn data modeling there can be instanes where you will want some sort of hyperbolic transformation of your data. In {healthyR.ai} this is easy with the use of the function hai_hyperbolic_vec() along with it’s corresponding augment and step functions.\n\n\nFunction\nThe function takes in a numeric vector as it’s argument and will transform the data with one of the following:\n\nsin\ncos\ntan\nsincos This will do: value = sin(x) * cos(x)\n\nThe full function call is:\n\nhai_hyperbolic_vec(.x, .scale_type = c(\"sin\", \"cos\", \"tan\", \"sincos\"))\n\n\n\nExample\n\nlibrary(dplyr)\nlibrary(healthyR.ai)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nlen_out <- 25\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n    ),\n  b = runif(len_out),\n  fv_sin = hai_hyperbolic_vec(b, .scale_type = \"sin\"),\n  fv_cos = hai_hyperbolic_vec(b, .scale_type = \"cos\"),\n  fv_sc  = hai_hyperbolic_vec(b, .scale_type = \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 25 × 5\n   date_col        b fv_sin fv_cos  fv_sc\n   <date>      <dbl>  <dbl>  <dbl>  <dbl>\n 1 2021-01-01 0.961  0.820   0.573 0.470 \n 2 2021-02-01 0.418  0.406   0.914 0.371 \n 3 2021-03-01 0.0729 0.0728  0.997 0.0726\n 4 2021-04-01 0.426  0.413   0.911 0.376 \n 5 2021-05-01 0.851  0.752   0.659 0.496 \n 6 2021-06-01 0.824  0.734   0.679 0.499 \n 7 2021-07-01 0.659  0.612   0.791 0.484 \n 8 2021-08-01 0.683  0.631   0.776 0.490 \n 9 2021-09-01 0.173  0.172   0.985 0.169 \n10 2021-10-01 0.345  0.338   0.941 0.318 \n# … with 15 more rows\n\n\n\n\nVisual\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")"
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html",
    "href": "posts/rtip-2022-11-09/index.html",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "",
    "text": "K-Means is a clustering algorithm that can be used to find potential clusters in your data.\nThe algorithm does require that you look at different values of K in order to assess which is the optimal value.\nIn the R package {healthyR.ai} there is a utility to do this."
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html#parameters",
    "href": "posts/rtip-2022-11-09/index.html#parameters",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "Parameters",
    "text": "Parameters\nThe parameters take the following arguments:\n\n.data - This is the data that should be an output of the hai_user_item_tbl() or it’s synonym, or should at least be in the user item matrix format.\n.centers - The maximum amount of centers you want to map to the k-means function. The default is 15."
  },
  {
    "objectID": "posts/rtip-2022-11-10/index.html",
    "href": "posts/rtip-2022-11-10/index.html",
    "title": "Reading Multiple Files with {purrr}",
    "section": "",
    "text": "Introduction\nThere may be times when you have multiple structured files in the same folder, maybe they are .csv files. For this short tip, we will say that they are.\nI will show the short script and then discuss it.\n\n# Library Load ----\nlibrary(dplyr)\nlibrary(purrr)\n\n# Set file path ----\nfolder    <- \"FileFolder\"\npath      <- \"C:/Some/Root/Path/\"\nfull_path <- paste0(path,folder,\"/\")\n\n# File List ----\nfile_list <- dir(full_path\n                 , pattern = \"\\\\.csv$\"\n                 , full.names = T)\n\n# Read Files ----\nfiles <- file_list %>%\n  map(read.csv) %>%\n  map(as_tibble)\n\n# Clean File Names ----\nfile_names <- file_list %>%\n  str_remove(full_path) %>%\n  str_replace(\n    pattern = \"_OldStuff.csv\", \n    replacement = \"_NewStuff.csv\"\n  )\n\nnames(files) <- file_names\n\nWe load in {dplyr} for the pipe and the as_tibble function. After this we set out to create the file path. I have chosen to do this in two separate pieces as I have had experience with needing to go through different folders in the same root directory. While this could further be scripted I leave it as is.\nfolder is the folder that has the files of interest, in this case the .csv files. We then get the root path to that folder but not including it, this is defined as path in the above. After we have both folder and path we can create the full_path by using paste0\nNow after this we use the base R function of dir to list out all of the files that fit the specific format of .csv with a regex pattern. I always want the name of the file as it allows me to go back to the file later and lets me name the files in the upcoming list later on.\nSince these are .csv files I use purrr::map and then read.csv to read in all of the .csv files in the list that was created, we then used map again and this time used as_tibble to make sure that each file is a tibble and not something else like data.frame\nSince I provided the argument of T to dir, full.names I can then get a character vector of the names of the files which then is applied to the file list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-14/index.html",
    "href": "posts/rtip-2022-11-14/index.html",
    "title": "Find Skewed Features with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes we may want to quickly find skewed features in a data set. This is easily achiveable using the {healthyR.ai} library. There is a simple function called hai_skewed_features(). We are going to go over this function today.\n\n\nFunction\nLet’s first take a look at the function call.\n\nhai_skewed_features(\n  .data, \n  .threshold = 0.6, \n  .drop_keys = NULL\n  )\n\nNow let’s take a look at the arguments that go to the parameters of the function.\n\n.data - The data.frame/tibble you are passing in.\n.threshold - A level of skewness that indicates where you feel a column should be considered skewed.\n.drop_keys - A c() character vector of columns you do not want passed to the function.\n\n\n\nExample\nHere are a couple of examples.\n\nlibrary(healthyR.ai)\n\nhai_skewed_features(mtcars)\n\n[1] \"mpg\"  \"hp\"   \"carb\"\n\nhai_skewed_features(mtcars, .drop_keys = \"hp\")\n\n[1] \"mpg\"  \"carb\""
  },
  {
    "objectID": "posts/rtip-2022-11-15/index.html",
    "href": "posts/rtip-2022-11-15/index.html",
    "title": "Auto Prep data for XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes we may want to quickly format some data in order to just pass it through some algorithm just to see what happens, how crazy are things, just to get an idea of what may lie ahead…a lot of prep.\nWith my r package {healthyR.ai} there is a set of prepper functions that will automatically do a ‘best effort’ to format you data to be used in the algorithm you choose (should it be supported).\nToday we will talk about [hai_xgboost_data_prepper()\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\nNow let’s go over the arguments that are passed to the function.\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\n\n\nExample\nLet’s go over some examples.\n\nlibrary(ggplot2)\nlibrary(healthyR.ai)\n\n# Regression\nhai_xgboost_data_prepper(.data = diamonds, .recipe_formula = price ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nreg_obj <- hai_xgboost_data_prepper(diamonds, price ~ .)\nget_juiced_data(reg_obj)\n\n# A tibble: 53,940 × 27\n   carat depth table     x     y     z price  cut_1  cut_2  cut_3  cut_4   cut_5\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1  0.23  61.5    55  3.95  3.98  2.43   326  0.359 -0.109 -0.522 -0.567 -0.315 \n 2  0.21  59.8    61  3.89  3.84  2.31   326  0.120 -0.436 -0.298  0.378  0.630 \n 3  0.23  56.9    65  4.05  4.07  2.31   327 -0.359 -0.109  0.522 -0.567  0.315 \n 4  0.29  62.4    58  4.2   4.23  2.63   334  0.120 -0.436 -0.298  0.378  0.630 \n 5  0.31  63.3    58  4.34  4.35  2.75   335 -0.359 -0.109  0.522 -0.567  0.315 \n 6  0.24  62.8    57  3.94  3.96  2.48   336 -0.120 -0.436  0.298  0.378 -0.630 \n 7  0.24  62.3    57  3.95  3.98  2.47   336 -0.120 -0.436  0.298  0.378 -0.630 \n 8  0.26  61.9    55  4.07  4.11  2.53   337 -0.120 -0.436  0.298  0.378 -0.630 \n 9  0.22  65.1    61  3.87  3.78  2.49   337 -0.598  0.546 -0.373  0.189 -0.0630\n10  0.23  59.4    61  4     4.05  2.39   338 -0.120 -0.436  0.298  0.378 -0.630 \n# … with 53,930 more rows, and 15 more variables: color_1 <dbl>, color_2 <dbl>,\n#   color_3 <dbl>, color_4 <dbl>, color_5 <dbl>, color_6 <dbl>, color_7 <dbl>,\n#   clarity_1 <dbl>, clarity_2 <dbl>, clarity_3 <dbl>, clarity_4 <dbl>,\n#   clarity_5 <dbl>, clarity_6 <dbl>, clarity_7 <dbl>, clarity_8 <dbl>\n\n# Classification\nhai_xgboost_data_prepper(Titanic, Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\ncla_obj <- hai_xgboost_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(cla_obj)\n\n# A tibble: 32 × 7\n       n Survived Class_X2nd Class_X3rd Class_Crew Sex_Male Age_Child\n   <dbl> <fct>         <dbl>      <dbl>      <dbl>    <dbl>     <dbl>\n 1     0 No                0          0          0        1         1\n 2     0 No                1          0          0        1         1\n 3    35 No                0          1          0        1         1\n 4     0 No                0          0          1        1         1\n 5     0 No                0          0          0        0         1\n 6     0 No                1          0          0        0         1\n 7    17 No                0          1          0        0         1\n 8     0 No                0          0          1        0         1\n 9   118 No                0          0          0        1         0\n10   154 No                1          0          0        1         0\n# … with 22 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-16/index.html",
    "href": "posts/rtip-2022-11-16/index.html",
    "title": "Cumulative Harmonic Mean with {TidyDensity}",
    "section": "",
    "text": "Introduction\nThere can be times in which you may want to see a cumulative statistic, maybe in this particular case it is the harmonic mean. Well with the {TidyDensity} it is possible with a function called chmean()\nLet’s take a look at the function.\n\n\nFunction\nHere is the function call, it is very simple as it is a vectorized function.\n\nchmean(.x)\n\nThe only argument you provide to this function is a numeric vector. Let’s take a quick look at the construction of the function.\n\nchmean <- function(.x) {\n  1 / (cumsum(1 / .x))\n}\n\n\n\nExamples\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\nx <- mtcars$mpg\n\nchmean(x)\n\n [1] 21.0000000 10.5000000  7.1891892  5.3813575  4.1788087  3.3949947\n [7]  2.7436247  2.4663044  2.2255626  1.9943841  1.7934398  1.6166494\n[13]  1.4784877  1.3474251  1.1928760  1.0701322  0.9975150  0.9677213\n[19]  0.9378663  0.9126181  0.8754572  0.8286539  0.7858140  0.7419753\n[25]  0.7143688  0.6961523  0.6779989  0.6632076  0.6364908  0.6165699\n[31]  0.5922267  0.5762786\n\nmtcars %>%\n  select(mpg) %>%\n  mutate(cum_har_mean = chmean(mpg)) %>%\n  head(10)\n\n                   mpg cum_har_mean\nMazda RX4         21.0    21.000000\nMazda RX4 Wag     21.0    10.500000\nDatsun 710        22.8     7.189189\nHornet 4 Drive    21.4     5.381358\nHornet Sportabout 18.7     4.178809\nValiant           18.1     3.394995\nDuster 360        14.3     2.743625\nMerc 240D         24.4     2.466304\nMerc 230          22.8     2.225563\nMerc 280          19.2     1.994384\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-17/index.html",
    "href": "posts/rtip-2022-11-17/index.html",
    "title": "Bootstrap Modeling with {purrr} and {modler}",
    "section": "",
    "text": "Introduction\nMany times in modeling we want to get the uncertainty in the model, well, bootstrapping to the rescue!\nI am going to go over a very simple example on how to use purrr and modelr for this situation. We will use the mtcars dataset.\n\n\nFunctions\nThe main functions that we are going to showcase are purrr::map() and modelr::bootstrap()\n\n\nExamples\nLet’s get right into it.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndf <- mtcars\n\nfit_boots <- df %>% \n  modelr::bootstrap(n = 200, id = 'boot_num') %>%\n  group_by(boot_num) %>%\n  mutate(fit = map(strap, ~lm(mpg ~ ., data = data.frame(.))))\n\nfit_boots\n\n# A tibble: 200 × 3\n# Groups:   boot_num [200]\n   strap                boot_num fit   \n   <list>               <chr>    <list>\n 1 <resample [32 x 11]> 001      <lm>  \n 2 <resample [32 x 11]> 002      <lm>  \n 3 <resample [32 x 11]> 003      <lm>  \n 4 <resample [32 x 11]> 004      <lm>  \n 5 <resample [32 x 11]> 005      <lm>  \n 6 <resample [32 x 11]> 006      <lm>  \n 7 <resample [32 x 11]> 007      <lm>  \n 8 <resample [32 x 11]> 008      <lm>  \n 9 <resample [32 x 11]> 009      <lm>  \n10 <resample [32 x 11]> 010      <lm>  \n# … with 190 more rows\n\n\nNow lets get our parameter estimates.\n\n# get parameters ####\nparams_boot <- fit_boots %>%\n  mutate(tidy_fit = map(fit, tidy)) %>%\n  unnest(cols = tidy_fit) %>%\n  ungroup()\n\n# get predictions\npreds_boot <- fit_boots %>%\n  mutate(augment_fit = map(fit, augment)) %>%\n  unnest(cols = augment_fit) %>%\n  ungroup()\n\nTime to visualize.\n\nlibrary(patchwork)\n\n# plot distribution of estimated parameters\np1 <- ggplot(params_boot, aes(estimate)) +\n  geom_histogram(col = 'black', fill = 'white') +\n  facet_wrap(~ term, scales = 'free') +\n  theme_minimal()\n\n# plot points with predictions\np2 <- ggplot() +\n  geom_line(aes(mpg, .fitted, group = boot_num), preds_boot, alpha = .03) +\n  geom_point(aes(mpg, .fitted), preds_boot, col = 'steelblue', alpha = 0.05) +\n  theme_minimal()\n  \n# plot both\np1 + p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-21/index.html",
    "href": "posts/rtip-2022-11-21/index.html",
    "title": "Bootstrap Modeling with Base R",
    "section": "",
    "text": "Introduction\nI have previously written about bootstrap modeling with {purrr} and {modelr} here. What if you would like to do some simple bootstrap modeling without importing a library? This itself is easy too!\n\n\nExample\nWe will be using a very simple for loop to accomplish this. You will find an excellent post on this on Stats StackExchange from Francisco Jos Goerlich Gisbert\n\nn    <- 2000\ndf   <- mtcars\npred <- numeric(0)\n\nlibrary(tictoc) # for timing\n\ntic()\nset.seed(123)\nfor (i in 1:n){\n  boot    <- sample(nrow(df), n, replace = TRUE)\n  fit     <- lm(mpg ~ wt, data = df[boot,])\n  pred[i] <- predict(fit, newdata = df[boot,]) +\n    sample(resid(fit), size = 1)\n}\ntoc()\n\n6.8 sec elapsed\n\n\nSo we can see that the process ran pretty quickly and the loop itself is not a very difficult one. Let’s explain a little.\nSo the boot object is a sampling of df which in this case is the mtcars data set. We took a sample with replacement from this data set. We took 2000 samples and did this 2000 times.\nNext we made the fit object by fitting a simple linear model to the data where mpg is a function of wt. Once this is done, we made out predictions.\nThat’s it!"
  },
  {
    "objectID": "posts/rtip-2022-11-22/index.html",
    "href": "posts/rtip-2022-11-22/index.html",
    "title": "Data Preprocessing Scale/Normalize with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nA large portion of data modeling occurrs not only in the data cleaning phase but also in the data preprocessing phase. This can include things like scaling or normalizing data before proceeding to the modeling phase. I will discuss one such function from my r package {healthyR.ai}. In this post I will go over hai_data_scale()\nThis is a {recipes} style step function and is tidymodels compliant.\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_data_scale(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"center\",\n  .range_min = 0,\n  .range_max = 1,\n  .scale_factor = 1\n)\n\nNow let’s go over the arguments that get supplied to the parameters of this function.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“center”\n“normalize”\n“range”\n“scale”\n\nrange_min - A single numeric value for the smallest value in the range. This defaults to 0.\n.range_max - A single numeric value for the largeest value in the range. This defaults to 1.\n.scale_factor - A numeric value of either 1 or 2 that scales the numeric inputs by one or two standard deviations. By dividing by two standard deviations, the coefficients attached to continuous predictors can be interpreted the same way as with binary inputs. Defaults to 1.\n\n\n\nExample\nNow let’s see it in action!\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndate_seq <- seq.Date(\n  from = as.Date(\"2013-01-01\"), \n  length.out = 100, \n  by = \"month\"\n)\n\nval_seq <- rep(rnorm(10, mean = 6, sd = 2), times = 10)\n\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col   value\n   <date>     <dbl>\n 1 2013-01-01  6.66\n 2 2013-02-01  6.66\n 3 2013-03-01  5.09\n 4 2013-04-01  6.94\n 5 2013-05-01  5.96\n 6 2013-06-01  6.18\n 7 2013-07-01  3.62\n 8 2013-08-01  7.31\n 9 2013-09-01  4.58\n10 2013-10-01  7.29\n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nnew_rec_obj <- hai_data_scale(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_scale = \"center\"\n)$scale_rec_obj\n\nnew_rec_obj %>% \n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.633 \n 2 2013-02-01  0.630 \n 3 2013-03-01 -0.935 \n 4 2013-04-01  0.909 \n 5 2013-05-01 -0.0676\n 6 2013-06-01  0.149 \n 7 2013-07-01 -2.41  \n 8 2013-08-01  1.28  \n 9 2013-09-01 -1.45  \n10 2013-10-01  1.26  \n# … with 90 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-28/index.html",
    "href": "posts/rtip-2022-11-28/index.html",
    "title": "Default Metric Sets with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nWhen modeling it is always good to understand your model performance against some metric The {tidymodels} package {yardstick} is a great resource for this.\nIn my R package {healthyR.ai} there are two functions that allow you to either minimize or maximize some cost function against your modeling problem.\nThese functions are: * hai_default_regression_metric_set() * hai_default_classification_metric_set()\n\n\nFunction\nThe functions themselves are {yardstick} metric set functions. Let’s take a look at them.\n\nlibrary(healthyR.ai)\n\nhai_default_classification_metric_set()\n\n# A tibble: 11 × 3\n   metric       class        direction\n   <chr>        <chr>        <chr>    \n 1 sensitivity  class_metric maximize \n 2 specificity  class_metric maximize \n 3 recall       class_metric maximize \n 4 precision    class_metric maximize \n 5 mcc          class_metric maximize \n 6 accuracy     class_metric maximize \n 7 f_meas       class_metric maximize \n 8 kap          class_metric maximize \n 9 ppv          class_metric maximize \n10 npv          class_metric maximize \n11 bal_accuracy class_metric maximize \n\nhai_default_regression_metric_set()\n\n# A tibble: 6 × 3\n  metric class          direction\n  <chr>  <chr>          <chr>    \n1 mae    numeric_metric minimize \n2 mape   numeric_metric minimize \n3 mase   numeric_metric minimize \n4 smape  numeric_metric minimize \n5 rmse   numeric_metric minimize \n6 rsq    numeric_metric maximize \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-29/index.html",
    "href": "posts/rtip-2022-11-29/index.html",
    "title": "Working with Lists",
    "section": "",
    "text": "Introduction\nIn R there are many times where we will work with lists. I won’t go into why lists are great or really the structure of a list but rather simply working with them.\n\n\nExample\nFirst let’s make a list.\n\nl <- list(\n  letters,\n  1:26,\n  rnorm(26)\n)\n\nl\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26\n\n[[3]]\n [1] -1.5647537840 -1.3080486753  1.3331315389 -0.5490502644 -0.4467608750\n [6] -1.5876952894  0.2292049732 -0.2885449316  1.4614499298 -0.0864987690\n[11]  0.5686850031 -0.3897819578  0.1776603862 -1.1326372302 -1.8651290164\n[16]  1.2676006036  0.2405115523 -1.0506728047  1.4069277686 -1.0125778892\n[21] -0.7687818102 -0.1325350681  0.3639485041  0.0005700058 -1.0698214370\n[26]  1.1972767040\n\n\nNow let’s look at somethings we can do with lists. First, let’s see if we can get the class of each item in the list. We are going to use lapply() for this.\n\nlapply(l, class)\n\n[[1]]\n[1] \"character\"\n\n[[2]]\n[1] \"integer\"\n\n[[3]]\n[1] \"numeric\"\n\n\nNow, let’s perform some simple operations on each item of the list.\n\nlapply(l, length)\n\n[[1]]\n[1] 26\n\n[[2]]\n[1] 26\n\n[[3]]\n[1] 26\n\ntry(lapply(l, sum))\n\nError in FUN(X[[i]], ...) : invalid 'type' (character) of argument\n\n\nOk so we see taking the sum of the first element of the list in lapply() did not work because of a class type mismatch. Let’s see how we can get around this an only apply the sum function to a numeric type. To do this we can rely on {purrr} by using a function map_if()\n\nlibrary(purrr)\n\nmap_if(l, is.numeric, sum)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 351\n\n[[3]]\n[1] -5.006323\n\n\n\nmap_if(l, is.numeric, mean)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 13.5\n\n[[3]]\n[1] -0.1925509\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-30/index.html",
    "href": "posts/rtip-2022-11-30/index.html",
    "title": "Generate Random Walk Data with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nGenerating random walk data for timesieries analysis does not have to be difficult, and in fact is not. It can be generated for multiple simulations and have a tidy output. How? ts_random_walk() from the {healthyR.ts} package. Let’s take a look at the function.\n\n\nFunction\nHere is the full function call.\n\nts_random_walk(\n  .mean = 0,\n  .sd = 0.1,\n  .num_walks = 100,\n  .periods = 100,\n  .initial_value = 1000\n)\n\nNow let’s look at the arguments to the parameters.\n\n.mean - The desired mean of the random walks\n.sd - The standard deviation of the random walks\n.num_walks - The number of random walks you want generated\n.periods - The length of the random walk(s) you want generated\n.initial_value - The initial value where the random walks should start\n\nThe underlying data of this function is generated by rnorm()\n\n\nExample\nLet’s take a look at an example and see some visuals.\n\nlibrary(healthyR.ts)\nlibrary(ggplot2)\n\ndf <- ts_random_walk(.num_walks = 100)\n\ndf\n\n# A tibble: 10,000 × 4\n     run     x        y cum_y\n   <dbl> <dbl>    <dbl> <dbl>\n 1     1     1 -0.144    856.\n 2     1     2  0.00648  862.\n 3     1     3  0.0726   924.\n 4     1     4 -0.152    784.\n 5     1     5  0.0228   802.\n 6     1     6 -0.0455   765.\n 7     1     7  0.0972   840.\n 8     1     8 -0.234    643.\n 9     1     9 -0.0501   611.\n10     1    10 -0.0358   589.\n# … with 9,990 more rows\n\n\nThere are attributes attached to the output of this function, let’s see what they are.\n\natb <- attributes(df)\n\nnames_to_print <- names(atb)[which(names(atb) != \"row.names\")]\n\natb[names_to_print]\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$names\n[1] \"run\"   \"x\"     \"y\"     \"cum_y\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 0.1\n\n$.num_walks\n[1] 100\n\n$.periods\n[1] 100\n\n$.initial_value\n[1] 1000\n\n\nNow lets visualize.\n\ndf %>%\n   ggplot(\n       mapping = aes(\n           x = x\n           , y = cum_y\n           , color = factor(run)\n           , group = factor(run)\n        )\n    ) +\n    geom_line(alpha = 0.8) +\n    ts_random_walk_ggplot_layers(df)\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-01/index.html",
    "href": "posts/rtip-2022-12-01/index.html",
    "title": "Extract Boilerplate Workflow Metrics with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nWhen working with the {tidymodels} framework there are ways to pull model metrics from a workflow, since {healthyR.ai} is built on and around the {tidyverse} and {tidymodels} we can do the same. This post will focus on the function hai_auto_wflw_metrics()\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_auto_wflw_metrics(.data)\n\nThe only parameter is .data and this is strictly the output object of one of the hai_auto_ boiler plate functions\n\n\nExample\nSince this function requires the input from an hai_auto function, we will walk through an example with the iris data set. We are going to use the hai_auto_knn() to classify the Species.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_knn_data_prepper(data, Species ~ .)\n\nauto_knn <- hai_auto_knn(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\",\n  .grid_size = 2,\n  .num_cores = 4\n)\n\nhai_auto_wflw_metrics(auto_knn)\n\n# A tibble: 22 × 9\n   neighbors weight_func dist_power .metric  .esti…¹  mean     n std_err .config\n       <int> <chr>            <dbl> <chr>    <chr>   <dbl> <int>   <dbl> <chr>  \n 1         8 rank             0.888 accuracy multic… 0.95     25 0.00652 Prepro…\n 2         8 rank             0.888 bal_acc… macro   0.962    25 0.00471 Prepro…\n 3         8 rank             0.888 f_meas   macro   0.947    25 0.00649 Prepro…\n 4         8 rank             0.888 kap      multic… 0.922    25 0.0102  Prepro…\n 5         8 rank             0.888 mcc      multic… 0.925    25 0.00964 Prepro…\n 6         8 rank             0.888 npv      macro   0.975    25 0.00351 Prepro…\n 7         8 rank             0.888 ppv      macro   0.949    25 0.00663 Prepro…\n 8         8 rank             0.888 precisi… macro   0.949    25 0.00663 Prepro…\n 9         8 rank             0.888 recall   macro   0.949    25 0.00633 Prepro…\n10         8 rank             0.888 sensiti… macro   0.949    25 0.00633 Prepro…\n# … with 12 more rows, and abbreviated variable name ¹​.estimator\n\n\nAs we see this pulls out the full metric table from the workflow.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-05/index.html",
    "href": "posts/rtip-2022-12-05/index.html",
    "title": "Naming Items in a List with {purrr}, {dplyr}, or {healthyR}",
    "section": "",
    "text": "Introduction\nMany times when we are working with a data set we will want to break it up into groups and place them into a list and work with them in that fashion. With this it can be useful to the elements of the list named by the column that the data was split upon. Let’s use the iris set as an example where we split on Species.\nThere are two main functions that we will use in this scenario, namely purrr:map() and dplyr::group_split(), you could also use the split function from base r for this.\nWe will also go over how simple this is using the {healthyR} package. Let’s look at the function from {healthyR}\n\n\nFunction\nFull function call.\n\nnamed_item_list(.data, .group_col)\n\nThere are only two arguments to supply.\n\n.data - The data.frame/tibble.\n.group_col - The column that contains the groupings.\n\nThat’s it.\n\n\nExamples\nLet’s jump into it.\n\nlibrary(purrr)\nlibrary(dplyr)\n\ndata_tbl <- iris\n\ndata_tbl_list <- data_tbl %>%\n  group_split(Species)\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n[[1]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n[[2]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n[[3]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\ndata_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\n[[1]]\n[1] \"setosa\"\n\n[[2]]\n[1] \"versicolor\"\n\n[[3]]\n[1] \"virginica\"\n\n\nNow lets go ahead and apply the names.\n\nnames(data_tbl_list) <- data_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nLet’s now see how we do this in {healthyR}\n\nlibrary(healthyR)\n\nnamed_item_list(iris, Species)\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nIf you use this in conjunction with the healthyR function save_to_excel() then it will write an excel file with a tab for each named item in the list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-06/index.html",
    "href": "posts/rtip-2022-12-06/index.html",
    "title": "Z-Score Scaling Step Recipe with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes one may find it useful or necessary to scale their data during a modeling or analysis phase. One of these such transformations is the z-score scaling.\nThis is done simply by performing the below transform where x is simply some numeric vector:\n\\[ z_x = (x - mu(x))/sd(x) \\]\nLet’s take a look at the recipe function called step_hai_scale_zscore\n\n\nFunction\nHere is the full function call:\n\nstep_hai_scale_zscore(\n  recipe,\n  ...,\n  role = \"predictor\",\n  trained = FALSE,\n  columns = NULL,\n  skip = FALSE,\n  id = rand_id(\"hai_scale_zscore\")\n)\n\nHere are the arguments to the function.\n\nrecipe - A recipe object. The step will be added to the sequence of operations for this recipe.\n... - One or more selector functions to choose which variables that will be used to create the new variables. The selected variables should have class numeric\nrole - For model terms created by this step, what analysis role should they be assigned?. By default, the function assumes that the new variable columns created by the original variables will be used as predictors in a model.\ntrained - A logical to indicate if the quantities for preprocessing have been estimated.\ncolumns - A character string of variables that will be used as inputs. This field is a placeholder and will be populated once recipes::prep() is used.\nskip - A logical. Should the step be skipped when the recipe is baked by bake.recipe()? While all operations are baked when prep.recipe() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations.\nid - A character string that is unique to this step to identify it.\n\n\n\nExample\nHere is a simple example.\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndf <- iris |>\n  as_tibble() |>\n  select(Species, Sepal.Length)\n\nrec_obj <- recipe(Sepal.Length ~ ., data = df) %>%\n  step_hai_scale_zscore(Sepal.Length)\n\nrec_obj\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nZero-One Scale Transformation on Sepal.Length\n\nsummary(rec_obj)\n\n# A tibble: 2 × 4\n  variable     type      role      source  \n  <chr>        <list>    <chr>     <chr>   \n1 Species      <chr [3]> predictor original\n2 Sepal.Length <chr [2]> outcome   original\n\n\nNow let’s take a look at the differences.\n\nlibrary(ggplot2)\nlibrary(plotly)\n\ndf_tbl <- get_juiced_data(rec_obj)\n\ndf_tbl |>\n  purrr::set_names(\"Species\",\"Sepal_Length\",\"Scaled_Sepal_Length\") |>\n  ggplot(aes(x = Sepal_Length)) +\n  geom_histogram(color = \"black\", fill = \"lightgreen\") +\n  geom_histogram(aes(x = Scaled_Sepal_Length), \n                 color = \"black\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    y = \"Count\",\n    x = \"Sepal Length\",\n    title = \"Speal.Length: Original vs. Z-Score Scaled\",\n    subtitle = \"Original (Light Green) Scaled (Steelblue)\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-07/index.html",
    "href": "posts/rtip-2022-12-07/index.html",
    "title": "Create Multiple {parsnip} Model Specs with {purrr}",
    "section": "",
    "text": "Introduction\nIf you want to generate multiple parsnip model specifications at the same time then it’s really not to hard. This sort of thing is being addressed in an upcoming package of mine called {tidyaml}\nThis post is going to be quick and simple, I will showcase how you can generate many different model specifications in one go. I will also discuss the function create_model_spec() that will allow you to do this with a simple function call once the package is actually released.\n\n\nFunction\nHere is the function call for the create_model_spec() for once it is release.\n\ncreate_model_spec(\n  .parsnip_eng = list(\"lm\"),\n  .mode = list(\"regression\"),\n  .parsnip_fns = list(\"linear_reg\"),\n  .return_tibble = TRUE\n)\n\nHere are the arguments to the function. * .parsnip_eng - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c(‘lm’, ‘glm’) * .mode- The input must be a list. The default is ‘regression’ * .parsnip_fns - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(“linear_reg”,“cubist_rules”) * .return_tibble - The default is TRUE. FALSE will return a list object.\n\n\nExample\nHere is the function at work.\n\nlibrary(tidyaml)\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow that we have seen what is to come in the future, let’s take a look at a pseudo solution that is easy to replicate now.\n\n# Load the purrr package\nlibrary(purrr)\nlibrary(parsnip)\n\n# Create a list of parsnip engines\nengines <- list(\n  engine1 = \"lm\",\n  engine2 = \"glm\",\n  engine3 = \"randomForest\"\n)\n\n# Create a list of parsnip call names\nparsnip_calls <- list(\n  call1 = \"linear_reg\",\n  call2 = \"linear_reg\",\n  call3 = \"rand_forest\"\n)\n\n# Use pmap() to create a list of parsnip model specs from the list of engines\n# and parsnip call names\n# Set the mode argument to \"regression\"\nmodel_specs <- pmap(list(engines, parsnip_calls), function(engine, call) {\n  match.fun(call)(engine = engine, mode = \"regression\")\n})\n\n# Print the list of model specs to the console\nmodel_specs\n\n$engine1\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$engine2\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$engine3\nRandom Forest Model Specification (regression)\n\nComputational engine: randomForest \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-08/index.html",
    "href": "posts/rtip-2022-12-08/index.html",
    "title": "Create a Faceted Historgram Plot with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nOne of the most important steps in data analysis is visualizing the distribution of your data. This can help you identify patterns, outliers, and trends in your data, and can also provide valuable insights into the relationships between different variables.\nOne way to visualize data distributions is by using histograms. A histogram is a graphical representation of the distribution of a numeric variable. It shows the number of observations (or the frequency) within each bin or range of values.\nIn this blog post, we will showcase the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create faceted histograms of numeric and factor data in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nhai_histogram_facet_plot(\n  .data,\n  .bins = 10,\n  .scale_data = FALSE,\n  .ncol = 5,\n  .fct_reorder = FALSE,\n  .fct_rev = FALSE,\n  .fill = \"steelblue\",\n  .color = \"white\",\n  .scale = \"free\",\n  .interactive = FALSE\n)\n\nHere are the parameters and the arguments that get passed to them.\n\n.data - The data you want to pass to the function.\n.bins - The number of bins for the histograms.\n.scale_data - This is a boolean set to FALSE. TRUE will use hai_scale_zero_one_vec() to [0, 1] scale the data.\n.ncol - The number of columns for the facet_warp argument.\n.fct_reorder - Should the factor column be reordered? TRUE/FALSE, default of FALSE\n.fct_rev - Should the factor column be reversed? TRUE/FALSE, default of FALSE\n.fill - Default is steelblue\n.color - Default is ‘white’\n.scale - Default is ‘free’\n.interactive - Default is FALSE, TRUE will produce a {plotly} plot.\n\n\n\nExamples\nLet’s take a look at some example.\n\nlibrary(healthyR.ai, quietly = TRUE)\n\nhai_histogram_facet_plot(mtcars)\n\n\n\n\nNow lets scale the data and review.\n\nhai_histogram_facet_plot(mtcars, .scale_data = TRUE)\n\n\n\n\nLet’s take a look the iris data set now.\n\noutput <- hai_histogram_facet_plot(iris, .interactive = TRUE)\noutput$plot\n\n\n\n\n\nIn this blog post, we showcased the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create histogram plots and faceted histograms in R. The hai_histogram_facet_plot() function allows you to quickly and easily visualize the distribution of your data, and can provide valuable insights into the relationships between different variables\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-13/index.html",
    "href": "posts/rtip-2022-12-13/index.html",
    "title": "Mixture Distributions with {TidyDensity}",
    "section": "",
    "text": "Introduction\nA mixture distribution is a type of probability distribution that is created by combining two or more simpler distributions. This allows us to model complex data that may have multiple underlying patterns. For example, a mixture distribution could be used to model a dataset that includes both continuous and discrete variables.\nTo create a mixture distribution, we first need to specify the individual distributions that will be combined, as well as the weights that determine how much each distribution contributes to the overall mixture. Once we have these components, we can use them to calculate the probability of any given value occurring in the mixture distribution.\nMixture distributions can be useful in a variety of applications, such as data analysis and machine learning. In data analysis, they can be used to model data that is not well-described by a single distribution, and in machine learning, they can be used to improve the performance of predictive models. Overall, mixture distributions are a powerful tool for understanding and working with complex data.\n\n\nFunction\nLet’s take a look a function in {TidyDensity} that allows us to do this. At this moment, weights are not a parameter to the function.\n\ntidy_mixture_density(...)\n\nNow let’s take a look at the arguments that get supplied to the ... parameter.\n\n... - The random data you want to pass. Example rnorm(50,0,1) or something like tidy_normal(.mean = 5, .sd = 1)\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\n\noutput <- tidy_mixture_density(\n  rnorm(100, 0, 1), \n  tidy_normal(.mean = 5, .sd = 1)\n)\n\nAs you can see, you can enter a function that outputs a numeric vector or you can use a {TidyDensity} distribution function.\nLet’s take a look at the outputs.\n\noutput$data\n\n$dist_tbl\n# A tibble: 150 × 2\n       x       y\n   <int>   <dbl>\n 1     1  0.442 \n 2     2  1.80  \n 3     3  0.571 \n 4     4 -0.0365\n 5     5  0.854 \n 6     6 -0.634 \n 7     7 -0.189 \n 8     8 -0.415 \n 9     9  1.36  \n10    10  0.107 \n# … with 140 more rows\n\n$dens_tbl\n# A tibble: 150 × 2\n       x         y\n   <dbl>     <dbl>\n 1 -4.17 0.0000995\n 2 -4.08 0.000145 \n 3 -3.98 0.000207 \n 4 -3.89 0.000294 \n 5 -3.79 0.000413 \n 6 -3.70 0.000574 \n 7 -3.60 0.000788 \n 8 -3.51 0.00107  \n 9 -3.41 0.00145  \n10 -3.32 0.00193  \n# … with 140 more rows\n\n$input_data\n$input_data$`rnorm(100, 0, 1)`\n  [1]  0.44169781  1.80418306  0.57133927 -0.03649729  0.85387119 -0.63383074\n  [7] -0.18854658 -0.41451222  1.36023418  0.10726858  0.08526992 -0.64879496\n [13]  0.69255412 -0.75735669  0.19705920 -0.17721516 -0.63079170 -1.39983310\n [19]  1.01755199 -0.83631414  0.72912414 -0.14737137  1.27082258 -1.04753889\n [25] -0.16141490  0.22198899  2.83598596 -0.22484669 -0.58487594 -0.62746477\n [31] -0.81873031  1.74559087  1.36529721  1.45023471 -0.06258668  2.14467649\n [37]  0.10043517 -0.67990809  2.85050168 -1.45216256  0.01049808  0.22827703\n [43] -0.51146361  0.43143915 -0.59915348  1.61324991 -0.58580448 -0.46120961\n [49]  0.98191810 -0.31593955  0.86164296  1.18808250  1.09066101  0.39150090\n [55]  0.50730674  1.88640675  1.55522681 -0.65149477 -0.27561149 -0.31867192\n [61]  0.08555271 -1.00047014  1.12127311 -1.23597493  0.96384070  0.99097697\n [67] -0.25932523  0.25407058 -0.35294377 -0.72055148 -0.40429088 -0.08843004\n [73]  0.95498089 -0.68453125  1.67531797 -0.20665261  0.57318766 -0.12758793\n [79] -0.38044927  1.81833828  1.05959931  0.08519174  0.16865694 -0.15828443\n [85]  0.08736815  0.70222886  1.27180668  0.76483122 -0.43573173  0.02909088\n [91] -1.31286933 -0.09244617  0.22188836 -0.88909052  1.22243358  0.48397190\n [97]  0.82291445  0.46595188  0.68619052 -1.65739185\n\n$input_data$`tidy_normal(.mean = 5, .sd = 1)`\n# A tibble: 50 × 7\n   sim_number     x     y    dx       dy      p     q\n   <fct>      <int> <dbl> <dbl>    <dbl>  <dbl> <dbl>\n 1 1              1  6.20  1.24 0.000230 0.886   6.20\n 2 1              2  3.95  1.40 0.000639 0.146   3.95\n 3 1              3  4.59  1.55 0.00158  0.342   4.59\n 4 1              4  3.16  1.70 0.00349  0.0326  3.16\n 5 1              5  5.03  1.86 0.00689  0.513   5.03\n 6 1              6  4.89  2.01 0.0122   0.455   4.89\n 7 1              7  5.49  2.16 0.0195   0.687   5.49\n 8 1              8  6.78  2.32 0.0283   0.962   6.78\n 9 1              9  5.17  2.47 0.0376   0.566   5.17\n10 1             10  6.36  2.62 0.0464   0.913   6.36\n# … with 40 more rows\n\n\nAnd now the visuals that come with it.\n\noutput$plots\n\n$line_plot\n\n\n\n\n\n\n$dens_plot\n\n\n\n\n\nThe function also lists the input functions as well.\n\noutput$input_fns\n\n[[1]]\nrnorm(100, 0, 1)\n\n[[2]]\ntidy_normal(.mean = 5, .sd = 1)\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-14/index.html",
    "href": "posts/rtip-2022-12-14/index.html",
    "title": "Distribution Summaries with {TidyDensity}",
    "section": "",
    "text": "Introduction\n{TidyDensity} is an R package that provides tools for working with probability distributions in a tidy data format. One of the key functions in the package is tidy_distribution_summary_tbl(), which allows users to quickly and easily get summary information about a probability distribution.\nThe tidy_distribution_summary_tbl() function takes a vector of data as input and returns a table with basic statistics about the distribution of the data. This includes the mean, standard deviation, kurtosis, and skewness of the data, as well as other useful information.\nUsing tidy_distribution_summary_tbl(), users can easily get a high-level overview of their data, which can be useful for exploratory data analysis, data visualization, and other tasks. The function is designed to work seamlessly with the other tools in the {TidyDensity} package, making it easy to combine with other operations and build complex data analysis pipelines.\nOverall, TidyDensity and its tidy_distribution_summary_tbl() function are valuable tools for anyone working with probability distributions in R. Whether you are a seasoned data scientist or a beginner, TidyDensity can help you quickly and easily explore and understand your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_distribution_summary_tbl(.data, ...)\n\nHere are the arguments that go to the parameters.\n\n.data - The data that is going to be passed from a a tidy_ distribution function.\n... - This is the grouping variable that gets passed to dplyr::group_by() and dplyr::select().\n\n\n\nExample\nNow let’s go over a simple example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <- tidy_normal(.num_sims = 5)\ntb <- tidy_beta(.num_sims = 5)\n\ntidy_distribution_summary_tbl(tn) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> -0.044964\n$ median_val <dbl> -0.0266966\n$ std_val    <dbl> 1.020322\n$ min_val    <dbl> -2.834123\n$ max_val    <dbl> 3.336879\n$ skewness   <dbl> 0.03115634\n$ kurtosis   <dbl> 2.772527\n$ range      <dbl> 6.171002\n$ iqr        <dbl> 1.447849\n$ variance   <dbl> 1.041057\n$ ci_low     <dbl> -1.873091\n$ ci_high    <dbl> 1.868382\n\ntidy_distribution_summary_tbl(tn, sim_number) |>\n  glimpse()\n\nRows: 5\nColumns: 13\n$ sim_number <fct> 1, 2, 3, 4, 5\n$ mean_val   <dbl> -0.09684833, -0.13886169, 0.23257556, -0.32487778, 0.103192…\n$ median_val <dbl> -0.1358051, -0.2550682, 0.3069263, -0.1334922, 0.2898412\n$ std_val    <dbl> 1.1231699, 1.0954659, 0.8902380, 0.9270631, 0.9919932\n$ min_val    <dbl> -2.834123, -2.340575, -1.963215, -2.396105, -1.827744\n$ max_val    <dbl> 3.336879, 1.987640, 2.066451, 1.526231, 2.093211\n$ skewness   <dbl> 0.352771389, 0.132723834, -0.282840344, -0.191853538, 0.006…\n$ kurtosis   <dbl> 3.652828, 2.169309, 2.749967, 2.332081, 2.409223\n$ range      <dbl> 6.171002, 4.328215, 4.029666, 3.922336, 3.920956\n$ iqr        <dbl> 1.5256470, 1.6335396, 0.9368546, 1.3968485, 1.3469671\n$ variance   <dbl> 1.2615106, 1.2000455, 0.7925236, 0.8594460, 0.9840505\n$ ci_low     <dbl> -1.834548, -1.844197, -1.428713, -2.193065, -1.626225\n$ ci_high    <dbl> 1.860755, 1.858576, 1.644153, 1.090125, 1.976371\n\ndata_tbl <- tidy_combine_distributions(tn, tb)\n\ntidy_distribution_summary_tbl(data_tbl) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> 0.2413251\n$ median_val <dbl> 0.3687409\n$ std_val    <dbl> 0.8030476\n$ min_val    <dbl> -2.834123\n$ max_val    <dbl> 3.336879\n$ skewness   <dbl> -0.7608556\n$ kurtosis   <dbl> 4.248452\n$ range      <dbl> 6.171002\n$ iqr        <dbl> 0.7835065\n$ variance   <dbl> 0.6448855\n$ ci_low     <dbl> -1.695096\n$ ci_high    <dbl> 1.585147\n\ntidy_distribution_summary_tbl(data_tbl, dist_type) |>\n  glimpse()\n\nRows: 2\nColumns: 13\n$ dist_type  <fct> \"Gaussian c(0, 1)\", \"Beta c(1, 1, 0)\"\n$ mean_val   <dbl> -0.0449640, 0.5276142\n$ median_val <dbl> -0.0266966, 0.5301650\n$ std_val    <dbl> 1.0203220, 0.2944871\n$ min_val    <dbl> -2.834123047, 0.001236575\n$ max_val    <dbl> 3.3368786, 0.9992146\n$ skewness   <dbl> 0.03115634, -0.08744219\n$ kurtosis   <dbl> 2.772527, 1.751248\n$ range      <dbl> 6.171002, 0.997978\n$ iqr        <dbl> 1.447849, 0.511105\n$ variance   <dbl> 1.04105699, 0.08672268\n$ ci_low     <dbl> -1.87309115, 0.04220623\n$ ci_high    <dbl> 1.8683817, 0.9771898"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html",
    "href": "posts/rtip-2022-12-15/index.html",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "The R package {healthyR.ts}, is an R package that allows users to easily plot and analyze their time series data. The package includes a variety of functions, but one of the standout features is the ts_sma_plot() function, which allows users to quickly visualize their time series data and any number of simple moving averages (SMAs) of their choosing.\nSMAs are a common tool used by analysts and investors to smooth out short-term fluctuations in data and identify longer-term trends. By overlaying SMAs of different time periods on top of the original time series data, the ts_sma_plot() function makes it easy to compare and contrast different time periods and identify potential trends and patterns in the data.\nWith {healthyR.ts} and the ts_sma_plot() function, users can quickly and easily gain valuable insights into their time series data and make more informed decisions based on the trends and patterns they uncover.\nOk enough of that, let’s see the function."
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#data",
    "href": "posts/rtip-2022-12-15/index.html#data",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Data",
    "text": "Data\n\nout$data\n\n# A tibble: 288 × 5\n   index     date_col   value sma_order sma_value\n   <yearmon> <date>     <dbl> <fct>         <dbl>\n 1 Jan 1949  1949-01-01   112 3               NA \n 2 Feb 1949  1949-02-01   118 3              121.\n 3 Mar 1949  1949-03-01   132 3              126.\n 4 Apr 1949  1949-04-01   129 3              127.\n 5 May 1949  1949-05-01   121 3              128.\n 6 Jun 1949  1949-06-01   135 3              135.\n 7 Jul 1949  1949-07-01   148 3              144.\n 8 Aug 1949  1949-08-01   148 3              144 \n 9 Sep 1949  1949-09-01   136 3              134.\n10 Oct 1949  1949-10-01   119 3              120.\n# … with 278 more rows"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#plots",
    "href": "posts/rtip-2022-12-15/index.html#plots",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Plots",
    "text": "Plots\n\nout$plots$static_plot\n\nWarning: Removed 7 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nout$plots$interactive_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-19/index.html",
    "href": "posts/rtip-2022-12-19/index.html",
    "title": "Viewing Different Versions of the Same Statistical Distribution with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIn statistics, it is often useful to view different versions of the same statistical distribution. For example, when working with the normal distribution, it may be helpful to see how the distribution changes as the mean and standard deviation are varied.\nOne way to do this is by using the R library {TidyDensity}, which has a function called tidy_multi_single_dist(). This function allows a user to easily generate multiple versions of the same statistical distribution which can be plotted on the same graph, with each version representing a different combination of mean and standard deviation.\nTo use this function, the user simply needs to specify the distribution they want to plot (e.g. “normal”), the range of values for the mean and standard deviation, and the number of versions they want to plot. The function will then generate a plot showing the different versions of the distribution, with each version represented by a different color.\nThere are several reasons why it might be a good idea to view different versions of the same statistical distribution. For one, it can help the user understand how the shape of the distribution changes as the mean and standard deviation are varied. This can be particularly useful for distributions that have a wide range of possible values for the mean and standard deviation, such as the normal distribution.\nIn addition, viewing different versions of the same distribution can also help the user identify patterns and trends in the data. For example, the user may notice that the distribution becomes more spread out as the standard deviation increases, or that the distribution shifts to the left or right as the mean changes.\nOverall, the TidyDensity function tidy_multi_single_dist() is a useful tool for anyone interested in visualizing different versions of the same statistical distribution. Whether you are a student learning about statistics for the first time, or an experienced data scientist looking to better understand your data, this function can help you gain a deeper understanding of the underlying distribution and identify patterns and trends in your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_multi_single_dist(\n  .tidy_dist = NULL, \n  .param_list = list()\n  )\n\nNow let’s look at the arguments that go to the parameters.\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the {TidyDensity} ‘tidy_’ distribution function.\n\n\n\nExample\nLet’s run through an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <-tidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 200,\n    .mean = c(-1, 0, 1),\n    .sd = 1,\n    .num_sims = 3\n  )\n)\n\nNow that we have generated the data, let’s take a look and see if these different distributions have indeed been created.\n\ntidy_distribution_summary_tbl(tn, dist_name) |>\n  select(dist_name, mean_val, std_val)\n\n# A tibble: 3 × 3\n  dist_name         mean_val std_val\n  <fct>                <dbl>   <dbl>\n1 Gaussian c(-1, 1)  -1.03     0.988\n2 Gaussian c(0, 1)    0.0136   1.01 \n3 Gaussian c(1, 1)    0.990    1.02 \n\n\nLook’s good there, now let’s visualize.\n\ntn %>%\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-20/index.html",
    "href": "posts/rtip-2022-12-20/index.html",
    "title": "Random Walks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a type of stochastic process that can be used to model the movement of a particle or system over time. At each time step, the position of the particle is updated based on a random step drawn from a given probability distribution. This process can be represented as a sequence of independent and identically distributed (i.i.d.) random variables, and the resulting path traced by the particle is known as a random walk.\nRandom walks have a wide range of applications, including modeling the movement of stock prices, animal migration, and the spread of infectious diseases. They are also a fundamental concept in probability and statistics, and have been studied extensively in the literature.\nThe {TidyDensity} package provides a convenient way to generate and visualize random walks using the tidy_random_walk() function. This function takes a probability distribution as an argument, and generates a random walk by sampling from this distribution at each time step. For example, to generate a random walk with normally distributed steps, we can use the tidy_normal() function as follows:\n\nlibrary(TidyDensity)\n\n# Generate a random walk with normally distributed steps\nrw <- tidy_random_walk(tidy_normal())\n\nThe resulting object rw is a tibble with the typical tidy_ distribution columns and one augmented column called random_walk_value. The columns that are output are:\n\nsim_number The current simulation number from the tidy_ distribution\nx (You can think of this as time t)\ny The randomly generated value.\ndx & dy The density estimates of y at x\np & q The probability and quantile values of y\nrandom_walk_value The random walk value generated from tidy_random_walk() (You can think of this as the position of the particle at time t or the x)\n\nTo visualize the random walk, we can use the tidy_random_walk_autoplot() function, which creates a ggplot object showing the position of the particle at each time step. For example:\n\n# Visualize the random walk\ntidy_random_walk_autoplot(rw)\n\nThis will produce a plot showing the trajectory of the particle over time. You can customize the appearance of the plot by passing additional arguments to the tidy_random_walk_autoplot() function, such as the geom argument to specify the type of plot to use (e.g. geom = “line” for a line plot, or geom = “point” for a scatter plot).\nIn summary, the {TidyDensity} package provides a convenient and user-friendly interface for generating and visualizing random walks. With the tidy_random_walk() and tidy_random_walk_autoplot() functions, you can easily explore the behavior of random walks and their applications in a wide range of contexts.\nLet’s take a look at these functions.\n\n\nFunction\nFirstly we will look at the tidy_random_walk() function.\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\nHere are the arguments that get provided to the parameters of this function.\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\nNow let’s do the same with the tidy_random_walk_autoplot() function.\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\nHere are the arguments that get provided to the parameters.\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExamples\nLet’s go over some examples.\n\nlibrary(TidyDensity)\n\ndist_data <- tidy_normal(.sd = .1, .num_sims = 5)\n\ntidy_random_walk(.data = dist_data, .value_type = \"cum_sum\") %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nAnd another.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nNow let’s get an interactive one.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot(.interactive = TRUE)\n\n\n\n\n\nOne last example, let’s use a different distribution. Let’s use a cauchy distribution.\n\ntidy_cauchy(.num_sims = 9, .location = .5) %>%\n  tidy_random_walk(.value_type = \"cum_sum\", .sample = TRUE) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-21/index.html",
    "href": "posts/rtip-2022-12-21/index.html",
    "title": "Distribution Statistics with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re working with statistical distributions in R, you may be interested in the {TidyDensity} package. This package provides a set of functions for creating, manipulating, and visualizing probability distributions in a tidy format. One of these functions is tidy_chisquare(), which allows you to create a chi-square distribution with a specified number of degrees of freedom and a non-centrality parameter.\nOnce you’ve created a chi-square distribution using tidy_chisquare(), you may want to get some summary statistics about the distribution. This is where the util_chisquare_stats_tbl() function comes in handy. This function takes a chi-square distribution (created with tidy_chisquare()) as input and returns a tibble with several statistics about the distribution.\nSome of the statistics included in the table are:\n\nMean: The mean of the chi-square distribution, also known as the expected value.\nVariance: The variance of the chi-square distribution, which is a measure of how spread out the data is.\nSkewness: The skewness of the chi-square distribution, which is a measure of the symmetry of the data.\nKurtosis: The kurtosis of the chi-square distribution, which is a measure of the peakedness of the data.\n\nTo use the util_chisquare_stats_tbl() function, you’ll need to install and load the {TidyDensity} package first. Then, you can create a chi-square distribution using tidy_chisquare() and pass it to util_chisquare_stats_tbl() like this:\n\n# install and load TidyDensity\ninstall.packages(\"TidyDensity\")\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# create a chi-square distribution with 5 degrees of freedom\ndistribution <- tidy_chisquare(.df = 5)\n\n# get statistics about the distribution\nutil_chisquare_stats_tbl(distribution) |>\n  glimpse()\n\nThe output will be a table with the mean, variance, skewness, and kurtosis of the chi-square distribution. These statistics can be useful for understanding the characteristics of the distribution and making statistical inferences.\nOverall, the {TidyDensity} package is a useful tool for working with statistical distributions in R. The util_chisquare_stats_tbl() function is just one of many functions available in the package that can help you analyze and understand your data. Give it a try and see how it can help with your statistical analysis!\n\n\nFunction\nLet’s take a look at the full function call.\n\nutil_chisquare_stats_tbl(.data)\n\nLet’s take a look at the arguments that get supplied to the function parameters.\n\n.data - The data being passed from a tidy_ distribution function.\n\n\n\nExample\nNow for a full example with output.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntidy_chisquare() %>%\n  util_chisquare_stats_tbl() %>%\n  glimpse()\n\nRows: 1\nColumns: 17\n$ tidy_function     <chr> \"tidy_chisquare\"\n$ function_call     <chr> \"Chisquare c(1, 1)\"\n$ distribution      <chr> \"Chisquare\"\n$ distribution_type <chr> \"continuous\"\n$ points            <dbl> 50\n$ simulations       <dbl> 1\n$ mean              <dbl> 1\n$ median            <dbl> 0.3333333\n$ mode              <chr> \"undefined\"\n$ std_dv            <dbl> 1.414214\n$ coeff_var         <dbl> 1.414214\n$ skewness          <dbl> 2.828427\n$ kurtosis          <dbl> 15\n$ computed_std_skew <dbl> 1.132669\n$ computed_std_kurt <dbl> 3.894553\n$ ci_lo             <dbl> 0.002189912\n$ ci_hi             <dbl> 6.521727\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-22/index.html",
    "href": "posts/rtip-2022-12-22/index.html",
    "title": "Listing Functions and Parameters",
    "section": "",
    "text": "Introduction\nI got a little bored one day and decided I wanted to list out all of the functions inside of a package along with their parameters in a tibble. Not sure if this serves any particular purpose or not, I was just bored.\nThis does not work for packages that have data as an export like {healthyR} or {healthyR.data} but it will work for packages like {TidyDensity}.\nLet’s run through it\n\n\nExamples\nHere we go.\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:TidyDensity\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nbootstrap_density_augment\n.data\nbootstrap_density_augment(.data)\n\n\nbootstrap_p_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_p_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_p_vec\n.x\nbootstrap_p_vec(.x)\n\n\nbootstrap_q_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_q_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_q_vec\n.x\nbootstrap_q_vec(.x)\n\n\nbootstrap_stat_plot\nc(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\nbootstrap_stat_plot(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\n\n\nbootstrap_unnest_tbl\n.data\nbootstrap_unnest_tbl(.data)\n\n\ncgmean\n.x\ncgmean(.x)\n\n\nchmean\n.x\nchmean(.x)\n\n\nci_hi\nc(“.x”, “.na_rm”)\nci_hi(“.x”, “.na_rm”)\n\n\nci_lo\nc(“.x”, “.na_rm”)\nci_lo(“.x”, “.na_rm”)\n\n\nckurtosis\n.x\nckurtosis(.x)\n\n\ncmean\n.x\ncmean(.x)\n\n\ncmedian\n.x\ncmedian(.x)\n\n\ncolor_blind\nNULL\ncolor_blind(NULL)\n\n\ncsd\n.x\ncsd(.x)\n\n\ncskewness\n.x\ncskewness(.x)\n\n\ncvar\n.x\ncvar(.x)\n\n\ndist_type_extractor\n.x\ndist_type_extractor(.x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\ntd_scale_color_colorblind\nc(“…”, “theme”)\ntd_scale_color_colorblind(“…”, “theme”)\n\n\ntd_scale_fill_colorblind\nc(“…”, “theme”)\ntd_scale_fill_colorblind(“…”, “theme”)\n\n\ntidy_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_bernoulli\nc(“.n”, “.prob”, “.num_sims”)\ntidy_bernoulli(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_beta\nc(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\ntidy_beta(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\n\n\ntidy_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_bootstrap\nc(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\ntidy_bootstrap(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\n\n\ntidy_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_cauchy\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_cauchy(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_chisquare\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_chisquare(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_combine_distributions\n…\ntidy_combine_distributions(…)\n\n\ntidy_combined_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_combined_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_distribution_comparison\nc(“.x”, “.distribution_type”)\ntidy_distribution_comparison(“.x”, “.distribution_type”)\n\n\ntidy_distribution_summary_tbl\nc(“.data”, “…”)\ntidy_distribution_summary_tbl(“.data”, “…”)\n\n\ntidy_empirical\nc(“.x”, “.num_sims”, “.distribution_type”)\ntidy_empirical(“.x”, “.num_sims”, “.distribution_type”)\n\n\ntidy_exponential\nc(“.n”, “.rate”, “.num_sims”)\ntidy_exponential(“.n”, “.rate”, “.num_sims”)\n\n\ntidy_f\nc(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\ntidy_f(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\n\n\ntidy_four_autoplot\nc(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_four_autoplot(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_gamma\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_gamma(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_beta\nc(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_beta(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_pareto\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_pareto(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_hypergeometric\nc(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\ntidy_hypergeometric(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\n\n\ntidy_inverse_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_exponential\nc(“.n”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_exponential(“.n”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_gamma\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_gamma(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_normal\nc(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\ntidy_inverse_normal(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\n\n\ntidy_inverse_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_inverse_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_weibull\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_weibull(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_kurtosis_vec\n.x\ntidy_kurtosis_vec(.x)\n\n\ntidy_logistic\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_logistic(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_lognormal\nc(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\ntidy_lognormal(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\n\n\ntidy_mixture_density\n…\ntidy_mixture_density(…)\n\n\ntidy_multi_dist_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_multi_dist_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_multi_single_dist\nc(“.tidy_dist”, “.param_list”)\ntidy_multi_single_dist(“.tidy_dist”, “.param_list”)\n\n\ntidy_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_normal\nc(“.n”, “.mean”, “.sd”, “.num_sims”)\ntidy_normal(“.n”, “.mean”, “.sd”, “.num_sims”)\n\n\ntidy_paralogistic\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_paralogistic(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_pareto1\nc(“.n”, “.shape”, “.min”, “.num_sims”)\ntidy_pareto1(“.n”, “.shape”, “.min”, “.num_sims”)\n\n\ntidy_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\ntidy_random_walk\nc(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\ntidy_random_walk(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\n\n\ntidy_random_walk_autoplot\nc(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\ntidy_random_walk_autoplot(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\n\n\ntidy_range_statistic\n.x\ntidy_range_statistic(.x)\n\n\ntidy_scale_zero_one_vec\n.x\ntidy_scale_zero_one_vec(.x)\n\n\ntidy_skewness_vec\n.x\ntidy_skewness_vec(.x)\n\n\ntidy_stat_tbl\nc(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\ntidy_stat_tbl(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\n\n\ntidy_t\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_t(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_uniform\nc(“.n”, “.min”, “.max”, “.num_sims”)\ntidy_uniform(“.n”, “.min”, “.max”, “.num_sims”)\n\n\ntidy_weibull\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_weibull(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_zero_truncated_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_zero_truncated_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_zero_truncated_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\nutil_bernoulli_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_bernoulli_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_bernoulli_stats_tbl\n.data\nutil_bernoulli_stats_tbl(.data)\n\n\nutil_beta_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_beta_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_beta_stats_tbl\n.data\nutil_beta_stats_tbl(.data)\n\n\nutil_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_binomial_stats_tbl\n.data\nutil_binomial_stats_tbl(.data)\n\n\nutil_cauchy_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_cauchy_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_cauchy_stats_tbl\n.data\nutil_cauchy_stats_tbl(.data)\n\n\nutil_chisquare_stats_tbl\n.data\nutil_chisquare_stats_tbl(.data)\n\n\nutil_exponential_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_exponential_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_exponential_stats_tbl\n.data\nutil_exponential_stats_tbl(.data)\n\n\nutil_f_stats_tbl\n.data\nutil_f_stats_tbl(.data)\n\n\nutil_gamma_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_gamma_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_gamma_stats_tbl\n.data\nutil_gamma_stats_tbl(.data)\n\n\nutil_geometric_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_geometric_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_geometric_stats_tbl\n.data\nutil_geometric_stats_tbl(.data)\n\n\nutil_hypergeometric_param_estimate\nc(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\nutil_hypergeometric_param_estimate(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\n\n\nutil_hypergeometric_stats_tbl\n.data\nutil_hypergeometric_stats_tbl(.data)\n\n\nutil_logistic_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_logistic_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_logistic_stats_tbl\n.data\nutil_logistic_stats_tbl(.data)\n\n\nutil_lognormal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_lognormal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_lognormal_stats_tbl\n.data\nutil_lognormal_stats_tbl(.data)\n\n\nutil_negative_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_negative_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_negative_binomial_stats_tbl\n.data\nutil_negative_binomial_stats_tbl(.data)\n\n\nutil_normal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_normal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_normal_stats_tbl\n.data\nutil_normal_stats_tbl(.data)\n\n\nutil_pareto_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_pareto_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_pareto_stats_tbl\n.data\nutil_pareto_stats_tbl(.data)\n\n\nutil_poisson_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_poisson_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_poisson_stats_tbl\n.data\nutil_poisson_stats_tbl(.data)\n\n\nutil_t_stats_tbl\n.data\nutil_t_stats_tbl(.data)\n\n\nutil_uniform_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_uniform_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_uniform_stats_tbl\n.data\nutil_uniform_stats_tbl(.data)\n\n\nutil_weibull_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_weibull_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_weibull_stats_tbl\n.data\nutil_weibull_stats_tbl(.data)\n\n\n\n\n\nAnother example.\n\nlibrary(healthyverse)\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:healthyverse\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\nexpr\nexpr\nexpr(expr)\n\n\nhealthyverse_conflicts\nNULL\nhealthyverse_conflicts(NULL)\n\n\nhealthyverse_deps\nc(“recursive”, “repos”)\nhealthyverse_deps(“recursive”, “repos”)\n\n\nhealthyverse_packages\ninclude_self\nhealthyverse_packages(inlude_self)\n\n\nhealthyverse_sitrep\nNULL\nhealthyverse_sitrep(NULL)\n\n\nhealthyverse_update\nc(“recursive”, “repos”)\nhealthyverse_update(“recursive”, “repos”)\n\n\nsym\nx\nsym(x)\n\n\nsyms\nx\nsyms(x)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-29/index.html",
    "href": "posts/rtip-2022-12-29/index.html",
    "title": "Gartner Magic Chart and its usefulness in healthcare analytics with {healthyR}",
    "section": "",
    "text": "Introduction\nThe Gartner Magic Chart is a powerful tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. It was developed by Dr. James Gartner in the early 2000s as a way to visualize the relationship between two key metrics, for example: Excess Length of Stay (ELOS) and Excess Readmit Rate.\nIn healthcare, length of stay (LOS) refers to the amount of time a patient spends in the hospital. Excess LOS is the difference between the actual LOS of a patient and the expected LOS for that patient, based on their diagnosis and other factors. Excess readmit rate is the percentage of patients who are readmitted to the hospital within a certain time period after being discharged, above and beyond what is expected based on their diagnosis and other factors.\nThe Gartner Magic Chart can plot excess LOS on the x-axis and excess readmit rate on the y-axis. The resulting chart is divided into four quadrants, with the top right quadrant representing high excess LOS and high excess readmit rate, the bottom left quadrant representing low excess LOS and low excess readmit rate, and the other two quadrants representing intermediate values of these metrics.\nOne of the key benefits of the Gartner Magic Chart is that it allows healthcare professionals to quickly and easily identify areas of concern and opportunities for improvement. For example, if a hospital has a high excess LOS and a high excess readmit rate, it may be an indication that the hospital is not effectively managing patient care and is instead relying on costly and unnecessary readmissions to address problems that could have been avoided in the first place.\nThe Gartner Magic Chart can also be used to identify trends over time, allowing healthcare professionals to track progress and see the impact of changes they have made to patient care processes.\nIf you are interested in creating a Gartner Magic Chart for your own healthcare data, the R package {healthyR} has a convenient function called gartner_magic_chart_plt() that allows you to easily create this chart from data supplied by the end user. Simply input your excess LOS and excess readmit rate data, and the function will generate the chart for you.\nIn summary, the Gartner Magic Chart is a valuable tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. By using the gartner_magic_chart_plt() function from the {healthyR} package, you can easily create this chart for your own data and start using it to improve patient care and outcomes.\n\n\nFunction\nLet’s take a look at the full function call for gartner_magic_chart_plt().\n\ngartner_magic_chart_plt(\n  .data,\n  .x_col,\n  .y_col,\n  .point_size_col = NULL,\n  .y_lab,\n  .x_lab,\n  .plt_title,\n  .tl_lbl,\n  .tr_lbl,\n  .br_lbl,\n  .bl_lbl\n)\n\nNow let’s take a look at the arguments to the parameters.\n\n.data - The data set you want to plot\n.x_col - The x-axis for the plot\n.y_col - The y-axis for the plot\n.point_size_col - The default is NULL, if you want to size the dots by a column in the data.frame/tibble then enter the column name here.\n.y_lab - The y-axis label\n.x_lab - The x-axis label\n.plt_title - The title of the plot\n.tl_lbl - The top left label\n.tr_lbl - The top right label\n.br_lbl - The bottom right label\n.bl_lbl - The bottom left label\n\n\n\nExample\nLet’s see the function in action.\n\nlibrary(dplyr)\nlibrary(healthyR)\n\ndata_tbl <- tibble(\n    x = rnorm(100, 0, 1),\n    y = rnorm(100, 0, 1),\n    z = abs(x) + abs(y)\n )\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = z,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)\n\n\n\n\nExample two.\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = NULL,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)"
  },
  {
    "objectID": "posts/rtip-2023-01-03/index.html",
    "href": "posts/rtip-2023-01-03/index.html",
    "title": "Calendar Heatmap with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nCalendar heatmaps are a useful visualization tool for understanding patterns and trends in time series data. They are particularly useful for displaying daily data, as they allow for the visualization of multiple weeks or months at a time.\nThe ts_calendar_heatmap_plot() function from the R library {healthyR.ts} is a powerful tool for creating calendar heatmaps. This function takes in a time series object and creates a heatmap plot with daily data plotted on the calendar. The intensity of the color on each day corresponds to the value of the data for that day.\nOne application of calendar heatmaps is in understanding daily patterns in data such as website traffic or sales. For example, a business owner could use a calendar heatmap to identify trends in their daily sales data and make informed decisions about their operations. Similarly, a website owner could use a calendar heatmap to understand the daily traffic patterns on their site and optimize their content strategy accordingly.\nCalendar heatmaps are also useful for identifying anomalies or unusual patterns in time series data. For example, a calendar heatmap could be used to identify unexpected spikes or dips in daily sales data, or to identify unusual patterns in website traffic.\nIn addition to their practical applications, calendar heatmaps are also aesthetically pleasing and can be a fun way to visualize data. The ts_calendar_heatmap_plot() function allows for customization of the color palette and other visual options, making it easy to create visually appealing heatmaps.\nOverall, calendar heatmaps are a useful tool for understanding patterns and trends in daily time series data. The ts_calendar_heatmap_plot() function from the R library healthyR.ts is a powerful tool for creating calendar heatmaps and can be easily customized to suit the needs of the user.\n\n\nFunction\nLet’s take a look at the function.\n\nts_calendar_heatmap_plot(\n  .data,\n  .date_col,\n  .value_col,\n  .low = \"red\",\n  .high = \"green\",\n  .plt_title = \"\",\n  .interactive = TRUE\n)\n\nNow let’s see the arguments to the parameters.\n\n.data - The time-series data with a date column and value column.\n.date_col - The column that has the datetime values\n.value_col - The column that has the values\n.low - The color for the low value, must be quoted like “red”. The default is “red”\n.high - The color for the high value, must be quoted like “green”. The default is “green”\n.plt_title - The title of the plot\n.interactive - Default is TRUE to get an interactive plot using plotly::ggplotly(). It can be set to FALSE to get a ggplot plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\n\ndata_tbl <- data.frame(\n  date_col = seq.Date(\n    from = as.Date(\"2020-01-01\"),\n    to   = as.Date(\"2022-06-01\"),\n    length.out = 365*2 + 180\n    ),\n  value = rnorm(365*2+180, mean = 100)\n)\n\nts_calendar_heatmap_plot(\n  .data          = data_tbl\n  , .date_col    = date_col\n  , .value_col   = value\n  , .interactive = TRUE\n)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-05/index.html",
    "href": "posts/rtip-2023-01-05/index.html",
    "title": "More Randomwalks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a mathematical concept that have found various applications in fields such as economics, biology, and computer science. At a high level, a random walk refers to a process in which a set of objects move in a random direction at each time step. The path that the objects take over time forms a random walk.\nOne of the main uses of random walks is in modeling the behavior of stock prices. In the stock market, prices can be thought of as performing a random walk because they are influenced by a variety of unpredictable factors such as market trends, news events, and investor sentiment. By modeling stock prices as a random walk, it is possible to make predictions about future price movements and to understand the underlying factors that drive these movements.\nAnother application of random walks is in studying the movement patterns of animals. For example, biologists have used random walk models to understand the foraging behavior of ants and the migration patterns of animals such as birds and whales.\nOne interesting aspect of random walks is that they can be generated with different statistical distributions. For example, a random walk could be generated with a standard normal distribution (mean = 0, standard deviation = 1) or with a distribution that has a different mean and standard deviation. By looking at random walks with different distributional parameters, it is possible to understand how the underlying distribution affects the overall shape and pattern of the random walk.\nTo generate random walks with different distributional parameters, you can use the R package {TidyDensity}. This package provides functions for generating random walks and visualizing them using density plots. With {TidyDensity}, you can easily compare random walks with different mean and standard deviation values to see how these parameters affect the shape of the random walk.\nIn summary, random walks are a useful tool for modeling the behavior of various systems over time. They are particularly useful for understanding the movement patterns of stock prices and animals, and can be generated with different statistical distributions using the R package {TidyDensity}.\n\n\nFunctions\nThere are a couple of functions that we are going to use, below you will find them with their full function call and parameter arguments.\ntidy_multi_single_dist()\n\ntidy_multi_single_dist(.tidy_dist = NULL, .param_list = list())\n\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the TidyDensity tidy_ distribution function.\n\ntidy_random_walk()\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\ntidy_random_walk_autoplot()\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExample\n\nlibrary(TidyDensity)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 250,\n    .mean = 0,\n    .sd = c(.025, .05, .1, .15),\n    .num_sims = 25\n  )\n) %>%\n  tidy_random_walk(.initial_value = 1000, .value_type = \"cum_prod\") %>%\n  tidy_random_walk_autoplot() +\n  facet_wrap(~ dist_name, scales = \"free\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-09/index.html",
    "href": "posts/rtip-2023-01-09/index.html",
    "title": "New Release of {healthyR.ts}",
    "section": "",
    "text": "Introduction\nHello R users!\nI am excited to announce a new update to the {healthyR.ts} package: the ts_brownian_motion() function.\nThis function allows you to easily simulate brownian motion, also known as a Wiener process, using just a few parameters. You can specify the length of the simulation using the ‘.time’ parameter, the number of simulations to run using the ‘.num_sims’ parameter, the time step size (standard deviation) using the ‘.delta_time’ parameter, and the initial value (which is set to 0 by default) using the ‘.initial_value’ parameter.\nBut what is brownian motion, and why might you want to simulate it? Brownian motion is a random process that describes the movement of particles suspended in a fluid. It is named after the botanist Robert Brown, who observed the random movement of pollen grains suspended in water under a microscope in the 19th century.\nIn finance, brownian motion is often used to model the movement of stock prices over time. By simulating brownian motion, you can get a sense of how prices might fluctuate in the future, and use this information to inform your investment decisions.\nI hope that the ts_brownian_motion() function will be a useful tool for anyone interested in simulating brownian motion, whether for financial modeling or any other application. Give it a try and see what you can do with it!\nRight now the function is a bit slow at .num_sims > 500 so I am working on optimizing it. I will also later on be introducing the Geometric Brownian Motion to {healthyR.ts}\nAs always, we welcome feedback and suggestions for new features and improvements. Thank you for using the {healthyR.ts} package, and happy simulating!\n\n\nFunction\nHere is the full function call:\n\nts_brownian_motion(\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = 1,\n  .initial_value = 0\n)\n\n\n\nExample\nA simple example of the output.\n\nlibrary(healthyR.ts)\n\nts_brownian_motion()\n\n# A tibble: 1,010 × 3\n   sim_number     t     y\n   <fct>      <dbl> <dbl>\n 1 1              0  0   \n 2 1              1  1.46\n 3 1              2  2.68\n 4 1              3  2.78\n 5 1              4  3.07\n 6 1              5  3.43\n 7 1              6  3.05\n 8 1              7  4.43\n 9 1              8  6.04\n10 1              9  6.89\n# … with 1,000 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-10/index.html",
    "href": "posts/rtip-2023-01-10/index.html",
    "title": "Optimal Break Points for Histograms with {healthyR}",
    "section": "",
    "text": "Introduction\nHistogram binning is a technique used in data visualization to group continuous data into a set of discrete bins, or intervals. The purpose of histogram binning is to represent the distribution of a dataset in a graphical format, allowing for easy identification of patterns and outliers. However, there are several challenges that can arise when working with histogram binning.\nOne major challenge is determining the appropriate number of bins to use. If there are too few bins, the histogram may not accurately represent the underlying distribution of the data. On the other hand, if there are too many bins, the histogram may become cluttered and difficult to interpret. To overcome this challenge, there are several strategies that can be employed, such as the Scott’s normal reference rule, Freedman-Diaconis rule, or the Rice rule, to determine the optimal number of bins.\nAnother challenge with histogram binning is dealing with outliers. If a dataset has outliers, they can greatly skew the distribution of the data and make it difficult to interpret the histogram. One strategy to handle outliers is to use a log-scale on the x-axis, which can help to reduce their impact on the histogram. Alternatively, one could remove the outlier data points before creating the histogram.\nA further challenge is to choose the width of the bin that best represents the data. Too narrow bins might cause overfitting and too wide bins may cause loss of information. Different widths of bin can lead to a different representation of the data and hence a different conclusion. To overcome this, one could use the Freedman-Diaconis rule which take into consideration the range and the size of the sample to provide a robust and adaptive way to choose the width of the bin\nA simple solution to these challenges is the opt_bin() function in the {healthyR} library for R. This function uses an optimal binning algorithm to automatically determine the number of bins and bin widths that best represent the data. This can save a lot of time and effort when working with histograms and can help to ensure that the resulting histograms are accurate and easy to interpret.\nIn conclusion, histogram binning is a useful technique for visualizing the distribution of data, but it can be challenging to determine the appropriate number of bins and bin widths. Strategies such as Scott’s normal reference rule, Freedman-Diaconis rule, or the Rice rule can be used to determine the optimal number of bins. Outliers and bin width selection also can be challenges to take into account, and a function such as opt_bin() in the {healthyR} library can be used to overcome these challenges and create high-quality histograms with ease.\n\n\nFunction\nHere is the full call.\n\nopt_bin(.data, .value_col, .iters = 30)\n\nHere are the arguments that get passed to the parameters.\n\n.data - The data set in question\n.value_col - The column that holds the values\n.iters - How many times the cost function loop should run\n\nNow under I will provide some code and it’s explanation under the hood that exaplains how this works.\nHere is a breakdown of what each part of the code is doing:\n\nn <- 2:iters: this line creates a sequence of numbers starting at 2 and ending at the number specified by the variable “iters”\nc <- base::numeric(base::length(n)) and d <- c: These lines create two empty numeric vectors (arrays) called “c” and “d” with the same length as the sequence “n”\nfor (i in 1:length(n)) {...}: This is a for loop that iterates through each number in the sequence “n”\nd[i] <- diff(range( data ) ) / n[i]: Inside the loop, this line calculates the width of the bin for the current iteration by dividing the range of the data by the current number in the sequence “n”\nhp <- graphics::hist(data, breaks = edges, plot = FALSE) and ki <- hp$counts: This creates a histogram of the data using the current bin width and then gets the count of data points in each bin\nk <- mean(ki) and v <- sum((ki-k)^2/n[i]): this line uses the counts from the previous step to calculate the average count across all bins (k) and the variance of the counts across all bins (v)\nc[i] <- (2*k - v)/d[i]^2: this line calculates the cost function for the current bin width, which is based on k and v\nidx <- which.min(c) and opt_d <- d[idx]: this line finds the index in the “c” vector where the cost function is the lowest and stores that value in the variable “idx”\nedges <- seq(min(data), max(data), length = n[idx]) and edges <- tibble::as_tibble(edges): this line creates a new sequence of numbers representing the edges of the bins with the optimal bin width\nreturn(edges): this line returns the sequence of optimal bin edges that the function has determined\n\nOverall, this function will return an optimal set of bin edges for a histogram of the given data, the function uses an iterative process and consider the balance between the number of bins and the width of the bins to find the optimal width of the bins for the histogram. This could save a lot of time and effort for the data analysts as it can help to ensure that the resulting histograms are accurate and easy to interpret.\n\n\nExample\nLet’s look at some examples.\n\nlibrary(healthyR)\nlibrary(tidyverse)\n\ndf_tbl <- rnorm(n = 1000, mean = 0, sd = 1)\ndf_tbl <- df_tbl %>%\n  as_tibble()\n\nopt_bin(\n  .data = df_tbl,\n  .value_col = value\n  , .iters = 100\n)\n\n# A tibble: 6 × 1\n   value\n   <dbl>\n1 -3.04 \n2 -1.81 \n3 -0.585\n4  0.644\n5  1.87 \n6  3.10 \n\n\nNow lets user a smaller n to see how the output changes\n\nopt_bin(\n  .data = as_tibble(rnorm(n = 50)),\n  value,\n  100\n)\n\n# A tibble: 11 × 1\n     value\n     <dbl>\n 1 -1.69  \n 2 -1.24  \n 3 -0.797 \n 4 -0.351 \n 5  0.0944\n 6  0.540 \n 7  0.986 \n 8  1.43  \n 9  1.88  \n10  2.32  \n11  2.77  \n\n\nLet’s visualize.\n\nrn <- rnorm(50)\nhist(rn)\n\n\n\n\nNow let’s use opt_bin()\n\nhist(rn, breaks = opt_bin(as_tibble(rn), value) %>% pull())\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-12/index.html",
    "href": "posts/rtip-2023-01-12/index.html",
    "title": "An Update on {tidyAML}",
    "section": "",
    "text": "Introduction\nI have been doing a lot of work on a new package called {tidyAML}. {tidyAML} is a new R package that makes it easy to use the {tidymodels} ecosystem to perform automated machine learning (AutoML). This package provides a simple and intuitive interface that allows users to quickly generate machine learning models without worrying about the underlying details. It also includes a safety mechanism that ensures that the package will fail gracefully if any required extension packages are not installed on the user’s machine. With {tidyAML}, users can easily build high-quality machine learning models in just a few lines of code. Whether you are a beginner or an experienced machine learning practitioner, {tidyAML} has something to offer.\nSome ideas are that we should be able to generate regression models on the fly without having to actually go through the process of building the specification, especially if it is a non-tuning model, meaning we are not planing on tuning hyper-parameters like penalty and cost.\nThe idea is not to re-write the excellent work the {tidymodels} team has done (because it’s not possible) but rather to try and make an enhanced easy to use set of functions that do what they say and can generate many models and predictions at once.\nThis is similar to the great {h2o} package, but, {tidyAML} does not require java to be setup properly like {h2o} because {tidyAML} is built on {tidymodels}.\nThis package is not yet release, so you can only install from GitHub with the following:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/tidyAML\")\n\n\n\nExample\n\nlibrary(tidyAML)\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n11        11 stan_glmer      regression    linear_reg   <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 glm             regression    linear_reg   <spec[+]> \n3         3 glm             regression    poisson_reg  <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\",\"gee\"), \n                                 .parsnip_fns = \"linear_reg\")\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 gee             regression    linear_reg   <spec[+]> \n3         3 glm             regression    linear_reg   <spec[+]> \n\n\nAs shown we can easily select the models we want either by choosing the supported parsnip function like linear_reg() or by choose the desired engine, you can also use them both in conjunction with each other!\nNow, what if you want to create a non-tuning model spec without using the fast_regression_parsnip_spec_tbl() function. Well, you can. The function is called create_model_spec().\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow the reason we are here. Let’s take a look at the first function for modeling with tidyAML, fast_regression().\n\nlibrary(recipes)\nlibrary(dplyr)\nlibrary(purrr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(frt_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nNow lets take a look at a few different things in the frt_tbl.\n\nnames(frt_tbl)\n\n[1] \".model_id\"       \".parsnip_engine\" \".parsnip_mode\"   \".parsnip_fns\"   \n[5] \"model_spec\"      \"wflw\"            \"fitted_wflw\"     \"pred_wflw\"      \n\n\nLet’s look at a single model spec.\n\nfrt_tbl %>% slice(1) %>% select(model_spec) %>% pull() %>% pluck(1)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the wflw column.\n\nfrt_tbl %>% slice(1) %>% select(wflw) %>% pull() %>% pluck(1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe fitted wflw object.\n\nfrt_tbl %>% slice(1) %>% select(fitted_wflw) %>% pull() %>% pluck(1)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   11.77621      0.59296      0.01626     -0.03191     -0.55350     -5.30785  \n       qsec           vs           am         gear         carb  \n    0.97840      2.64023      1.68549      0.87059      0.58785  \n\nfrt_tbl %>% slice(1) %>% select(fitted_wflw) %>% pull() %>% pluck(1) %>%\n  broom::glance() %>%\n  glimpse()\n\nRows: 1\nColumns: 12\n$ r.squared     <dbl> 0.9085669\n$ adj.r.squared <dbl> 0.8382337\n$ sigma         <dbl> 2.337527\n$ statistic     <dbl> 12.91804\n$ p.value       <dbl> 3.367361e-05\n$ df            <dbl> 10\n$ logLik        <dbl> -47.07551\n$ AIC           <dbl> 118.151\n$ BIC           <dbl> 132.2877\n$ deviance      <dbl> 71.03241\n$ df.residual   <int> 13\n$ nobs          <int> 24\n\n\nAnd finally the predictions (this one I am probably going to change up).\n\nfrt_tbl %>% slice(1) %>% select(pred_wflw) %>% pull() %>% pluck(1)\n\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  17.4\n 2  28.4\n 3  17.2\n 4  10.7\n 5  13.4\n 6  17.0\n 7  22.8\n 8  14.3\n 9  22.4\n10  15.5\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-16/index.html",
    "href": "posts/rtip-2023-01-16/index.html",
    "title": "Auto K-Means with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nToday’s post is going to center around the automatic k-means functionality of {healthyR.ai}. I am not going to get into what it is or how it works, but rather the function call itself and how it works and what it puts out. The function is called hai_kmeans_automl. This function is a wrapper around the h2o::h2o.kmeans() function, but also does some processing to enhance the output at the end. Let’s get to it!\n\n\nFunction\nHere is the full function call.\n\nhai_kmeans_automl(\n  .data,\n  .split_ratio = 0.8,\n  .seed = 1234,\n  .centers = 10,\n  .standardize = TRUE,\n  .print_model_summary = TRUE,\n  .predictors,\n  .categorical_encoding = \"auto\",\n  .initialization_mode = \"Furthest\",\n  .max_iterations = 100\n)\n\nNow let’s go over the function arguments:\n\n.data - The data that is to be passed for clustering.\n.split_ratio - The ratio for training and testing splits.\n.seed - The default is 1234, but can be set to any integer.\n.centers - The default is 1. Specify the number of clusters (groups of data) in a data set.\n.standardize - The default is set to TRUE. When TRUE all numeric columns will be set to zero mean and unit variance.\n.print_model_summary - This is a boolean and controls if the model summary is printed to the console. The default is TRUE.\n.predictors - This must be in the form of c(“column_1”, “column_2”, … “column_n”)\n.categorical_encoding - Can be one of the following:\n\n“auto”\n“enum”\n“one_hot_explicit”\n“binary”\n“eigen”\n“label_encoder”\n“sort_by_response”\n“enum_limited”\n\n.initialization_mode - This can be one of the following:\n\n“Random”\n“Furthest (default)\n“PlusPlus”\n\n.max_iterations - The default is 100. This specifies the number of training iterations\n\n\n\nExamples\nTime for some examples.\n\nlibrary(healthyR.ai)\nlibrary(h2o)\n\nh2o.init()\n\noutput <- hai_kmeans_automl(\n  .data = iris,\n  .predictors = c(\n    \"Sepal.Width\", \"Sepal.Length\", \"Petal.Width\", \"Petal.Length\"\n    ),\n  .standardize = TRUE,\n  .split_ratio = .8\n)\n\nh2o.shutdown(prompt = FALSE)\n\nNow let’s take a look at the output. There are going to be 4 pieces of main output. Here they are:\n\ndata\nauto_kmeans_obj\nmodel_id\nscree_plt\n\nLet’s take a look at each one. First the data output which itself has 6 different objects in it.\n\noutput$data\n\n$splits\n$splits$training_tbl\n# A tibble: 123 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          5           3.6          1.4         0.2 setosa \n 5          5.4         3.9          1.7         0.4 setosa \n 6          4.6         3.4          1.4         0.3 setosa \n 7          5           3.4          1.5         0.2 setosa \n 8          4.4         2.9          1.4         0.2 setosa \n 9          5.4         3.7          1.5         0.2 setosa \n10          4.8         3.4          1.6         0.2 setosa \n# … with 113 more rows\n\n$splits$validate_tbl\n# A tibble: 27 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.6         3.1          1.5         0.2 setosa \n 2          4.9         3.1          1.5         0.1 setosa \n 3          5.8         4            1.2         0.2 setosa \n 4          5.1         3.5          1.4         0.3 setosa \n 5          5.7         3.8          1.7         0.3 setosa \n 6          5.1         3.8          1.5         0.3 setosa \n 7          5.4         3.4          1.7         0.2 setosa \n 8          5.1         3.7          1.5         0.4 setosa \n 9          5           3.4          1.6         0.4 setosa \n10          4.7         3.2          1.6         0.2 setosa \n# … with 17 more rows\n\n\n$metrics\n$metrics$training_metrics\n# A tibble: 2 × 3\n  centroid  size within_cluster_sum_of_squares\n     <dbl> <dbl>                         <dbl>\n1        1    87                         145. \n2        2    36                          38.3\n\n$metrics$validation_metrics\n# A tibble: 2 × 3\n  centroid  size within_cluster_sum_of_squares\n     <dbl> <dbl>                         <dbl>\n1        1    13                          35.7\n2        2    14                          13.4\n\n$metrics$cv_metric_summary\n# A tibble: 5 × 8\n  metric_name   mean    sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid cv_5_va…¹\n  <chr>        <dbl> <dbl>      <dbl>      <dbl>      <dbl>      <dbl>     <dbl>\n1 betweenss     62.5 12.4        66.7       72.7       71.6       42.5      59.1\n2 mse          NaN    0         NaN        NaN        NaN        NaN       NaN  \n3 rmse         NaN    0         NaN        NaN        NaN        NaN       NaN  \n4 tot_withinss  33.6  7.36       45.6       29.7       34.8       26.5      31.3\n5 totss         96.1 17.1       112.       102.       106.        69.0      90.3\n# … with abbreviated variable name ¹​cv_5_valid\n\n\n$original_data\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n[150 rows x 5 columns] \n\n$scree_data_tbl\n# A tibble: 2 × 2\n  centers   wss\n    <dbl> <dbl>\n1       1  745.\n2       2  226.\n\n$scoring_history_tbl\n# A tibble: 6 × 6\n  timestamp           duration     iterations number_of_clusters numbe…¹ withi…²\n  <chr>               <chr>             <dbl>              <dbl>   <dbl>   <dbl>\n1 2023-01-16 09:41:25 \" 0.241 sec\"          0                  0     NaN    NaN \n2 2023-01-16 09:41:25 \" 0.246 sec\"          1                  1     123   1003.\n3 2023-01-16 09:41:25 \" 0.247 sec\"          2                  1       0    488 \n4 2023-01-16 09:41:25 \" 0.250 sec\"          3                  2      26    311.\n5 2023-01-16 09:41:25 \" 0.251 sec\"          4                  2       2    184.\n6 2023-01-16 09:41:25 \" 0.251 sec\"          5                  2       0    183.\n# … with abbreviated variable names ¹​number_of_reassigned_observations,\n#   ²​within_cluster_sum_of_squares\n\n$model_summary_tbl\n# A tibble: 7 × 2\n  name                           value\n  <chr>                          <dbl>\n1 number_of_rows                  123 \n2 number_of_clusters                2 \n3 number_of_categorical_columns     0 \n4 number_of_iterations              5 \n5 within_cluster_sum_of_squares   183.\n6 total_sum_of_squares            488 \n7 between_cluster_sum_of_squares  305.\n\n\nNow lets take a look at the auto_kmeans_obj\n\noutput$auto_kmeans_obj\n\nModel Details:\n==============\n\nH2OClusteringModel: kmeans\nModel ID:  KMeans_model_R_1673880074548_1 \nModel Summary: \n  number_of_rows number_of_clusters number_of_categorical_columns\n1            123                  2                             0\n  number_of_iterations within_cluster_sum_of_squares total_sum_of_squares\n1                    5                     183.42511            488.00000\n  between_cluster_sum_of_squares\n1                      304.57489\n\n\nH2OClusteringMetrics: kmeans\n** Reported on training data. **\n\n\nTotal Within SS:  183.4251\nBetween SS:  304.5749\nTotal SS:  488 \nCentroid Statistics: \n  centroid     size within_cluster_sum_of_squares\n1        1 87.00000                     145.14706\n2        2 36.00000                      38.27805\n\nH2OClusteringMetrics: kmeans\n** Reported on validation data. **\n\n\nTotal Within SS:  49.05625\nBetween SS:  53.9303\nTotal SS:  102.9865 \nCentroid Statistics: \n  centroid     size within_cluster_sum_of_squares\n1        1 13.00000                      35.67618\n2        2 14.00000                      13.38007\n\nH2OClusteringMetrics: kmeans\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\n\nTotal Within SS:  167.8887\nBetween SS:  320.1113\nTotal SS:  488 \nCentroid statistics are not available.\nCross-Validation Metrics Summary: \n                  mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\nbetweenss    62.496490 12.417224  66.671760  72.679665  71.595474  42.470100\nmse                 NA  0.000000         NA         NA         NA         NA\nrmse                NA  0.000000         NA         NA         NA         NA\ntot_withinss 33.577747  7.357984  45.607327  29.705257  34.811970  26.512373\ntotss        96.074234 17.148642 112.279080 102.384926 106.407450  68.982475\n             cv_5_valid\nbetweenss     59.065456\nmse                  NA\nrmse                 NA\ntot_withinss  31.251806\ntotss         90.317260\n\n\nThe model id:\n\noutput$model_id\n\n[1] \"KMeans_model_R_1673880074548_1\"\n\n\nAnd finally the scree_plt.\n\noutput$scree_plt\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-17/index.html",
    "href": "posts/rtip-2023-01-17/index.html",
    "title": "Augmenting a Brownian Motion to a Time Series with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series analysis is a crucial tool for forecasting and understanding trends in various industries, including finance, economics, and engineering. However, traditional time series analysis methods can be limiting, and they may not always capture the complex dynamics of real-world data. That’s where the R package {healthyR.ts} comes in.\nThe {healthyR.ts} package is a powerful tool for time series analysis that offers a wide range of functions for cleaning, transforming, and analyzing time series data. One of its standout features is the ts_brownian_motion_augment() function, which allows you to add a brownian motion to a given time series dataset. This powerful tool can be used to simulate more realistic and complex scenarios, making it an invaluable tool for forecasters and data analysts.\nBrownian motion is a random walk process that can be used to model the movement of particles in a fluid. It has been widely used in mathematical finance, physics, and engineering to model the random movements of stock prices, pollutant concentrations, and other phenomena. By adding a brownian motion to a time series dataset, the ts_brownian_motion_augment() function allows users to capture the unpredictable and random nature of real-world data, making time series analysis more accurate and reliable.\nThe ts_brownian_motion_augment() function is easy to use and requires no prior knowledge of brownian motion or advanced mathematics. With just a few lines of code, users can quickly add a brownian motion to their time series dataset and begin analyzing the data with greater precision and confidence.\nThis set of functionality will be included in the next release which will be coming soon as it also speeds up the current ts_brownian_motion() function by 49x!\n\n\nFunction\nHere is the full function call.\n\nts_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = NULL\n)\n\nLet’s take a look at the arguments for the parameters.\n\n.data - The data.frame/tibble being augmented.\n.date_col - The column that holds the date.\n.value_col - The value that is going to get augmented. The last value of this column becomes the initial value internally.\n.time - How many time steps ahead.\n.num_sims - How many simulations should be run.\n.delta_time - Time step size.\n\n\n\nExample\nNow for an example.\n\nlibrary(tidyquant)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\ndf <- FANG %>%\n  filter(symbol == \"FB\") %>%\n  select(symbol, date, adjusted) %>%\n  filter_by_time(.date_var = date, .start_date = \"2016-01-01\") %>%\n  tq_mutate(select = adjusted, mutate_fun = periodReturn,\n            period = \"daily\", type = \"log\",\n            col_rename = \"daily_returns\")\n\nLet’s take a look at our initial data.\n\ndf\n\n# A tibble: 252 × 4\n   symbol date       adjusted daily_returns\n   <chr>  <date>        <dbl>         <dbl>\n 1 FB     2016-01-04    102.        0      \n 2 FB     2016-01-05    103.        0.00498\n 3 FB     2016-01-06    103.        0.00233\n 4 FB     2016-01-07     97.9      -0.0503 \n 5 FB     2016-01-08     97.3      -0.00604\n 6 FB     2016-01-11     97.5       0.00185\n 7 FB     2016-01-12     99.4       0.0189 \n 8 FB     2016-01-13     95.4      -0.0404 \n 9 FB     2016-01-14     98.4       0.0302 \n10 FB     2016-01-15     95.0      -0.0352 \n# … with 242 more rows\n\n\nNow let’s augment it with the brownian motion and see that data set before we visualize it.\n\ndf %>%\n  ts_brownian_motion_augment(\n    .date_col = date,\n    .num_sims = 50,\n    .value_col = daily_returns,\n    .delta_time = 0.00005\n  )\n\n# A tibble: 5,302 × 3\n   sim_number  date       daily_returns\n   <fct>       <date>             <dbl>\n 1 actual_data 2016-01-04       0      \n 2 actual_data 2016-01-05       0.00498\n 3 actual_data 2016-01-06       0.00233\n 4 actual_data 2016-01-07      -0.0503 \n 5 actual_data 2016-01-08      -0.00604\n 6 actual_data 2016-01-11       0.00185\n 7 actual_data 2016-01-12       0.0189 \n 8 actual_data 2016-01-13      -0.0404 \n 9 actual_data 2016-01-14       0.0302 \n10 actual_data 2016-01-15      -0.0352 \n# … with 5,292 more rows\n\n\nAs you see the function preserves the names of the input columns!\nNow, let’s see it!\n\ndf %>%\n  ts_brownian_motion_augment(\n    .date_col = date,\n    .num_sims = 50,\n    .value_col = daily_returns,\n    .delta_time = 0.00005\n  ) %>%\n  ggplot(aes(x = date, y = daily_returns\n             , group = sim_number, color = sim_number)) +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"FB Log Daily Returns for 2016\",\n    x = \"Date\",\n    y = \"Log Daily Returns\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-18/index.html",
    "href": "posts/rtip-2023-01-18/index.html",
    "title": "Geometric Brownian Motion with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nGeometric Brownian motion (GBM) is a widely used model in financial analysis for modeling the behavior of stock prices. It is a stochastic process that describes the evolution of a stock price over time, assuming that the stock price follows a random walk with a drift term and a volatility term.\nOne of the advantages of GBM is that it can capture the randomness and volatility of stock prices, which is a key feature of financial markets. GBM can also be used to estimate the expected return and volatility of a stock, which are important inputs for financial decision making.\nAnother advantage of GBM is that it can be used to generate simulations of future stock prices. These simulations can be used to estimate the probability of different outcomes, such as the probability of a stock price reaching a certain level in the future. This can be useful for risk management and for evaluating investment strategies.\nGBM is also very easy to implement, making it a popular choice among financial analysts and traders.\nThe equation for GBM is: \\[\ndS(t) = μS(t)dt + σS(t)dW(t)\n\\] Where:\n\\(dS(t)\\) is the change in the stock price at time \\(t\\)\n\\(S(t)\\) is the stock price at time \\(t\\)\n\\(μ\\) is the expected return of the stock\n\\(σ\\) is the volatility of the stock\n\\(dW(t)\\) is a Wiener process (a random variable that describes the rate of change of a random variable over time)\nIt’s important to keep in mind that GBM is a model and not always a perfect fit to real-world stock prices. However, it’s a widely accepted model due to its capability to captures the key characteristics of stock prices and its mathematical tractability.\nAttention R users! Are you looking for a reliable and accurate way to model stock prices? We have some exciting news for you! The next release of the R package {healthyR.ts} will include a new function, ts_geometric_brownian_motion(). This powerful function utilizes the geometric Brownian motion model to simulate stock prices, providing you with valuable insights and predictions for your financial analysis.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\nNow let’s go over the arguments to the parameters.\n\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.initial_value - Integer representing the initial value.\n.delta_time - Time step size.\n.return_tibble The default is TRUE. If set to FALSE then an object of class matrix will be returned.\n\n\n\nExample\nLet’s go over a few examples.\n\nlibrary(healthyR.ts)\n\nts_geometric_brownian_motion()\n\n# A tibble: 2,600 × 3\n   sim_number        t     y\n   <fct>         <int> <dbl>\n 1 sim_number 1      1   100\n 2 sim_number 2      1   100\n 3 sim_number 3      1   100\n 4 sim_number 4      1   100\n 5 sim_number 5      1   100\n 6 sim_number 6      1   100\n 7 sim_number 7      1   100\n 8 sim_number 8      1   100\n 9 sim_number 9      1   100\n10 sim_number 10     1   100\n# … with 2,590 more rows\n\n\nNow without returning a tibble object.\n\nts_geometric_brownian_motion(.num_sims = 5, .return_tibble = FALSE)\n\n      sim_number 1 sim_number 2 sim_number 3 sim_number 4 sim_number 5\n [1,]    100.00000     100.0000    100.00000    100.00000     100.0000\n [2,]    101.04170     100.6583    100.46420     99.68513     100.3776\n [3,]    101.58155     100.8959    100.03621     98.91656     101.5732\n [4,]    100.91680     100.7494     99.47735     98.57117     101.1525\n [5,]     99.96787     101.3298     98.70899     99.03101     101.1557\n [6,]     99.29069     101.4187     98.32176     98.33018     101.5584\n [7,]     99.40451     101.5124     98.26237     97.79356     101.4934\n [8,]     99.35345     101.0328     98.69587     97.46604     101.9630\n [9,]     97.94177     100.9534     98.32630     96.95231     102.1643\n[10,]     97.95812     101.3813     98.36934     96.64048     101.8546\n[11,]     98.47820     101.8262     98.21492     96.12851     102.5529\n[12,]     99.53016     102.5522     97.92270     95.97443     102.8912\n[13,]     98.82850     102.7482     96.66348     96.26008     103.1899\n[14,]     99.87335     102.9351     96.69635     96.15058     103.9259\n[15,]    101.03605     103.3796     96.60162     96.63562     103.3790\n[16,]    101.83475     103.1900     97.63875     96.00162     103.0422\n[17,]    102.10155     103.5851     97.12873     95.99579     103.0913\n[18,]    102.16085     103.2966     96.26772     95.95174     103.7034\n[19,]    102.35736     103.7429     96.37355     96.02805     102.8406\n[20,]    102.49297     104.5301     96.44318     96.28293     103.3507\n[21,]    102.36953     105.1809     96.87639     97.32625     104.0307\n[22,]    103.30672     104.7480     96.90017     97.16507     104.0751\n[23,]    103.55433     104.9848     97.40063     97.49375     102.6901\n[24,]    103.44429     104.3553     97.35982     97.39390     102.8163\n[25,]    103.23952     102.9840     97.30287     97.66737     103.2160\n[26,]    103.48365     103.6117     97.96290     97.91773     103.0579\nattr(,\".time\")\n[1] 25\nattr(,\".num_sims\")\n[1] 5\nattr(,\".mean\")\n[1] 0\nattr(,\".sigma\")\n[1] 0.1\nattr(,\".initial_value\")\n[1] 100\nattr(,\".delta_time\")\n[1] 0.002739726\nattr(,\".return_tibble\")\n[1] FALSE\n\n\nLet’s visualize the GBM at different levels of volatility.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ngbm <- rbind(\n  ts_geometric_brownian_motion(.sigma = 0.05) %>%\n    mutate(volatility = as.factor(\"A) Sigma = 5%\")),\n  ts_geometric_brownian_motion(.sigma = 0.1) %>%\n    mutate(volatility = as.factor(\"B) Sigma = 10%\")),\n  ts_geometric_brownian_motion(.sigma = .15) %>%\n    mutate(volatility = as.factor(\"C) Sigma = 15%\")),\n  ts_geometric_brownian_motion(.sigma = .2) %>%\n    mutate(volatility = as.factor(\"D) Sigma = 20%\"))\n)\n\ngbm %>%\n  ggplot(aes(x = t, y = y, group = sim_number, color = sim_number)) + \n  facet_wrap(~ volatility, scales = \"free\") +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-19/index.html",
    "href": "posts/rtip-2023-01-19/index.html",
    "title": "Boilerplate XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nXGBoost, short for “eXtreme Gradient Boosting,” is a powerful and popular machine learning library that is specifically designed for gradient boosting. It is an open-source library and is available in many programming languages, including R.\nGradient boosting is a technique that combines the predictions of multiple weak models to create a strong, more accurate model. XGBoost is an optimized version of gradient boosting that is designed to run faster and more efficiently than other implementations.\nLet’s take a look at a simple example of how to use XGBoost in R. We will use the iris dataset, a well-known dataset that contains 150 observations of iris flowers, each with four features (sepal length, sepal width, petal length, and petal width) and one target variable (the species of iris). Our goal is to train a model to predict the species of an iris flower based on its features.\nFirst, we need to install the “xgboost” package in R:\n\ninstall.packages(\"xgboost\")\n\nNext, we load the iris dataset and split it into training and test sets:\n\ndata(iris)\nset.seed(123)\nindices <- sample(1:nrow(iris), 0.8*nrow(iris))\ntrain_data <- iris[indices, 1:4]\ntrain_label <- iris[indices, 5]\ntest_data <- iris[-indices, 1:4]\ntest_label <- iris[-indices, 5]\n\nNow we can train our XGBoost model:\n\nlibrary(xgboost)\nxgb_model <- xgboost(\n  data = train_data, \n  label = train_label, \n  nrounds = 100, \n  objective = \"multi:softmax\", \n  num_class = 3\n  )\n\nHere, we specified the training data, labels, number of rounds (iterations) to run, the objective (multiclass classification) and the number of classes.\nFinally, we can use the trained model to make predictions on the test set:\n\npredictions <- predict(xgb_model, test_data)\n\nWe can also evaluate the performance of our model by comparing the predicted labels to the true labels using metrics such as accuracy:\n\naccuracy <- mean(predictions == test_label)\n\nIn this example, we used XGBoost to train a model to predict the species of iris flowers based on their features. We saw that XGBoost is a powerful and efficient library for gradient boosting, and it can be easily integrated into a R script.\nKeep in mind that this is a simple example, and in real-world scenarios, more preprocessing and parameter tuning is necessary to achieve optimal performance. Also, the dataset is small, and the number of rounds used is also small, which is not ideal for real-world scenarios. But this example shows the basic usage of XGBoost in R.\nOk, so, what’s the point? Is there a possibly easier way to do this…yes! You can use the boilerplace function hai_auto_xgboost() and it’s data prep helper hai_xgboost_data_prepper() from the {healthyR.ai} library. Let’s see how that works.\n\n\nFunction\nHere is the data prepper function and it’s arguments.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\nHere is the boilerplate function\n\nhai_auto_xgboost(\n  .data,\n  .rec_obj,\n  .splits_obj = NULL,\n  .rsamp_obj = NULL,\n  .tune = TRUE,\n  .grid_size = 10,\n  .num_cores = 1,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\"\n)\n\nHere are it’s arguments.\n\n.data - The data being passed to the function. The time-series object.\n.rec_obj - This is the recipe object you want to use. You can use hai_xgboost_data_prepper() an automatic recipe_object.\n.splits_obj - NULL is the default, when NULL then one will be created.\n.rsamp_obj - NULL is the default, when NULL then one will be created. It will default to creating an rsample::mc_cv() object.\n.tune - Default is TRUE, this will create a tuning grid and tuned workflow\n.grid_size - Default is 10\n.num_cores - Default is 1\n.best_metric - Default is “f_meas”. You can choose a metric depending on the model_type used. If regression then see hai_default_regression_metric_set(), if classification then see hai_default_classification_metric_set().\n.model_type - Default is classification, can also be regression.\n\n\n\nExample\nLet’s take a look at an example and it’s output. This is using {parsnip} under the hood.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_xgboost_data_prepper(data, Species ~ .)\n\nauto_xgb <- hai_auto_xgboost(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .num_cores = 1\n)\n\nThere are three main outputs to this function, which are:\n\nrecipe_info\nmodel_info\ntuned_info\n\nLet’s take a look at each. First the recipe_info\n\nauto_xgb$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nNow the model_info\n\nauto_xgb$model_info\n\n$model_spec\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune::tune()\n  min_n = tune::tune()\n  tree_depth = tune::tune()\n  learn_rate = tune::tune()\n  loss_reduction = tune::tune()\n  sample_size = tune::tune()\n\nComputational engine: xgboost \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune::tune()\n  min_n = tune::tune()\n  tree_depth = tune::tune()\n  learn_rate = tune::tune()\n  loss_reduction = tune::tune()\n  sample_size = tune::tune()\n\nComputational engine: xgboost \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 2.5 Mb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.10962507492329, max_depth = 13L, \n    gamma = 0.000498577409120534, colsample_bytree = 1, colsample_bynode = 1, \n    min_child_weight = 3L, subsample = 0.594320066112559), data = x$data, \n    nrounds = 1240L, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"multi:softprob\", num_class = 3L)\nparams (as set within xgb.train):\n  eta = \"0.10962507492329\", max_depth = \"13\", gamma = \"0.000498577409120534\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"3\", subsample = \"0.594320066112559\", nthread = \"1\", objective = \"multi:softprob\", num_class = \"3\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 4 \nniter: 1240\nnfeatures : 4 \nevaluation_log:\n    iter training_mlogloss\n       1        0.96929822\n       2        0.85785438\n---                       \n    1239        0.07815044\n    1240        0.07808817\n\n$was_tuned\n[1] \"tuned\"\n\nNow the tuned_info\n\nauto_xgb$tuned_info\n\n$tuning_grid\n# A tibble: 10 × 6\n   trees min_n tree_depth learn_rate loss_reduction sample_size\n   <int> <int>      <int>      <dbl>          <dbl>       <dbl>\n 1   926     6          2    0.0246        2.21e- 1       0.952\n 2  1510    25         14    0.00189       1.01e+ 1       0.424\n 3  1077    29          9    0.195         1.34e- 5       0.319\n 4   795    32          3    0.00102       1.64e- 3       0.686\n 5   368    22          4    0.00549       2.97e- 7       0.735\n 6  1240     3         13    0.110         4.99e- 4       0.594\n 7  1839    18          5    0.0501        1.67e- 7       0.273\n 8   139    11         10    0.0153        1.17e- 2       0.483\n 9   470    40          8    0.0906        6.79e-10       0.168\n10  1732    16         11    0.00667       9.19e- 9       0.883\n\n$cv_obj\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n$tuned_results\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics            .notes          \n   <list>          <chr>      <list>              <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 10]> <tibble [1 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 10]> <tibble [1 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 10]> <tibble [1 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 10]> <tibble [1 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 10]> <tibble [1 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 10]> <tibble [1 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 10]> <tibble [1 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 10]> <tibble [1 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 10]> <tibble [1 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 10]> <tibble [1 × 3]>\n# … with 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n$grid_size\n[1] 10\n\n$best_metric\n[1] \"f_meas\"\n\n$best_result_set\n# A tibble: 1 × 12\n  trees min_n tree_depth learn_rate loss_r…¹ sampl…² .metric .esti…³  mean     n\n  <int> <int>      <int>      <dbl>    <dbl>   <dbl> <chr>   <chr>   <dbl> <int>\n1  1240     3         13      0.110 0.000499   0.594 f_meas  macro   0.944    25\n# … with 2 more variables: std_err <dbl>, .config <chr>, and abbreviated\n#   variable names ¹​loss_reduction, ²​sample_size, ³​.estimator\n\n$tuning_grid_plot\n\n\n\n\nTuning Grid\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-23/index.html",
    "href": "posts/rtip-2023-01-23/index.html",
    "title": "ADF and Phillips-Perron Tests for Stationarity using lists",
    "section": "",
    "text": "Introduction\nA time series is a set of data points collected at regular intervals of time. Sometimes, the data points in a time series change over time in a predictable way. This is called a stationary time series. Other times, the data points change in an unpredictable way. This is called a non-stationary time series.\nImagine you are playing a game of catch with a friend. If you throw the ball back and forth at the same speed and distance, that’s like a stationary time series. But if you keep throwing the ball harder and farther, that’s like a non-stationary time series.\nThere are two tests that we can use to see if a time series is stationary or non-stationary. The first test is called the ADF test, which stands for Augmented Dickey-Fuller test. The second test is called the Phillips-Perron test.\nThe ADF test looks at the data points and checks to see if the average value of the data points is the same over time. If the average value is the same, then the time series is stationary. If the average value is not the same, then the time series is non-stationary.\nThe Phillips-Perron test is similar to the ADF test, but it is a bit more advanced. It checks to see if the data points are changing in a predictable way. If the data points are changing in a predictable way, then the time series is stationary. If the data points are changing in an unpredictable way, then the time series is non-stationary.\nSo, in short, The ADF test checks if the mean of the time series is constant over time and Phillips-Perron test checks if the variance of the time series is constant over time.\nNow, you can use these two tests to see if the time series you are studying is stationary or non-stationary, just like how you can use the game of catch to see if your throws are the same or different.\n\n\nFunction\nTo perform these test we can use two libraries, one is the {tseries} library for the adf.test() and the other is the {aTSA} for the pp.test()\nLet’s see some examples.\n\n\nExamples\nLet’s first make our time series obejcts and place them in a list.\n\nlibrary(tseries)\nlibrary(aTSA)\n\n# create time series objects\nts1 <- ts(rnorm(100), start = c(1990,1), frequency = 12)\nts2 <- ts(rnorm(100), start = c(1995,1), frequency = 12)\nts3 <- ts(rnorm(100), start = c(2000,1), frequency = 12)\n\n# create list of time series\nts_list <- list(ts1, ts2, ts3)\n\nNow let’s make our functions.\n\n# function to test for stationarity\nadf_is_stationary <- function(x) {\n  adf.test(x)$p.value > 0.05\n}\n\npp_is_stationary <- function(x) {\n  pp_df <- pp.test(x) |> as.data.frame() \n  pp_df$p.value > 0.05\n}\n\nTime to use lapply()!\n\n# apply function to each time series in list\nlapply(ts_list, adf_is_stationary)\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -10.97    0.01\n[2,]   1  -7.70    0.01\n[3,]   2  -5.23    0.01\n[4,]   3  -4.05    0.01\n[5,]   4  -4.03    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -11.48    0.01\n[2,]   1  -8.32    0.01\n[3,]   2  -5.81    0.01\n[4,]   3  -4.59    0.01\n[5,]   4  -4.63    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -11.42    0.01\n[2,]   1  -8.28    0.01\n[3,]   2  -5.77    0.01\n[4,]   3  -4.56    0.01\n[5,]   4  -4.59    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -10.60    0.01\n[2,]   1  -7.88    0.01\n[3,]   2  -5.96    0.01\n[4,]   3  -5.26    0.01\n[5,]   4  -4.90    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -10.64    0.01\n[2,]   1  -7.98    0.01\n[3,]   2  -6.08    0.01\n[4,]   3  -5.41    0.01\n[5,]   4  -5.08    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -10.58    0.01\n[2,]   1  -7.94    0.01\n[3,]   2  -6.06    0.01\n[4,]   3  -5.39    0.01\n[5,]   4  -5.07    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -9.19    0.01\n[2,]   1 -6.65    0.01\n[3,]   2 -5.41    0.01\n[4,]   3 -5.33    0.01\n[5,]   4 -4.77    0.01\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -9.14    0.01\n[2,]   1 -6.62    0.01\n[3,]   2 -5.39    0.01\n[4,]   3 -5.30    0.01\n[5,]   4 -4.75    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -9.11    0.01\n[2,]   1 -6.59    0.01\n[3,]   2 -5.36    0.01\n[4,]   3 -5.29    0.01\n[5,]   4 -4.73    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\n[[1]]\nlogical(0)\n\n[[2]]\nlogical(0)\n\n[[3]]\nlogical(0)\n\nlapply(ts_list, pp_is_stationary)\n\nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3  -111    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3  -110    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3  -110    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3  -101    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3  -101    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3  -101    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3 -92.9    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3 -92.9    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3   -93    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \n\n\n[[1]]\n[1] FALSE FALSE FALSE\n\n[[2]]\n[1] FALSE FALSE FALSE\n\n[[3]]\n[1] FALSE FALSE FALSE\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-24/index.html",
    "href": "posts/rtip-2023-01-24/index.html",
    "title": "Making Non Stationary Data Stationary",
    "section": "",
    "text": "Introduction\nIn the most basic sense for time series, a series is stationary if the properties of the generating process (the process that generates the data) do not change over time, the process remains constant. This does not mean the data does not change, it simply means the process does not change. You can bake a vanilla cake or a chocolate cake but you still cook it in the oven.\nA non-stationary time series is like a toy car that doesn’t run in a straight line. Sometimes it goes fast and sometimes it goes slow, so it’s hard to predict what it will do next. But, just like how you can fix a toy car by adjusting it, we can fix a non-stationary time series by making it “stationary.”\nOne way we can do this is by taking the difference in the time series vector. This is like taking the toy car apart and looking at how each piece moves. By subtracting one piece from another, we can see if they are moving at the same speed or not. If they are not, we can adjust them so they are moving at the same speed. This makes it easier to predict what the toy car will do next because it’s moving at a steady pace.\nAnother way we can make a non-stationary time series stationary is by taking the second difference of the log of the data. This is like looking at the toy car from a different angle. By taking the log of the data, we can see how much each piece has changed over time. Then, by taking the second difference, we can see if the changes are happening at the same rate or not. If they are not, we can adjust them so they are happening at the same rate.\nIn simple terms, these methods help to stabilize the time series by making the data move at a consistent speed, which allows for better predictions.\nIn summary, a non-stationary time series is like a toy car that doesn’t run in a straight line. By taking the difference in the time series vector or taking the second difference of the log of the data, we can fix the toy car and make it run in a straight line. This is helpful for making accurate predictions.\n\n\nFunction\nWe are going to use the adf.test() function from the {aTSA} library. Here is the function:\n\nadf.test(x, nlag = NULL, output = TRUE)\n\nHere are the arugments to the parameters.\n\nx- a numeric vector or time series.\nalternative - the lag order with default to calculate the test statistic. See details for the default.\noutput - a logical value indicating to print the test results in R console. The default is TRUE.\n\n\n\nExamples\nAs an example, we are going to use the R built in data set AirPassengers as our timeseries. This data is both cyclical and trending so it is good for this purpose.\n\nlibrary(aTSA)\n\nplot(AirPassengers)\n\n\n\n\nNow that we know what it looks like, lets see if it is stationary right off the bat.\n\nadf.test(AirPassengers)\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag      ADF p.value\n[1,]   0  0.04712   0.657\n[2,]   1 -0.35240   0.542\n[3,]   2 -0.00582   0.641\n[4,]   3  0.26034   0.718\n[5,]   4  0.82238   0.879\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -1.748   0.427\n[2,]   1 -2.345   0.194\n[3,]   2 -1.811   0.402\n[4,]   3 -1.536   0.509\n[5,]   4 -0.986   0.701\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -4.64    0.01\n[2,]   1 -7.65    0.01\n[3,]   2 -7.09    0.01\n[4,]   3 -6.94    0.01\n[5,]   4 -5.95    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nSo we can see that right off the bat that “Type 1” and “Type 2” fail as there is significant trend in this data as we can plainly see. Let’s see what happens when we take a simpmle diff() of the series.\n\nplot(diff(AirPassengers))\n\n\n\n\nLooking like its still going to fail, but let’s run the test anyways.\n\nadf.test(diff(AirPassengers))\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -8.58    0.01\n[2,]   1 -8.68    0.01\n[3,]   2 -8.13    0.01\n[4,]   3 -8.48    0.01\n[5,]   4 -6.59    0.01\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -8.58    0.01\n[2,]   1 -8.69    0.01\n[3,]   2 -8.17    0.01\n[4,]   3 -8.60    0.01\n[5,]   4 -6.70    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -8.55    0.01\n[2,]   1 -8.66    0.01\n[3,]   2 -8.14    0.01\n[4,]   3 -8.57    0.01\n[5,]   4 -6.69    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nThe adf.test comes back with a p.value <= 0.01 as the data is no longer presenting a trend, but as we can plainly see, the data has non constant variance overtime which we know we need. Here we will use the {TidyDensity} package to use the cvar() (cumulative variance) function to see the ongoing variance.\n\nlibrary(TidyDensity)\n\nplot(cvar(diff(AirPassengers)), type = \"l\")\n\n\n\n\nReject the null that the data is stationary. So lets proceed with a diff diff log of the data and see what we get. First let’s visualize.\n\nplot(diff(diff(log(AirPassengers))))\n\n\n\nplot(cvar(diff(diff(log(AirPassengers)))), type = \"l\")\n\n\n\n\nLooking good!\n\nadf.test(diff(diff(log(AirPassengers))))\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -15.90    0.01\n[2,]   1 -12.78    0.01\n[3,]   2  -9.28    0.01\n[4,]   3 -10.76    0.01\n[5,]   4  -9.72    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -15.85    0.01\n[2,]   1 -12.73    0.01\n[3,]   2  -9.24    0.01\n[4,]   3 -10.73    0.01\n[5,]   4  -9.68    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -15.79    0.01\n[2,]   1 -12.68    0.01\n[3,]   2  -9.21    0.01\n[4,]   3 -10.68    0.01\n[5,]   4  -9.64    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nVoila!\n\n\nReferences\n\nhttps://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322\nhttps://www.statology.org/dickey-fuller-test-in-r/"
  },
  {
    "objectID": "posts/rtip-2023-01-25/index.html",
    "href": "posts/rtip-2023-01-25/index.html",
    "title": "Simplifying List Filtering in R with purrr’s keep()",
    "section": "",
    "text": "Introduction\nThe {purrr} package in R is a powerful tool for working with lists and other data structures. One particularly useful function in the package is keep(), which allows you to filter a list by keeping only the elements that meet certain conditions.\nThe keep() function takes two arguments: the list to filter, and a function that returns a logical value indicating whether each element of the list should be kept. The function can be specified as an anonymous function or a named function, and it should take a single argument (the current element of the list).\nFor example, let’s say we have a list of numbers and we want to keep only the even numbers. We could use the keep() function with an anonymous function that checks the remainder of the current element divided by 2:\n\nlibrary(purrr)\n\nnumbers <- c(1, 2, 3, 4, 5, 6)\neven_numbers <- keep(numbers, function(x) x %% 2 == 0)\neven_numbers\n\n[1] 2 4 6\n\n\nWe see that this keeps [1] 2 4 6.\nThe purrr package also provides a convenient shorthand for this operation, .p, which can be used inside the keep function to return the element.\n\neven_numbers <- keep(numbers, ~ .x %% 2 == 0)\neven_numbers\n\n[1] 2 4 6\n\n\nYou can also use the keep() function to filter a list of other types of objects, such as strings or lists. For example, you could use it to keep only the strings that are longer than a certain length:\n\nwords <- c(\"cat\", \"dog\", \"elephant\", \"bird\")\nlong_words <- keep(words, function(x) nchar(x) > 4)\nlong_words\n\n[1] \"elephant\"\n\n\nWe see that this keeps “elephant” & “bird”.\nIn summary, the {purrr} package’s keep() function is a powerful tool for filtering lists in R, and the .p parameter can be used as a shorthand. It can be used to keep only the items in a list that meet a user-given condition, and it can be used with a variety of data types.\n\n\nFunction\nHere is the keep() function and it’s parameters.\n\nkeep(.x, .p, ...)\n\nHere are the arguments to the parameters.\n\n.x - A list or vector.\n.p - A predicate function (i.e. a function that returns either TRUE or FALSE) specified in one of the following ways:\n\nA named function, e.g. is.character.\nAn anonymous function, e.g. \\(x) all(x < 0) or function(x) all(x < 0).\nA formula, e.g. ~ all(.x < 0). You must use .x to refer to the first argument). Only recommended if you require backward compatibility with older versions of R.\n\n... - Additional arguments passed on to .p.\n\n\n\nExamples\nI recently came across wanting to filter a list that is given as an argument to a parameter. The function I am working for my upcoming {tidyAML} package has a function called create_workflow_set() that has a parameter .recipe_list which is set to list(). The user must only place recipes in this list or else I want it to fail. So I was able to write a quick check using keep() like so:\n\n# Checks ----\n# only keep() recipes\nrec_list <- purrr::keep(rec_list, ~ inherits(.x, \"recipe\"))\n\nVoila!"
  },
  {
    "objectID": "posts/tidydensity-20221007/index.html",
    "href": "posts/tidydensity-20221007/index.html",
    "title": "TidyDensity Primer",
    "section": "",
    "text": "This is going to serve as a sort of primer for the {TidyDensity} package.\nThe goal of {TidyDensity} is to make working with random numbers from different distributions easy. All tidy_ distribution functions provide the following components:\n\n[r_]\n[d_]\n[q_]\n[p_]\n\n\nInstallation\nYou can install the released version of {TidyDensity} from CRAN with:\ninstall.packages(\"TidyDensity\")\nAnd the development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/TidyDensity\")\n\n\nExample Data\nThis is a basic example which shows you how to solve a common problem, which is, how do we generate randomly generated data from a normal distribution of some mean, and some standard deviation with n points and sims number of simulations?\nWith the function tidy_normal() we can generate such data. All functions that are condsidered tidy_ distribution functions, meaning those that generate randomly generated data from some distribution, have the same API call structure.\nFor example, using tidy_normal() the full function call at it’s default is as follows:\ntidy_normal(.n = 50, .mean = 0, .sd = 1, .num_sims = 1).\nWhat this means is that we want to generate 50 points from a standard normal distribution of mean 0 and with a standard deviation of 1, and we want to generate a single simulation of this data.\nLet’s see an example below:\n\nsuppressPackageStartupMessages(library(TidyDensity))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(ggplot2))\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nset.seed(123)\ntidy_normal()\n\n# A tibble: 50 × 7\n   sim_number     x       y    dx       dy     p       q\n   <fct>      <int>   <dbl> <dbl>    <dbl> <dbl>   <dbl>\n 1 1              1 -0.560  -3.11 0.000256 0.288 -0.560 \n 2 1              2 -0.230  -2.98 0.000691 0.409 -0.230 \n 3 1              3  1.56   -2.85 0.00167  0.940  1.56  \n 4 1              4  0.0705 -2.72 0.00362  0.528  0.0705\n 5 1              5  0.129  -2.59 0.00707  0.551  0.129 \n 6 1              6  1.72   -2.45 0.0125   0.957  1.72  \n 7 1              7  0.461  -2.32 0.0201   0.678  0.461 \n 8 1              8 -1.27   -2.19 0.0298   0.103 -1.27  \n 9 1              9 -0.687  -2.06 0.0415   0.246 -0.687 \n10 1             10 -0.446  -1.93 0.0552   0.328 -0.446 \n# … with 40 more rows\n\n\nWhat comes back we see is a tibble. This is true for all functions in the {TidyDensity} library. It was a goal to return items that are consistent with the tidyverse.\nNow let’s talk a bit about what was actually returned. There are a few columns that are returned, these are referred to as the r, d, p, and q\n\n[r_] Shows as y and is the randomly generated data from the underlying distribution.\n[d_] Two components come back, dx and dy where these are generated from the [stats::density()] function with n set to .n from the function input.\n[p_] Shows as p and is the results of the p_ function, in this case pnorm() where the x of the input goes from 0-1 with .n points.\n[q_] Shows as q and is the results of the q_ function, in this case qnorm() where the x of the input goes from 0-1 with .n points.\n\nNow you will also see two more columns, namely, sim_number a factor column and x an integer column. The sim_number column represents the current simulation for which data was drawn, and the x represents the nth point in that simulation.\n\n\nVisualization Example\nWith data typically comes the need to see it! Show me the data! TidyDensity has a variety of autoplot functionality that will present only data from a tidy_ distribution function. We will take a look at output from tidy_normal() and set a see otherwise everytime this site is rendered the data would change.\n\nset.seed(123)\ntn <- tidy_normal(.n = 100, .num_sims = 6)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe can see that the plots are faily informative. There are the regular density plot, the quantile plot, probability and qq plots. The title and subtitle of these plots are generated from attributes that are attached to the output of the tidy_ distribution function. Let’s take a look at the attributes of tn\n\nattributes(tn)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n[235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n[253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n[271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n[289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n[307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n[325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n[343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n[361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n[379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n[397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n[415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n[433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n[451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n[469] 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n[487] 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n[505] 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n[523] 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n[541] 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n[559] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n[577] 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n[595] 595 596 597 598 599 600\n\n$names\n[1] \"sim_number\" \"x\"          \"y\"          \"dx\"         \"dy\"        \n[6] \"p\"          \"q\"         \n\n$distribution_family_type\n[1] \"continuous\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 1\n\n$.n\n[1] 100\n\n$.num_sims\n[1] 6\n\n$tibble_type\n[1] \"tidy_gaussian\"\n\n$ps\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$qs\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$param_grid\n# A tibble: 1 × 2\n  .mean   .sd\n  <dbl> <dbl>\n1     0     1\n\n$param_grid_txt\n[1] \"c(0, 1)\"\n\n$dist_with_params\n[1] \"Gaussian c(0, 1)\"\n\n\nI won’t go over them but as you can see, the attribute list can get long and has a lot of great information in it.\nNow what if we have simulations over 9? The legend would get fairly large making the visualization difficult to read.\nLet’s take a look at 20 simulations.\n\ntn <- tidy_normal(.n = 100, .num_sims = 20)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe see that the legend disappears! That’s great, but what if we still want to see what simulation is what? Well, make the plot interactive!\n\ntidy_autoplot(tn, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-2022-12-23/index.html",
    "href": "posts/weekly-rtip-2022-12-23/index.html",
    "title": "Simulating Time Series Model Forecasts with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series models are a powerful tool for forecasting future values of a time-dependent variable. These models are commonly used in a variety of fields, including finance, economics, and engineering, to predict future outcomes based on past data.\nOne important aspect of time series modeling is the ability to simulate model forecasts. This allows us to evaluate the performance of different forecasting methods and to compare the results of different models. Simulating forecasts also allows us to assess the uncertainty associated with our predictions, which can be especially useful when making important decisions based on the forecast.\nThere are several benefits to simulating time series model forecasts:\n\nImproved accuracy: By simulating forecasts, we can identify the best forecasting method for a given time series and optimize its parameters. This can lead to more accurate forecasts, especially for long-term predictions.\nEnhanced understanding: Simulating forecasts helps us to understand how different factors, such as seasonality and trend, affect the prediction. This understanding can inform our decision-making and allow us to make more informed predictions.\nImproved communication: Simulating forecasts allows us to present the uncertainty associated with our predictions, which can be useful for communicating the potential risks and benefits of different courses of action.\n\nThe R package {healthyR.ts} includes a function called ts_forecast_simulator() that can be used to simulate time series model forecasts. This function allows users to specify the forecasting method, the number of simulations to run, and the length of the forecast horizon. It also provides options for visualizing the results, including plots of the forecast distribution and summary statistics such as the mean and standard deviation of the forecasts.\nIn summary, simulating time series model forecasts is a valuable tool for improving the accuracy and understanding of our predictions, as well as for communicating the uncertainty associated with these forecasts. The ts_forecast_simulator() function in the {healthyR.ts} package is a useful tool for performing these simulations in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_forecast_simulator(\n  .model,\n  .data,\n  .ext_reg = NULL,\n  .frequency = NULL,\n  .bootstrap = TRUE,\n  .horizon = 4,\n  .iterations = 25,\n  .sim_color = \"steelblue\",\n  .alpha = 0.05\n)\n\nNow let’s take a look at the arguments that get provided to the parameters.\n\n.model - A forecasting model of one of the following from the forecast package:\n\nArima\nauto.arima\nets\nnnetar\nArima() with xreg\n\n.data - The data that is used for the .model parameter. This is used with timetk::tk_index()\n.ext_reg - A tibble or matrix of future xregs that should be the same length as the horizon you want to forecast.\n.frequency - This is for the conversion of an internal table and should match the time frequency of the data.\n.bootstrap - A boolean value of TRUE/FALSE. From forecast::simulate.Arima() Do simulation using resampled errors rather than normally distributed errors.\n.horizon - An integer defining the forecast horizon.\n.iterations - An integer, set the number of iterations of the simulation.\n.sim_color - Set the color of the simulation paths lines.\n.alpha - Set the opacity level of the simulation path lines.\n\nGreat, now let’s take a look at examples.\n\n\nExamples\n\nlibrary(healthyR.ts)\nlibrary(forecast)\n\nfit <- auto.arima(AirPassengers)\ndata_tbl <- ts_to_tbl(AirPassengers)\n\n# Simulate 50 possible forecast paths, with .horizon of 12 months\noutput <- ts_forecast_simulator(\n  .model        = fit\n  , .horizon    = 12\n  , .iterations = 50\n  , .data       = data_tbl\n)\n\nOk, so now we have our output object, which is a list object. Let’s see what it contains.\n\n\n\nForecast Simulation Output\n\n\nNow, let’s explore each element.\nFirst the forecast simulation data.\n\noutput$forecast_sim\n\n            sim_1    sim_2    sim_3    sim_4    sim_5    sim_6    sim_7\nJan 1961 445.7399 434.7175 462.6173 447.8849 453.5069 443.0944 464.4749\nFeb 1961 424.1606 423.3814 431.9512 420.4460 426.8245 423.4784 443.1545\nMar 1961 444.0015 467.6276 459.3452 450.2936 453.6116 461.3751 462.4351\nApr 1961 490.2370 504.9129 492.0632 491.2198 494.6883 489.6674 498.7902\nMay 1961 502.0907 517.7794 504.5420 503.7614 504.8544 504.1816 521.9044\nJun 1961 552.6359 588.4650 560.1199 567.5266 563.5353 552.8411 543.5723\nJul 1961 655.1872 666.8450 642.9804 653.4262 646.1559 648.1923 639.6587\nAug 1961 633.4657 635.5080 644.1405 650.1333 631.2688 630.4410 623.3784\nSep 1961 536.3259 551.6068 548.9176 554.8289 533.3672 553.5334 529.2182\nOct 1961 486.2815 513.2851 514.8842 513.2190 481.4636 495.1295 490.7402\nNov 1961 406.9061 428.0550 437.2211 435.3443 426.4892 425.9885 416.1940\nDec 1961 454.1048 478.1804 477.3923 475.0052 504.0761 486.3584 472.2933\n            sim_8    sim_9   sim_10   sim_11   sim_12   sim_13   sim_14\nJan 1961 444.3152 444.3203 453.5722 438.0253 438.0253 425.2956 458.6829\nFeb 1961 418.1653 418.2569 423.9229 413.1814 404.0688 403.3318 445.9250\nMar 1961 450.8166 437.2694 452.5463 433.7963 450.5365 433.6445 448.2279\nApr 1961 489.4801 501.5545 504.4428 488.2243 503.9232 476.7711 495.1634\nMay 1961 499.7523 499.2059 508.6748 496.3915 520.8837 489.1480 502.1013\nJun 1961 562.7791 574.2660 571.6261 560.3126 578.8797 564.5855 581.8097\nJul 1961 653.2719 655.1924 655.0359 647.2704 664.1505 660.7023 654.6549\nAug 1961 649.6533 639.4611 655.1187 610.5014 661.2034 606.4399 654.0751\nSep 1961 542.5110 548.6498 560.1903 505.3303 552.4413 515.2890 547.9581\nOct 1961 505.5032 490.4686 495.1026 464.1546 503.7735 479.0465 501.3405\nNov 1961 423.0761 414.9520 445.9182 399.1634 435.6692 405.7617 447.0409\nDec 1961 454.4311 452.3740 473.5319 435.0425 468.3699 449.6422 474.2666\n           sim_15   sim_16   sim_17   sim_18   sim_19   sim_20   sim_21\nJan 1961 455.4787 451.0585 459.4839 444.3242 435.1001 443.7523 444.9274\nFeb 1961 439.4881 422.0447 442.7488 427.5273 412.6110 430.0844 418.5412\nMar 1961 473.0626 449.9922 479.0381 454.4156 443.6824 454.0246 455.8716\nApr 1961 497.0203 501.5911 517.4542 495.3593 476.1640 489.3426 492.7721\nMay 1961 515.2215 526.5615 506.5715 487.1920 504.6323 500.4116 506.1908\nJun 1961 576.5544 581.4492 560.9466 560.1496 557.4915 551.0780 580.6906\nJul 1961 662.4222 675.8571 643.9878 645.8877 634.9354 649.0339 655.5332\nAug 1961 648.2923 653.8785 639.1892 624.4315 643.3871 630.6072 638.5270\nSep 1961 548.8370 547.7029 548.0178 529.7305 552.4617 533.3734 541.2910\nOct 1961 500.8130 500.6410 509.7552 481.4202 500.3374 484.8628 513.4386\nNov 1961 448.3088 427.7597 434.1320 424.8617 468.9838 421.0139 460.0366\nDec 1961 479.2161 468.6392 473.0734 460.6548 492.5949 445.3285 483.7701\n           sim_22   sim_23   sim_24   sim_25   sim_26   sim_27   sim_28\nJan 1961 431.9061 436.9803 445.6030 470.3241 438.1303 438.0355 442.8814\nFeb 1961 405.5096 406.4255 431.2780 451.1454 438.4957 414.3992 406.9653\nMar 1961 437.8384 437.3900 455.2998 472.2864 457.3227 452.7172 439.0830\nApr 1961 480.2212 479.1317 491.4019 520.8334 492.1962 497.7494 495.6963\nMay 1961 507.5101 491.4699 502.3217 522.6236 504.1139 498.8398 490.4093\nJun 1961 565.3556 555.2957 562.9435 569.6062 575.3107 565.8427 558.4052\nJul 1961 631.8589 643.0705 649.3241 656.5773 699.1111 660.5835 656.7441\nAug 1961 636.9621 620.3416 635.3089 658.5513 666.7588 655.0455 634.5556\nSep 1961 533.4397 546.0061 537.0957 552.6849 578.5525 563.2023 557.6656\nOct 1961 493.0531 483.6816 489.7577 517.3445 535.8427 509.2783 506.7000\nNov 1961 421.6430 429.6623 419.7854 427.1834 456.2713 429.0018 434.0667\nDec 1961 452.2728 457.5337 460.9375 470.2428 514.2062 482.2928 490.3966\n           sim_29   sim_30   sim_31   sim_32   sim_33   sim_34   sim_35\nJan 1961 455.3775 444.3272 461.2236 433.8962 464.4749 438.0355 439.1887\nFeb 1961 426.9441 418.2812 447.4852 424.0317 409.7277 415.1395 433.4188\nMar 1961 457.7136 440.5773 468.7090 447.5754 438.9636 443.0306 420.3961\nApr 1961 505.5824 483.8879 508.5020 490.2527 475.6410 497.7693 480.1376\nMay 1961 502.1128 495.2827 526.9527 488.3082 510.2163 524.8456 504.3970\nJun 1961 564.2560 565.1836 571.4699 554.6782 548.1972 582.0468 557.3177\nJul 1961 675.1975 656.7991 674.3476 643.9736 631.1844 665.9546 650.1157\nAug 1961 642.3301 604.3293 649.4941 612.8896 632.0786 640.5119 626.3405\nSep 1961 546.0228 520.2840 550.9040 533.3240 544.4996 540.7844 555.0268\nOct 1961 517.2212 480.8524 494.4332 467.0373 505.1177 495.0939 511.8599\nNov 1961 436.9353 409.0263 423.5813 407.0598 433.9868 420.4208 435.3652\nDec 1961 477.7085 444.2295 463.2785 447.8678 471.4819 477.0888 486.3902\n           sim_36   sim_37   sim_38   sim_39   sim_40   sim_41   sim_42\nJan 1961 458.2243 453.5069 442.9633 437.7823 456.7181 439.1887 444.2746\nFeb 1961 437.0087 430.5385 443.4436 434.5281 441.0503 415.1167 410.6375\nMar 1961 450.4477 456.5806 462.1142 454.3194 453.8004 433.9958 450.8734\nApr 1961 507.2234 499.3962 524.0761 495.3350 498.6223 465.6067 489.4286\nMay 1961 521.1416 518.5159 541.6983 514.5651 504.7369 474.7817 504.5533\nJun 1961 579.8129 580.3206 598.3710 558.8196 564.7209 544.2263 569.3150\nJul 1961 685.2838 663.2947 685.4005 652.3353 664.4497 622.2188 654.6807\nAug 1961 671.5436 671.9516 655.8918 630.8036 628.7970 615.9232 637.1259\nSep 1961 566.4307 567.5127 576.3038 539.0021 523.1752 530.4702 525.4079\nOct 1961 506.4949 510.5275 513.7364 488.4346 483.6546 479.5688 482.5080\nNov 1961 455.9362 453.4418 460.7156 417.2549 413.5535 408.8823 397.0385\nDec 1961 487.4205 481.4040 488.4877 455.6214 481.9331 453.4355 448.3142\n           sim_43   sim_44   sim_45   sim_46   sim_47   sim_48   sim_49\nJan 1961 456.7181 448.9127 444.1944 439.1887 444.3762 443.9540 449.6957\nFeb 1961 415.5481 417.4993 432.0998 418.8621 411.7694 419.4677 432.5796\nMar 1961 441.1190 446.8376 453.2327 448.5254 448.2663 470.9662 455.3371\nApr 1961 484.0184 474.0491 510.7521 487.2088 508.1952 505.5636 510.9074\nMay 1961 474.5845 493.5429 520.5754 493.9236 521.0746 494.3940 527.4129\nJun 1961 540.3995 553.2761 571.1053 570.3663 579.1209 553.5803 571.8349\nJul 1961 649.1646 657.5268 653.2605 657.5574 676.0876 607.8539 654.0949\nAug 1961 643.3467 635.9533 636.3980 656.9203 673.6242 590.6108 632.1774\nSep 1961 539.6460 532.2568 541.7711 546.2181 564.7882 492.5588 501.7329\nOct 1961 490.9799 483.5566 507.1510 500.1024 505.8764 471.2807 448.5220\nNov 1961 412.2859 415.6316 428.0756 413.5517 434.7768 384.6290 382.9796\nDec 1961 461.1189 422.9218 457.3822 459.4905 484.2661 434.8731 429.3802\n           sim_50\nJan 1961 442.9633\nFeb 1961 382.3990\nMar 1961 411.8363\nApr 1961 459.1659\nMay 1961 458.3228\nJun 1961 524.1910\nJul 1961 615.5164\nAug 1961 614.8716\nSep 1961 506.6345\nOct 1961 444.9304\nNov 1961 383.3882\nDec 1961 426.0341\n\noutput$forecast_sim_tbl\n\n# A tibble: 600 × 4\n       x     y n        id\n   <dbl> <dbl> <chr> <int>\n 1 1961.  446. sim_1     1\n 2 1961.  424. sim_1     2\n 3 1961.  444. sim_1     3\n 4 1961.  490. sim_1     4\n 5 1961.  502. sim_1     5\n 6 1961.  553. sim_1     6\n 7 1962.  655. sim_1     7\n 8 1962.  633. sim_1     8\n 9 1962.  536. sim_1     9\n10 1962.  486. sim_1    10\n# … with 590 more rows\n\n\nThe time series that was used.\n\noutput$time_series\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nThe fitted values in two different formats\n\noutput$fitted_values\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1949 111.9353 117.9664 131.9662 128.9774 120.9892 134.9782 147.9692 147.9731\n1950 115.4270 121.3807 138.5312 137.2522 127.5180 139.7865 158.9159 166.3207\n1951 132.8130 151.5147 164.7662 165.5934 153.1413 186.9835 201.0564 197.1598\n1952 171.2150 174.9573 205.3873 181.9983 189.5111 191.9692 229.3659 230.2887\n1953 197.4932 205.0669 211.9492 214.7030 229.4790 261.9978 259.3789 272.3394\n1954 205.8921 206.1940 236.4993 236.1932 227.3492 247.7618 281.4459 303.3972\n1955 229.7559 221.0431 274.5806 260.1968 270.5411 299.1164 345.0987 346.2332\n1956 283.9172 273.9637 307.7212 313.9008 312.5977 358.8094 415.5070 394.8641\n1957 313.6903 307.2137 343.5753 347.2471 353.0923 409.4687 455.6903 452.6105\n1958 346.2681 328.3116 377.2767 360.7205 361.5534 432.0632 479.8147 490.8438\n1959 346.5194 335.2349 385.8185 384.9676 406.8419 485.3013 531.0698 553.0149\n1960 418.4120 397.2579 454.0138 420.2105 468.2327 523.6974 603.6761 623.9211\n          Sep      Oct      Nov      Dec\n1949 135.9874 119.0049 104.0187 118.0778\n1950 156.2023 139.1631 119.1047 128.9974\n1951 185.1990 158.4085 140.6879 169.2312\n1952 220.7255 190.3056 172.8446 191.9035\n1953 238.8716 218.7628 194.3686 207.1910\n1954 261.6971 232.5912 199.3564 222.5606\n1955 309.8394 277.5670 246.5073 264.0691\n1956 363.2702 318.0959 271.0990 310.9291\n1957 409.6920 354.9782 312.2741 341.2514\n1958 438.0323 360.3301 317.3131 346.5759\n1959 454.6235 412.1130 357.5358 384.6475\n1960 513.8591 450.7760 410.8955 439.9468\n\noutput$fitted_values_tbl\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949   112.\n 2 Feb 1949   118.\n 3 Mar 1949   132.\n 4 Apr 1949   129.\n 5 May 1949   121.\n 6 Jun 1949   135.\n 7 Jul 1949   148.\n 8 Aug 1949   148.\n 9 Sep 1949   136.\n10 Oct 1949   119.\n# … with 134 more rows\n\n\nThe residual values in two different formats\n\noutput$residual_values\n\n               Jan           Feb           Mar           Apr           May\n1949   0.064663218   0.033565844   0.033806149   0.022551853   0.010753877\n1950  -0.426993657   4.619296276   2.468817592  -2.252222539  -2.517970381\n1951  12.187004737  -1.514734464  13.233788623  -2.593406722  18.858703025\n1952  -0.215049450   5.042657391 -12.387298452  -0.998279160  -6.511066526\n1953  -1.493243603  -9.066863638  24.050789583  20.296981218  -0.479038408\n1954  -1.892145409 -18.194023466  -1.499295070  -9.193240870   6.650775967\n1955  12.244087324  11.956865808  -7.580632008   8.803192574  -0.541058565\n1956   0.082775370   3.036286487   9.278782426  -0.900756132   5.402300489\n1957   1.309660582  -6.213658164  12.424731881   0.752860797   1.907693986\n1958  -6.268081829 -10.311568418 -15.276674427 -12.720532082   1.446551672\n1959  13.480639348   6.765147100  20.181485627  11.032373098  13.158068772\n1960  -1.412019863  -6.257914445 -35.013829003  40.789527421   3.767286916\n               Jun           Jul           Aug           Sep           Oct\n1949   0.021825883   0.030792890   0.026927580   0.012574909  -0.004856125\n1950   9.213518228  11.084130208   3.679258769   1.797714396  -6.163078961\n1951  -8.983477860  -2.056432959   1.840247841  -1.199018264   3.591516610\n1952  26.030752750   0.634055619  11.711292178 -11.725472510   0.694363698\n1953 -18.997830355   4.621111100  -0.339432546  -1.871647726  -7.762821892\n1954  16.238163730  20.554095186 -10.397216564  -2.697121208  -3.591173672\n1955  15.883576851  18.901348128   0.766758698   2.160561460  -3.567021842\n1956  15.190550278  -2.507027255  10.135908860  -8.270245332 -12.095862166\n1957  12.531339373   9.309735715  14.389487605  -5.692026179  -7.978198549\n1958   2.936791273  11.185298228  14.156225803 -34.032306461  -1.330101426\n1959 -13.301336524  16.930226311   5.985059029   8.376509562  -5.112953069\n1960  11.302583143  18.323916561 -17.921058274  -5.859106651  10.223989361\n               Nov           Dec\n1949  -0.018746667  -0.077775679\n1950  -5.104668785  11.002554045\n1951   5.312079856  -3.231170377\n1952  -0.844622054   2.096450492\n1953 -14.368618246  -6.190983141\n1954   3.643587684   6.439397882\n1955  -9.507327199  13.930943896\n1956  -0.099013624  -4.929146809\n1957  -7.274091927  -5.251369244\n1958  -7.313133943  -9.575869505\n1959   4.464243872  20.352533059\n1960 -20.895479201  -7.946822359\n\noutput$residual_values_tbl\n\n# A tibble: 144 × 2\n   index        value\n   <yearmon>    <dbl>\n 1 Jan 1949   0.0647 \n 2 Feb 1949   0.0336 \n 3 Mar 1949   0.0338 \n 4 Apr 1949   0.0226 \n 5 May 1949   0.0108 \n 6 Jun 1949   0.0218 \n 7 Jul 1949   0.0308 \n 8 Aug 1949   0.0269 \n 9 Sep 1949   0.0126 \n10 Oct 1949  -0.00486\n# … with 134 more rows\n\n\nThe input data itself\n\noutput$input_data\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949    112\n 2 Feb 1949    118\n 3 Mar 1949    132\n 4 Apr 1949    129\n 5 May 1949    121\n 6 Jun 1949    135\n 7 Jul 1949    148\n 8 Aug 1949    148\n 9 Sep 1949    136\n10 Oct 1949    119\n# … with 134 more rows\n\n\nThe time series simulations\n\noutput$sim_ts_tbl\n\n# A tibble: 600 × 5\n   index         x     y n        id\n   <yearmon> <dbl> <dbl> <chr> <int>\n 1 Jan 1961  1961.  446. sim_1     1\n 2 Feb 1961  1961.  424. sim_1     2\n 3 Mar 1961  1961.  444. sim_1     3\n 4 Apr 1961  1961.  490. sim_1     4\n 5 May 1961  1961.  502. sim_1     5\n 6 Jun 1961  1961.  553. sim_1     6\n 7 Jul 1961  1962.  655. sim_1     7\n 8 Aug 1961  1962.  633. sim_1     8\n 9 Sep 1961  1962.  536. sim_1     9\n10 Oct 1961  1962.  486. sim_1    10\n# … with 590 more rows\n\n\nNow, the visuals, first the static ggplot\n\noutput$ggplot\n\n\n\n\nThe interactive plotly plot.\n\noutput$plotly_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-2023-01-06/index.html",
    "href": "posts/weekly-rtip-2023-01-06/index.html",
    "title": "Brownian Motion",
    "section": "",
    "text": "Introduction\nBrownian motion, also known as the random motion of particles suspended in a fluid, is a phenomenon that was first described by Scottish botanist Robert Brown in 1827. It occurs when a particle is subjected to a series of random collisions with the molecules in the fluid.\nThe motion of the particle can be described mathematically using the following equation:\n\\[ \\frac{dx_t}{dt} = \\mu + \\sigma \\cdot W_t \\]\nWhere \\(x_t\\) represents the position of the particle at time t, \\(\\mu\\) is the drift coefficient, \\(\\sigma\\) is the diffusion coefficient, and \\(W_t\\) is a Wiener process (a type of random process).\nBrownian motion has a number of important applications, including in the field of finance. It is used to model the random movements of financial assets, such as stocks, over time. It can also be used to estimate the volatility of an asset, as well as to calculate the prices of financial derivatives such as options.\nIn physics, Brownian motion is used to study the behavior of small particles suspended in a fluid, as well as to understand the properties of fluids at the molecular level. It has also been used to study the motion of biological molecules, such as proteins, within cells.\nOverall, Brownian motion is a fundamental concept that has wide-ranging applications in a variety of fields, including finance, physics, and biology.\n\n\nFunction\nLet’s take a look at a function to produce such results. This type of functionality will be coming to my R packages {TidyDensity} and to {healthyR.ts}\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(forcats)\n\nbrownian_motion <- function(T, N, delta_t) {\n  # T: total time of the simulation (in seconds)\n  # N: number of simulations to generate\n  # delta_t: time step size (in seconds)\n  \n  # Initialize empty data.frame to store the simulations\n  sim_data <- data.frame()\n  \n  # Generate N simulations\n  for (i in 1:N) {\n    # Initialize the current simulation with a starting value of 0\n    sim <- c(0)\n    \n    # Generate the brownian motion values for each time step\n    for (t in 1:(T / delta_t)) {\n      sim <- c(sim, sim[t] + rnorm(1, mean = 0, sd = sqrt(delta_t)))\n    }\n    \n    # Bind the time steps, simulation values, and simulation number together\n    # in a data.frame and add it to the result\n    sim_data <- rbind(\n      sim_data, \n      data.frame(\n        t = seq(0, T, delta_t), \n        y = sim, \n        sim_number = i\n        )\n      ) %>%\n      as_tibble()\n  }\n  \n  sim_data <- sim_data %>%\n    mutate(sim_number = as_factor(sim_number)\n                  )\n  return(sim_data)\n}\n\nWe see that the internal variable sim is set to 0, this in the future will be set to an initial value that a user can provide.\n\n\nExamples\nLet’s take a look at a couple of examples.\n\nbrownian_motion(40, 25, .2) %>%\n  ggplot(aes(x = t, y = y, group = sim_number, color = sim_number)) +\n  geom_line() +\n  geom_smooth(se = FALSE, \n              aes(group = FALSE), \n              color = \"black\", \n              linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    x = \"Time t\",\n    y = \"Brownian Motion Value y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nNow lets take a look at the change in a few different ones at the same time.\n\nbm_tbl <- rbind(\n  brownian_motion(40, 25, .2) %>%\n    mutate(label = \"20% Volatility\"),\n  brownian_motion(40, 25, .1) %>%\n    mutate(label = \"10% Volatility\"),\n  brownian_motion(40, 25, .05) %>%\n    mutate(label = \"5% Volatility\"),\n  brownian_motion(40, 25, .025) %>%\n    mutate(label = \"2.5% Volatility\")\n)\n\nggplot(bm_tbl, aes(x = t, y = y, group = sim_number, color = sim_number)) +\n  geom_line() +\n  facet_wrap(~ label, scales = \"free\") +\n    geom_smooth(se = FALSE, \n              aes(group = FALSE), \n              color = \"black\", \n              linetype = \"dashed\") +\n  theme_minimal() +\n    labs(\n    x = \"Time t\",\n    y = \"Brownian Motion Value y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "title": "PCA with healthyR.ai",
    "section": "",
    "text": "In this post we are going to talk about how you can perform principal component analysis in R with {healthyR.ai} in a tidyverse compliant fashion.\nThe specific function we are going to discuss on this post is pca_your_recipe()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "title": "PCA with healthyR.ai",
    "section": "Library Load",
    "text": "Library Load\n\npacman::p_load(\n  \"healthyR.ai\",\n  \"healthyR.data\",\n  \"timetk\",\n  \"dplyr\",\n  \"purrr\",\n  \"rsample\",\n  \"recipes\"\n)\n\nNow that we have our libraries loaded lets get the data."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "title": "PCA with healthyR.ai",
    "section": "Data",
    "text": "Data\n\ndata_tbl <- healthyR_data %>%\n  select(visit_end_date_time) %>%\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by       = \"month\",\n    value     = n()\n  ) %>%\n  set_names(\"date_col\", \"value\") %>%\n  filter_by_time(\n    .date_var = date_col,\n    .start_date = \"2013\",\n    .end_date = \"2020\"\n  )\n\nhead(data_tbl, 5)\n\n# A tibble: 5 × 2\n  date_col            value\n  <dttm>              <int>\n1 2013-01-01 00:00:00  2082\n2 2013-02-01 00:00:00  1719\n3 2013-03-01 00:00:00  1796\n4 2013-04-01 00:00:00  1865\n5 2013-05-01 00:00:00  2028\n\n\nNow for the splits object."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "title": "PCA with healthyR.ai",
    "section": "Splits",
    "text": "Splits\n\nsplits <- initial_split(data = data_tbl, prop = 0.8)\n\nsplits\n\n<Training/Testing/Total>\n<76/19/95>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "title": "PCA with healthyR.ai",
    "section": "Recipe and Output",
    "text": "Recipe and Output\nNow it is time for the recipe and the output objects.\n\nrec_obj <- recipe(value ~ ., training(splits)) %>%\n  step_timeseries_signature(date_col) %>%\n  step_rm(matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\"))\n\noutput_list <- pca_your_recipe(rec_obj, .data = data_tbl)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "title": "PCA with healthyR.ai",
    "section": "PCA Transform",
    "text": "PCA Transform\n\noutput_list$pca_transform\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nTimeseries signature features from date_col\nVariables removed matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\")\nCentering for recipes::all_numeric()\nScaling for recipes::all_numeric()\nSparse, unbalanced variable filter on recipes::all_numeric()\nPCA extraction with recipes::all_numeric_predictors()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "title": "PCA with healthyR.ai",
    "section": "Variable Loadings",
    "text": "Variable Loadings\n\noutput_list$variable_loadings\n\n# A tibble: 169 × 4\n   terms                 value component id       \n   <chr>                 <dbl> <chr>     <chr>    \n 1 date_col_index.num -0.00137 PC1       pca_bVc37\n 2 date_col_year       0.0529  PC1       pca_bVc37\n 3 date_col_half      -0.385   PC1       pca_bVc37\n 4 date_col_quarter   -0.434   PC1       pca_bVc37\n 5 date_col_month     -0.437   PC1       pca_bVc37\n 6 date_col_wday      -0.0159  PC1       pca_bVc37\n 7 date_col_qday      -0.0608  PC1       pca_bVc37\n 8 date_col_yday      -0.437   PC1       pca_bVc37\n 9 date_col_mweek      0.0537  PC1       pca_bVc37\n10 date_col_week      -0.438   PC1       pca_bVc37\n# … with 159 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "title": "PCA with healthyR.ai",
    "section": "Variable Variance",
    "text": "Variable Variance\n\noutput_list$variable_variance\n\n# A tibble: 52 × 4\n   terms       value component id       \n   <chr>       <dbl>     <int> <chr>    \n 1 variance 5.14             1 pca_bVc37\n 2 variance 2.08             2 pca_bVc37\n 3 variance 1.47             3 pca_bVc37\n 4 variance 1.40             4 pca_bVc37\n 5 variance 1.07             5 pca_bVc37\n 6 variance 0.684            6 pca_bVc37\n 7 variance 0.583            7 pca_bVc37\n 8 variance 0.519            8 pca_bVc37\n 9 variance 0.0534           9 pca_bVc37\n10 variance 0.000231        10 pca_bVc37\n# … with 42 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Estimates",
    "text": "PCA Estimates\n\noutput_list$pca_estimates\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nTraining data contained 76 data points and no missing data.\n\nOperations:\n\nTimeseries signature features from date_col [trained]\nVariables removed date_col_year.iso, date_col_month.xts, date_col_hour, d... [trained]\nCentering for value, date_col_index.num, date_col_year, date_... [trained]\nScaling for value, date_col_index.num, date_col_year, date_... [trained]\nSparse, unbalanced variable filter removed date_col_day, date_col_mday, date_col_m... [trained]\nPCA extraction with date_col_index.num, date_col_year, date_col_half... [trained]"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Juiced Estimates",
    "text": "PCA Juiced Estimates\n\noutput_list$pca_juiced_estimates\n\n# A tibble: 76 × 8\n   date_col              value date_col…¹ date_…²     PC1     PC2     PC3    PC4\n   <dttm>                <dbl> <ord>      <ord>     <dbl>   <dbl>   <dbl>  <dbl>\n 1 2018-06-01 00:00:00  0.676  June       Friday   0.733   1.23   -1.30    0.536\n 2 2016-01-01 00:00:00 -0.133  January    Friday   3.51   -0.102  -0.500  -0.556\n 3 2013-05-01 00:00:00  1.67   May        Wednes…  1.15   -2.07   -1.68   -0.319\n 4 2018-10-01 00:00:00  0.337  October    Monday  -2.30    0.499   1.91   -1.92 \n 5 2016-09-01 00:00:00 -0.130  September  Thursd… -1.08    0.0972  0.410   2.64 \n 6 2016-07-01 00:00:00 -0.364  July       Friday  -0.0591  0.0211 -0.333   1.11 \n 7 2020-02-01 00:00:00 -0.646  February   Saturd…  3.08    2.48   -0.0249  0.214\n 8 2020-08-01 00:00:00 -1.42   August     Saturd… -0.491   2.60    0.142   1.88 \n 9 2018-03-01 00:00:00  0.243  March      Thursd…  2.74    0.848  -1.53    0.260\n10 2015-05-01 00:00:00 -0.0148 May        Friday   1.18   -0.636  -1.88   -0.147\n# … with 66 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "title": "PCA with healthyR.ai",
    "section": "PCA Baked Data",
    "text": "PCA Baked Data\n\noutput_list$pca_baked_data\n\n# A tibble: 95 × 8\n   date_col            value date_col_month…¹ date_…²    PC1   PC2    PC3    PC4\n   <dttm>              <dbl> <ord>            <ord>    <dbl> <dbl>  <dbl>  <dbl>\n 1 2013-01-01 00:00:00 1.86  January          Tuesday  3.60  -2.64  1.13  -0.871\n 2 2013-02-01 00:00:00 0.596 February         Friday   2.93  -1.76 -0.532  0.424\n 3 2013-03-01 00:00:00 0.864 March            Friday   2.62  -1.95 -2.24   0.643\n 4 2013-04-01 00:00:00 1.10  April            Monday   2.09  -2.68  1.39  -0.867\n 5 2013-05-01 00:00:00 1.67  May              Wednes…  1.15  -2.07 -1.68  -0.319\n 6 2013-06-01 00:00:00 0.923 June             Saturd…  0.612 -1.57 -2.01   0.919\n 7 2013-07-01 00:00:00 1.29  July             Monday  -0.669 -2.41  2.29  -0.420\n 8 2013-08-01 00:00:00 1.18  August           Thursd… -0.628 -1.76 -0.165  1.96 \n 9 2013-09-01 00:00:00 0.714 September        Sunday  -1.11  -2.19  0.910  2.25 \n10 2013-10-01 00:00:00 1.40  October          Tuesday -2.55  -1.91 -0.128 -1.48 \n# … with 85 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Data Frame",
    "text": "PCA Variance Data Frame\n\noutput_list$pca_variance_df\n\n# A tibble: 13 × 6\n   PC    var_explained var_pct_txt cum_var_pct cum_var_pct_txt ou_threshold\n   <chr>         <dbl> <chr>             <dbl> <chr>           <fct>       \n 1 PC1   0.395         39.55%            0.395 39.55%          Under       \n 2 PC2   0.160         16.02%            0.556 55.57%          Under       \n 3 PC3   0.113         11.29%            0.669 66.86%          Under       \n 4 PC4   0.107         10.75%            0.776 77.61%          Over        \n 5 PC5   0.0824        8.24%             0.858 85.85%          Over        \n 6 PC6   0.0526        5.26%             0.911 91.11%          Over        \n 7 PC7   0.0449        4.49%             0.956 95.59%          Over        \n 8 PC8   0.0400        4.00%             0.996 99.59%          Over        \n 9 PC9   0.00411       0.41%             1.00  100.00%         Over        \n10 PC10  0.0000178     0.00%             1.00  100.00%         Over        \n11 PC11  0.000000712   0.00%             1.00  100.00%         Over        \n12 PC12  0.000000273   0.00%             1.00  100.00%         Over        \n13 PC13  0.00000000196 0.00%             1     100.00%         Over"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Rotation Data Frame",
    "text": "PCA Rotation Data Frame\n\noutput_list$pca_rotation_df\n\n# A tibble: 13 × 13\n        PC1      PC2      PC3     PC4     PC5     PC6      PC7     PC8      PC9\n      <dbl>    <dbl>    <dbl>   <dbl>   <dbl>   <dbl>    <dbl>   <dbl>    <dbl>\n 1 -0.00137  0.671    0.116   -0.0521 -0.183   0.0165  0.0471   0.0277 -0.0177 \n 2  0.0529   0.667    0.116   -0.0610 -0.173   0.0146  0.0466   0.0313  0.00904\n 3 -0.385    0.0115   0.222    0.173   0.140   0.217   0.250   -0.0112  0.802  \n 4 -0.434    0.00824  0.0752  -0.0427  0.0615  0.135   0.00843  0.0332 -0.272  \n 5 -0.437    0.00278 -0.00879  0.0737 -0.0734  0.0167  0.00135 -0.0264 -0.213  \n 6 -0.0159   0.265   -0.406    0.274   0.468   0.254  -0.520   -0.366   0.0484 \n 7 -0.0608  -0.0200  -0.325    0.480  -0.542  -0.476  -0.0318  -0.244   0.187  \n 8 -0.437    0.00435 -0.00655  0.0740 -0.0733  0.0149  0.00231 -0.0303 -0.216  \n 9  0.0537  -0.164    0.562   -0.0242 -0.414   0.267  -0.572   -0.285   0.0519 \n10 -0.438    0.00638 -0.00736  0.0676 -0.0704  0.0236  0.00470 -0.0266 -0.219  \n11  0.250   -0.0238   0.208    0.420   0.0436  0.301   0.544   -0.492  -0.293  \n12 -0.0474   0.0762   0.516    0.142   0.460  -0.676  -0.108   -0.144  -0.0636 \n13  0.123    0.00776  0.150    0.666   0.0183  0.152  -0.161    0.676  -0.111  \n# … with 4 more variables: PC10 <dbl>, PC11 <dbl>, PC12 <dbl>, PC13 <dbl>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Scree Plot",
    "text": "PCA Variance Scree Plot\n\noutput_list$pca_variance_scree_plt"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Loadings Plot",
    "text": "PCA Loadings Plot\n\noutput_list$pca_loadings_plt\n\n\n\noutput_list$pca_loadings_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "title": "PCA with healthyR.ai",
    "section": "Top N Loadings Plots",
    "text": "Top N Loadings Plots\n\noutput_list$pca_top_n_loadings_plt\n\n\n\noutput_list$pca_top_n_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "",
    "text": "Minimal coding ML is not something that is unheard of and is rather prolific, think h2o and pycaret just to name two. There is also no shortage available for R with the h2o interface, and tidyfit. There are also similar low-code workflows in my r package {healthyR.ai}. Today I will specifically go through the workflow for Automatic KNN classification for the Iris data set where we will classify the Species."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Recipe Output",
    "text": "Recipe Output\n\nauto_knn$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\nCentering and scaling for recipes::all_numeric()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Model Info",
    "text": "Model Info\n\nauto_knn$model_info$was_tuned\n\n[1] \"tuned\"\n\n\n\nauto_knn$model_info$model_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$wflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$fitted_wflw\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(13L,     data, 5), distance = ~1.69935879141092, kernel = ~\"rank\")\n\nType of response variable: nominal\nMinimal misclassification: 0.03571429\nBest kernel: rank\nBest k: 13"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Tuning Info",
    "text": "Tuning Info\n\nauto_knn$tuned_info$tuning_grid\n\n# A tibble: 10 × 3\n   neighbors weight_func  dist_power\n       <int> <chr>             <dbl>\n 1         4 triangular        0.764\n 2        11 rectangular       0.219\n 3         5 gaussian          1.35 \n 4        14 triweight         0.351\n 5         5 biweight          1.05 \n 6         9 optimal           1.87 \n 7         7 cos               0.665\n 8        11 inv               1.18 \n 9        13 rank              1.70 \n10         1 epanechnikov      1.58 \n\n\n\nauto_knn$tuned_info$cv_obj\n\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$tuned_results\n\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics           .notes          \n   <list>          <chr>      <list>             <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 7]> <tibble [0 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 7]> <tibble [0 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 7]> <tibble [0 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 7]> <tibble [0 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 7]> <tibble [0 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 7]> <tibble [0 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 7]> <tibble [0 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 7]> <tibble [0 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 7]> <tibble [0 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 7]> <tibble [0 × 3]>\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$grid_size\n\n[1] 10\n\n\n\nauto_knn$tuned_info$best_metric\n\n[1] \"f_meas\"\n\n\n\nauto_knn$tuned_info$best_result_set\n\n# A tibble: 1 × 9\n  neighbors weight_func dist_power .metric .estima…¹  mean     n std_err .config\n      <int> <chr>            <dbl> <chr>   <chr>     <dbl> <int>   <dbl> <chr>  \n1        13 rank              1.70 f_meas  macro     0.957    25 0.00655 Prepro…\n# … with abbreviated variable name ¹​.estimator\n\n\n\nauto_knn$tuned_info$tuning_grid_plot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nauto_knn$tuned_info$plotly_grid_plot\n\n\n\n\n\nVoila!\nThank you for reading."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "title": "Time Series Lag Correlation Plots",
    "section": "",
    "text": "In time series analysis there is something called a lag. This simply means we take a look at some past event from some point in time t. This is a non-statistical method for looking at a relationship between a timeseries and its lags.\n{healthyR.ts} has a function called ts_lag_correlation(). This function, as described by it’s name, provides more than just a simple lag plot.\nThis function provides a lot of extra information for the end user. First let’s go over the function call."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Call",
    "text": "Function Call\nHere is the full call:\n\nts_lag_correlation(\n  .data,\n  .date_col,\n  .value_col,\n  .lags = 1,\n  .heatmap_color_low = \"white\",\n  .heatmap_color_hi = \"steelblue\"\n)\n\nHere are the arguments that get supplied to the different parameters.\n\n.data - A tibble of time series data\n.date_col - A date column\n.value_col - The value column being analyzed\n.lags - This is a vector of integer lags, ie 1 or c(1,6,12)\n.heatmap_color_low - What color should the low values of the heatmap of the correlation matrix be, the default is ‘white’\n.heatmap_color_hi - What color should the low values of the heatmap of the correlation matrix be, the default is ‘steelblue’"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Return",
    "text": "Function Return\nThe function itself returns a list object. The list has the following elements in it:\nData Elements\n\nlag_list\nlag_tbl\ncorrelation_lag_matrix\ncorrelation_lag_tbl\n\nPlot Elements\n\nlag_plot\nplotly_lag_plot\ncorrelation_heatmap\nplotly_heatmap"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Data Elements",
    "text": "Data Elements\nHere are the data elements.\n\noutput$data$lag_list\n\n[[1]]\n# A tibble: 143 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 1       118          112\n 2 1       132          118\n 3 1       129          132\n 4 1       121          129\n 5 1       135          121\n 6 1       148          135\n 7 1       148          148\n 8 1       136          148\n 9 1       119          136\n10 1       104          119\n# … with 133 more rows\n\n[[2]]\n# A tibble: 141 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 3       129          112\n 2 3       121          118\n 3 3       135          132\n 4 3       148          129\n 5 3       148          121\n 6 3       136          135\n 7 3       119          148\n 8 3       104          148\n 9 3       118          136\n10 3       115          119\n# … with 131 more rows\n\n[[3]]\n# A tibble: 138 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 6       148          112\n 2 6       148          118\n 3 6       136          132\n 4 6       119          129\n 5 6       104          121\n 6 6       118          135\n 7 6       115          148\n 8 6       126          148\n 9 6       141          136\n10 6       135          119\n# … with 128 more rows\n\n[[4]]\n# A tibble: 132 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 12      115          112\n 2 12      126          118\n 3 12      141          132\n 4 12      135          129\n 5 12      125          121\n 6 12      149          135\n 7 12      170          148\n 8 12      170          148\n 9 12      158          136\n10 12      133          119\n# … with 122 more rows\n\n\nThis is a list of all the tibbles of the different lags that were chosen.\n\noutput$data$lag_tbl\n\n# A tibble: 554 × 4\n   lag   value lagged_value lag_title\n   <fct> <dbl>        <dbl> <fct>    \n 1 1       118          112 Lag: 1   \n 2 1       132          118 Lag: 1   \n 3 1       129          132 Lag: 1   \n 4 1       121          129 Lag: 1   \n 5 1       135          121 Lag: 1   \n 6 1       148          135 Lag: 1   \n 7 1       148          148 Lag: 1   \n 8 1       136          148 Lag: 1   \n 9 1       119          136 Lag: 1   \n10 1       104          119 Lag: 1   \n# … with 544 more rows\n\n\nThis is the long lag tibble with all of the lags in it.\n\noutput$data$correlation_lag_matrix\n\n                value value_lag1 value_lag3 value_lag6 value_lag12\nvalue       1.0000000  0.9542938  0.8186636  0.7657001   0.9905274\nvalue_lag1  0.9542938  1.0000000  0.8828054  0.7726530   0.9492382\nvalue_lag3  0.8186636  0.8828054  1.0000000  0.8349550   0.8218493\nvalue_lag6  0.7657001  0.7726530  0.8349550  1.0000000   0.7780911\nvalue_lag12 0.9905274  0.9492382  0.8218493  0.7780911   1.0000000\n\n\nThis is the correlation matrix.\n\noutput$data$correlation_lag_tbl\n\n# A tibble: 25 × 3\n   name        data_names value\n   <fct>       <fct>      <dbl>\n 1 value       value      1    \n 2 value_lag1  value      0.954\n 3 value_lag3  value      0.819\n 4 value_lag6  value      0.766\n 5 value_lag12 value      0.991\n 6 value       value_lag1 0.954\n 7 value_lag1  value_lag1 1    \n 8 value_lag3  value_lag1 0.883\n 9 value_lag6  value_lag1 0.773\n10 value_lag12 value_lag1 0.949\n# … with 15 more rows\n\n\nThis is the correlation lag tibble"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Plot Elements",
    "text": "Plot Elements\n\noutput$plots$lag_plot\n\n\n\n\nThe Lag Plot itself.\n\noutput$plots$plotly_lag_plot\n\n\n\n\n\nA plotly version of the lag plot.\n\noutput$plots$correlation_heatmap\n\n\n\n\nA heatmap of the correlations.\n\noutput$plots$plotly_heatmap\n\n\n\n\n\nA plotly version of the correlation heatmap.\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "title": "Create QQ Plots for Time Series Models with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nA Q-Q plot, or quantile-quantile plot, is a graphical tool for comparing two sets of data to assess whether they come from the same distribution. In the context of time series modeling, a Q-Q plot can be used to check whether the residuals of a fitted time series model follow the normal distribution. This is important because many time series models, such as the autoregressive moving average (ARMA) model, assume that the residuals are normally distributed.\nTo create a Q-Q plot, the data are first sorted in ascending order and then divided into quantiles. The quantiles of the first dataset are then plotted against the quantiles of the second dataset. If the two datasets come from the same distribution, the points on the Q-Q plot will fall approximately on a straight line. Deviations from this line can indicate departures from the assumed distribution.\nFor example, if we have a time series dataset and we fit an ARMA model to it, we can use a Q-Q plot to check whether the residuals of the fitted model are normally distributed. If the Q-Q plot shows that the residuals do not follow the normal distribution, we may need to consider using a different time series model that does not assume normality of the residuals.\nIn summary, Q-Q plots are a useful tool for assessing the distribution of a dataset and for checking whether a time series model has produced satisfactory residuals.\nIn the R package {healthyR.ts} there is a function to view the QQ plot. This function is called ts_qq_plot() and it is meant to work with a calibration tibble from the excellent {modeltime} which is a {parsnip} extension package.\n\n\nFunction\nLet’s take a look at the full function call and the arguments that get provided to the parameters.\n\nts_qq_plot(\n  .calibration_tbl, \n  .model_id = NULL, \n  .interactive = FALSE\n  )\n\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nLet’s work through an example, and since we already spoke about ARMA let’s try out an ARMA model.\n\nlibrary(healthyR.ts)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(recipes)\nlibrary(parsnip)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_qq_plot(calibration_tbl, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "title": "Model Scedacity Plots with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nScedacity plots are a useful tool for evaluating the performance of time series models and identifying trends or patterns in the data. They are a type of scatter plot that compares the predicted values produced by a model to the observed values in the data, with a diagonal reference line indicating perfect agreement between the two.\nThe {healthyR.ts} package in R provides a convenient function for creating scedacity plots for time series data, called ts_scedacity_scatter_plot(). This function takes as input a calibration tibble which you would get from using the {modeltime} library, and produces a scedacity plot showing the predicted values against the observed values.\nOne of the main benefits of using a scedacity plot is that it allows you to visualize the accuracy of the model’s predictions. If the points on the plot fall close to the reference line, it indicates that the model is able to accurately predict the values in the data. On the other hand, if the points are scattered far from the reference line, it suggests that the model is not performing well and may need to be improved or refined.\nIn addition to evaluating the accuracy of the model, scedacity plots can also be used to identify trends or patterns in the data. For example, if there is a clear upward or downward trend in the points on the plot, it may indicate that the model is over- or under-estimating the values in the data. By identifying these trends, you can adjust the model or try different approaches to improve its performance.\nOverall, scedacity plots are a useful tool for evaluating the performance of time series models and identifying trends or patterns in the data. The ts_scedacity_scatter_plot() function in the {healthyR.ts} package makes it easy to create these plots and gain insights into the performance of your time series models.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_scedacity_scatter_plot(\n  .calibration_tbl,\n  .model_id = NULL,\n  .interactive = FALSE\n)\n\nLet’s take a look at the arguments that get provided to the parameters.\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(recipes)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nmodel_spec_mars <- mars(mode = \"regression\") %>%\n  set_engine(\"earth\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nwflw_fit_mars <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_mars) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima, wflw_fit_mars)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_scedacity_scatter_plot(calibration_tbl)\n\n\n\n\nNow the interactive plot.\n\nts_scedacity_scatter_plot(calibration_tbl, .interactive = TRUE)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "title": "Event Analysis with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime-to-event analysis, also known as survival analysis, is a statistical technique used to analyze the length of time until an event occurs. This type of analysis is often used in fields such as healthcare, engineering, and finance to understand the factors that influence the likelihood of an event occurring and to make predictions about future events.\nIn economics, an event study is a statistical technique used to analyze the effect of a specific event on a particular market or financial instrument. Event studies are commonly used in finance to understand how events, such as the announcement of a new product, the release of financial earnings, or a change in government policy, may impact the price or performance of a company’s stock or other financial instruments.\nTo conduct an event study, analysts typically collect data on the performance of a market or financial instrument before and after the event in question. This data is then used to estimate the effect of the event on the market or instrument.\nThere are several different methods that can be used to conduct an event study, including the market model, the abnormal return method, and the buy-and-hold abnormal return method. These methods allow analysts to quantify the effect of the event on the market or instrument and to identify any changes in market behavior that may have occurred as a result of the event.\nOverall, event studies are a valuable tool for understanding how specific events may impact financial markets and instruments, and are widely used in finance and economics to inform investment decisions and to better understand market behavior.\nIn this post we are going to examine a function from the R package {healthyR.ts} has a function called ts_time_event_analysis_tbl() that will help us understand what happens after a specified event, in this instance it will always be some percentage decrease or increase in a value.\nThere is a great article from Investopedia on this economic topic here\n\n\nFunction\nThe function is ts_time_event_analysis_tbl() and it’s complimentary plotting function ts_event_analysis_plot().\nHere is the tibble data return function.\n\nts_time_event_analysis_tbl(\n  .data,\n  .date_col,\n  .value_col,\n  .percent_change = 0.05,\n  .horizon = 12,\n  .precision = 2,\n  .direction = \"forward\",\n  .filter_non_event_groups = TRUE\n)\n\nLet’s take a look at the arguments to the parameters for this one.\n\n.data - The date.frame/tibble that holds the data.\n.date_col - The column with the date value.\n.value_col - The column with the value you are measuring.\n.percent_change - This defaults to 0.05 which is a 5% increase in the value_col.\n.horizon - How far do you want to look back or ahead.\n.precision - The default is 2 which means it rounds the lagged 1 value percent change to 2 decimal points. You may want more for more finely tuned results, this will result in fewer groupings.\n.direction - The default is forward. You can supply either forward, backwards or both.\nfilter_non_event_groups - The default is TRUE, this drops groupings with no events on the rare occasion it does occur.\n\nNow the plotting function.\n\nts_event_analysis_plot(\n  .data,\n  .plot_type = \"mean\",\n  .plot_ci = TRUE,\n  .interactive = FALSE\n)\n\n\n.data - The data that comes from the ts_time_event_analysis_tbl()\n.plot_type - The default is “mean” which will show the mean event change of the output from the analysis tibble. The possible values for this are: mean, median, and individual.\n.plot_ci - The default is TRUE. This will only work if you choose one of the aggregate plots of either “mean” or “median”\n.interactive - The default is FALSE. TRUE will return a plotly plot.\n\n\n\nExamples\nLet’s go through a couple examples using the AirPassengers data. We will first transform it into a tibble and then we will use a look period of 6. Let’s see the data output and then we will visualize.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndf <- ts_to_tbl(AirPassengers) %>% select(-index)\n\nevent_tbl <- ts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"both\"\n)\n\nglimpse(event_tbl)\n\nRows: 33\nColumns: 18\n$ rowid                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ date_col             <date> 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value                <dbl> 118, 132, 129, 121, 135, 148, 148, 199, 184, 162,…\n$ lag_val              <dbl> 112, 118, 132, 129, 121, 135, 148, 199, 199, 184,…\n$ adj_diff             <dbl> 6, 14, -3, -8, 14, 13, 0, 0, -15, -22, -16, 20, 5…\n$ relative_change_raw  <dbl> 0.05357143, 0.11864407, -0.02272727, -0.06201550,…\n$ relative_change      <dbl> 0.05, 0.12, -0.02, -0.06, 0.12, 0.10, 0.00, 0.00,…\n$ pct_chg_mark         <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ event_base_change    <dbl> 0.00000000, 0.11864407, -0.02272727, -0.06201550,…\n$ group_number         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ numeric_group_number <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ group_event_number   <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ x                    <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ mean_event_change    <dbl> 0.00000000, 0.03849647, -0.06815622, -0.04991040,…\n$ median_event_change  <dbl> 0.00000000, 0.07222222, -0.06217617, -0.06201550,…\n$ event_change_ci_low  <dbl> 0.00000000, -0.06799693, -0.11669576, -0.09692794…\n$ event_change_ci_high <dbl> 0.000000000, 0.116322976, -0.024699717, 0.0073964…\n$ event_type           <fct> Before, Before, Before, Before, Before, Before, A…\n\n\nLet’s visualize!\n\nts_event_analysis_plot(\n  .data = event_tbl\n)\n\n\n\n\nLet’s see the median now.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\"\n)\n\n\n\n\nNow let’s see it as an interactive plot.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\",\n  .interactive = TRUE\n)\n\n\n\n\n\nNow let’s see all the individual groups.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"individual\",\n  .interactive = TRUE\n)\n\n\n\n\n\nSingle direction plotting.\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"backward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nAnd…\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"forward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-lists-2023-01-20/index.html",
    "href": "posts/weekly-rtip-lists-2023-01-20/index.html",
    "title": "Another Post on Lists",
    "section": "",
    "text": "Introduction\nManipulating lists in R is a powerful tool for organizing and analyzing data. Here are a few common ways to manipulate lists:\n\nIndexing: Lists can be indexed using square brackets “[ ]” and numeric indices. For example, to access the first element of a list called “mylist”, you would use the expression “mylist[1]”.\nSubsetting: Lists can be subsetted using the same square bracket notation, but with a logical vector indicating which elements to keep. For example, to select all elements of “mylist” that are greater than 5, you would use the expression “mylist[mylist > 5]”.\nModifying elements: Elements of a list can be modified by assigning new values to them using the assignment operator “<-”. For example, to change the third element of “mylist” to 10, you would use the expression “mylist[3] <- 10”.\nAdding elements: New elements can be added to a list using the concatenation operator “c()” or the “append()” function. For example, to add the number 7 to the end of “mylist”, you would use the expression “mylist <- c(mylist, 7)”.\nRemoving elements: Elements can be removed from a list using the “-” operator. For example, to remove the second element of “mylist”, you would use the expression “mylist <- mylist[-2]”.\n\n\n\nExamples\nHere is an example of how these methods can be used to manipulate a list in R:\n\nmylist <- list(1,2,3,4,5)\n\n# Indexing\nmylist[[1]] # Returns 1\n\n[1] 1\n\n# Subsetting\nmylist[mylist > 3] # Returns 4 & 5\n\n[[1]]\n[1] 4\n\n[[2]]\n[1] 5\n\n# Modifying elements\nmylist[[3]] <- 10\nmylist # Returns 1 2 10 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n# Adding elements\nmylist <- c(mylist, 7)\nmylist # Returns 1 2 10 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n[[6]]\n[1] 7\n\n# Removing elements\nmylist[-3]\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 4\n\n[[4]]\n[1] 5\n\n[[5]]\n[1] 7\n\nmylist # Returns 1 2 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n[[6]]\n[1] 7\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidyaml-2023-01-13/index.html",
    "href": "posts/weekly-rtip-tidyaml-2023-01-13/index.html",
    "title": "The building of {tidyAML}",
    "section": "",
    "text": "Introduction\nYesterday I posted on An Update to {tidyAML} where I was discussing some of my thought process and how things could potentially work for the package.\nToday I want to showcase how the function fast_regression_parsnip_spec_tbl() and it’s complimentary function fast_classification_parsnip_spec_tbl() actually work or maybe don’t work for that matter.\nWe are going to pick on fast_regression_parsnip_spec_tbl() in today’s post. The point of it is that it creates a tibble of parsnip regression model specifications. This will create a tibble of 46 different regression model specifications which can be filtered. The model specs are created first and then filtered out. This will only create models for regression problems. To find all of the supported models in this package you can visit the parsnip search page\n\n\nFunction\nFirst let’s take a look at the function call itself.\n\nfast_regression_parsnip_spec_tbl(\n  .parsnip_fns = \"all\", \n  .parsnip_eng = \"all\"\n  )\n\nNow let’s take a look at the arguments:\n\n.parsnip_fns - The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(\"linear_reg\",\"cubist_rules\")\n.parsnip_eng - The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c('lm', 'glm')\n\nThe workhorse to this function is the internal_make_spec_tbl() function. This is the one that will be the subject of the post. Let’s take a look at it’s inner workings, afterall this is open source.\n\ninternal_make_spec_tbl <- function(.data){\n\n  # Checks ----\n  df <- dplyr::as_tibble(.data)\n\n  nms <- unique(names(df))\n\n  if (!\".parsnip_engine\" %in% nms | !\".parsnip_mode\" %in% nms | !\".parsnip_fns\" %in% nms){\n    rlang::abort(\n      message = \"The model tibble must come from the class/reg to parsnip function.\",\n      use_cli_format = TRUE\n    )\n  }\n\n  # Make tibble ----\n  mod_spec_tbl <- df %>%\n    dplyr::mutate(\n      model_spec = purrr::pmap(\n        dplyr::cur_data(),\n        ~ match.fun(..3)(mode = ..2, engine = ..1)\n      )\n    ) %>%\n    # add .model_id column\n    dplyr::mutate(.model_id = dplyr::row_number()) %>%\n    dplyr::select(.model_id, dplyr::everything())\n\n  # Return ----\n  return(mod_spec_tbl)\n\n}\n\nLet’s examine this (and it is currently changing form in a github issue). Firstly, we are taking in a data.frame/tibble that has to have certain names in it (this is going to change and look for a class instead). Once this determination is TRUE we then proceed to the meat and potatoes of it. The internal mod_spec_tbl is made using mutate, pmap, cur_data and match.fun. What this does essentially is the following:\n\nmutate a column called model_spec\nUse the {purrr} function pmap which maps over several columns in parallel to create the model spec.\nInside of the pmap we use cur_data() to get the current line where we match the function using match.fun (which takes a character string of the function, this means the library needs to be loaded) we supply the column it is in and then we supply the arguments we want.\nWe give it a numeric model id\nWe then ensure that the .model_id column is first.\n\n\n\nExample\nLet’s see it in action!\n\nlibrary(tidyAML) # Not yet available, you can install from GitHub though\n\nfast_regression_parsnip_spec_tbl()\n\n# A tibble: 46 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n# … with 36 more rows\n\n\nSo we see we get a nicely generated tibble of output that matchs a model spec to the .model_id and to the appropriate parsnip engine and mode\nWe can also choose the models we may want by giving either arguments to the .parsnip_engine parameter or .parsnip_fns or both.\n\nlibrary(dplyr)\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n11        11 stan_glmer      regression    linear_reg   <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 glm             regression    linear_reg   <spec[+]> \n3         3 glm             regression    poisson_reg  <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = \"glm\") %>%\n  pull(model_spec)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n[[2]]\nPoisson Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "",
    "text": "Many times someone may want to see a summary or cumulative statistic for a given set of data or even from several simulations of data. I went over bootstrap plotting earlier this month, and this is a form of what we will go over today although slightly more restrictive.\nI have decided to make today my weekly r-tip because tomorrow is Thanksgiving here in the US and I am taking an extended holiday so I won’t be back until Monday.\nToday’s function and weekly tip is on tidy_stat_tbl(). It is meant to be used with a tidy_ distribution function. Let’s take a look."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Single Simulation",
    "text": "Single Simulation\nLet’s go over some examples. Firstly, we will go over all the different .return_type’s of a single simulation of tidy_normal() using the quantile function.\nVector Output BE CAREFUL IT USES SAPPLY\n\nlibrary(TidyDensity)\n\nset.seed(123)\ntn <- tidy_normal()\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = quantile,\n  na.rm = TRUE,\n  probs = c(0.025, 0.5, 0.975)\n  )\n\n      sim_number_1\n2.5%   -1.59190149\n50%    -0.07264039\n97.5%   1.77074730\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n$sim_number_1\n       2.5%         50%       97.5% \n-1.59190149 -0.07264039  1.77074730 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nNow let’s take a look with multiple simulations."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Multiple Simulations",
    "text": "Multiple Simulations\nLet’s set our simulation count to 5. While this is not a large amount it will serve as a good illustration on the outputs.\n\nns <- 5\nf  <- quantile\nnr <- TRUE\np  <- c(0.025, 0.975)\n\nOk let’s run the same simulations but with the updated params.\nVector Output BE CAREFUL IT USES SAPPLY\n\nset.seed(123)\ntn <- tidy_normal(.num_sims = ns)\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = f,\n  na.rm = nr,\n  probs = p\n  )\n\n      sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n2.5%     -1.591901    -1.474945    -1.656679    -1.258156    -1.309749\n97.5%     1.770747     1.933653     1.894424     2.098923     1.943384\n\ntidy_stat_tbl(\n  tn, y, .return_type = \"vector\",\n  .fns = f, na.rm = nr\n)\n\n     sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n0%    -1.96661716   -2.3091689   -2.0532472  -1.31080153   -1.3598407\n25%   -0.55931702   -0.3612969   -0.9505826  -0.49541417   -0.7140627\n50%   -0.07264039    0.1525789   -0.3048700  -0.07675993   -0.2240352\n75%    0.69817699    0.6294358    0.2900859   0.55145766    0.5287605\n100%   2.16895597    2.1873330    2.1001089   3.24103993    2.1988103\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\n$sim_number_2\n        0%        25%        50%        75%       100% \n-2.3091689 -0.3612969  0.1525789  0.6294358  2.1873330 \n\n$sim_number_3\n        0%        25%        50%        75%       100% \n-2.0532472 -0.9505826 -0.3048700  0.2900859  2.1001089 \n\n$sim_number_4\n         0%         25%         50%         75%        100% \n-1.31080153 -0.49541417 -0.07675993  0.55145766  3.24103993 \n\n$sim_number_5\n        0%        25%        50%        75%       100% \n-1.3598407 -0.7140627 -0.2240352  0.5287605  2.1988103 \n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr, \n  probs = p\n)\n\n$sim_number_1\n     2.5%     97.5% \n-1.591901  1.770747 \n\n$sim_number_2\n     2.5%     97.5% \n-1.474945  1.933653 \n\n$sim_number_3\n     2.5%     97.5% \n-1.656679  1.894424 \n\n$sim_number_4\n     2.5%     97.5% \n-1.258156  2.098923 \n\n$sim_number_5\n     2.5%     97.5% \n-1.309749  1.943384 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <chr>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <chr> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <fct>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <fct> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nOk, now that we have shown that, let’s ratchet up the simulations so we can see the true difference in using the .use_data_tbl parameter when simulations are large. We are going to use {rbenchmark} for"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Benchmarking",
    "text": "Benchmarking\nHere we go. We are going to make a tidy_bootstrap() of the mtcars$mpg data which will produce 2000 simulations, we will replicate this 25 times.\n\nlibrary(rbenchmark)\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# Get the interesting vector, well for this anyways\nx <- mtcars$mpg\n\n# Bootstrap the vector (2k simulations is default)\ntb <- tidy_bootstrap(x) %>%\n  bootstrap_unnest_tbl()\n\nbenchmark(\n  \"tibble\" = {\n    tidy_stat_tbl(tb, y, IQR, \"tibble\")\n  },\n  \"data.table\" = {\n    tidy_stat_tbl(tb, y, IQR, .use_data_table = TRUE, type = 7)\n  },\n  \"sapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"vector\")\n  },\n  \"lapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"list\")\n  },\n  replications = 25,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) %>%\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1 data.table           25    4.11    1.000      3.33     0.11\n2     lapply           25   24.14    5.873     20.02     0.38\n3     sapply           25   25.11    6.109     21.01     0.28\n4     tibble           25   33.18    8.073     27.45     0.51\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html",
    "title": "Updates to {healthyverse} packages",
    "section": "",
    "text": "I have made several updates to {healthyverse}, this has resulted in new releases to CRAN for {healthyR.ai}, {healthyR.ts}, and {TidyDesnsity}."
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "title": "Updates to {healthyverse} packages",
    "section": "TidyDensity",
    "text": "TidyDensity\nFor TidyDensity a new distribution was added, welcome tidy_bernoulli(). This distribution also comes with the standard util_distname_param_estimate() and the util_distname_stats_tbl() functions. Let’s take a look at the function calls.\n\ntidy_bernoulli(.n = 50, .prob = 0.1, .num_sims = 1)\n\nutil_bernoulli_param_estimate(.x, .auto_gen_empirical = TRUE)\n\nutil_bernoulli_stats_tbl(.data)\n\nLet’s see them in use.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntb <- tidy_bernoulli()\n\ntb\n\n# A tibble: 50 × 7\n   sim_number     x     y      dx     dy     p     q\n   <fct>      <int> <int>   <dbl>  <dbl> <dbl> <dbl>\n 1 1              1     0 -0.338  0.0366   0.9     0\n 2 1              2     0 -0.304  0.0866   0.9     0\n 3 1              3     0 -0.270  0.187    0.9     0\n 4 1              4     0 -0.236  0.369    0.9     0\n 5 1              5     0 -0.201  0.663    0.9     0\n 6 1              6     0 -0.167  1.09     0.9     0\n 7 1              7     0 -0.133  1.63     0.9     0\n 8 1              8     1 -0.0988 2.22     1       1\n 9 1              9     0 -0.0646 2.76     0.9     0\n10 1             10     0 -0.0304 3.14     0.9     0\n# … with 40 more rows\n\nutil_bernoulli_param_estimate(tb$y)\n\n$combined_data_tbl\n# A tibble: 100 × 8\n   sim_number     x     y      dx     dy     p     q dist_type\n   <fct>      <int> <dbl>   <dbl>  <dbl> <dbl> <dbl> <fct>    \n 1 1              1     0 -0.338  0.0366  0.92     0 Empirical\n 2 1              2     0 -0.304  0.0866  0.92     0 Empirical\n 3 1              3     0 -0.270  0.187   0.92     0 Empirical\n 4 1              4     0 -0.236  0.369   0.92     0 Empirical\n 5 1              5     0 -0.201  0.663   0.92     0 Empirical\n 6 1              6     0 -0.167  1.09    0.92     0 Empirical\n 7 1              7     0 -0.133  1.63    0.92     0 Empirical\n 8 1              8     1 -0.0988 2.22    1        0 Empirical\n 9 1              9     0 -0.0646 2.76    0.92     0 Empirical\n10 1             10     0 -0.0304 3.14    0.92     0 Empirical\n# … with 90 more rows\n\n$parameter_tbl\n# A tibble: 1 × 8\n  dist_type samp_size   min   max  mean variance sum_x  prob\n  <chr>         <int> <dbl> <dbl> <dbl>    <dbl> <dbl> <dbl>\n1 Bernoulli        50     0     1  0.08   0.0736     4  0.08\n\nutil_bernoulli_stats_tbl(tb) %>%\n  glimpse()\n\nRows: 1\nColumns: 18\n$ tidy_function      <chr> \"tidy_bernoulli\"\n$ function_call      <chr> \"Bernoulli c(0.1)\"\n$ distribution       <chr> \"Bernoulli\"\n$ distribution_type  <chr> \"discrete\"\n$ points             <dbl> 50\n$ simulations        <dbl> 1\n$ mean               <dbl> 0.1\n$ mode               <chr> \"0\"\n$ coeff_var          <dbl> 0.09\n$ skewness           <dbl> 2.666667\n$ kurtosis           <dbl> 5.111111\n$ mad                <dbl> 0.5\n$ entropy            <dbl> 0.325083\n$ fisher_information <dbl> 11.11111\n$ computed_std_skew  <dbl> 3.096281\n$ computed_std_kurt  <dbl> 10.58696\n$ ci_lo              <dbl> 0\n$ ci_hi              <dbl> 1"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ai",
    "text": "healthyR.ai\nThis was a minor patch release that exported some previously internal only functions and fixed an error with the custom recipe steps. One of the functions that has been exported is hai_data_impute()\nLet’s take a look.\n\nhai_data_impute(\n  .recipe_object = NULL,\n  ...,\n  .seed_value = 123,\n  .type_of_imputation = \"mean\",\n  .number_of_trees = 25,\n  .neighbors = 5,\n  .mean_trim = 0,\n  .roll_statistic,\n  .roll_window = 5\n)\n\nLet’s take a look at an example of it’s use.\n\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(healthyR.ai)\n\ndate_seq <- seq.Date(from = as.Date(\"2013-01-01\"), length.out = 100, by = \"month\")\nval_seq <- rep(c(rnorm(9), NA), times = 10)\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 NA     \n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nhai_data_impute(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_imputation = \"roll\",\n  .roll_statistic = median\n)$impute_rec_obj %>%\n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 -0.322 \n# … with 90 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ts",
    "text": "healthyR.ts\nThis was a minor patch release fixing the function ts_lag_correlation() when the column that was the value was not explicitly called…value.\nThank you!"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "",
    "text": "Many times in the real world we have a data set which is actually a sample as we typically do not know what the actual population is. This is where bootstrapping tends to come into play. It allows us to get a hold on what the possible parameter values are by taking repeated samples of the data that is available to us.\nAt it’s core it is a resampling method with replacement where it assigns measures of accuracy to the sample estimates. Here is the Wikipedia Article for bootstrapping.\nIn this post I am going to go over how to use the bootstrap function set with {TidyDensity}. You can find the pkgdown site with all function references here: TidyDensity"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Mean",
    "text": "Cumulative Mean\n\ntb %>%\n  bootstrap_stat_plot(.value = y)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE)\n\n\n\n\n\nYou can see from this output that the statistic you choose is printed in the chart title and on the y axis, the caption will also tell you how many simulations are present. Lets look at skewness as another example."
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Skewness",
    "text": "Cumulative Skewness\n\nsc <- \"cskewness\"\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE,\n                      .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .stat = sc,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE,\n                      .show_groups = TRUE,\n                      .stat = sc)\n\n\n\n\n\nVolia!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Steve On Data",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nI am using this as a site to host all of the tips and tricks for R/SQL and data that I will post to my LinkedIn, Twitter and Telegram channels."
  },
  {
    "objectID": "posts/rtip-2023-01-26/index.html",
    "href": "posts/rtip-2023-01-26/index.html",
    "title": "Transforming Your Data: A Guide to Popular Methods and How to Implement Them with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nTransforming data refers to the process of changing the scale or distribution of a variable in order to make it more suitable for analysis. There are many different methods for transforming data, and each has its own specific use case.\n\nBox-Cox: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses a power transformation to adjust the scale of the data.\nBasis Spline: This is a type of non-parametric regression that uses splines (piecewise polynomials) to model the relationship between a dependent variable and one or more independent variables.\nLog: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses the logarithm function to adjust the scale of the data.\nLogit: This is a method for transforming binary data (i.e., data with only two possible values) into a continuous scale. It uses the logistic function to adjust the scale of the data.\nNatural Spline: This is a type of non-parametric regression that uses splines (piecewise polynomials) to model the relationship between a dependent variable and one or more independent variables, where the splines are chosen to be as smooth as possible.\nRectified Linear Unit (ReLU): This is a type of activation function used in artificial neural networks. It is used to introduce non-linearity in the output of a neuron.\nSquare Root: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses the square root function to adjust the scale of the data.\nYeo-Johnson: This is a power transformation that works well for data that is positively or negatively skewed. It is a generalization of the Box-Cox transformation and handles zero and negative data.\n\nThe R library {healthyR.ai} provides a function called hai_data_transform() that allows users to easily apply any of these transforms to their data. The function takes in the data and the type of transformation as arguments, and returns the transformed data. This makes it easy for users to experiment with different transformations and see which one works best for their data.\n\n\nFunction\nLet’s take a look at the full function call.\n\nhai_data_transform(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"log\",\n  .bc_limits = c(-5, 5),\n  .bc_num_unique = 5,\n  .bs_deg_free = NULL,\n  .bs_degree = 3,\n  .log_base = exp(1),\n  .log_offset = 0,\n  .logit_offset = 0,\n  .ns_deg_free = 2,\n  .rel_shift = 0,\n  .rel_reverse = FALSE,\n  .rel_smooth = FALSE,\n  .yj_limits = c(-5, 5),\n  .yj_num_unique = 5\n)\n\nNow let’s go over the arguments to the parameters.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“boxcox”\n“bs”\n“log”\n“logit”\n“ns”\n“relu”\n“sqrt”\n“yeojohnson\n\n.bc_limits - A length 2 numeric vector defining the range to compute the transformation parameter lambda.\n.bc_num_unique - An integer to specify minimum required unique values to evaluate for a transformation\n.bs_deg_free - The degrees of freedom for the spline. As the degrees of freedom for a spline increase, more flexible and complex curves can be generated. When a single degree of freedom is used, the result is a rescaled version of the original data.\n.bs_degree - Degree of polynomial spline (integer).\n.log_base - A numeric value for the base.\n.log_offset - An optional value to add to the data prior to logging (to avoid log(0))\n.logit_offset - A numeric value to modify values of the columns that are either one or zero. They are modifed to be x - offset or offset respectively.\n.ns_deg_free - The degrees of freedom for the natural spline. As the degrees of freedom for a natural spline increase, more flexible and complex curves can be generated. When a single degree of freedom is used, the result is a rescaled version of the original data.\n.rel_shift - A numeric value dictating a translation to apply to the data.\n.rel_reverse - A logical to indicate if the left hinge should be used as opposed to the right hinge.\n.rel_smooth - A logical indicating if hte softplus function, a smooth approximation to the rectified linear transformation, should be used.\n.yj_limits - A length 2 numeric vector defining the range to compute the transformation parameter lambda.\n.yj_num_unique - An integer where data that have less possible values will not be evaluated for a transformation.\n\n\n\nExamples\nLet’s look over some examples. For an example data set we are going to pick on the mtcars data set as the histogram will prove to be skewed which makes it a good candidate to test these transformations on.\n\ninstall.packages(\"healthyR.ai\")\n\nNow that we have {healthyR.ai} installed we can get to work. It does use the {recipes} package underneath so you will need to have that installed as well. Let’s look at the histogram of mtcars now.\n\nmpg_vec <- mtcars$mpg\n\nhist(mpg_vec)\n\n\n\nplot(density(mpg_vec))\n\n\n\n\nFirst up, Box-Cox\n\nlibrary(healthyR.ai)\nlibrary(recipes)\n\nro <- recipe(mpg ~ wt, data = mtcars)\n\nboxcox_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"boxcox\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(boxcox_vec))\n\n\n\n\nBasis Spline\n\nbs_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"bs\"\n)$scale_rec_obj %>%\n  get_juiced_data()\n\nplot(density(bs_vec$mpg_bs_1))\n\n\n\nplot(density(bs_vec$mpg_bs_2))\n\n\n\nplot(density(bs_vec$mpg_bs_3))\n\n\n\n\nLog\n\nlog_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"log\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(log_vec))\n\n\n\n\nYeo-Johnson\n\nyj_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"yeojohnson\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(yj_vec))\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyr-2023-01-27/index.html",
    "href": "posts/weekly-rtip-healthyr-2023-01-27/index.html",
    "title": "Service Line Grouping with {healthyR}",
    "section": "",
    "text": "Introduction\nHealthcare data analysis can be a complex and time-consuming task, but it doesn’t have to be. Meet {healthyR}, your new go-to R package for all things healthcare data analysis. With {healthyR}, you can easily and efficiently analyze your healthcare data, and make sense of the information it contains.\nOne of the key features of {healthyR} is the service_line_augment() function. This function is designed to help you quickly and easily append a vector to a data.frame or tibble that is passed to the .data parameter. In order to use this function, you will need a data.frame or tibble with a principal diagnosis column, a principal procedure column, and a column for the DRG number. These are needed so that the function can join the dx_cc_mapping and px_cc_mapping columns to provide the service line.\nThe service_line_augment() function is especially useful for analyzing healthcare data that is coded using ICD Version 10. This version of the ICD coding system is widely used in the healthcare industry, and the service_line_augment() function is specifically designed to work with it. With this function, you can quickly and easily append a vector to your data.frame or tibble that provides the service line for each visit.\nIn addition to the service_line_augment() function, {healthyR} also includes a wide range of other useful tools and functions for healthcare data analysis. Whether you’re looking to analyze claims data, clinical data, or any other type of healthcare data, {healthyR} has you covered.\nSo why wait? Download {healthyR} today and start making sense of your healthcare data! With {healthyR}, you can easily and efficiently analyze your healthcare data, and make sense of the information it contains.\n\n\nFunction\nLet’s take a look at the full function call.\n\nservice_line_augment(.data, .dx_col, .px_col, .drg_col)\n\nNow let’s look at the arguments to the parameters.\n\n.data - The data being passed that will be augmented by the function.\n.dx_col - The column containing the Principal Diagnosis for the discharge.\n.px_col - The column containing the Principal Coded Procedure for the discharge. It is possible that this could be blank.\n.drg_col - The DRG Number coded to the inpatient discharge.\n\nNow for some examples.\n\n\nExample\nFirst if you have not already, install {healthyR}\n\ninstall.packages(\"healthyR\")\n\nHere we go.\n\nlibrary(healthyR)\n\ndf <- data.frame(\n  dx_col = \"F10.10\",\n  px_col = NA,\n  drg_col = \"896\"\n)\n\nservice_line_augment(\n  .data = df,\n  .dx_col = dx_col,\n  .px_col = px_col,\n  .drg_col = drg_col\n)\n\n# A tibble: 1 × 4\n  dx_col px_col drg_col service_line \n  <chr>  <lgl>  <chr>   <chr>        \n1 F10.10 NA     896     alcohol_abuse\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-30/index.html",
    "href": "posts/rtip-2023-01-30/index.html",
    "title": "{healthyR.ts}: The New and Improved Library for Time Series Analysis",
    "section": "",
    "text": "Introduction\nAre you looking for a powerful and efficient library for time series analysis? Look no further than {healthyR.ts}! This library has recently been updated with new functions and improvements, making it easier for you to analyze and visualize your time series data.\nOne of the new functions in {healthyR.ts} is ts_geometric_brownian_motion(). This function allows you to generate multiple Brownian motion simulations at once, saving you time and effort. With this feature, you can easily generate multiple simulations to compare and analyze different scenarios.\nAnother new function, ts_brownian_motion_augment(), enables you to add a Brownian motion to a time series that you provide. This is a great tool for analyzing the impact of random variations on your data.\nThe ts_geometric_brownian_motion_augment() function generates a geometric Brownian motion, allowing you to study the effects of compounding growth or decay in your time series data. And, with the ts_brownian_motion_plot() function, you can easily plot both augmented and non-augmented Brownian motion plots, giving you a visual representation of your data.\nIn addition to the new functions, {healthyR.ts} has also made several minor fixes and improvements. For example, the ts_brownian_motion() function has been updated and optimized, resulting in a 49x speedup due to vectorization. Additionally, all Brownian motion functions now have an attribute of .motion_type, making it easier to track and identify your data.\nWith all of these new features and improvements, {healthyR.ts} is the ideal library for anyone looking to analyze and visualize time series data. So, if you want to take your time series analysis to the next level, install {healthyR.ts} today!\n\n\nFunction\nLet’s take a look at the new functions.\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\nIts arguments.\n\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.initial_value - Integer representing the initial value.\n.delta_time - Time step size.\n.return_tibble - The default is TRUE. If set to FALSE then an object of class matrix will be returned.\n\n\nts_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = NULL\n)\n\nIts arguments.\n\n.data - The data.frame/tibble being augmented.\n.date_col - The column that holds the date.\n.value_col - The value that is going to get augmented. The last value of this column becomes the initial value internally.\n.time - How many time steps ahead.\n.num_sims - How many simulations should be run.\n.delta_time - Time step size.\n\n\nts_geometric_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .num_sims = 10,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .delta_time = 1/365\n)\n\nIts arguments.\n\n.data - The data you are going to pass to the function to augment.\n.date_col - The column that holds the date\n.value_col - The column that holds the value\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.delta_time - Time step size.\n\n\nts_brownian_motion_plot(\n  .data, \n  .date_col, \n  .value_col, \n  .interactive = FALSE\n)\n\nIts arguments.\n\n.data - The data you are going to pass to the function to augment.\n.date_col - The column that holds the date\n.value_col - The column that holds the value\n.interactive - The default is FALSE, TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nFirst make sure you install {healthyR.ts} if you do not yet already have it, otherwise update it to gain th enew functionality.\n\ninstall.packages(\"healthyR.ts\")\n\nNow let’s take a look at ts_geometric_brownian_motion().\n\nlibrary(healthyR.ts)\n\nts_geometric_brownian_motion()\n\n# A tibble: 2,600 × 3\n   sim_number        t     y\n   <fct>         <int> <dbl>\n 1 sim_number 1      1   100\n 2 sim_number 2      1   100\n 3 sim_number 3      1   100\n 4 sim_number 4      1   100\n 5 sim_number 5      1   100\n 6 sim_number 6      1   100\n 7 sim_number 7      1   100\n 8 sim_number 8      1   100\n 9 sim_number 9      1   100\n10 sim_number 10     1   100\n# … with 2,590 more rows\n\n\nNow let’s take a look at ts_brownian_motion_augment().\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value\n)\n\n# A tibble: 1,041 × 3\n   sim_number  date_col    value\n   <fct>       <date>      <dbl>\n 1 actual_data 2022-01-01 -0.303\n 2 actual_data 2022-01-02 -1.17 \n 3 actual_data 2022-01-03 -1.44 \n 4 actual_data 2022-01-04 -0.682\n 5 actual_data 2022-01-05 -2.31 \n 6 actual_data 2022-01-06 -1.19 \n 7 actual_data 2022-01-07 -0.454\n 8 actual_data 2022-01-08 -1.83 \n 9 actual_data 2022-01-09  0.659\n10 actual_data 2022-01-10 -0.150\n# … with 1,031 more rows\n\n\nNow ts_geometric_brownian_motion_augment().\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_geometric_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value\n)\n\n# A tibble: 291 × 3\n   sim_number  date_col    value\n   <fct>       <date>      <dbl>\n 1 actual_data 2022-01-01 -1.47 \n 2 actual_data 2022-01-02 -1.63 \n 3 actual_data 2022-01-03  1.01 \n 4 actual_data 2022-01-04  1.44 \n 5 actual_data 2022-01-05 -1.05 \n 6 actual_data 2022-01-06 -0.599\n 7 actual_data 2022-01-07 -0.393\n 8 actual_data 2022-01-08  1.06 \n 9 actual_data 2022-01-09 -0.121\n10 actual_data 2022-01-10 -0.349\n# … with 281 more rows\n\n\nNow for ts_brownian_motion_plot().\n\nts_geometric_brownian_motion() %>%\n  ts_brownian_motion_plot(.date_col = t, .value_col = y)\n\n\n\n\n\nts_brownian_motion() %>%\n  ts_brownian_motion_plot(t, y, .interactive = TRUE)\n\n\n\n\n\nAnd with the augmenting functions\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value,\n  .time = 30,\n  .num_sims = 30\n) %>%\n  ts_brownian_motion_plot(date_col, value, TRUE)\n\n\n\n\n\nAnd with a static ggplot2 plot.\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value,\n  .time = 30,\n  .num_sims = 30\n) %>%\n  ts_brownian_motion_plot(date_col, value)\n\n\n\n\nThank you for reading, and Voila!"
  },
  {
    "objectID": "posts/rtip-2023-01-31/index.html",
    "href": "posts/rtip-2023-01-31/index.html",
    "title": "Median: A Simple Way to Detect Excess Events Over Time with {healthyR}",
    "section": "",
    "text": "Introduction\nAs we collect data over time, it’s important to look for patterns and trends that can help us understand what’s happening. One common way to do this is to look at the median of the data. The median is the middle value of a set of numbers, and it can be a useful tool for detecting whether there is an excess of events, either positive or negative, occurring over time.\nBenefits of Looking at Median:\n\nShows the central tendency: The median gives us a good idea of the central tendency of the data. This can help us understand what’s typical and what’s not.\nResistant to outliers: Unlike the mean, the median is not affected by outliers. This means that if there are a few extreme values in the data, the median will not be skewed by them.\nEasy to understand: The median is easy to understand, even for people who are not familiar with statistics.\n\nUsing the R Library {healthyR} provides a convenient way to perform median analysis. The function ts_median_excess_plt() can be used to plot the median of an event over time and detect any excess events that may be occurring. This function is designed to be user-friendly, so even if you’re not an expert in statistics, you can still use it to gain valuable insights into your data.\nIn conclusion, looking at the median of an event over time can be a useful tool for detecting excess events, either positive or negative. The R library {healthyR} provides a convenient way to perform this analysis with the function ts_median_excess_plt(). Give it a try and see what insights you can uncover in your own data!\n\n\nFunction\nHere is the full function call.\n\nts_median_excess_plt(\n  .data,\n  .date_col,\n  .value_col,\n  .x_axis,\n  .ggplot_group_var,\n  .years_back\n)\n\nHere are its arguments.\n\n.data - The data that is being analyzed, data must be a tibble/data.frame.\n.date_col - The column of the tibble that holds the date.\n.value_col - The column that holds the value of interest.\n.x_axis - What is the be the x-axis, day, week, etc.\n.ggplot_group_var - The variable to group the ggplot on.\n.years_back - How many yeas back do you want to go in order to compute the median value.\n\n\n\nExample\nFirst make sure you have the package installed.\n\ninstall.packages(\"healthyR\")\n\nNow for an example. The data is required to be in a certain format, this function is dated, meaning it was one of the first ones I wrote so I will be taking time to improve it in the future. We are using data from my {healthyR.data]} package.\n\nlibrary(healthyR.data)\nlibrary(lubridate)\nlibrary(healthyR)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(ggplot2)\n\ndf <- healthyR_data %>%\n  filter_by_time(\n    .date_var = visit_start_date_time,\n    .start_date = \"2012\",\n    .end_date = \"2019\"\n  ) %>%\n  filter(ip_op_flag == \"I\") %>%\n  select(visit_id, visit_start_date_time) %>%\n  mutate(\n    visit_start_date_time = as.Date(visit_start_date_time, \"%Y%M%D\"),\n    record = 1\n    ) %>%\n  summarise_by_time(\n    .date_var = visit_start_date_time,\n    visits = sum(record)\n  ) %>%\n  ts_signature_tbl(\n    .date_col = visit_start_date_time\n  )\n\nOk now that we have our data, let’s take a look at it using glimpse()\n\nglimpse(df)\n\nRows: 2,922\nColumns: 30\n$ visit_start_date_time <date> 2012-01-01, 2012-01-02, 2012-01-03, 2012-01-04,…\n$ visits                <dbl> 34, 52, 53, 44, 46, 55, 42, 29, 50, 55, 50, 43, …\n$ index.num             <dbl> 1325376000, 1325462400, 1325548800, 1325635200, …\n$ diff                  <dbl> NA, 86400, 86400, 86400, 86400, 86400, 86400, 86…\n$ year                  <int> 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ year.iso              <int> 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ half                  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ quarter               <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ month                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ month.xts             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ month.lbl             <ord> January, January, January, January, January, Jan…\n$ day                   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ hour                  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ minute                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ second                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hour12                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ am.pm                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ wday                  <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, …\n$ wday.xts              <int> 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, …\n$ wday.lbl              <ord> Sunday, Monday, Tuesday, Wednesday, Thursday, Fr…\n$ mday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ qday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ yday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ mweek                 <int> 5, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, …\n$ week                  <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, …\n$ week.iso              <int> 52, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3,…\n$ week2                 <int> 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ week3                 <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, …\n$ week4                 <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, …\n$ mday7                 <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, …\n\n\nNow to visualize it.\n\ndf %>%\n  ts_median_excess_plt(\n    .date_col = visit_start_date_time,\n    .value_col = visits,\n    .x_axis = month.lbl,\n    .ggplot_group_var = year,\n    .years_back = 3\n  ) +\n  labs(\n    y = \"Excess Visits\",\n    title = \"Excess Visits by Month YoY\"\n  ) + \n  theme(axis.text.x=element_text(angle = -90, hjust = 0))\n\n\n\n\nSo from here what we can see is that looking back in time over the visits data that the current year (the max year in the data) shows that it is significantly under previous years median values by month.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-01/index.html",
    "href": "posts/rtip-2023-02-01/index.html",
    "title": "Attributes in R Functions: An Overview",
    "section": "",
    "text": "R is a powerful programming language that is widely used for data analysis, visualization, and machine learning. One of the features of R that makes it versatile and flexible is the ability to assign attributes to functions. Attributes are metadata associated with an object in R, and they can be used to store additional information about the function or to modify the behavior of the function.\nIn this blog post, we will discuss what attributes are, how they can be useful, and how they can be used inside R functions.\n\n\nAttributes are pieces of information that are stored alongside an object in R. Functions are objects in R, and they can have attributes associated with them. Some of the common attributes associated with functions in R include:\n\nformals: This attribute stores the arguments of the function and their default values.\nsrcref: This attribute stores the source code of the function, including the line numbers of the code.\nenvironment: This attribute stores the environment in which the function was defined.\n\n\n\n\nAttributes can be useful in R functions in several ways, including:\n\nDebugging: Attributes can be used to store information that can be used to debug functions. For example, the srcref attribute can be used to retrieve the source code of the function and the line numbers of the code, which can be useful when trying to identify the source of an error.\nMetadata: Attributes can be used to store metadata about the function, such as the author, version, and date of creation. This information can be used to keep track of the function and to provide information about its purpose and usage.\nModifying Function Behavior: Attributes can be used to modify the behavior of the function. For example, the environment attribute can be used to set the environment in which the function is executed. This can be useful when creating closures or when using functions in a specific context.\n\n\n\n\nTo access or modify the attributes of a function in R, you can use the attributes() function. For example, to retrieve the formals attribute of a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\nformals(f)\n\n$x\n\n\n$y\n\n\nTo add an attribute to a function, you can use the attr() function. For example, to add a version attribute to a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattr(f, \"version\") <- \"1.0\"\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\n$version\n[1] \"1.0\"\n\n\nTo remove an attribute from a function, you can use the attributes() function with the NULL value. For example, to remove the version attribute from a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattr(f, \"version\") <- \"1.0\"\nattributes(f)$version <- NULL\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\n\nConclusion\nAttributes are a useful feature in R functions that can be used to store additional information about the function, to debug the function, and to modify its behavior. By using attributes, you can make your functions more versatile, flexible, and easier to work with."
  },
  {
    "objectID": "posts/rtip-2023-02-02/index.html",
    "href": "posts/rtip-2023-02-02/index.html",
    "title": "Diverging Lollipop Chart: A Visual Tool for Comparing Data with {healthyR}",
    "section": "",
    "text": "Introduction\nA diverging lollipop chart is a useful tool for comparing data that falls into two categories, usually indicated by different colors. This type of chart is particularly well-suited for comparing the differences between two data sets and for identifying which data points are contributing most to the differences.\nThe R package {healthyR} offers a function called diverging_lollipop_plt() that can be used to create a diverging lollipop chart. This function has several parameters that can be used to customize the chart to meet your specific needs.\nIn conclusion, the diverging lollipop chart is a useful tool for comparing data sets and can provide insights into the differences between two sets of data. The diverging_lollipop_plt() function from the {healthyR} package is a great option for creating this type of chart, as it offers a range of customization options to meet your specific needs. Whether you’re working with data related to business, finance, or any other field, a diverging lollipop chart can be a valuable tool in your visual analysis toolkit.\n\n\nFunction\nLet’s take a look at the full function call.\n\ndiverging_lollipop_plt(\n  .data,\n  .x_axis,\n  .y_axis,\n  .plot_title = NULL,\n  .plot_subtitle = NULL,\n  .plot_caption = NULL,\n  .interactive = FALSE\n)\n\nNow lets see the arguments that get provided to the parameters.\n\n.data - The data to pass to the function, must be a tibble/data.frame.\n.x_axis - The data that is passed to the x-axis. This will also be the x and xend parameters of the geom_segment\n.y_axis - The data that is passed to the y-axis. This will also equal the parameters of yend and label\n.plot_title - Default is NULL\n.plot_subtitle - Default is NULL\n.plot_caption - Default is NULL\n.interactive - Default is FALSE. TRUE returns a plotly plot\n\n\n\nExample\nLet’s see an example.\n\nlibrary(healthyR)\n\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata(\"mtcars\")\nmtcars$car_name <- rownames(mtcars)\nmtcars$mpg_z <- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), 2)\nmtcars$mpg_type <- ifelse(mtcars$mpg_z < 0, \"below\", \"above\")\nmtcars <- mtcars[order(mtcars$mpg_z), ]  # sort\nmtcars$car_name <- factor(mtcars$car_name, levels = mtcars$car_name)\n\ndiverging_lollipop_plt(\n  .data = mtcars, \n  .x_axis = car_name,\n  .y_axis = mpg_z\n)\n\n\n\n\nNow let’s also see the interactive chart.\n\ndiverging_lollipop_plt(\n  .data = mtcars, \n  .x_axis = car_name,\n  .y_axis = mpg_z,\n  .interactive = TRUE\n)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-03/index.html",
    "href": "posts/rtip-2023-02-03/index.html",
    "title": "The Argument Matcher: A Function for Selecting the Right Arguments {tidyAML}",
    "section": "",
    "text": "Introduction\nI am working on finishing up a few things with my new R package {tidyAML} before I release it to CRAN. One of those things is the ability of a user to build a model using a command that might be something like generate_model(). One of the things that is necessary to do is to match the function arguments from the generate_model() to the actual parsnip call.\nThis is where and argument matcher of sorts may come in handy. I am doing this because it will take one most step of abstraction away, and instead of say calling linear_reg() or mars() or something like that, you can just instead use generate_model() and type in your engine or the parsnip function call there.\nNow I am not one hundred percent certain that I’ll actually implement this or not, but the exercise was fun enough that I decided to share it. So let’s get into it.\n\n\nFunction\nHere is the current state of the function.\n\nargument_matcher <- function(.f = \"linear_reg\", .args = list()){\n  \n  # TidyEval ----\n  fns <- as.character(.f)\n  \n  fns_args <- formalArgs(fns)\n  fns_args_list <- as.list(fns_args)\n  names(fns_args_list) <- fns_args\n  \n  arg_list <- .args\n  arg_list_names <- unique(names(arg_list))\n  \n  l <- list(arg_list, fns_args_list)\n  \n  arg_idx <- which(arg_list_names %in% fns_args_list)\n  bad_arg_idx <- which(!arg_list_names %in% fns_args_list)\n  \n  bad_args <- arg_list[bad_arg_idx]\n  bad_arg_names <- unique(names(bad_args))\n  \n  final_args <- arg_list[arg_idx]\n  \n  # Return ----\n  if (length(bad_arg_names > 0)){\n    rlang::inform(\n      message = paste0(\"bad arguments passed: \", bad_arg_names),\n      use_cli_format = TRUE\n    )\n  }\n\n  return(final_args)\n}\n\nWhen working with R functions, it’s not uncommon to encounter a situation where you need to pass arguments to another function. This can be especially challenging when the arguments are not properly matched. Fortunately, the argument_matcher function provides an elegant solution to this problem.\nThe argument_matcher function takes two arguments: .f and .args. The .f argument is a string that specifies the name of the function you want to pass arguments to, while the .args argument is a list that contains the arguments you want to pass to the specified function.\nThe argument_matcher function first uses the formalArgs function to extract the formal arguments of the specified function and store them in fns_args. The names of the formal arguments are then used to create a list, fns_args_list.\nNext, the function extracts the names of the arguments in .args and stores them in arg_list_names. It then checks if the names of the arguments in .args match the names of the formal arguments of the specified function, and stores the matching arguments in final_args. Any arguments that don’t match the formal arguments are stored in bad_args, and a warning message is printed indicating that bad arguments were passed.\nThe final step is to return the final_args list, which contains only the arguments that match the formal arguments of the specified function.\nIn conclusion, the argument_matcher function is a useful tool for ensuring that arguments are properly matched when passed to another function. Whether you’re working with linear regression models or any other type of function, the argument_matcher function will help you select the right arguments and avoid common errors.\n\n\nExample\nLet’s see a simple example.\n\nsuppressPackageStartupMessages(library(tidymodels))\n\nargument_matcher(\n  .args = list(\n    mode = \"regression\", \n    engine = \"lm\",\n    cost = 0.5,\n    trees = 1, \n    mtry = 1\n    )\n  )\n\nbad arguments passed: cost\nbad arguments passed: trees\nbad arguments passed: mtry\n\n\n$mode\n[1] \"regression\"\n\n$engine\n[1] \"lm\"\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-06/inex.html",
    "href": "posts/rtip-2023-02-06/inex.html",
    "title": "Cumulative Measurement Functions with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re looking for an easy-to-use package to calculate cumulative statistics in R, you may want to check out the TidyDensity package. This package offers several functions to calculate cumulative measurements, including mean, median, standard deviation, variance, skewness, kurtosis, harmonic mean, and geometric mean.\n\ncgmean(): Cumulative Geometric Mean\n\nThe cgmean() function calculates the cumulative geometric mean of a set of values. This is the nth root of the product of the first n elements of the set. It’s a useful measurement for sets of values that are multiplied together, such as growth rates.\n\nchmean(): Cumulative Harmonic Mean\n\nThe chmean() function calculates the cumulative harmonic mean of a set of values. This is the inverse of the arithmetic mean of the reciprocals of the values. It’s commonly used for sets of values that represent rates, such as speeds.\n\nckurtosis(): Cumulative Kurtosis\n\nThe ckurtosis() function calculates the cumulative kurtosis of a set of values. Kurtosis is a measure of the peakedness of a distribution, relative to a normal distribution. The cumulative kurtosis calculates the kurtosis of a set of values up to a specific point in the set.\n\ncmean(): Cumulative Mean\n\nThe cmean() function calculates the cumulative mean of a set of values. It’s a measure of the average of the values up to a specific point in the set.\n\ncmedian(): Cumulative Median\n\nThe cmedian() function calculates the cumulative median of a set of values. It’s the value that separates the lower half of the set from the upper half, up to a specific point in the set.\n\ncsd(): Cumulative Standard Deviation\n\nThe csd() function calculates the cumulative standard deviation of a set of values. Standard deviation is a measure of the spread of values in a set. The cumulative standard deviation calculates the standard deviation up to a specific point in the set.\n\ncskewness(): Cumulative Skewness\n\nThe cskewness() function calculates the cumulative skewness of a set of values. Skewness is a measure of the asymmetry of a distribution. The cumulative skewness calculates the skewness up to a specific point in the set.\n\ncvar(): Cumulative Variance\n\nThe cvar() function calculates the cumulative variance of a set of values. Variance is a measure of the spread of values in a set. The cumulative variance calculates the variance up to a specific point in the set.\nIn conclusion, the {TidyDensity} package offers several functions for calculating cumulative statistics, including mean, median, standard deviation, variance, skewness, kurtosis, harmonic mean, and geometric mean. These functions make it easy to calculate cumulative statistics for sets of values in R.\n\n\nFunctions\nAll of the functions perform work strictly on a vector. Because of this I will not go over the function calls separately because they all follow the vectorized for of fun(.x) where .x is the argument passed to the cumulative function.\n\n\nExamples\nHere I will go over some examples of each function use the AirPassengers data set.\n\nlibrary(TidyDensity)\n\nv <- AirPassengers\n\nLet’s start at the top.\nCumulative Geometric Mean:\n\nhead(cgmean(v))\n\n[1] 112.0000 114.9609 120.3810 122.4802 122.1827 124.2311\n\ntail(cgmean(v))\n\n[1] 249.6135 251.1999 252.4577 253.5305 254.2952 255.2328\n\nplot(cgmean(v), type = \"l\")\n\n\n\n\nCumulative Harmonic Mean:\n\nhead(chmean(v))\n\n[1] 112.00000  57.46087  40.03378  30.55222  24.39304  20.66000\n\ntail(chmean(v))\n\n[1] 1.636832 1.632423 1.627194 1.621471 1.614757 1.608744\n\nplot(chmean(v), type = \"l\")\n\n\n\n\nCumulative Kurtosis:\n\nhead(ckurtosis(v))\n\n[1]      NaN 1.000000 1.500000 1.315839 1.597316 1.597850\n\ntail(ckurtosis(v))\n\n[1] 2.668951 2.795314 2.733117 2.674195 2.649894 2.606228\n\nplot(ckurtosis(v), type = \"l\")\n\n\n\n\nCumulative Mean:\n\nhead(cmean(v))\n\n[1] 112.0000 115.0000 120.6667 122.7500 122.4000 124.5000\n\ntail(cmean(v))\n\n[1] 273.1367 275.5143 277.1631 278.4577 279.2378 280.2986\n\nplot(cmean(v), type = \"l\")\n\n\n\n\nCumulative Median:\n\nhead(cmedian(v))\n\n[1] 112.0 115.0 118.0 123.5 121.0 125.0\n\ntail(cmedian(v))\n\n[1] 259.0 261.5 264.0 264.0 264.0 265.5\n\nplot(cmedian(v), type = \"l\")\n\n\n\n\nCumulative Standard Deviation:\n\nhead(csd(v))\n\n[1]        NA  4.242641 10.263203  9.358597  8.142481  8.916277\n\ntail(csd(v))\n\n[1] 115.0074 117.9956 119.1924 119.7668 119.7083 119.9663\n\nplot(csd(v), type = \"l\")\n\n\n\n\nCumulative Skewness:\n\nhead(cskewness(v))\n\n[1]         NaN  0.00000000  0.44510927 -0.14739157 -0.02100016 -0.18544758\n\ntail(cskewness(v))\n\n[1] 0.5936970 0.6471651 0.6349071 0.6145579 0.5972102 0.5770682\n\nplot(cskewness(v), type = \"l\")\n\n\n\n\nCumulative Variance:\n\nhead(cvar(v))\n\n[1]        NA  18.00000 105.33333  87.58333  66.30000  79.50000\n\ntail(cvar(v))\n\n[1] 13226.70 13922.96 14206.84 14344.08 14330.07 14391.92\n\nplot(cvar(v), type = \"l\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-07/index.html",
    "href": "posts/rtip-2023-02-07/index.html",
    "title": "Subsetting Named Lists in R",
    "section": "",
    "text": "Introduction\nIn R, lists are a fundamental data structure that allows us to store multiple objects of different data types under a single name. Often times, we want to extract certain elements of a list based on their names, and this can be accomplished through the use of the subset function. In this blog post, we will take a look at how to use the grep function to subset named lists in R.\nFirst, we will create a list object as follows:\n\nasc_list &lt;- list(\n  Facility = 1:10,\n  State = 11:20,\n  National = 21:30\n)\n\nWe now have a list with three elements, each with a different name. Next, we want to make sure that our list does not contain any 0 length items. This can be achieved by using the lapply function and the length function:\n\nasc_list &lt;- asc_list[lapply(asc_list, length) &gt; 0]\n\nThe lapply function applies the length function to each element of the list, and returns a logical vector indicating whether each element is of length greater than 0. By using the square bracket operator, we can extract only those elements for which the logical value is TRUE.\nNext, we create a character vector of possible items that we want to match on:\n\npatterns &lt;- c(\"state\",\"faci\")\n\nWe can now pass this vector of patterns to the grep function, along with the names of our list and the ignore.case argument set to TRUE. The grep function returns the indices of the elements in our list that match the given pattern:\n\nasc_list[grep(\n  paste(patterns, collapse = \"|\"),\n  names(asc_list),\n  ignore.case = TRUE\n  )]\n\n$Facility\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$State\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nThe result of this code is a new list that contains only the elements of our original list whose names match either “state” or “faci”. The paste function is used to join the patterns in the vector into a single string, with the | character separating each pattern. This allows us to search for multiple patterns at once.\nIn conclusion, the grep function is a powerful tool for sub-setting named lists in R, especially when we have multiple patterns that we want to match on. By combining the grep function with other R functions such as lapply and length, we can extract specific elements from our lists with ease."
  },
  {
    "objectID": "posts/rtip-2023-02-08/index.html",
    "href": "posts/rtip-2023-02-08/index.html",
    "title": "Creating an R Project Directory",
    "section": "",
    "text": "Introduction\nWhen working in R I find it best to create a new project when working on something. This keeps all of the data and scripts in one location. This also means that if you are not careful the directory you have your project in can become quite messy. This used to happen to me with regularity, then I got smart and wrote a script that would standardize how projects are built for me.\nI find it important to have different fodlers for different parts of a project. This does not mean I will use them all for every project but that is fine, you can either comment that portion out or just delete the files that are created.\n\n\nFunction\nHere is what I do broken down into different steps. First, I see if the package {fs} is installed, and if not, then install it, and finally load it.\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\nNext we create a character vector of folder paths that will exist inside of the main project folder itself.\n\nfolders <- c(\n  \"00_Scripts\"\n  , \"00_Data\"\n  , \"01_Queries\"\n  , \"02_Data_Manipulation\"\n  , \"03_Viz\"\n  , \"04_TS_Modeling\"\n  , \"99_Automations\"\n)\n\nNow that the folders we want are spelt out, we can create them.\n\nfs::dir_create(\n  path = folders\n)\n\nNow that is done, it’s off to creating a few files that I personally almost always use. I do a lot of work out of a data warehouse so a connection file is needed. We also need a disconnection function.\n\n# DSS Connection \ndb_connect <- function() {\n  db_con <- LICHospitalR::db_connect()\n  \n  return(db_con)\n  \n}\n\n# Disconnect from Database\ndb_disconnect <- function(.connection) {\n  \n  DBI::dbDisconnect(\n    conn = db_connect()\n  )\n  \n}\n\nNow, let’s load in the typical libraries. You can modify this to suit your own needs.\n\n# Library Load\n\nlibrary_load <- function(){\n  \n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"DBI\"\n    , \"odbc\"\n    , \"janitor\"\n    , \"dplyr\"\n    , \"tibble\"\n    , \"tidyr\"\n    , \"LICHospitalR\"\n    , \"modeltime\"\n  )\n  \n}\n\nOk so now the functions have been created, let’s dump them!\n\ndb_funs <- c(\"db_connect\",\"db_disconnect\")\ndump(\n  list = db_funs,\n  file = \"00_Scripts/db_con_obj.R\"\n)\n\nlib_funs <- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Scripts/library_load.R\"\n)\n\n\n\nExample\nHere is the full script!\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\nfolders <- c(\n  \"00_Scripts\"\n  , \"00_Data\"\n  , \"01_Queries\"\n  , \"02_Data_Manipulation\"\n  , \"03_Viz\"\n  , \"04_TS_Modeling\"\n  , \"99_Automations\"\n)\n\nfs::dir_create(\n  path = folders\n)\n\n\nfile_create(\"01_Queries/query_functions.R\")\nfile_create(\"02_Data_Manipulation/data_functions.R\")\nfile_create(\"03_Viz/viz_functions.R\")\nfile_create(\"04_TS_Modeling/ts_functions.R\")\n\n# DSS Connection \ndb_connect <- function() {\n  db_con <- LICHospitalR::db_connect()\n  \n  return(db_con)\n  \n}\n\n# Disconnect from Database\ndb_disconnect <- function(.connection) {\n  \n  DBI::dbDisconnect(\n    conn = db_connect()\n  )\n  \n}\n\n# Library Load\n\nlibrary_load <- function(){\n  \n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"DBI\"\n    , \"odbc\"\n    , \"janitor\"\n    , \"dplyr\"\n    , \"tibble\"\n    , \"tidyr\"\n    , \"LICHospitalR\"\n    , \"modeltime\"\n  )\n  \n}\n\ndb_funs <- c(\"db_connect\",\"db_disconnect\")\ndump(\n  list = db_funs,\n  file = \"00_Scripts/db_con_obj.R\"\n)\n\nlib_funs <- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Scripts/library_load.R\"\n)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-09/index.html",
    "href": "posts/rtip-2023-02-09/index.html",
    "title": "Creating and Predicting Fast Regression Parsnip Models with {tidyAML}",
    "section": "",
    "text": "Introduction\nI am almost ready for a first release of my R package {tidyAML}. The purpose of this is to act as a way of quickly generating models using the parsnip package and keeping things inside of the tidymodels framework allowing users to seamlessly create models in tidyAML but pluck and move them over to tidymodels should they prefer. This is because I believe that software should be interchangeable and work well with other libraries. Today I am going to showcase how the function fast_regression()\n\n\nFunction\nLet’s take a look at the function.\n\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL\n)\n\nHere are the arguments to the function:\n\n.data - The data being passed to the function for the regression problem\n.rec_obj - The recipe object being passed.\n.parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported.\n.parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported.\n.split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample\n.split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type.\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(tidyAML)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(purrr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfast_reg_tbl <- fast_regression(\n  .data = mtcars,\n  .rec_obj = rec_obj,\n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(fast_reg_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nLet’s take a look at the model spec.\n\nfast_reg_tbl %>% slice(1) %>% pull(model_spec) %>% pluck(1)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the wflw column.\n\nfast_reg_tbl %>% slice(1) %>% pull(wflw) %>% pluck(1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe Fitted workflow.\n\nfast_reg_tbl %>% slice(1) %>% pull(fitted_wflw) %>% pluck(1)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n -15.077267     1.107474     0.001161    -0.001014     4.010199    -1.280324  \n       qsec           vs           am         gear         carb  \n   0.512318    -0.488014     2.430052     4.353568    -2.546043  \n\n\nAnd lastly tne predicted workflow column.\n\nfast_reg_tbl %>% slice(1) %>% pull(pred_wflw) %>% pluck(1)\n\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.7\n 2  28.2\n 3  18.9\n 4  12.0\n 5  14.8\n 6  15.4\n 7  14.7\n 8  20.0\n 9  11.2\n10  19.1\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrdata-2023-02-10/index.html",
    "href": "posts/weekly-rtip-healthyrdata-2023-02-10/index.html",
    "title": "Get the Current Hospital Data Set from CMS with {healthyR.data}",
    "section": "",
    "text": "Introduction\nGetting data for health care in the US can sometimes be hard. With my R package {healthyR.data} I am hoping to alleviate some of that pain.\nRight now the package is bring actively developed from what was a simple yet sleepy simulated administrative data set is getting supercharged into a a full blow package that will retrieve data from outside sources. One such source is CMS.\nAt the start, and this is going to be a long road, I have started to build some functionality around getting the current hospital data from CMS. Let’s take a look at how it works.\n\n\nFunction\nHere is the function which has no parameters. This function will download the current and the official hospital data sets from the CMS.gov website.\nThe function makes use of a temporary directory and file to save and unzip the data. This will grab the current Hospital Data Files, unzip them and return a list of tibbles with each tibble named after the data file.\nThe function returns a list object with all of the current hospital data as a tibble. It does not save the data anywhere so if you want to save it you will have to do that manually.\nThis also means that you would have to store the data as a variable in order to access the data later on. It does have a given attributes and a class so that it can be piped into other functions.\n\ncurrent_hosp_data()\n\nNow let’s see it in action.\n\n\nExample\nWe will download the current hospital data sets and take a look.\n\nlibrary(healthyR.data)\nlibrary(dplyr)\n\ncurrent_hospital_dataset <- current_hosp_data()\n\nThis function downloads 70 files. Let’s see which ones have been downloaded.\n\nnames(current_hospital_dataset)\n\n [1] \"ASC_Facility.csv\"                                                \n [2] \"ASC_National.csv\"                                                \n [3] \"ASC_State.csv\"                                                   \n [4] \"ASCQR_OAS_CAHPS_BY_ASC.csv\"                                      \n [5] \"ASCQR_OAS_CAHPS_NATIONAL.csv\"                                    \n [6] \"ASCQR_OAS_CAHPS_STATE.csv\"                                       \n [7] \"CJR_PY6_Quality_Reporting_July_2022_Production_File.csv\"         \n [8] \"CMS_PSI_6_decimal_file.csv\"                                      \n [9] \"Complications_and_Deaths_Hospital.csv\"                           \n[10] \"Complications_and_Deaths_National.csv\"                           \n[11] \"Complications_and_Deaths_State.csv\"                              \n[12] \"Data_Updates_January_2023.csv\"                                   \n[13] \"Footnote_Crosswalk.csv\"                                          \n[14] \"FY_2023_HAC_Reduction_Program_Hospital.csv\"                      \n[15] \"FY_2023_Hospital_Readmissions_Reduction_Program_Hospital.csv\"    \n[16] \"FY2021_Distribution_of_Net_Change_in_Base_Op_DRG_Payment_Amt.csv\"\n[17] \"FY2021_Net_Change_in_Base_Op_DRG_Payment_Amt.csv\"                \n[18] \"FY2021_Percent_Change_in_Medicare_Payments.csv\"                  \n[19] \"FY2021_Value_Based_Incentive_Payment_Amount.csv\"                 \n[20] \"HCAHPS_Hospital.csv\"                                             \n[21] \"HCAHPS_National.csv\"                                             \n[22] \"HCAHPS_State.csv\"                                                \n[23] \"Healthcare_Associated_Infections_Hospital.csv\"                   \n[24] \"Healthcare_Associated_Infections_National.csv\"                   \n[25] \"Healthcare_Associated_Infections_State.csv\"                      \n[26] \"Hospital_General_Information.csv\"                                \n[27] \"HOSPITAL_QUARTERLY_MSPB_6_DECIMALS.csv\"                          \n[28] \"hvbp_clinical_outcomes.csv\"                                      \n[29] \"hvbp_efficiency_and_cost_reduction.csv\"                          \n[30] \"hvbp_person_and_community_engagement.csv\"                        \n[31] \"hvbp_safety.csv\"                                                 \n[32] \"hvbp_tps.csv\"                                                    \n[33] \"IPFQR_QualityMeasures_Facility.csv\"                              \n[34] \"IPFQR_QualityMeasures_National.csv\"                              \n[35] \"IPFQR_QualityMeasures_State.csv\"                                 \n[36] \"Maternal_Health_Hospital.csv\"                                    \n[37] \"Maternal_Health_National.csv\"                                    \n[38] \"Maternal_Health_State.csv\"                                       \n[39] \"Measure_Dates.csv\"                                               \n[40] \"Medicare_Hospital_Spending_by_Claim.csv\"                         \n[41] \"Medicare_Hospital_Spending_Per_Patient_Hospital.csv\"             \n[42] \"Medicare_Hospital_Spending_Per_Patient_National.csv\"             \n[43] \"Medicare_Hospital_Spending_Per_Patient_State.csv\"                \n[44] \"OAS_CAHPS_Footnotes.csv\"                                         \n[45] \"OQR_OAS_CAHPS_BY_HOSPITAL.csv\"                                   \n[46] \"OQR_OAS_CAHPS_NATIONAL.csv\"                                      \n[47] \"OQR_OAS_CAHPS_STATE.csv\"                                         \n[48] \"Outpatient_Imaging_Efficiency_Hospital.csv\"                      \n[49] \"Outpatient_Imaging_Efficiency_National.csv\"                      \n[50] \"Outpatient_Imaging_Efficiency_State.csv\"                         \n[51] \"Payment_National.csv\"                                            \n[52] \"Payment_State.csv\"                                               \n[53] \"Payment_and_Value_of_Care_Hospital.csv\"                          \n[54] \"PCH_HCAHPS_HOSPITAL.csv\"                                         \n[55] \"PCH_HCAHPS_NATIONAL.csv\"                                         \n[56] \"PCH_HCAHPS_STATE.csv\"                                            \n[57] \"PCH_HEALTHCARE_ASSOCIATED_INFECTIONS_HOSPITAL.csv\"               \n[58] \"PCH_ONCOLOGY_CARE_MEASURES_HOSPITAL.csv\"                         \n[59] \"PCH_OUTCOMES_HOSPITAL.csv\"                                       \n[60] \"PCH_OUTCOMES_NATIONAL.csv\"                                       \n[61] \"Timely_and_Effective_Care_Hospital.csv\"                          \n[62] \"Timely_and_Effective_Care_National.csv\"                          \n[63] \"Timely_and_Effective_Care_State.csv\"                             \n[64] \"Unplanned_Hospital_Visits_Hospital.csv\"                          \n[65] \"Unplanned_Hospital_Visits_National.csv\"                          \n[66] \"Unplanned_Hospital_Visits_State.csv\"                             \n[67] \"VA_IPF.csv\"                                                      \n[68] \"VA_TE.csv\"                                                       \n[69] \"Value_of_Care_National.csv\"                                      \n[70] \"Veterans_Health_Administration_Provider_Level_Data.csv\"          \n\n\nMore to come in the future!"
  },
  {
    "objectID": "posts/rtip-2023-02-13/index.html",
    "href": "posts/rtip-2023-02-13/index.html",
    "title": "Off to CRAN! {tidyAML}",
    "section": "",
    "text": "Introduction\nAre you tired of spending hours tuning and testing different machine learning models for your regression or classification problems? The new R package {tidyAML} is here to simplify the process for you! tidyAML is a simple interface for automatic machine learning that fits the tidymodels framework, making it easier for you to solve regression and classification problems.\nThe tidyAML package has been designed with the goal of providing a simple API that automates the entire machine learning pipeline, from data preparation to model selection, training, and prediction. This means that you no longer have to spend hours tuning and testing different models; tidyAML will do it all for you, saving you time and effort.\nIn this initial release (version 0.0.1), tidyAML introduces a number of new features and minor fixes to improve the overall user experience. Here are some of the updates in this release:\nNew Features:\n\nmake_regression_base_tbl() and make_classification_base_tbl() functions for creating base tables for regression and classification problems, respectively.\ninternal_make_spec_tbl() function for making the specification table for the machine learning pipeline.\ninternal_set_args_to_tune() function for setting arguments to tune the models. This has not yet been implemented in a true working fashion but might be useful for feedback in this initial release.\ncreate_workflow_set() function for creating a set of workflows to test different models.\nget_model(), extract_model_spec(), extract_wflw(), extract_wflw_fit(), and extract_wflw_pred() functions for extracting different parts of the machine learning pipeline.\nmatch_args() function for matching arguments between the base and specification tables.\n\nMinor Fixes and Improvements:\n\nUpdates to fast_classification_parsnip_spec_tbl() and fast_regression_parsnip_spec_tbl() to use the make_regression and make_classification functions and the internal_make_spec_tbl() function.\nAddition of a class for the base table functions and using that class in internal_make_spec_tbl().\nUpdate to the DESCRIPTION for R >= 3.4.0.\n\nIn conclusion, tidyAML is a game-changer for those looking to automate the machine learning pipeline. It provides a simple API that eliminates the need for manual tuning and testing of different models. With the updates in this initial release, the tidyAML package is sure to make your machine learning journey easier and more efficient.\n\n\nFunction\nThere are too many functions to go over in this post so you can find them all here\n\n\nExamples\nEven though there are many functions to go over, we can showcase some with a small useful example. So let’s get at it!\n\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(dplyr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\n\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n  )\n\nglimpse(frt_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nNow let’s go through the extractors.\nThe get_model() function.\n\nget_model(frt_tbl, 2) |>\n  glimpse()\n\nRows: 1\nColumns: 8\n$ .model_id       <int> 2\n$ .parsnip_engine <chr> \"glm\"\n$ .parsnip_mode   <chr> \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, glm, TRUE…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>]\n\n\nThe extract_model_spec() function.\n\nextract_model_spec(frt_tbl, 1)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOr do multiples:\n\nextract_model_spec(frt_tbl, 1:2)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nThe extract_wflw() function.\n\nextract_wflw(frt_tbl, 1)\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOr do multiples:\n\nextract_wflw(frt_tbl, c(1, 2))\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n[[2]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nThe extract_wflw_fit() function.\n\nextract_wflw_fit(frt_tbl, 1)\n\n[[1]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\n\nOr do multiples:\n\nextract_wflw_fit(frt_tbl, 1:2)\n\n[[1]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\n\n[[2]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::gaussian, data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\nDegrees of Freedom: 23 Total (i.e. Null);  13 Residual\nNull Deviance:      935.1 \nResidual Deviance: 121.5    AIC: 131\n\n\nFinally the extract_wflw_pred() function.\n\nextract_wflw_pred(frt_tbl, 2)\n\n[[1]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n\nOr do multiples:\n\nextract_wflw_pred(frt_tbl, 1:2)\n\n[[1]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n[[2]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-14/index.html",
    "href": "posts/rtip-2023-02-14/index.html",
    "title": "An example of using {box}",
    "section": "",
    "text": "Introduction\nToday I am going to make a short post on the R package {box} which was showcased to me quite nicely by Michael Miles. It was informative and I was able to immediately see the usefulness of the {box} library.\nSo what is ‘box’? Well here is the description straight from their site:\n\n‘box’ allows organising R code in a more modular way, via two mechanisms:\n\nIt enables writing modular code by treating files and folders of R code as independent (potentially nested) modules, without requiring the user to wrap reusable code into packages.\nIt provides a new syntax to import reusable code (both from packages and from modules) which is more powerful and less error-prone than library or require, by limiting the number of names that are made available.\n\n\nSo let’s see how it all works.\n\n\nFunction\nThe main portion of the script looks like this:\n\n# Main script\n\n# Script setup --------------------------------------\n\n# Load box modules\nbox::use(. / box / global_options / global_options)\nbox::use(. / box / io / imports)\nbox::use(. / box / io / exports)\nbox::use(. / box / mod / mod)\n\n# Load global options\nglobal_options$set_global_options() \n\n\n# Main script ---------------------------------------\n\n# Load data, process it, and export results\nall_data <- getOption('data_dir') |> \n  \n  # Load all data\n  imports$load_all() |> \n  \n  # Modify dataset\n  mod$modify_data() |> \n  \n  # Export data\n  exports$export_data()\n\nSo what does this do? Well it is grabbing data from a predefined location, modifying it and then re-exporting it. Now let’s look at all the code that is behind it, which allows us to do these things and then you will see the power of using box\n\n\nExample\nLet’s take a look at the global options settings.\n\n# Set global options\n#' @export\nset_global_options <- function() {\n  options(\n    look_ups = 'look-ups/',\n    data_dir = 'data/input/'\n  )\n}\n\nOk 6 lines, boxed down to one.\nNow the import function.\n\n# Function for importing data\n\n#' @export\nload_all <- function(file_path) {\n  \n  box::use(purrr)\n  box::use(vroom)\n  \n  file_path |> \n    \n    # Get all csv files from folder\n    list.files(full.names = TRUE) |> \n    \n    # Set list names\n    purrr$set_names(\\(file) basename(file)) |> \n    \n    # Load all csvs into list\n    purrr$map(\\(file) vroom$vroom(file))\n\n}\n\nNow the modify_data function.\n\n# Function for modifying data\n\n#' @export\nmodify_data <- function(df_list) {\n  \n  box::use(dplyr)\n  box::use(purrr)\n  \n  map_fun <- function(df) {\n    \n    df |> \n      dplyr$select(name:mass) |> \n      dplyr$mutate(lol = height * mass) |> \n      dplyr$filter(lol > 1500)\n  }\n  \n  # Apply mapping function to list\n  purrr$map(df_list, map_fun)\n  \n}\n\nOk again, a big savings here, instead of the above we simply call mod$modify_data() which makes things clearner and also modular in that we can go to a very specific spot in our proejct to fix an error or add/subtract functionality.\nLastly the export.\n\n# Function for exporting data\n\n#' @export\nexport_data <- function(df_list) {\n  \n  box::use(vroom)\n  box::use(purrr)\n  \n  # Export data\n  purrr$map2(.x = df_list,\n             .y = names(df_list),\n             ~vroom$vroom_write(x = .x,\n                               file = paste0('data/output/', \n                                             .y),\n                               delim = ','))\n  \n}\n\nVoila! I think to even a fresh user, the power of boxing your functions is fairly apparent and to the advanced user, eyes are most likely glowing!"
  },
  {
    "objectID": "posts/rtip-2023-02-15/index.html",
    "href": "posts/rtip-2023-02-15/index.html",
    "title": "Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nAre you interested in visualizing time series data in a clear and concise way? The R package {healthyR.ts} provides a variety of tools for time series analysis and visualization, including the ts_ma_plot() function.\nThe ts_ma_plot() function is designed to help you quickly and easily create moving average plots for time series data. This function takes several arguments, including the data you want to visualize, the date column from your data, the value column from your data, and the frequency of the aggregation.\nOne of the great features of ts_ma_plot() is that it can handle both weekly and monthly data frequencies, making it a flexible tool for analyzing a variety of time series data. If you pass in a frequency other than “weekly” or “monthly”, the function will default to weekly, so it’s important to ensure that your data is aggregated at the appropriate frequency.\nWith ts_ma_plot(), you can create a variety of plots to help you better understand your time series data. The function allows you to add up to three different titles to your plot, helping you to organize and communicate your findings effectively. The main_title argument sets the title for the main plot, while the secondary_title and tertiary_title arguments set the titles for the second and third plots, respectively.\nIf you’re interested in using ts_ma_plot() for your own time series data, you’ll first need to preprocess your data so that it’s in the appropriate format for this function. Once you’ve done that, though, ts_ma_plot() can help you to quickly identify trends and patterns in your data that might not be immediately apparent from a raw data set.\nIn summary, ts_ma_plot() is a powerful and flexible tool for visualizing time series data. Whether you’re working with weekly or monthly data, this function can help you to quickly and easily create moving average plots that can help you to better understand your data. If you’re interested in time series analysis, be sure to check out {healthyR.ts} and give ts_ma_plot() a try!\n\n\nFunction\nHere is the full function call.\n\nts_ma_plot(\n  .data,\n  .date_col,\n  .value_col,\n  .ts_frequency = \"monthly\",\n  .main_title = NULL,\n  .secondary_title = NULL,\n  .tertiary_title = NULL\n)\n\nNow for the arguments to the parameters.\n\n.data: the data you want to visualize, which should be pre-processed and the aggregation should match the .frequency argument.\n.date_col: the data column from the .data argument that contains the dates for your time series.\n.value_col: the data column from the .data argument that contains the values for your time series.\n.ts_frequency: the frequency of the aggregation, which should be quoted as “weekly” or “monthly”. If not specified, the function defaults to weekly.\n.main_title: the title of the main plot.\n.secondary_title: the title of the second plot.\n.tertiary_title: the title of the third plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndata_tbl <- ts_to_tbl(AirPassengers) |>\n  select(-index)\n\noutput <- ts_ma_plot(\n  .data = data_tbl,\n  .date_col = date_col,\n  .value_col = value\n)\n\nLet’s take a look at each piece of the output.\n\noutput$data_trans_xts |> head()\n\n           value ma12\n1949-01-01   112   NA\n1949-02-01   118   NA\n1949-03-01   132   NA\n1949-04-01   129   NA\n1949-05-01   121   NA\n1949-06-01   135   NA\n\n\n\noutput$data_diff_xts_a |> head()\n\n              diff_a\n1949-01-01        NA\n1949-02-01  5.357143\n1949-03-01 11.864407\n1949-04-01 -2.272727\n1949-05-01 -6.201550\n1949-06-01 11.570248\n\n\n\noutput$data_diff_xts_b |> head()\n\n           diff_b\n1949-01-01     NA\n1949-02-01     NA\n1949-03-01     NA\n1949-04-01     NA\n1949-05-01     NA\n1949-06-01     NA\n\n\n\noutput$data_summary_tbl\n\n# A tibble: 144 × 5\n   date_col   value  ma12 diff_a diff_b\n   <date>     <dbl> <dbl>  <dbl>  <dbl>\n 1 1949-01-01   112    NA   0         0\n 2 1949-02-01   118    NA   5.36      0\n 3 1949-03-01   132    NA  11.9       0\n 4 1949-04-01   129    NA  -2.27      0\n 5 1949-05-01   121    NA  -6.20      0\n 6 1949-06-01   135    NA  11.6       0\n 7 1949-07-01   148    NA   9.63      0\n 8 1949-08-01   148    NA   0         0\n 9 1949-09-01   136    NA  -8.11      0\n10 1949-10-01   119    NA -12.5       0\n# … with 134 more rows\n\n\n\noutput$pgrid\n\n\n\n\n\noutput$xts_plt"
  },
  {
    "objectID": "posts/rtip-2023-02-16/index.html",
    "href": "posts/rtip-2023-02-16/index.html",
    "title": "Officially on CRAN {tidyAML}",
    "section": "",
    "text": "Introduction\nI’m excited to announce that the R package {tidyAML} is now officially available on CRAN! This package is designed to make it easy for users to perform automated machine learning (AutoML) using the tidymodels ecosystem. With a simple and intuitive interface, tidyAML allows users to quickly generate high-quality machine learning models without worrying about the underlying details.\nOne of the key features of tidyAML is its ability to generate regression models on the fly, without the need to build a full specification or tune hyper-parameters. This makes it ideal for users who want to quickly build a machine learning model without spending a lot of time on the setup process.\ntidyAML is also designed to be easy to use, with a set of functions that are straightforward and can generate many models and predictions at once. And because it’s built on top of the tidymodels ecosystem, users don’t need to worry about setting up additional packages or dependencies.\nWe’re also happy to announce that tidyAML will be added to the R package {healthyverse} and pushed to CRAN this week. This means that users who install {healthyverse} will automatically get access to tidyAML, as well as other popular packages like ggplot2, dplyr, and tidyr.\nWhether you’re a beginner or an experienced machine learning practitioner, tidyAML is a powerful tool that can help you quickly generate high-quality models with minimal setup. We hope you’ll give it a try and let us know what you think!"
  },
  {
    "objectID": "posts/rtip-2023-02-17/index.html",
    "href": "posts/rtip-2023-02-17/index.html",
    "title": "Converting a {tidyAML} tibble to a {workflowsets}",
    "section": "",
    "text": "The {tidyAML} package is an R package that provides a set of tools for building regression/classification models on the fly with minimal input required. In this post we will discuss the create_workflow_set() function.\nThe create_workflow_set function is a function in the tidyAML package that is used to create a workflowset object from the workflowsets package. A workflow is a sequence of tasks that can be executed in a specific order, and is often used in data analysis and machine learning to automate data processing and model fitting. The create_workflow_set function takes as input a YAML specification of a set of workflows, and returns a list of workflow objects that can be executed using the tidymodels package and its associated packages.\nThe create_workflow_set function is particularly useful when working with the tidymodels package and the parsnip framework. The tidymodels package is a collection of packages for modeling and machine learning in R that provides a consistent interface for building, tuning, and evaluating machine learning models. The parsnip package is part of the tidymodels ecosystem and provides a way to specify a wide range of models in a consistent manner.\n\n\nTo use the create_workflow_set function with tidymodels andparsnip, you will need to provide a recipe or recipes as a list to the .recipe_list parameter and a model_spec tibble that you would get from something like fast_regression_parsnip_spec_tbl(), other classes will be supported in the future.\nThe reason this was done was because I did not want to force users to remain inside of tidyAML perhaps and most likely there are other packages out there that are more suited to an end users specific problem at hand."
  },
  {
    "objectID": "posts/rtip-2023-02-22/index.html",
    "href": "posts/rtip-2023-02-22/index.html",
    "title": "Calibrate and Plot a Time Series with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nIn time series analysis, it is common to split the data into training and testing sets to evaluate the accuracy of a model. However, it is important to ensure that the model is calibrated on the training set before evaluating its performance on the testing set. The {healthyR.ts} library provides a function called calibrate_and_plot() that simplifies this process.\n\n\nFunction\nHere is the full function call:\n\ncalibrate_and_plot(\n  ...,\n  .type = \"testing\",\n  .splits_obj,\n  .data,\n  .print_info = TRUE,\n  .interactive = FALSE\n)\n\nHere are the arguments to the parameters:\n\n... - The workflow(s) you want to add to the function.\n.type - Either the training(splits) or testing(splits) data.\n.splits_obj - The splits object.\n.data - The full data set.\n.print_info - The default is TRUE and will print out the calibration accuracy tibble and the resulting plotly plot.\n.interactive - The defaults is FALSE. This controls if a forecast plot is interactive or not via plotly.\n\n\n\nExample\nBy default, calibrate_and_plot() will print out a calibration accuracy tibble and a resulting plotly plot. This can be controlled with the print_info argument, which is set to TRUE by default. If you prefer a non-interactive forecast plot, you can set the interactive argument to FALSE.\nHere’s an example of how to use the calibrate_and_plot() function:\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(workflows)\nlibrary(rsample)\n\n# Get the Data\ndata <- ts_to_tbl(AirPassengers) |>\n  select(-index)\n\n# Split the data into training and testing sets\nsplits <- time_series_split(\n   data\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\n# Make the recipe object\nrec_obj <- recipe(value ~ ., data = training(splits))\n\n# Make the Model\nmodel_spec <- linear_reg(\n   mode = \"regression\"\n   , penalty = 0.5\n   , mixture = 0.5\n) |>\n   set_engine(\"lm\")\n\n# Make the workflow object\nwflw <- workflow() |>\n   add_recipe(rec_obj) |>\n   add_model(model_spec) |>\n   fit(training(splits))\n\n# Get our output\noutput <- calibrate_and_plot(\n  wflw\n  , .type = \"training\"\n  , .splits_obj = splits\n  , .data = data\n  , .print_info = FALSE\n  , .interactive = TRUE\n )\n\nThe resulting output will include a calibration accuracy tibble and a plotly plot showing the original time series data along with the fitted values for the training set.\nLet’s take a look at the output.\n\noutput$calibration_tbl\n\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc .type .calibration_data \n      <int> <list>     <chr>       <chr> <list>            \n1         1 <workflow> LM          Test  <tibble [132 × 4]>\n\n\n\noutput$model_accuracy\n\n# A tibble: 1 × 9\n  .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq\n      <int> <chr>       <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 LM          Test   31.4  12.0  1.31  11.9  41.7 0.846\n\n\nAnd…\n\noutput$plot\n\n\n\n\n\nOverall, the calibrate_and_plot() function is a useful tool for simplifying the process of calibrating time series models on a training set and evaluating their performance on a testing set.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-23/index.html",
    "href": "posts/rtip-2023-02-23/index.html",
    "title": "Data Preppers with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nThere are many different methods that one can choose from in order to model their data. This brings with it a fundamental issue of how to prepare your data for the specified algorithm. With the [{healthyR.ai}] package there are many different functions in this family that will help solve this issue for some algorithms but of course not all, that would be utterly exhausting for me to do on my own.\nIn healthyR.ai I call these Data Preppers because they prep the data you supply to the format necessary for the algorithm to function properly.\nLet’s take a look at one.\n\n\nFunction\nHere we are going to use the hai_c50_data_prepper(.data, .recipe_formula) function.\n\nhai_c50_data_prepper(.data, .recipe_formula)\n\nHere are the simple arguments:\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::recipe() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the iris data then the formula would most likely be something like Species ~ .\n\n\n\nExample\nHere is a small example:\n\nlibrary(healthyR.ai)\n\nhai_c50_data_prepper(.data = Titanic, .recipe_formula = Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\n\nrec_obj <- hai_c50_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(rec_obj)\n\n# A tibble: 32 × 5\n   Class Sex    Age       n Survived\n   <fct> <fct>  <fct> <dbl> <fct>   \n 1 1st   Male   Child     0 No      \n 2 2nd   Male   Child     0 No      \n 3 3rd   Male   Child    35 No      \n 4 Crew  Male   Child     0 No      \n 5 1st   Female Child     0 No      \n 6 2nd   Female Child     0 No      \n 7 3rd   Female Child    17 No      \n 8 Crew  Female Child     0 No      \n 9 1st   Male   Adult   118 No      \n10 2nd   Male   Adult   154 No      \n# … with 22 more rows\n\n\nHere are the rest of the data-preppers at the time of writing this article:\n\nhai_c50_data_prepper()\nhai_cubist_data_prepper()\nhai_earth_data_prepper()\nhai_glmnet_data_prepper()\nhai_knn_data_prepper()\nhai_ranger_data_prepper()\nhai_svm_poly_data_prepper()\nhai_svm_rbf_data_prepper()\nhai_xgboost_data_prepper()\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-27/index.html",
    "href": "posts/rtip-2023-02-27/index.html",
    "title": "Quickly Generate Nested Time Series Models",
    "section": "",
    "text": "Introduction\nThere are many approaches to modeling time series data in R. One of the types of data that we might come across is a nested time series. This means the data is grouped simply by one or more keys. There are many methods in which to accomplish this task. This will be a quick post, but if you want a longer more detailed and quite frankly well written out one, then this is a really good article\n\n\nExampmle\nLet’s just get to it with a very simple example, the motivation here isn’t to be all encompassing, but rather to just showcase it is possible for those who may not know it is.\n\nlibrary(healthyR.data)\nlibrary(dplyr)\nlibrary(timetk)\n\nts_tbl <- healthyR_data |> \n  filter(ip_op_flag == \"I\") |> \n  select(visit_end_date_time, service_line, length_of_stay) |>\n  mutate(visit_end_date_time = as.Date(visit_end_date_time)) |>\n  group_by(service_line) |>\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by = \"month\",\n    los = mean(length_of_stay)\n  ) |>\n  ungroup()\n\nglimpse(ts_tbl)\n\nRows: 2,148\nColumns: 3\n$ service_line        <chr> \"Alcohol Abuse\", \"Alcohol Abuse\", \"Alcohol Abuse\",…\n$ visit_end_date_time <date> 2011-09-01, 2011-10-01, 2011-11-01, 2011-12-01, 2…\n$ los                 <dbl> 3.666667, 3.181818, 4.380952, 3.464286, 3.677419, …\n\n\n\nlibrary(forecast)\nlibrary(broom)\nlibrary(tidyr)\n\nglanced_models <- ts_tbl |> \n  nest_by(service_line) |> \n  mutate(AA = list(auto.arima(data$los))) |> \n  mutate(perf = list(glance(AA))) |> \n  unnest(cols = c(perf))\n\nglanced_models |>\n  select(-data)\n\n# A tibble: 23 × 7\n# Groups:   service_line [23]\n   service_line                  AA         sigma logLik   AIC   BIC  nobs\n   <chr>                         <list>     <dbl>  <dbl> <dbl> <dbl> <int>\n 1 Alcohol Abuse                 <fr_ARIMA> 2.22  -241.   493.  506.   109\n 2 Bariatric Surgery For Obesity <fr_ARIMA> 0.609  -80.1  168.  178.    88\n 3 CHF                           <fr_ARIMA> 0.963 -152.   309.  314.   110\n 4 COPD                          <fr_ARIMA> 0.987 -155.   315.  320.   110\n 5 CVA                           <fr_ARIMA> 1.50  -201.   407.  412.   110\n 6 Carotid Endarterectomy        <fr_ARIMA> 6.27  -166.   335.  339.    51\n 7 Cellulitis                    <fr_ARIMA> 1.07  -163.   329.  335.   110\n 8 Chest Pain                    <fr_ARIMA> 0.848 -139.   281.  287.   110\n 9 GI Hemorrhage                 <fr_ARIMA> 1.21  -179.   361.  366.   111\n10 Joint Replacement             <fr_ARIMA> 1.65  -196.   396.  401.   102\n# … with 13 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-28/index.html",
    "href": "posts/rtip-2023-02-28/index.html",
    "title": "Open a File Folder in R",
    "section": "",
    "text": "Inroduction\nWhen writing a function, it is possible that you may want to ask the user where they want the data stored and if they want to open the file folder after the download has taken place. Well we can do this in R by invoking the shell.exec() command where we use a variable like f_path that is the path to the folder. We are going to go over a super simple example.\n\n\nFunction\nHere is the function:\n\nshell.exec(file)\n\nHere are the arguments.\n\nfile - file, directory or URL to be opened.\n\nNow let’s go over a simple example\n\n\nExample\nHere we go.\n\n# Create a temporary file to store the zip file\nf_path <- utils::choose.dir()\n\n# Open file folder?\nif (.open_folder){\n    shell.exec(f_path)\n}\n\nIf in our function creation we make a variable .open_folder and set it equal to TRUE then the if statement will execute and shell.exec(f_path) will open the specified path set by utils::choose.dir()\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-01/index.html",
    "href": "posts/rtip-2023-03-01/index.html",
    "title": "Text Processing Made Easy with {healthyR}’s sql_left(), sql_mid(), and sql_right() Functions in R",
    "section": "",
    "text": "Introduction\nAre you tired of manually manipulating text data in R? Do you find yourself frequently needing to extract substrings from long strings or to grab just the first few characters of a string? If so, you’re in luck! The {healthyR} library has three functions that will make your text processing tasks much easier: sql_left(), sql_mid(), and sql_right().\n\n\nFunction\nHere are the function calls, I will also make the source avilable in the same cell so steal this code!!\n\n# LEFT\nsql_left(\"text\", 3)\n\nsql_left <- function(.text, .num_char) {\n    base::substr(.text, 1, .num_char)\n}\n\n# MID\nsql_mid(\"this is some text\", 6, 2)\n\nsql_mid <- function(.text, .start_num, .num_char) {\n    base::substr(.text, .start_num, .start_num + .num_char - 1)\n}\n\n# RIGHT\nsql_right(\"this is some more text\", 3)\n\nsql_right <- function(.text, .num_char) {\n    base::substr(.text, base::nchar(.text) - (.num_char-1), base::nchar(.text))\n}\n\n\n\nExample\nLet’s start with sql_left(). This function is similar to the LEFT() function in SQL and Excel, in that it returns the specified number of characters from the beginning of a string. For example, if we have the string “Hello, world!”, and we want to grab just the first three characters, we can use sql_left() like this:\n\nlibrary(healthyR)\nsql_left(\"Hello, world!\", 3)\n\n[1] \"Hel\"\n\n\nThis will return the string “Hel”.\nNext up is sql_mid(). This function is similar to the SUBSTRING() and MID() functions in SQL and Excel, in that it returns a specified portion of a string. The first argument is the string itself, the second argument is the starting position of the substring, and the third argument is the length of the substring. For example, if we have the string “This is some text”, and we want to grab the two characters starting at position six, we can use sql_mid() like this:\n\nsql_mid(\"This is some text\", 6, 2)\n\n[1] \"is\"\n\n\nThis will return the string “is”.\nFinally, we have sql_right(). This function is similar to the RIGHT() function in SQL and Excel, in that it returns the specified number of characters from the end of a string. For example, if we have the string “This is some more text”, and we want to grab just the last three characters, we can use sql_right() like this:\n\nsql_right(\"This is some more text\", 3)\n\n[1] \"ext\"\n\n\nThis will return the string “ext”.\nThese three functions can be extremely helpful when working with text data in R. They can save you time and effort, and make your code more concise and readable. So next time you find yourself needing to manipulate text data, remember to reach for sql_left(), sql_mid(), and sql_right()!\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-02/index.html",
    "href": "posts/rtip-2023-03-02/index.html",
    "title": "Forecasting Timeseries in a list with R",
    "section": "",
    "text": "Introduction\nIn this article, we will discuss how to perform an ARIMA forecast on nested data or data that is in a list using R programming language. This is a common scenario in which we have data stored in a list format, where each element of the list corresponds to a different time series. We will use the R programming language, specifically the “forecast” package, to perform the ARIMA forecast.\nFirst, we will need to load the required packages and data. For this example, we will use the “AirPassengers” dataset which is included in the “datasets” package. This dataset contains the number of international airline passengers per month from 1949 to 1960. We will then create a list containing subsets of this data for each year.\n\nlibrary(forecast)\n\nyearly_data <- split(AirPassengers, f = ceiling(seq_along(AirPassengers)/12))\n\nyearly_data\n\n$`1`\n [1] 112 118 132 129 121 135 148 148 136 119 104 118\n\n$`2`\n [1] 115 126 141 135 125 149 170 170 158 133 114 140\n\n$`3`\n [1] 145 150 178 163 172 178 199 199 184 162 146 166\n\n$`4`\n [1] 171 180 193 181 183 218 230 242 209 191 172 194\n\n$`5`\n [1] 196 196 236 235 229 243 264 272 237 211 180 201\n\n$`6`\n [1] 204 188 235 227 234 264 302 293 259 229 203 229\n\n$`7`\n [1] 242 233 267 269 270 315 364 347 312 274 237 278\n\n$`8`\n [1] 284 277 317 313 318 374 413 405 355 306 271 306\n\n$`9`\n [1] 315 301 356 348 355 422 465 467 404 347 305 336\n\n$`10`\n [1] 340 318 362 348 363 435 491 505 404 359 310 337\n\n$`11`\n [1] 360 342 406 396 420 472 548 559 463 407 362 405\n\n$`12`\n [1] 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nIn the above code, we use the “split” function to split the data into yearly subsets. The “f” parameter is used to specify the grouping variable which, in this case, is the sequence of numbers from 1 to the length of the dataset divided by 12, rounded up to the nearest integer. This creates a list of 12 elements, one for each year.\n\n\nFunction\nNext, we will define a function that takes a single element of the list, fits an ARIMA model, and generates a forecast.\n\narima_forecast <- function(x){\n  fit <- auto.arima(x)\n  forecast(fit)\n}\n\nThis function takes a single argument “x” which is one of the elements of the list. We use the “auto.arima” function from the “forecast” package to fit an ARIMA model to the data. The “forecast” function is then used to generate a forecast based on this model.\n\n\nExample\nWe can now use the “lapply” function to apply this function to each element of the list.\n\nforecasts <- lapply(yearly_data, arima_forecast)\n\nThe “lapply” function applies the “arima_forecast” function to each element of the “yearly_data” list and returns a list of forecasts.\nFinally, we can extract and plot the forecasts for a specific year.\n\nplot(forecasts[[5]])\n\n\n\n\nNow lets take a look at them all.\n\npar(mfrow = c(2,1))\n\npurrr::map(forecasts, plot)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$`1`\n$`1`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 132.2237 126.4744 126.4744 126.4744 126.4744 126.4744 126.4744 126.4744\n [9] 126.4744 126.4744\n\n$`1`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 120.1608 113.7751\n14 110.0828 101.4056\n15 110.0828 101.4056\n16 110.0828 101.4056\n17 110.0828 101.4056\n18 110.0828 101.4056\n19 110.0828 101.4056\n20 110.0828 101.4056\n21 110.0828 101.4056\n22 110.0828 101.4056\n\n$`1`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 144.2865 150.6722\n14 142.8660 151.5432\n15 142.8660 151.5432\n16 142.8660 151.5432\n17 142.8660 151.5432\n18 142.8660 151.5432\n19 142.8660 151.5432\n20 142.8660 151.5432\n21 142.8660 151.5432\n22 142.8660 151.5432\n\n\n$`2`\n$`2`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 153.8708 139.5919 139.5919 139.5919 139.5919 139.5919 139.5919 139.5919\n [9] 139.5919 139.5919\n\n$`2`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 136.3778 127.1175\n14 115.8789 103.3260\n15 115.8789 103.3260\n16 115.8789 103.3260\n17 115.8789 103.3260\n18 115.8789 103.3260\n19 115.8789 103.3260\n20 115.8789 103.3260\n21 115.8789 103.3260\n22 115.8789 103.3260\n\n$`2`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 171.3638 180.6240\n14 163.3048 175.8577\n15 163.3048 175.8577\n16 163.3048 175.8577\n17 163.3048 175.8577\n18 163.3048 175.8577\n19 163.3048 175.8577\n20 163.3048 175.8577\n21 163.3048 175.8577\n22 163.3048 175.8577\n\n\n$`3`\n$`3`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 173.6413 170.0479 170.0479 170.0479 170.0479 170.0479 170.0479 170.0479\n [9] 170.0479 170.0479\n\n$`3`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 153.5404 142.8995\n14 146.6452 134.2565\n15 146.6452 134.2565\n16 146.6452 134.2565\n17 146.6452 134.2565\n18 146.6452 134.2565\n19 146.6452 134.2565\n20 146.6452 134.2565\n21 146.6452 134.2565\n22 146.6452 134.2565\n\n$`3`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 193.7423 204.3831\n14 193.4506 205.8393\n15 193.4506 205.8393\n16 193.4506 205.8393\n17 193.4506 205.8393\n18 193.4506 205.8393\n19 193.4506 205.8393\n20 193.4506 205.8393\n21 193.4506 205.8393\n22 193.4506 205.8393\n\n\n$`4`\n$`4`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 194.0074 194.0119 194.0147 194.0164 194.0174 194.0180 194.0184 194.0186\n [9] 194.0187 194.0188\n\n$`4`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 169.7973 156.9812\n14 165.6741 150.6730\n15 164.2944 148.5614\n16 163.8005 147.8051\n17 163.6201 147.5288\n18 163.5539 147.4272\n19 163.5296 147.3898\n20 163.5207 147.3761\n21 163.5175 147.3711\n22 163.5163 147.3692\n\n$`4`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 218.2176 231.0336\n14 222.3497 237.3509\n15 223.7350 239.4680\n16 224.2322 240.2276\n17 224.4146 240.5059\n18 224.4821 240.6088\n19 224.5071 240.6469\n20 224.5165 240.6611\n21 224.5200 240.6664\n22 224.5213 240.6684\n\n\n$`5`\n$`5`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 206.8929 210.7977 213.3851 215.0996 216.2356 216.9884 217.4872 217.8178\n [9] 218.0368 218.1819\n\n$`5`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 178.2600 163.1026\n14 176.4492 158.2662\n15 176.8082 157.4455\n16 177.5860 157.7275\n17 178.3181 158.2458\n18 178.8949 158.7294\n19 179.3167 159.1104\n20 179.6134 159.3893\n21 179.8176 159.5856\n22 179.9562 159.7208\n\n$`5`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 235.5258 250.6831\n14 245.1461 263.3291\n15 249.9620 269.3246\n16 252.6131 272.4716\n17 254.1531 274.2255\n18 255.0819 275.2475\n19 255.6578 275.8641\n20 256.0221 276.2462\n21 256.2559 276.4879\n22 256.4076 276.6430\n\n\n$`6`\n$`6`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 245.0709 240.0400 240.0400 240.0400 240.0400 240.0400 240.0400 240.0400\n [9] 240.0400 240.0400\n\n$`6`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 212.6687 195.5160\n14 196.9893 174.1996\n15 196.9893 174.1996\n16 196.9893 174.1996\n17 196.9893 174.1996\n18 196.9893 174.1996\n19 196.9893 174.1996\n20 196.9893 174.1996\n21 196.9893 174.1996\n22 196.9893 174.1996\n\n$`6`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 277.4731 294.6259\n14 283.0907 305.8803\n15 283.0907 305.8803\n16 283.0907 305.8803\n17 283.0907 305.8803\n18 283.0907 305.8803\n19 283.0907 305.8803\n20 283.0907 305.8803\n21 283.0907 305.8803\n22 283.0907 305.8803\n\n\n$`7`\n$`7`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 278.0001 278.0001 278.0002 278.0002 278.0002 278.0002 278.0002 278.0002\n [9] 278.0002 278.0002\n\n$`7`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 236.8903 215.1282\n14 228.5879 202.4307\n15 225.3145 197.4243\n16 223.9224 195.2953\n17 223.3147 194.3659\n18 223.0466 193.9559\n19 222.9278 193.7742\n20 222.8751 193.6936\n21 222.8516 193.6577\n22 222.8412 193.6418\n\n$`7`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 319.1098 340.8720\n14 327.4123 353.5695\n15 330.6859 358.5760\n16 332.0780 360.7051\n17 332.6857 361.6345\n18 332.9538 362.0445\n19 333.0726 362.2262\n20 333.1254 362.3069\n21 333.1488 362.3427\n22 333.1592 362.3587\n\n\n$`8`\n$`8`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 349.0540 373.2678 369.7906 348.0549 325.4487 315.1915 319.8599 332.7645\n [9] 344.2812 348.1670\n\n$`8`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 315.6225 297.9249\n14 322.1404 295.0752\n15 314.7795 285.6584\n16 292.8344 263.6024\n17 266.5768 235.4118\n18 252.9822 220.0505\n19 257.0954 223.8699\n20 269.7958 236.4622\n21 280.1875 246.2583\n22 283.2781 248.9280\n\n$`8`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 382.4855 400.1831\n14 424.3952 451.4604\n15 424.8018 453.9229\n16 403.2754 432.5074\n17 384.3206 415.4855\n18 377.4009 410.3325\n19 382.6243 415.8498\n20 395.7332 429.0668\n21 408.3750 442.3042\n22 413.0559 447.4061\n\n\n$`9`\n$`9`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 378.9729 406.5723 408.7509 392.6048 372.9147 361.5778 362.0569 370.2398\n [9] 379.1516 383.6927\n\n$`9`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 336.2126 313.5766\n14 342.0963 307.9648\n15 339.3660 302.6358\n16 323.1265 286.3469\n17 300.2319 261.7560\n18 285.7363 245.5882\n19 285.5516 245.0521\n20 293.6654 253.1294\n21 301.8675 260.9558\n22 305.8147 264.5885\n\n$`9`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 421.7333 444.3692\n14 471.0482 505.1797\n15 478.1359 514.8660\n16 462.0831 498.8627\n17 445.5975 484.0734\n18 437.4193 477.5674\n19 438.5622 479.0617\n20 446.8142 487.3503\n21 456.4356 497.3473\n22 461.5707 502.7968\n\n\n$`10`\n$`10`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 391.9249 381.5489 381.5489 381.5489 381.5489 381.5489 381.5489 381.5489\n [9] 381.5489 381.5489\n\n$`10`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 331.8921 300.1126\n14 304.6704 263.9734\n15 304.6704 263.9734\n16 304.6704 263.9734\n17 304.6704 263.9734\n18 304.6704 263.9734\n19 304.6704 263.9734\n20 304.6704 263.9734\n21 304.6704 263.9734\n22 304.6704 263.9734\n\n$`10`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 451.9577 483.7372\n14 458.4274 499.1244\n15 458.4274 499.1244\n16 458.4274 499.1244\n17 458.4274 499.1244\n18 458.4274 499.1244\n19 458.4274 499.1244\n20 458.4274 499.1244\n21 458.4274 499.1244\n22 458.4274 499.1244\n\n\n$`11`\n$`11`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 408.4203 410.7762 412.3990 413.5168 414.2868 414.8171 415.1824 415.4340\n [9] 415.6074 415.7268\n\n$`11`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 342.2241 307.1820\n14 330.3960 287.8452\n15 326.1006 280.4170\n16 324.5481 277.4509\n17 324.0788 276.3255\n18 324.0270 275.9656\n19 324.1175 275.9106\n20 324.2390 275.9632\n21 324.3506 276.0422\n22 324.4407 276.1168\n\n$`11`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 474.6165 509.6586\n14 491.1565 533.7072\n15 498.6974 544.3810\n16 502.4855 549.5827\n17 504.4948 552.2480\n18 505.6072 553.6686\n19 506.2474 554.4543\n20 506.6291 554.9049\n21 506.8641 555.1726\n22 507.0128 555.3367\n\n\n$`12`\n$`12`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 502.9998 476.0531 476.0531 476.0531 476.0531 476.0531 476.0531 476.0531\n [9] 476.0531 476.0531\n\n$`12`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 437.2687 402.4728\n14 387.1722 340.1214\n15 387.1722 340.1214\n16 387.1722 340.1214\n17 387.1722 340.1214\n18 387.1722 340.1214\n19 387.1722 340.1214\n20 387.1722 340.1214\n21 387.1722 340.1214\n22 387.1722 340.1214\n\n$`12`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 568.7308 603.5267\n14 564.9341 611.9848\n15 564.9341 611.9848\n16 564.9341 611.9848\n17 564.9341 611.9848\n18 564.9341 611.9848\n19 564.9341 611.9848\n20 564.9341 611.9848\n21 564.9341 611.9848\n22 564.9341 611.9848\n\ndev.off()\n\nnull device \n          1 \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-03/index.html",
    "href": "posts/rtip-2023-03-03/index.html",
    "title": "Simple examples of pmap() from {purrr}",
    "section": "",
    "text": "Introduction\nThe pmap() function in R is part of the purrr library, which is a package designed to make it easier to work with functions that operate on vectors, lists, and other types of data structures.\nThe pmap() function is used to apply a function to a list of arguments, where each element in the list contains the arguments for a single function call. The function is applied in parallel, meaning that each call is executed concurrently, which can help speed up computations when working with large datasets.\nHere is the basic syntax of the pmap() function:\n\npmap(.l, .f, ...)\n\nwhere:\n\n.l - is a list of arguments, where each element of the list contains the arguments for a single function call.\n.f - is the function to apply to the arguments in .l.\n... - is used to pass additional arguments to .f.\n\nThe pmap() function returns a list, where each element of the list contains the output of a single function call.\nLet’s define a function for an example.\n\n\nFunction\n\nmy_function <- function(a, b, c) {\n  # do something with a, b, and c\n  return(a + b + c)\n}\n\nA very simple function that just adds up the elements passed.\nNow let’s go over a couple simple examples.\n\n\nExample\n\nlibrary(purrr)\nlibrary(TidyDensity)\n\n\n# create a list of vectors with your arguments\nmy_args <- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(7, 8, 9)\n)\n\n# apply your function to each combination of arguments in parallel\nresults <- pmap(my_args, my_function)\n\n# print the results\nprint(results)\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 18\n\n\nNow lets see a couple more examples.\n\nargsl <- list(\n  c(100, 100, 100, 100), # this is .n\n  c(0,1,2,3),            # this is .mean\n  c(4,3,2,1),            # this is .sd\n  c(10,10,10,10)         # this is .num_sims\n)\n\npmap(argsl, tidy_normal)\n\n[[1]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx        dy     p      q\n   <fct>      <int>  <dbl> <dbl>     <dbl> <dbl>  <dbl>\n 1 1              1  3.56  -15.0 0.0000353 0.814  3.56 \n 2 1              2 -0.433 -14.6 0.0000679 0.457 -0.433\n 3 1              3 -1.93  -14.3 0.000125  0.315 -1.93 \n 4 1              4  1.68  -14.0 0.000219  0.663  1.68 \n 5 1              5  4.18  -13.7 0.000369  0.852  4.18 \n 6 1              6  0.805 -13.4 0.000596  0.580  0.805\n 7 1              7  7.99  -13.1 0.000922  0.977  7.99 \n 8 1              8 -1.61  -12.8 0.00137   0.344 -1.61 \n 9 1              9  1.83  -12.5 0.00195   0.676  1.83 \n10 1             10  6.66  -12.1 0.00267   0.952  6.66 \n# … with 990 more rows\n\n[[2]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx        dy      p      q\n   <fct>      <int>  <dbl> <dbl>     <dbl>  <dbl>  <dbl>\n 1 1              1 -0.335 -9.02 0.0000814 0.328  -0.335\n 2 1              2  2.00  -8.82 0.000162  0.630   2.00 \n 3 1              3 -0.238 -8.62 0.000304  0.340  -0.238\n 4 1              4  1.17  -8.41 0.000544  0.523   1.17 \n 5 1              5  1.50  -8.21 0.000921  0.567   1.50 \n 6 1              6  4.68  -8.01 0.00148   0.890   4.68 \n 7 1              7  4.59  -7.81 0.00227   0.884   4.59 \n 8 1              8 -1.18  -7.61 0.00331   0.233  -1.18 \n 9 1              9  2.35  -7.40 0.00460   0.673   2.35 \n10 1             10 -3.73  -7.20 0.00610   0.0574 -3.73 \n# … with 990 more rows\n\n[[3]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx       dy     p      q\n   <fct>      <int>  <dbl> <dbl>    <dbl> <dbl>  <dbl>\n 1 1              1  4.42  -3.98 0.000118 0.886  4.42 \n 2 1              2  2.24  -3.86 0.000211 0.547  2.24 \n 3 1              3 -0.207 -3.73 0.000369 0.135 -0.207\n 4 1              4  3.32  -3.61 0.000622 0.745  3.32 \n 5 1              5  0.999 -3.48 0.00101  0.308  0.999\n 6 1              6  4.08  -3.36 0.00160  0.851  4.08 \n 7 1              7  5.81  -3.23 0.00244  0.972  5.81 \n 8 1              8  6.11  -3.11 0.00362  0.980  6.11 \n 9 1              9  2.30  -2.98 0.00518  0.560  2.30 \n10 1             10  0.231 -2.86 0.00718  0.188  0.231\n# … with 990 more rows\n\n[[4]]\n# A tibble: 1,000 × 7\n   sim_number     x     y      dx       dy       p     q\n   <fct>      <int> <dbl>   <dbl>    <dbl>   <dbl> <dbl>\n 1 1              1 3.41  -0.635  0.000128 0.658   3.41 \n 2 1              2 0.415 -0.557  0.000243 0.00487 0.415\n 3 1              3 3.24  -0.479  0.000440 0.593   3.24 \n 4 1              4 3.73  -0.401  0.000758 0.768   3.73 \n 5 1              5 4.22  -0.324  0.00124  0.889   4.22 \n 6 1              6 3.70  -0.246  0.00193  0.757   3.70 \n 7 1              7 4.35  -0.168  0.00288  0.911   4.35 \n 8 1              8 1.50  -0.0899 0.00408  0.0672  1.50 \n 9 1              9 2.58  -0.0120 0.00551  0.336   2.58 \n10 1             10 3.41   0.0658 0.00713  0.661   3.41 \n# … with 990 more rows\n\npmap(argsl, tidy_normal) |>\n  map(tidy_autoplot)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-06/index.html",
    "href": "posts/rtip-2023-03-06/index.html",
    "title": "Simple examples of imap() from {purrr}",
    "section": "",
    "text": "Introduction\nThe imap() function is a powerful tool for iterating over a list or a vector while also keeping track of the index or names of the elements. This function applies a given function to each element of a list, along with the name or index of that element, and returns a new list with the results.\nThe imap() function takes two main arguments: x and .f. x is the list or vector to iterate over, and .f is the function to apply to each element. The .f function takes two arguments: x and i, where x is the value of the element and i is the index or name of the element.\n\n\nFunction\nHere is the imap() function.\n\nimap(.x, .f, ...)\n\nHere is the documentation from the function page:\n\n.x - A list or atomic vector.\n.f - A function, specified in one of the following ways:\n\nA named function, e.g. paste.\nAn anonymous function, e.g. (x, idx) x + idx or function(x, idx) x + idx.\nA formula, e.g. ~ .x + .y. You must use .x to refer to the current element and .y to refer to the current index. Only recommended if you require backward compatibility with older versions of R.\n\n... - Additional arguments passed on to the mapped function. We now generally recommend against using … to pass additional (constant) arguments to .f. Instead use a shorthand anonymous function:\n\n\n# Instead of\nx |> map(f, 1, 2, collapse = \",\")\n# do:\nx |> map(\\(x) f(x, 1, 2, collapse = \",\"))\n\nThis makes it easier to understand which arguments belong to which function and will tend to yield better error messages.\n\n\nExample\nHere’s an example of using imap() with a simple list of integers:\n\nlibrary(purrr)\n\n# create a list of integers\nmy_list <- list(1, 2, 3, 4, 5)\n\n# define a function to apply to each element of the list\nmy_function <- function(x, i) {\n  paste(\"The element at index\", i, \"is\", x)\n}\n\n# apply the function to each element of the list using imap()\nresult <- imap(my_list, my_function)\n\n# print the result\nprint(result)\n\n[[1]]\n[1] \"The element at index 1 is 1\"\n\n[[2]]\n[1] \"The element at index 2 is 2\"\n\n[[3]]\n[1] \"The element at index 3 is 3\"\n\n[[4]]\n[1] \"The element at index 4 is 4\"\n\n[[5]]\n[1] \"The element at index 5 is 5\"\n\n\nIn this example, we create a list of integers called my_list. We define a function called my_function that takes two arguments: x, which is the value of each element in the list, and i, which is the index of that element. We then use imap() to apply my_function to each element of my_list, passing both the value and the index of the element as arguments. The result is a new list where each element contains the output of my_function applied to the corresponding element of my_list.\nNow let’s take a look at a slightly more complex example. In this case, we will use imap() to iterate over a list of data frames, apply a function to each data frame that subsets the data to include only certain columns, and return a new list of data frames with the subsetted data.\n\n# create a list of data frames\nmy_list <- list(\n  data.frame(x = 1:5, y = c(\"a\", \"b\", \"c\", \"d\", \"e\")),\n  data.frame(x = 6:10, y = c(\"f\", \"g\", \"h\", \"i\", \"j\")),\n  data.frame(x = 11:15, y = c(\"k\", \"l\", \"m\", \"n\", \"o\"))\n)\n\n# define a function to apply to each element of the list\nmy_function <- function(df, i) {\n  # subset the data to include only the x column\n  df_subset <- df[, \"x\", drop = FALSE]\n  # rename the column to include the index of the element\n  colnames(df_subset) <- paste(\"x_\", i, sep = \"\")\n  # return the subsetted data frame\n  return(df_subset)\n}\n\n# apply the function to each element of the list using imap\nresult <- imap(my_list, my_function)\n\n# print the result\nprint(result)\n\n[[1]]\n  x_1\n1   1\n2   2\n3   3\n4   4\n5   5\n\n[[2]]\n  x_2\n1   6\n2   7\n3   8\n4   9\n5  10\n\n[[3]]\n  x_3\n1  11\n2  12\n3  13\n4  14\n5  15\n\n\nIn this example, we create a list of three data frames called my_list. We define a function called my_function that takes two arguments: df, which is the value of each element in the list (a data frame), and i, which is the index of that element. The function subsets the data frame to include only the x column, renames the column to include the index of the element, and returns the subsetted data frame.\nWe use imap() to apply my_function to each element of my_list, passing both the data frame and the index of the element as arguments. The result is a new list of data frames, where each data frame contains only the x column from the original data frame, with a new name that includes the index of the element.\nAs you can see, the output is a list of three data frames, each containing only the x column from the corresponding original data frame, with a new name that includes the index of the element.\nIn summary, the imap() function from the R library purrr is a useful tool for iterating over a list or a vector while also keeping track of the index or names of the elements. The function takes a list or a vector as its first argument, and a function as its second argument, which takes two arguments: the value of each element, and the index or name of that element. The function returns a new list or vector with the results of applying the function to each element of the original list or vector. This function is particularly useful for complex data structures, where the index or name of each element is important for further data analysis or processing.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-07/index.html",
    "href": "posts/rtip-2023-03-07/index.html",
    "title": "tidy_bernoulli() with {data.table}",
    "section": "",
    "text": "Introduction\nSo I was challanged by Adrian Antico to learn data.table, so yesterday I started with a single function from my package {TidyDensity} called tidy_bernoulli().\nSo let’s see how I did (hint, works but needs a lot of improvement, so I’ll learn it.)\n\n\nFunction\nLet’s see the function in data.table\n\nlibrary(data.table)\nlibrary(tidyr)\nlibrary(stats)\nlibrary(purrr)\n\nnew_func &lt;- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data &lt;- data.table(sim_number = factor(seq(1, num_sims, 1)))\n  \n  # Group the data by sim_number and add columns for x and y\n  sim_data[, `:=` (\n    x = list(1:n),\n    y = list(stats::rbinom(n = n, size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, `:=` (\n    d = list(density(unlist(y), n = n)[c(\"x\", \"y\")] |&gt;\n      set_names(\"dx\", \"dy\") |&gt;\n      as_tibble())\n  ), by = sim_number]\n  \n  # Compute the p-values for the y values and add a column for p\n  sim_data[, `:=` (\n    p = list(stats::pbinom(unlist(y), size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, `:=` (\n    q = list(stats::qbinom(unlist(p), size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Unnest the columns for x, y, d, p, and q\n  sim_data &lt;- sim_data[, \n                       unnest(\n                         .SD, \n                         cols = c(\"x\", \"y\", \"d\", \"p\", \"q\")\n                         ), \n                       by = sim_number]\n  \n  # Remove the grouping\n  sim_data[, sim_number := as.factor(sim_number)]\n  \n  return(sim_data)\n}\n\n\n\nExample\nNow, let’s see the output of the original function tidy_bernoulli() and new_func().\n\nlibrary(TidyDensity)\nn &lt;- 50\npr &lt;- 0.1\nsims &lt;- 5\n\nset.seed(123)\ntb &lt;- tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n\nset.seed(123)\nnf &lt;- new_func(n = n, num_sims = sims, pr = pr)\n\nprint(tb)\n\n# A tibble: 250 × 7\n   sim_number     x     y      dx     dy     p     q\n   &lt;fct&gt;      &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1              1     0 -0.405  0.0292   0.9     0\n 2 1              2     0 -0.368  0.0637   0.9     0\n 3 1              3     0 -0.331  0.129    0.9     0\n 4 1              4     0 -0.294  0.243    0.9     0\n 5 1              5     1 -0.258  0.424    1       1\n 6 1              6     0 -0.221  0.688    0.9     0\n 7 1              7     0 -0.184  1.03     0.9     0\n 8 1              8     0 -0.147  1.44     0.9     0\n 9 1              9     0 -0.110  1.87     0.9     0\n10 1             10     0 -0.0727 2.25     0.9     0\n# ℹ 240 more rows\n\nprint(nf)\n\n     sim_number  x y         dx          dy   p q\n  1:          1  1 0 -0.4053113 0.029196114 0.9 0\n  2:          1  2 0 -0.3683598 0.063683226 0.9 0\n  3:          1  3 0 -0.3314083 0.129227066 0.9 0\n  4:          1  4 0 -0.2944568 0.242967496 0.9 0\n  5:          1  5 1 -0.2575054 0.424395426 1.0 1\n ---                                             \n246:          5 46 0  1.2575054 0.057872104 0.9 0\n247:          5 47 0  1.2944568 0.033131931 0.9 0\n248:          5 48 1  1.3314083 0.017621873 1.0 1\n249:          5 49 1  1.3683598 0.008684076 1.0 1\n250:          5 50 0  1.4053113 0.003981288 0.9 0\n\n\nOk so at least the output is identical which is a good sign. Now let’s benchmark the two solutions.\n\nlibrary(rbenchmark)\nlibrary(dplyr)\n\nbenchmark(\n  \"original\" = {\n    tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n  },\n  \"data.table\" = {\n    new_func(n = n, pr = pr, num_sims = sims)\n  },\n  replications = 100,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) |&gt;\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1   original          100    3.18    1.000      2.89     0.12\n2 data.table          100    4.89    1.538      4.49     0.11\n\n\nYeah, needs some work but it’s a start."
  },
  {
    "objectID": "posts/rtip-2023-03-08/index.html",
    "href": "posts/rtip-2023-03-08/index.html",
    "title": "Getting NYS Home Heating Oil Prices with {rvest}",
    "section": "",
    "text": "Introduction\nIf you live in New York and rely on heating oil to keep your home warm during the colder months, you know how important it is to keep track of heating oil prices. Fortunately, with a bit of R code, you can easily access the latest heating oil prices in New York.\nThe code uses the {dplyr} package to clean and manipulate the data, as well as the {timetk} package to plot the time series. Here’s a breakdown of what the code does:\n\nFirst, it loads the necessary packages and sets the URL for the data source.\nNext, it reads the HTML from the URL using the read_html function from the xml2 package.\nIt then uses the html_node function from the rvest package to extract the HTML node that contains the data table.\n\nThe resulting data table is then cleaned and transformed using dplyr functions such as html_table, as_tibble, set_names, select, mutate, and arrange.\nFinally, the resulting time series data is plotted using plot_time_series from the timetk package.\nTo run this code, you will need to have these packages installed on your machine. You can install them using the install.packages function in R. Here’s how you can install the packages:\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"xml2\")\ninstall.packages(\"rvest\")\ninstall.packages(\"tibble\")\ninstall.packages(\"purrr\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"timetk\")\n\nOnce you have installed the packages, you can copy and paste the code into your R console or RStudio and run it to get the latest heating oil prices in New York.\nIn conclusion, the code above provides a simple and efficient way to access and visualize heating oil prices in New York using R. By keeping track of these prices, you can make informed decisions about when to buy heating oil and how much to purchase, ultimately saving you money on your heating bills.\n\n\nExample\nNow let’s run it!\n\nurl  <- \"https://www.eia.gov/opendata/qb.php?sdid=PET.W_EPD2F_PRS_SNY_DPG.W\"\npage <- xml2::read_html(url)\nnode <- rvest::html_node(\n    x = page\n    , xpath = \"/html/body/div[1]/section/div/div/div[2]/div[1]/table\"\n)\nny_tbl <- node |>\n    rvest::html_table() |>\n    tibble::as_tibble() |>\n    purrr::set_names('series_name','period','frequency','value','units') |>\n    dplyr::select(period, frequency, value, units, series_name) |>\n    dplyr::mutate(period = lubridate::ymd(period)) |>\n    dplyr::arrange(period)\n\nny_tbl |>\n    timetk::plot_time_series(.date_var = period, .value = value)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-09/index.html",
    "href": "posts/rtip-2023-03-09/index.html",
    "title": "Multiple Solutions to speedup tidy_bernoulli() with {data.table}",
    "section": "",
    "text": "Introduction\nI had just recently posted on making an attempt to speedup computations with my package {TidyDensity} using a purely data.table solution, yes of course I can use {dtplyr} or {tidytable} but that not the challenge put to me.\nMy original attempt was worse than the original solution of tidy_bernoulli(). After I posted on Mastadon, LinkedIn and Reddit, I recieved potential solutions from each site by users. Let’s check them out below.\n\n\nFunction\nFirst let’s load in the necessary libraries.\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(rbenchmark)\nlibrary(TidyDensity)\n\nNow let’s look at the different solutions.\n\n# My original new function\nnew_func &lt;- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data &lt;- data.table(sim_number = factor(seq(1, num_sims, 1)))\n\n  # Group the data by sim_number and add columns for x and y\n  sim_data[, `:=` (\n    x = list(1:n),\n    y = list(stats::rbinom(n = n, size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, `:=` (\n    d = list(density(unlist(y), n = n)[c(\"x\", \"y\")] |&gt;\n               set_names(\"dx\", \"dy\") |&gt;\n               as_tibble())\n  ), by = sim_number]\n\n  # Compute the p-values for the y values and add a column for p\n  sim_data[, `:=` (\n    p = list(stats::pbinom(unlist(y), size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, `:=` (\n    q = list(stats::qbinom(unlist(p), size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Unnest the columns for x, y, d, p, and q\n  sim_data &lt;- sim_data[,\n                       unnest(\n                         .SD,\n                         cols = c(\"x\", \"y\", \"d\", \"p\", \"q\")\n                       ),\n                       by = sim_number]\n\n  # Remove the grouping\n  sim_data[, sim_number := as.factor(sim_number)]\n\n  return(sim_data)\n}\n\nreddit_func &lt;- function(num_sims, n, pr) {\n  sim_dat &lt;- data.table(sim_number = rep(1:num_sims,each=n),\n                        x          = rep(1:n,num_sims))\n\n  sim_dat[, y := stats::rbinom(n = n, size = 1, prob = pr), by=sim_number]\n  sim_dat[, c(\"dx\",\"dy\") := density(y,n=n)[c(\"x\",\"y\")]    , by=sim_number]\n  sim_dat[, p := stats::pbinom(y, size = 1, prob = pr)    , by=sim_number]\n  sim_dat[, q := stats::qbinom(p, size = 1, prob = pr)    , by=sim_number]\n  \n  return(sim_dat)\n}\n\nmastadon_func &lt;- function(num_sims, n, pr){\n  sim_data &lt;- data.table(sim_number = 1:num_sims\n  )[, `:=`( x = .(1:n), y= .(rbinom(n = n, size = 1, prob = pr))), sim_number\n  ][, `:=`( d = .(density(unlist(y), n = n)[c('x','y')] |&gt; \n                    as.data.table() |&gt; \n                    setnames(c('dx','dy'))\n                  )\n            ), sim_number\n  ][, `:=`( p = .(pbinom(unlist(y), size = 1, prob = pr))), sim_number\n  ][, `:=`( q = .(qbinom(unlist(p), size = 1, prob = pr))), sim_number]\n\n    cbind(\n      sim_data[, lapply(.SD, unlist), by = sim_number, .SDcol = c('x','y','p','q')],\n      rbindlist(sim_data$d)\n    ) |&gt;\n    setcolorder(c('sim_number','x','y','dx','dy'))\n    \n    return(sim_data)\n}\n\nlinkedin_func &lt;- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data &lt;- CJ(sim_number = factor(1:num_sims), x = 1:n)\n\n  # Group the data by sim_number and add columns for x and y\n  sim_data[, y := stats::rbinom(n = .N, size = 1, prob = pr)]\n\n\n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, c(\"dx\", \"dy\") := density(y, n = n)[c(\"x\", \"y\")], by = sim_number]\n\n  # Compute the p-values for the y values and add a column for p\n  sim_data[, p := stats::pbinom(y, size = 1, prob = pr)]\n\n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, q := stats::qbinom(p, size = 1, prob = pr)]\n  setkey(sim_data, NULL) # needed only to compare with new_func\n  return(sim_data)\n}\n\nAll of the functions work in the same set of three arguments as input: * num_sims: an integer value that specifies the number of simulations to run * n: an integer value that specifies the sample size * pr: a numeric value that specifies the probability of success\nThe functions use the data.table package to create a data table named sim_dat/sim_data. The data table has two columns: sim_number and x. The sim_number column represents the simulation number, and x column represents the observation number.\nThe functions then generate random binary data using the rbinom function from the stats package. The function generates n binary data points for each simulation number (sim_number) using the input parameter pr as the probability of success. The resulting binary data points are stored in the y column of sim_dat/data.\nNext, the function calculates the density of y using the density function from the stats package. The function calculates the density separately for each simulation number (sim_number) and stores the resulting values in the dx and dy columns of sim_dat/data.\nThe functions then calculate the cumulative probability (p) of each binary data point using the pbinom function from the stats package. The function calculates the cumulative probability separately for each simulation number (sim_number) and stores the resulting values in the p column of sim_dat.\nFinally, the functions calculate the inverse of the cumulative probability (q) using the qbinom function from the stats package. The function calculates the inverse of the cumulative probability separately for each simulation number (sim_number) and stores the resulting values in the q column of sim_dat.\nThe functions then return the data table containing the results of the simulations.\n\n\nExample\nHow do they stack up to each other? Lets see!\n\nn &lt;- 50\npr &lt;- 0.1\nnum_sims &lt;- sims &lt;- 5\n\nbenchmark(\n  \"tidy_bernoulli()\" = {\n    tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n  },\n  \"my.first.attempt\" = {\n    new_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"linkedin.attempt\" = {\n    linkedin_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"mastadon.attempt\" = {\n    mastadon_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"reddit.attempt\" = {\n    reddit_func(n = n, pr = pr, num_sims = sims)\n  },\n  replications = 200,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) |&gt;\n  arrange(relative)\n\n              test replications elapsed relative user.self sys.self\n1 linkedin.attempt          200    0.95    1.000      0.92     0.02\n2   reddit.attempt          200    1.03    1.084      1.00     0.03\n3 mastadon.attempt          200    1.60    1.684      1.52     0.06\n4 tidy_bernoulli()          200    5.83    6.137      5.40     0.37\n5 my.first.attempt          200    8.66    9.116      8.17     0.14\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-10/index.html",
    "href": "posts/rtip-2023-03-10/index.html",
    "title": "Use of the apply family of functions",
    "section": "",
    "text": "In this post I will talk about the use of the R functions apply(), lapply(), sapply(), tapply(), and vapply() with examples.\nThese functions are all designed to help users apply a function to a set of data in R, but they differ in their input and output types, as well as in the way they handle missing values and other complexities. By using the right function for your particular problem, you can make your code more efficient and easier to read.\nLet’s start with the basics.\n\n\nBefore we dive into the details of each function, let’s define some terms:\n\nA vector is a one-dimensional array of data, like a list of numbers or strings.\nA matrix is a two-dimensional array of data, like a table of numbers.\nA data frame is a two-dimensional object that can hold different types of data, like a spreadsheet.\nA list is a collection of objects, which can be of different types, like a shopping bag full of different items.\n\nEach of the five functions we’ll discuss here takes a list as input (although some can also take vectors or matrices). Let’s create a list object to use in our examples:\n\nmy_list <- list(\n  a = c(1, 2, 3),\n  b = matrix(1:6, nrow = 2),\n  c = data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\")),\n  d = c(4, NA, 6),\n  e = list(\"foo\", \"bar\", \"baz\")\n)\n\nThis list contains five elements:\n\nA vector of numbers (a)\nA matrix of numbers (b)\nA data frame with two columns (c)\nA vector of numbers with a missing value (d)\nA list of character strings (e)\n\nNow that we have our data, let’s look at each of the functions in turn.\n\n\n\napply()\nThe apply() function applies a function to the rows or columns of a matrix or array. It is most commonly used with matrices, but can also be used with higher-dimensional arrays. The function takes three arguments:\n\nThe matrix or array to apply the function to\nThe margin (1 for rows, 2 for columns, or a vector of dimensions)\nThe function to apply\n\nLet’s apply the mean() function to the columns of our matrix in my_list$b:\n\napply(my_list$b, 2, mean)\n\n[1] 1.5 3.5 5.5\n\n\nThis will return a vector of means for each column of the matrix\nlapply()\nThe lapply() function applies a function to each element of a list and returns a list of the results. It takes two arguments:\n\nThe list to apply the function to\nThe function to apply\n\nLet’s apply the class() function to each element of our list:\n\nlapply(my_list, class)\n\n$a\n[1] \"numeric\"\n\n$b\n[1] \"matrix\" \"array\" \n\n$c\n[1] \"data.frame\"\n\n$d\n[1] \"numeric\"\n\n$e\n[1] \"list\"\n\n\nThis will return a list of the classes of each element.\nsapply()\nThe sapply() function is similar to lapply(), but it simplifies the output to a vector or matrix if possible. It takes the same two arguments as lapply():\n\nThe list to apply the function\nThe function to apply\n\nLet’s apply the length() function to each element of our list using sapply():\n\nsapply(my_list, length)\n\na b c d e \n3 6 2 3 3 \n\n\nThis will return a vector of lengths for each element.\ntapply()\nThe tapply() function applies a function to subsets of a vector or data frame, grouped by one or more factors. It takes three arguments:\n\nThe vector or data frame to apply the function to\nThe factor(s) to group the data by\nThe function to apply\n\nLet’s apply the mean() function to the elements of our vector my_list$d, grouped by whether they are missing or not:\n\ntapply(my_list$d, !is.na(my_list$d), mean)\n\nFALSE  TRUE \n   NA     5 \n\n\nThis will return a vector of means for each group where they are NOT NA.\nvapply()\nThe vapply() function is similar to sapply(), but allows the user to specify the output type and length, making it more efficient and less prone to errors. It takes four arguments:\n\nThe list to apply the function to\nThe function to apply\nThe output type of the function\nThe length of the output vector or matrix\n\nLet’s apply the length() function to each element of our list, specifying that the output type is an integer and the length is 1:\n\nvapply(my_list, length, integer(1))\n\na b c d e \n3 6 2 3 3 \n\n\nThis will return a matrix of lengths for each element, with 1 row:"
  },
  {
    "objectID": "posts/rtip-2023-03-17/index.html",
    "href": "posts/rtip-2023-03-17/index.html",
    "title": "Use of the apply family of functions",
    "section": "",
    "text": "I am thrilled to announce that the R universe of packages {healthyverse} has surpassed 60,000 downloads! Thank you to everyone who has downloaded and used these packages, your support is greatly appreciated.\nFor those who are not familiar, the {healthyverse} package is a collection of R packages focused on data science and analysis with an emphasis on healthcare. These packages are ever evolving and for the most part still in an experimental stage, but are maturing. These packages are:\n\n{healthyR}\n{healthyR.ts}\n{healthyR.ai}\n{healthyR.data}\n{TidyDensity}\n{tidyAML}\n\nI am continuously working on updates and improvements to the {healthyverse} package, and I hope to release them soon. Some of the updates include bug fixes, new functionality, and enhancements to existing functions.\nAdditionally, I want to encourage users who are interested in contributing to the {healthyverse} package to submit pull requests. Contributions can be in the form of bug fixes, new functions, or enhancements to existing ones. I am always open to feedback and suggestions on how to improve these packages.\nOnce again, thank you to everyone who has downloaded and used the {healthyverse} package. Your support motivates me to continue working on this project and making it the best it can be.\n\n\n\n60k"
  },
  {
    "objectID": "posts/rtip-2023-03-21/index.html",
    "href": "posts/rtip-2023-03-21/index.html",
    "title": "Getting the CCI30 Index Current Makeup",
    "section": "",
    "text": "Introduction\nThe CCI30 Crypto Index is a cryptocurrency index that tracks the performance of the top 30 cryptocurrencies by market capitalization. It was created in 2017 by a team of researchers and analysts from the CryptoCompare and MVIS indices.\nThe CCI30 Crypto Index is designed to provide a broad-based and representative measure of the cryptocurrency market’s overall performance. It includes a diverse range of cryptocurrencies, such as Bitcoin, Ethereum, Litecoin, Ripple, and many others. The index is weighted by market capitalization, with each cryptocurrency’s weight determined by its market capitalization relative to the total market capitalization of all 30 cryptocurrencies.\nThe CCI30 Crypto Index has become a popular benchmark for the cryptocurrency market, as it offers a comprehensive view of the market’s performance, rather than just focusing on one particular cryptocurrency. It is often used by investors, traders, and researchers to analyze trends and make investment decisions.\nOne notable feature of the CCI30 Crypto Index is that it is rebalanced every quarter. This means that the composition of the index is adjusted to reflect changes in the market capitalization of the constituent cryptocurrencies. This helps to ensure that the index remains representative of the overall cryptocurrency market.\nOverall, the CCI30 Crypto Index provides a useful tool for tracking the performance of the cryptocurrency market. It is a valuable resource for investors, traders, and researchers who are interested in this exciting and rapidly evolving field.\n\n\nCode Explanation\nLet’s break it down step by step:\n\nThe first line of the code loads the “dplyr” package, which provides a set of functions for data manipulation.\nThe second line of the code reads the HTML code from the website “https://cci30.com/” using the “read_html” function from the “xml2” package.\nThe next two blocks of code extract two tables from the HTML document using the “html_node” function from the “rvest” package. The tables are located at two different XPaths in the HTML document.\nThe extracted tables are then converted into tibbles using the “as_tibble” function from the “tibble” package. The tibbles are further transformed by selecting only the columns from the second to the fifth column using the “select” function from the “dplyr” package.\nThe column names of the tibbles are then set using the “set_names” function from the “purrr” package.\nFinally, the two tibbles are combined using the “union” function from the “dplyr” package, and the resulting tibble is printed to the console.\n\nIn summary, the code is extracting two tables from a website, transforming them into tibbles, selecting a subset of columns, renaming the columns, and combining them into a single tibble.\n\n\nExample\n\ncci30 <- xml2::read_html(\"https://cci30.com/\")\n\ntbl1 <- cci30 |>\n    rvest::html_node(xpath = \"/html/body/div[2]/div/div/div/div[2]/div[1]/table\") |>\n    rvest::html_table(header = 1) |>\n    tibble::as_tibble() |>\n    dplyr::select(2:5) |>\n    purrr::set_names(\n        \"Coin\",\"Price\",\"Mkt Cap\",\"Daily Change\"\n    )\n\ntbl2 <- cci30 |>\n    rvest::html_node(xpath = \"/html/body/div[2]/div/div/div/div[2]/div[2]/table\") |>\n    rvest::html_table(header = 1) |>\n    tibble::as_tibble() |>\n    dplyr::select(2:5) |>\n    purrr::set_names(\n        \"Coin\",\"Price\",\"Mkt Cap\",\"Daily Change\"\n    )\n\ntbl <- tbl1 |>\n  dplyr::union(tbl2) |>\n  knitr::kable()\n\ntbl\n\n\n\n\nCoin\nPrice\nMkt Cap\nDaily Change\n\n\n\n\nBitcoin\n$27,767.24\n$536,553,055,078\n0.17%\n\n\nEthereum\n$1,735.32\n$212,357,972,798\n0.04%\n\n\nBNB\n$332.92\n$52,565,516,823\n0.13%\n\n\nXRP\n$0.37\n$19,087,613,742\n0.13%\n\n\nCardano\n$0.33\n$11,547,419,916\n0.05%\n\n\nPolygon\n$1.10\n$9,643,536,324\n0.17%\n\n\nDogecoin\n$0.07\n$9,484,198,878\n0.34%\n\n\nSolana\n$22.18\n$8,507,167,040\n0.12%\n\n\nPolkadot\n$6.10\n$7,119,808,610\n0.08%\n\n\nShiba Inu\n$0.00\n$6,169,390,592\n0.24%\n\n\nTRON\n$0.07\n$5,936,468,687\n0.14%\n\n\nLitecoin\n$78.42\n$5,685,409,727\n0.40%\n\n\nAvalanche\n$16.64\n$5,418,625,875\n0.06%\n\n\nUniswap\n$6.19\n$4,716,487,304\n0.19%\n\n\nChainlink\n$7.06\n$3,649,558,739\n0.06%\n\n\nCosmos\n$11.56\n$3,309,216,299\n0.03%\n\n\nUNUS SED LEO\n$3.35\n$3,195,413,769\n-0.55%\n\n\nToncoin\n$2.38\n$2,907,590,168\n-0.62%\n\n\nMonero\n$151.58\n$2,767,118,876\n-0.01%\n\n\nEthereum Classic\n$19.58\n$2,741,016,944\n-6.84%\n\n\nOKB\n$44.34\n$2,660,455,327\n0.56%\n\n\nBitcoin Cash\n$130.60\n$2,526,173,508\n-0.48%\n\n\nStellar\n$0.09\n$2,293,156,708\n-0.04%\n\n\nCronos\n$0.07\n$1,787,408,658\n1.05%\n\n\nNEAR Protocol\n$2.00\n$1,728,135,015\n0.26%\n\n\nVeChain\n$0.02\n$1,665,251,562\n0.33%\n\n\nQuant\n$126.33\n$1,525,183,149\n0.31%\n\n\nInternet Computer\n$5.11\n$1,516,076,264\n0.19%\n\n\nAlgorand\n$0.21\n$1,498,361,340\n0.41%\n\n\nApeCoin\n$4.06\n$1,496,070,125\n0.12%"
  },
  {
    "objectID": "posts/rtip-2023-03-22/index.html",
    "href": "posts/rtip-2023-03-22/index.html",
    "title": "Some Examples of Cumulative Mean with {TidyDensity}",
    "section": "",
    "text": "Introduction\nCumulative mean is a statistical measure that calculates the mean of a set of numbers up to a certain point in time or after a certain number of observations. It is also known as a running average or moving average.\nCumulative mean can be useful in a variety of contexts. For example:\n\nTracking progress: Cumulative mean can be used to track progress over time. For instance, a teacher might use it to track the average test scores of her students throughout the school year.\nAnalyzing trends: Cumulative mean can help identify trends in data. For example, a business might use it to track the average revenue generated by a new product over the course of several months.\nSmoothing data: Cumulative mean can be used to smooth out fluctuations in data. For instance, a meteorologist might use it to calculate the average temperature over the course of a year, which would help to smooth out the effects of daily temperature fluctuations.\n\nIn summary, cumulative mean is a useful statistical measure that can help track progress, analyze trends, and smooth out fluctuations in data.\n\n\nFunction\nThe function we will review is cmean() from the {TidyDensity} R package. Let’s take a look at it.\n\ncmean()\n\nThe only argument is .x which is a numeric vector as this is a vectorized function. Let’s see it in use.\n\n\nExample\nFirst let’s load in TidyDensity\n\nlibrary(TidyDensity)\n\nOk now let’s make some data. For this we are going to use the simple rnorm() function.\n\nx <- rnorm(100)\n\nhead(x)\n\n[1] -0.8293250 -1.2983499  2.2782337 -0.1521549  0.6859169  0.3809020\n\n\nOk, now that we have our vector, let’s run it through the function and see what it outputs and then we will graph it.\n\ncmx <- cmean(x)\nhead(cmx)\n\n[1] -0.8293249774 -1.0638374319  0.0501862766 -0.0003990095  0.1368641726\n[6]  0.1775371452\n\n\nNow let’s graph it.\n\nplot(cmx, type = \"l\")\n\n\n\n\nOk nice, so can we do this on grouped data or lists of data? Of course! First let’s use a for loop to generate a list of rnorm() values.\n\n# Initialize an empty list to store the generated values\nmy_list <- list()\n\n# Generate values using rnorm(5) in a for loop and store them in the list\nfor (i in 1:5) {\n  my_list[[i]] <- rnorm(100)\n}\n\n# Print the generated list\npurrr::map(my_list, head)\n\n[[1]]\n[1] -0.8054353 -0.4596541 -0.2362475  1.1486398 -0.7242154  0.5184610\n\n[[2]]\n[1]  0.3243327  0.7170802 -0.5963424 -1.0307104  0.3388504  0.5717486\n\n[[3]]\n[1]  1.7360816 -1.0359467 -0.3206138 -1.2157684 -0.8841356  0.1856481\n\n[[4]]\n[1] -1.1401642 -0.4437817 -0.2555245 -0.1809040 -0.2131763 -0.1251750\n\n[[5]]\n[1]  0.08835903 -1.79153379 -2.15010900  0.67344844  1.06125849  0.99848796\n\n\nNow that we have our list object let’s go ahead and plot the values out after we pass the data through cmean().\n\nlibrary(purrr)\n\nmy_list |>\n  map(\\(x) x |> cmean() |> plot(type = \"l\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n\nFrom here I think it is easy to see how one could do this on gruoped data as well with dplyr’s group_by()."
  },
  {
    "objectID": "posts/rtip-2023-03-24/index.html",
    "href": "posts/rtip-2023-03-24/index.html",
    "title": "How fast do the files read in?",
    "section": "",
    "text": "I will demonstrate how to generate a 1,000 row and column matrix with random numbers in R, and then save it in different file formats. I will also show how to get the file size of each saved object and benchmark how long it takes to read in each file using different functions.\n\n\nTo generate a 1,000 row and column matrix with random numbers, we can use the matrix() function and the runif() function in R. Here’s the code to generate the matrix:\n\n# set seed for reproducibility\nset.seed(123)\n\n# number of rows/columns in matrix\nn <- 1000\n\n# generate matrix with random normal values\nmat <- matrix(runif(n^2), nrow = n) \n\nThis code sets the random number generator seed to ensure that the same random numbers are generated every time the code is run. It then generates a vector of 1,000^2 random numbers using the runif() function, and creates a matrix with 1,000 columns using the matrix() function.\n\n\n\nWe can save the generated matrix in different file formats using different functions in R. Here are the functions we will use for each file format:\n\nCSV: write.csv()\nRDS: saveRDS()\nFST: write_fst()\nArrow: write_feather()\n\nHere’s the code to save the matrix in each file format:\n\nlibrary(fst)\nlibrary(arrow)\n\n# Save matrix in different file formats\nwrite.csv(mat, \"matrix.csv\", row.names=FALSE)\nsaveRDS(mat, \"matrix.rds\")\nwrite_fst(as.data.frame(mat), \"matrix.fst\")\nwrite_feather(as_arrow_table(as.data.frame(mat)), \"matrix.arrow\")\n\nThis code saves the matrix in each file format using the corresponding function, with the file name specified as the second argument. Getting the file size of each saved object\nTo get the file size of each saved object, we can use the file.size() function in R. Here’s the code to get the file size of each saved object:\n\n# Get file size of each saved object\ncsv_size <- file.size(\"matrix.csv\")  / (1024^2)\nrds_size <- file.size(\"matrix.rds\") / (1024^2)\nfst_size <- file.size(\"matrix.fst\") / (1024^2)\narrow_size <- file.size(\"matrix.arrow\") / (1024^2)\n\n# Print file size in human-readable format\nprint(paste(\"CSV file size in MB:\", format(csv_size, units=\"auto\")))\n\n[1] \"CSV file size in MB: 17.17339\"\n\nprint(paste(\"RDS file size in MB:\", format(rds_size, units=\"auto\")))\n\n[1] \"RDS file size in MB: 5.079627\"\n\nprint(paste(\"FST file size in MB:\", format(fst_size, units=\"auto\")))\n\n[1] \"FST file size in MB: 7.700841\"\n\nprint(paste(\"Arrow file size in MB:\", format(arrow_size, units=\"auto\")))\n\n[1] \"Arrow file size in MB: 6.705355\"\n\n\nThis code uses the file.size() function to get the file size of each object, and stores the file size of each object in a separate variable.\nFinally, it prints the file size of each object in a human-readable format using the format() function with the units=“auto” argument. The units=“auto” argument automatically chooses the most appropriate unit (e.g., KB, MB, GB) based on the file size.\n\n\n\nTo benchmark how long it takes to read in each file, we can use the {rbenchmark} package in R. In this example, we will compare the read times for the CSV file using four different functions: read.csv(), read_csv() from the {readr} package, fread() from the {data.table} package, and vroom() from the {vroom} package. We will also benchmark the read times for the RDS file using readRDS(), the FST file using read_fst(), and the Arrow file using read_feather().\nHere’s the code to benchmark the read times:\n\n# Load rbenchmark package\nlibrary(rbenchmark)\nlibrary(readr)\nlibrary(data.table)\nlibrary(vroom)\nlibrary(dplyr)\n\nn = 30\n\n# Benchmark read times for CSV file\nbenchmark(\n  # CSV File\n  \"read.csv\" = {\n    a <- read.csv(\"matrix.csv\")\n  },\n  \"read_csv\" = {\n    b <- read_csv(\"matrix.csv\")\n  },\n  \"fread\" = {\n    c <- fread(\"matrix.csv\")\n  },\n  \"vroom alltrep false\" = {\n    d <- vroom(\"matrix.csv\")\n  },\n  \"vroom alltrep true\" = {\n    dd <- vroom(\"matrix.csv\", altrep = TRUE)\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               fread           30    1.35    1.000      0.90     0.20\n2  vroom alltrep true           30    6.59    4.881      3.58     1.71\n3 vroom alltrep false           30    6.62    4.904      3.43     1.62\n4            read.csv           30   33.86   25.081     26.15     0.22\n5            read_csv           30   82.39   61.030     20.39     3.47\n\n# RDS File\nbenchmark(\n  # RDS File\n  \"readRDS\" = {\n    e <- readRDS(\"matrix.rds\")\n  },\n  \"read_rds\" = {\n    f <- read_rds(\"matrix.rds\")\n  },\n  \n  # Repications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n      test replications elapsed relative user.self sys.self\n1 read_rds           30    0.95    1.000      0.74     0.01\n2  readRDS           30    0.97    1.021      0.74     0.02\n\n# FST / Arrow\nbenchmark(\n  # FST\n  \"read_fst\" = {\n    g <- read_fst(\"matrix.fst\")\n  },\n  \n  # Arrow\n  \"arrow\" = {\n    h <- read_feather(\"matrix.arrow\")\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n      test replications elapsed relative user.self sys.self\n1 read_fst           30    0.21    1.000      0.05     0.12\n2    arrow           30    3.00   14.286      1.60     0.11\n\n\nThis code loads the {rbenchmark} package, and uses the benchmark() function to compare the read times for each file format. We specify the function to use for each file format, and set the number of replications to 10. Conclusion\nIn this blog post, we demonstrated how to generate a large matrix with random numbers in R, and how to save it in different file formats. We also showed how to get the file size of each saved object, and benchmarked the read times for each file format using different functions.\nThis example demonstrates the importance of choosing the appropriate file format and read function for your data. Depending on the size of your data and the requirements of your analysis, some file formats and functions may be more efficient than others."
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html",
    "href": "posts/rtip-2023-03-27/index.html",
    "title": "How fast does a compressed file in?",
    "section": "",
    "text": "I received an email over the weekend in regards to my last post not containing the reading in of gz compressed file(s) for the benchmarking. While this was not an over site per-se it was a good reminder that people would probably be interested in seeing this as well.\nBenchmarking is the process of measuring and comparing the performance of different programs, tools, or configurations in order to identify which one is the most efficient for a specific task. It is a critical step in software development that can help developers identify performance bottlenecks and improve the overall performance of their applications.\nIn this post I create a square matrix and then convert it to a data.frame (2,000 rows by 2,000 columns) and then saved it as a gz compressed csv file. The benchmark compares different R packages and functions, including base R, data.table, vroom, and readr, and measures their relative speeds based on the time it takes to read in the .csv.gz file.\nHere are some pro’s of trying things different ways and properly benchmarking them:\n\nIdentify the most efficient solution: Benchmarking can help you identify the most efficient solution for a specific task. By measuring the relative speeds of different programs or tools, you can determine which one is the fastest and use it to improve the performance of your application.\nOptimize resource utilization: Benchmarking can help you optimize resource utilization by identifying programs or tools that consume more resources than others. By choosing the most resource-efficient solution, you can reduce the cost of running your application and improve its scalability.\nAvoid premature optimization: Benchmarking can help you avoid premature optimization by measuring the performance of different programs or tools before you start optimizing them. By identifying the slowest parts of your application, you can focus your optimization efforts on the most critical areas and avoid wasting time optimizing code that doesn’t need it.\nKeep up with technology: Benchmarking can help you keep up with technology by comparing the performance of different tools and libraries. By staying up to date with the latest technologies, you can improve the performance of your application and stay ahead of your competitors.\nImprove code quality: Benchmarking can help you improve the quality of your code by identifying performance bottlenecks and areas for optimization. By optimizing your code, you can improve its maintainability, reliability, and readability.\n\nIn conclusion, benchmarking is an essential tool for software developers that can help them identify the most efficient solutions for their applications. By measuring the relative speeds of different programs or tools, developers can optimize resource utilization, avoid premature optimization, keep up with technology, and improve the quality of their code."
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#base-r",
    "href": "posts/rtip-2023-03-27/index.html#base-r",
    "title": "How fast does a compressed file in?",
    "section": "Base R",
    "text": "Base R\n\nread.csv()\nread.table()"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#data.table",
    "href": "posts/rtip-2023-03-27/index.html#data.table",
    "title": "How fast does a compressed file in?",
    "section": "data.table",
    "text": "data.table\n\nfread"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#vroom",
    "href": "posts/rtip-2023-03-27/index.html#vroom",
    "title": "How fast does a compressed file in?",
    "section": "vroom",
    "text": "vroom\n\nvroom() with altrep = FALSE\nvroom() with altrep = TRUE"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#readr",
    "href": "posts/rtip-2023-03-27/index.html#readr",
    "title": "How fast does a compressed file in?",
    "section": "readr",
    "text": "readr\n\nread_csv()"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#benchmarking",
    "href": "posts/rtip-2023-03-27/index.html#benchmarking",
    "title": "How fast does a compressed file in?",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nlibrary(rbenchmark)\nlibrary(data.table)\nlibrary(readr)\nlibrary(vroom)\nlibrary(dplyr)\n\nn <- 30\n\nbenchmark(\n  # Base R\n  \"read.table\" = {\n    a <- read.table(\"matrix.csv.gz\", sep = \",\")\n  },\n  \"read.csv\" = {\n    b <- read.csv(\"matrix.csv.gz\", sep = \",\")\n  },\n  \n  # data.table\n  \"fread\" = {\n    c <- fread(\"matrix.csv.gz\", sep = \",\")\n  },\n  \n  # vroom\n  \"vroom alltrep false\" = {\n    d <- vroom(\"matrix.csv.gz\", delim = \",\")\n  },\n  \"vroom alltrep true\" = {\n    e <- vroom(\"matrix.csv.gz\", delim = \",\", altrep = TRUE)\n  },\n  \n  # readr\n  \"readr\" = {\n    f <- read_csv(\"matrix.csv.gz\")\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               fread           30   19.44    1.000     13.56     1.59\n2  vroom alltrep true           30   22.06    1.135     10.54     2.63\n3 vroom alltrep false           30   24.75    1.273     10.22     2.84\n4          read.table           30   94.34    4.853     79.02     0.64\n5            read.csv           30  143.28    7.370    115.64     0.74\n6               readr           30  177.61    9.136     50.37    10.05\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html",
    "href": "posts/rtip-2023-03-28/index.html",
    "title": "How fast does a compressed file in Part 2",
    "section": "",
    "text": "Yesterday I posted on performing a benchmark on reading in a compressed .csv.gz file of a 2,000 by 2,000 data.frame. It was brought to my attention by someone on Mastadon (@mariviere@fediscience.org - https://fediscience.org/@mariviere) that I should also use {duckdb} and {arrow} so I will perform the same analysis as yesterday but I will also add in the two aforementioned packages."
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html#make-the-data",
    "href": "posts/rtip-2023-03-28/index.html#make-the-data",
    "title": "How fast does a compressed file in Part 2",
    "section": "Make the Data",
    "text": "Make the Data\nLet’s make that dataset again:\n\nlibrary(R.utils)\n\n# create a 1000 x 1000 matrix of random numbers\ndf <- matrix(rnorm(2000000), nrow = 2000, ncol = 2000) |>\n  as.data.frame()\n\n# Make and save gzipped file\nwrite.csv(df, \"df.csv\")\ngzip(\n  filename = \"df.csv\", \n  destname = \"df.csv.gz\",\n  overwrite = FALSE, \n  remove = TRUE\n)"
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html#benchmarking",
    "href": "posts/rtip-2023-03-28/index.html#benchmarking",
    "title": "How fast does a compressed file in Part 2",
    "section": "Benchmarking",
    "text": "Benchmarking\nTime to benchmark\n\nlibrary(rbenchmark)\nlibrary(data.table)\nlibrary(readr)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(vroom)\nlibrary(dplyr)\nlibrary(DBI)\n\nn <- 30\n\nbenchmark(\n  # Base R\n  \"read.table\" = {\n    a <- read.table(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \"read.csv\" = {\n    b <- read.csv(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \n  # data.table\n  \"fread\" = {\n    c <- fread(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \n  # vroom\n  \"vroom alltrep false\" = {\n    d <- vroom(\"df.csv.gz\", delim = \",\", col_types = \"d\")\n  },\n  \"vroom alltrep true\" = {\n    e <- vroom(\"df.csv.gz\", delim = \",\", altrep = TRUE, col_types = \"d\")\n  },\n  \n  # readr\n  \"readr\" = {\n    f <- read_csv(\"df.csv.gz\", col_types = \"d\")\n  },\n  \n  # Arrow\n  \"arrow\" = {\n    g <- open_csv_dataset(\"df.csv.gz\")\n  },\n  \n  # DuckDB\n  \"duckdb\" = {\n    con <- dbConnect(duckdb())\n    h <- duckdb_read_csv(\n      conn = con,\n      name = \"df\",\n      files = \"C:\\\\Users\\\\ssanders\\\\Documents\\\\GitHub\\\\steveondata\\\\posts\\\\rtip-2023-03-28\\\\df.csv.gz\"\n    )\n    dbDisconnect(con)\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               arrow           30    3.01    1.000      5.04     0.25\n2               fread           30   28.28    9.395     19.56     4.30\n3 vroom alltrep false           30   31.89   10.595     26.25    10.75\n4  vroom alltrep true           30   33.72   11.203     25.75    10.67\n5              duckdb           30   94.09   31.259     90.70     2.77\n6               readr           30   98.28   32.651    113.05    45.12\n7          read.table           30  109.97   36.535    107.78     1.24\n8            read.csv           30  153.79   51.093    152.44     0.56\n\n\nImportant note is the session info on the pc I am using to write this:\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] dplyr_1.1.1       vroom_1.6.1       arrow_11.0.0.3    duckdb_0.7.1-1   \n [5] DBI_1.1.3         readr_2.1.4       data.table_1.14.8 rbenchmark_1.0.0 \n [9] R.utils_2.12.2    R.oo_1.25.0       R.methodsS3_1.8.2\n\nloaded via a namespace (and not attached):\n [1] pillar_1.9.0      compiler_4.2.3    tools_4.2.3       digest_0.6.31    \n [5] bit_4.0.5         jsonlite_1.8.4    evaluate_0.20     lifecycle_1.0.3  \n [9] tibble_3.2.1      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] rstudioapi_0.14   parallel_4.2.3    yaml_2.3.7        xfun_0.38        \n[17] fastmap_1.1.1     knitr_1.42        generics_0.1.3    vctrs_0.6.1      \n[21] htmlwidgets_1.6.2 hms_1.1.3         bit64_4.0.5       tidyselect_1.2.0 \n[25] glue_1.6.2        R6_2.5.1          fansi_1.0.4       rmarkdown_2.21   \n[29] tzdb_0.3.0        purrr_1.0.1       magrittr_2.0.3    htmltools_0.5.5  \n[33] assertthat_0.2.1  utf8_1.2.3        crayon_1.5.2     \n\n Sys.info() |> \n   as.data.frame() |> \n   tibble::rownames_to_column() |> \n   as_tibble() |> \n   slice(1,2,3,5)\n\n# A tibble: 4 × 2\n  rowname `Sys.info()`\n  <chr>   <chr>       \n1 sysname Windows     \n2 release 10 x64      \n3 version build 19045 \n4 machine x86-64      \n\n memory.profile() |>\n   as.data.frame()\n\n            memory.profile()\nNULL                       1\nsymbol                 24303\npairlist              642504\nclosure                11189\nenvironment             4009\npromise                22963\nlanguage              189766\nspecial                   47\nbuiltin                  701\nchar                 2039073\nlogical                18866\ninteger               108132\ndouble                 20060\ncomplex                    5\ncharacter             160381\n...                       21\nany                        0\nlist                   58500\nexpression                 5\nbytecode               41555\nexternalptr            12382\nweakref                13860\nraw                    10113\nS4                      1362\n\n gc()\n\n           used  (Mb) gc trigger  (Mb) max used  (Mb)\nNcells  3363479 179.7    5830931 311.5  5830931 311.5\nVcells 32950395 251.4   81254422 620.0 81254324 620.0"
  },
  {
    "objectID": "posts/rtip-2023-03-29/index.html",
    "href": "posts/rtip-2023-03-29/index.html",
    "title": "A Bootstrapped Time Series Model with auto.arima() from {forecast}",
    "section": "",
    "text": "Introduction\nTime series analysis is a powerful tool for understanding and predicting patterns in data that vary over time. In this tutorial, we will use the AirPassengers dataset to create a bootstrapped timeseries model in R.\nThe AirPassengers dataset The AirPassengers dataset contains data on the number of passengers traveling on international flights per month from 1949 to 1960. To begin, we will load the dataset into R and plot it to get an idea of the data’s structure and any underlying patterns.\n\nlibrary(forecast)\n\ndata(AirPassengers)\nplot(AirPassengers, main = \"International Airline Passengers 1949-1960\")\n\n\n\n\nFrom the plot, we can see that there is a clear upward trend in the data, as well as some seasonality.\n\n\nCreating a bootstrapped timeseries model\nNow that we have an idea of the structure of the data, we can create a bootstrapped timeseries model using the auto.arima() function from the {forecast} package. The auto.arima() function uses an automated algorithm to determine the best model for a given timeseries.\n\nset.seed(123)\nn <- length(AirPassengers)\nn_boot <- 1000\n\n# create bootstrap sample indices\nboot_indices <- replicate(n_boot, sample(1:n, replace = TRUE))\n\n# create list to store models\nmodels <- list()\n\n# create bootstrapped models\nfor(i in 1:n_boot) {\n  boot_data <- AirPassengers[boot_indices[, i]]\n  models[[i]] <- auto.arima(boot_data)\n}\n\nmodels[[1]]\n\nSeries: boot_data \nARIMA(0,0,0) with non-zero mean \n\nCoefficients:\n          mean\n      275.5347\ns.e.    9.5443\n\nsigma^2 = 13209:  log likelihood = -887.01\nAIC=1778.02   AICc=1778.1   BIC=1783.96\n\n\nIn the code above, we first set a seed to ensure reproducibility of our results. We then specify the length of the timeseries and the number of bootstrap iterations we want to run. We create a list to store the models and a set of bootstrap sample indices.\nWe then loop through each bootstrap iteration, creating a new dataset from the original timeseries by sampling with replacement using the boot_indices. We use the auto.arima() function to create a timeseries model for each bootstrap sample and store it in our models list.\n\n\nSummarizing and plotting residuals\nNow that we have created our bootstrapped timeseries models, we can summarize and plot the residuals of each model to get an idea of how well our models fit the data.\n\n# create list to store residuals\nresiduals <- list()\n\n# create residuals for each model\nfor(i in 1:n_boot) {\n  boot_data <- AirPassengers[boot_indices[, i]]\n  residuals[[i]] <- residuals(models[[i]])\n}\n\n# summarize residuals\nresidual_means <- sapply(residuals, mean)\nresidual_sd <- sapply(residuals, sd)\n\n# plot residuals\npar(mfrow = c(2, 1))\n\nhist(\n  residual_means, \n  main = \"Bootstrapped Model Residuals\", \n  xlab = \"Mean Residuals\"\n  )\nhist(\n  residual_sd, \n  col = \"red\", \n  main = \"\", \n  xlab = \"SD Residuals\"\n  )\n\n\n\npar(mfrow = c(1,1))\n\nIn the code above, we create a list to store the residuals for each model, loop through each model to create residuals using the residuals() function, and summarize the residuals by taking the mean and standard deviation of each set of residuals.\nWe then plot the mean residuals and standard deviations for each model using the plot() function and add a legend to indicate the meaning of the two lines.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html",
    "href": "posts/rtip-2023-04-03/index.html",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "",
    "text": "In this blog post, we will be discussing how to create a Shiny application in R that will download and extract data from a zip file and allow users to choose which data they would like to see presented to them in the app from a selection drop-down menu. We will be using the current_hosp_data() function to obtain and read in the data. This function is in the upcoming release for the {healthyR.data} package."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#install",
    "href": "posts/rtip-2023-04-03/index.html#install",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Install",
    "text": "Install\n\ninstall.packages(c(\"shiny\",\"shinythemes\"))"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#current-hospital-data",
    "href": "posts/rtip-2023-04-03/index.html#current-hospital-data",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Current Hospital Data",
    "text": "Current Hospital Data\nHere is the current_hospital_data() function:\n\ncurrent_hosp_data <- function() {\n\n  # URL for file\n  url <- \"https://data.cms.gov/provider-data/sites/default/files/archive/Hospitals/current/hospitals_current_data.zip\"\n\n  # Create a temporary directory to process the zip file\n  tmp_dir <- tempdir()\n  download_location <- file.path(tmp_dir, \"download.zip\")\n  extract_location <- file.path(tmp_dir, \"extract\")\n\n  # Download the zip file to the temporary location\n  utils::download.file(\n    url = url,\n    destfile = download_location\n  )\n\n  # Unzip the file\n  utils::unzip(download_location, exdir = extract_location)\n\n  # Read the csv files into a list\n  csv_file_list <- list.files(\n    path = extract_location,\n    pattern = \"\\\\.csv$\",\n    full.names = TRUE\n  )\n\n  # make named list\n  csv_names <-\n    stats::setNames(\n      object = csv_file_list,\n      nm =\n        csv_file_list |>\n        basename() |>\n        gsub(pattern = \"\\\\.csv$\", replacement = \"\") |>\n        janitor::make_clean_names()\n    )\n\n  # Process CSV Files\n  parse_csv_file <- function(file) {\n    # Normalize the path to use C:/path/to/file structure\n    normalizePath(file, \"/\") |>\n      # read in the csv file and use check.names = FALSE because some of\n      # the names are very long\n      utils::read.csv(check.names = FALSE) |>\n      dplyr::as_tibble() |>\n      # clean the field names\n      janitor::clean_names()\n  }\n\n  list_of_tables <- lapply(csv_names, parse_csv_file)\n\n  unlink(tmp_dir, recursive = TRUE)\n\n  # Return the tibbles\n  # Add and attribute and a class type to the object\n  attr(list_of_tables, \".list_type\") <- \"current_hosp_data\"\n  class(list_of_tables) <- c(\"current_hosp_data\", class(list_of_tables))\n\n  list_of_tables\n}"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#app-file",
    "href": "posts/rtip-2023-04-03/index.html#app-file",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "App File",
    "text": "App File\nNext, let’s create a new file called app.R. In this file, we will create the Shiny app. The app will have a user interface (UI) and a server.\nThe UI is responsible for creating the layout of the app, while the server is responsible for processing the data and responding to user input.\nFirst, let’s create the UI. The UI will consist of a drop-down menu that will allow users to choose which data they would like to see presented to them in the app.\n\nlibrary(shiny)\nlibrary(shinythemes)\n\nhosp_data <- current_hosp_data()\n\nui <- fluidPage(theme = shinytheme(\"cerulean\"),\n                \n                # Set up the dropdown menu\n                selectInput(inputId = \"table\", \n                            label = \"Select a table:\", \n                            choices = names(hosp_data), \n                            selected = NULL),\n                \n                # Set up the table output\n                tableOutput(outputId = \"table_output\")\n)\n\nThe fluidPage() function creates a new Shiny app page. We also specify the theme using the {shinythemes} package. The selectInput() function creates the drop-down menu, which allows users to select which data they would like to see presented to them in the app. The choices argument is set to the names of the tables in the current_hosp_data() object. The tableOutput() function creates the output for the selected table."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#server",
    "href": "posts/rtip-2023-04-03/index.html#server",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Server",
    "text": "Server\nNext, let’s create the server. The server will be responsible for processing the data and generating the output based on user input.\n\nserver <- function(input, output) {\n    \n    # Load the data into a reactive object\n    data <- reactive(hosp_data)\n    \n    # Set up the table output\n    output$table_output <- renderTable({\n        # Get the selected table\n        table_selected <- input$table\n        \n        # Get the table from the data object\n        table_data <- data()[[table_selected]]\n        \n        # Return the table data\n        table_data\n    })\n}\n\nThe reactive() function is used to create a reactive object that will load the data when the app starts. The renderTable() function generates the output for the selected table. It does this by getting the selected table from the drop-down menu, getting the table data from the reactive data object, and returning the table data."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#shiny-app",
    "href": "posts/rtip-2023-04-03/index.html#shiny-app",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Shiny App",
    "text": "Shiny App\nFinally, we need to run the appl using the shinyApp() function:\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#pros-and-cons",
    "href": "posts/rtip-2023-04-03/index.html#pros-and-cons",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nPros:\n\nThe app is easy to use, and users can quickly select which data they would like to see presented to them in the app.\nThe current_hosp_data() function is only called once when the app starts, which can save time and resources if the function is time-consuming or resource-intensive.\n\nCons:\n\nThe app will not update if the data in the zip file changes. Users will need to restart the app to see the updated data.\nThe app loads all the data into memory when it starts, which can be an issue if the data is large and memory-intensive."
  },
  {
    "objectID": "posts/rtip-2023-04-04/index.html",
    "href": "posts/rtip-2023-04-04/index.html",
    "title": "A sample Shiny App to view Forecasts on the AirPassengers Data",
    "section": "",
    "text": "Hello! In this code, we are making a program that will help us predict the number of air passengers in the future. Let me explain what each part of the code does, step by step.\nFirst, we need to load some tools that will help us create the program. These tools are called “packages.” We use the library() function to load them. The packages we need are called shiny, forecast, and ggplot2.\n\n\n\nlibrary(shiny)\nlibrary(forecast)\nlibrary(ggplot2)\n\n\n\n\nNext, we need some data to work with. We will use a dataset of the number of air passengers each month from 1949 to 1960. We load this dataset using the data() function.\n\ndata(AirPassengers)\n\n\n\n\nNow, we need to create the user interface, or UI. This is what the user will see and interact with. In this case, we will create a simple app with a title, a dropdown menu to choose a forecasting model, and a plot and table to display the forecast results. We use the fluidPage() function to create the UI, and we define the UI elements inside it.\n\nui <- fluidPage(\n  titlePanel(\"AirPassengers Forecast\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"model\", label = \"Choose a model:\",\n                  choices = c(\"auto.arima\", \"ets\", \"holtwinters\"))\n    ),\n    mainPanel(\n      plotOutput(outputId = \"forecast_plot\"),\n      tableOutput(outputId = \"forecast_table\")\n    )\n  )\n)\n\n\n\n\nNow, we need to define the server. The server is where the program does the calculations and generates the output based on what the user selects in the UI. We define the server inside the function(input, output) argument.\n\nserver <- function(input, output) {\n\nInside the server, we need to create a reactive expression that generates the forecast based on the model the user selects. We use an if statement to check which model the user selected, and then we use the corresponding function to generate the forecast.\n\n\n\nforecast_data <- reactive({\n    if (input$model == \"auto.arima\") {\n      fit <- auto.arima(AirPassengers)\n      forecast(fit)\n    } else if (input$model == \"ets\") {\n      fit <- ets(AirPassengers)\n      forecast(fit)\n    } else {\n      fit <- hw(AirPassengers)\n      forecast(fit)\n    }\n  })\n\n\n\n\nThe renderPlot() function tells the program to create a plot based on the reactive expression we defined earlier. We use the plotOutput() function in the UI to display the plot.\n\noutput$forecast_plot <- renderPlot({\n    plot(forecast_data())\n  })\n\nSimilarly, the renderTable() function tells the program to create a table based on the reactive expression we defined earlier. We use the tableOutput() function in the UI to display the table.\n\noutput$forecast_table <- renderTable({\n    forecast_data()$mean\n  })\n\nFinally, we run the app using the shinyApp() function, with the UI and server arguments.\n\nshinyApp(ui = ui, server = server)\n\nAnd that’s it! This program allows the user to choose a forecasting model, and then generates a plot and table with the predicted number of air passengers based on that model.\nHere is the Full code block”\n\n# Load required packages\nlibrary(shiny)\nlibrary(forecast)\nlibrary(ggplot2)\n\n# Load AirPassengers dataset\ndata(AirPassengers)\n\n# Define UI\nui <- fluidPage(\n  \n  # Title of the app\n  titlePanel(\"AirPassengers Forecast\"),\n  \n  # Sidebar with input controls\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"model\", label = \"Choose a model:\",\n                  choices = c(\"auto.arima\", \"ets\", \"holtwinters\"))\n    ),\n    \n    # Output plot and table\n    mainPanel(\n      plotOutput(outputId = \"forecast_plot\"),\n      tableOutput(outputId = \"forecast_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Reactive expression to create forecast based on selected model\n  forecast_data <- reactive({\n    if (input$model == \"auto.arima\") {\n      fit <- auto.arima(AirPassengers)\n      forecast(fit)\n    } else if (input$model == \"ets\") {\n      fit <- ets(AirPassengers)\n      forecast(fit)\n    } else {\n      fit <- hw(AirPassengers)\n      forecast(fit)\n    }\n  })\n  \n  # Output plot\n  output$forecast_plot <- renderPlot({\n    plot(forecast_data())\n    #checkresiduals(forecast_data())\n  })\n  \n  # Output table\n  output$forecast_table <- renderTable({\n    forecast_data()$mean\n  })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-05/index.html",
    "href": "posts/rtip-2023-04-05/index.html",
    "title": "Looking at Daily Log Returns with tidyquant, TidyDensity, and Shiny",
    "section": "",
    "text": "In this blog post, we’ll walk through how to create a shiny application that allows users to analyze the weekly returns of FAANG stocks (AAPL, AMZN, FB, GOOGL, and NFLX) using the {tidyquant} and {TidyDensity} packages in R.\n\n\nThe first section of the code sets up the necessary R packages and creates the UI for the shiny app. The packages we’ll be using are:\n\nshiny: for creating interactive web applications in R\ntidyquant: for easily getting and analyzing financial data in R\nTidyDensity: for computing and visualizing probability distributions in a tidy way\ndplyr: for manipulating data in a tidy way\nDT: for creating interactive and scrollable data tables\n\nAnalysts assemble your packages!\n\nlibrary(shiny)\nlibrary(tidyquant)\nlibrary(TidyDensity)\nlibrary(dplyr)\nlibrary(DT)\n\nThe UI consists of a title panel, a sidebar panel, and a main panel. The sidebar panel contains a select input that allows users to choose which FAANG stock to analyze, as well as a numeric input for the number of simulations to run. The main panel contains two sections: one for the tidy_autoplot() output (a plot of the stock returns), and one for the tidy_empirical() output (a table of the log returns).\n\n\n\nThe second section of the code defines the server function for the shiny app. The server function takes the input values from the UI (i.e. the selected stock and number of simulations) and uses them to get and analyze the stock data.\nTo get the stock data, we use the tq_get() function from the tidyquant package to retrieve the adjusted stock prices for the selected security from January 1, 2010 to the present. We then use the tq_transmute() function to compute the weekly log returns of the stock and rename the resulting column to “log_return”.\nThe tidy_empirical() function from the TidyDensity package is used to compute the empirical distribution of the log returns. The resulting table is displayed using the renderDT() function from the DT package, which creates a scrollable data table that can be sorted and filtered.\nThe tidy_autoplot() function is used to create a plot of the log returns, which is displayed using the renderPlot() function.\n\n\n\nThe final section of the code runs the shiny app using the ui and server functions.\nOverall, this shiny app provides a simple and interactive way for users to analyze the weekly returns of FAANG stocks using tidyquant and TidyDensity in R. By allowing users to choose which stock to analyze and how many simulations to run, the app provides a customizable way to explore the empirical distributions of the log returns."
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html",
    "href": "posts/rtip-2023-04-06/index.html",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "",
    "text": "BRVM"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_ticker_desc-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_ticker_desc-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_ticker_desc() function",
    "text": "The BRVM_ticker_desc() function\nIt receives no argument and returns BRVM tickers information such as its full name, sector and country.\n\n# Display tickers of BRVM\ntickers <- BRVM_ticker_desc()\ntickers\n\n\n\nWarning: package 'kableExtra' was built under R version 4.2.3\n\n\n\n\n \n  \n    Ticker \n    Company name \n    Sector \n    Country \n  \n \n\n  \n    ABJC \n    SERVAIR ABIDJAN  COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    BICC \n    BICI COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    BNBC \n    BERNABE COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    BOAB \n    BANK OF AFRICA BENIN \n    FINANCE \n    BENIN \n  \n  \n    BOABF \n    BANK OF AFRICA BURKINA FASO \n    FINANCE \n    BURKINA FASO \n  \n  \n    BOAC \n    BANK OF AFRICA COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    BOAM \n    BANK OF AFRICA MALI \n    FINANCE \n    MALI \n  \n  \n    BOAN \n    BANK OF AFRICA NIGER \n    FINANCE \n    NIGER \n  \n  \n    BOAS \n    BANK OF AFRICA SENEGAL \n    FINANCE \n    SENEGAL \n  \n  \n    CABC \n    SICABLE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    CBIBF \n    CORIS BANK INTERNATIONAL BURKINA FASO \n    FINANCE \n    BURKINA FASO \n  \n  \n    CFAC \n    CFAO MOTORS COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    CIEC \n    CIE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    ECOC \n    ECOBANK COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    ETIT \n    Ecobank Transnational Incorporated TOGO \n    FINANCE \n    TOGO \n  \n  \n    FTSC \n    FILTISAC COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    NEIC \n    NEI-CEDA COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    NSBC \n    NSIA BANQUE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    NTLC \n    NESTLE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    ONTBF \n    ONATEL BURKINA FASO \n    PUBLIC SERVICE \n    BURKINA FASO \n  \n  \n    ORAC \n    ORANGE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    ORGT \n    ORAGROUP TOGO \n    FINANCE \n    TOGO \n  \n  \n    PALC \n    PALM COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    PRSC \n    TRACTAFRIC MOTORS COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    SAFC \n    SAFCA COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SCRC \n    SUCRIVOIRE COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SDCC \n    SODE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    SDSC \n    BOLLORE TRANSPORT & LOGISTICS COTE D'IVOIRE \n    TRANSPORT \n    IVORY COAST \n  \n  \n    SEMC \n    CROWN SIEM COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SGBC \n    SOCIETE GENERALE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SHEC \n    VIVO ENERGY COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    SIBC \n    SOCIETE IVOIRIENNE DE BANQUE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SICC \n    SICOR COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SIVC \n    AIR LIQUIDE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SLBC \n    SOLIBRA COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SMBC \n    SMB COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SNTS \n    SONATEL SENEGAL \n    PUBLIC SERVICE \n    SENEGAL \n  \n  \n    SOGC \n    SOGB COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SPHC \n    SAPH COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    STAC \n    SETAO COTE D'IVOIRE \n    OTHER \n    IVORY COAST \n  \n  \n    STBC \n    SITAB COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SVOC \n    MOVIS COTE D'IVOIRE \n    TRANSPORT \n    IVORY COAST \n  \n  \n    TTLC \n    TOTAL COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    TTLS \n    TOTAL SENEGAL \n    DISTRIBUTION \n    SENEGAL \n  \n  \n    TTRC \n    TRITURAF Ste en Liquid \n    INDUSTRY \n    IVORY COAST \n  \n  \n    UNLC \n    UNILEVER COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    UNXC \n    UNIWAX COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_index-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_index-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_index() function :",
    "text": "The BRVM_index() function :\nIt receives no argument and returns a table of updated data (with as table header: indexes, previous closing, closing, change (%), Year to Date Change) on all the indices available on the BRVM exchange.\n\n\n\n\n \n  \n    Indexes \n    Previous closing \n    Closing \n    Change (%) \n    Year to Date Change \n  \n \n\n  \n    BRVM-30 \n    99.71 \n    99.75 \n    0.04 \n    0.00 \n  \n  \n    BRVM - AGRICULTURE \n    281.76 \n    281.25 \n    -0.18 \n    -0.66 \n  \n  \n    BRVM - OTHER SECTOR \n    1295.58 \n    1357.27 \n    4.76 \n    -7.32 \n  \n  \n    BRVM - COMPOSITE \n    199.37 \n    199.46 \n    0.05 \n    0.85 \n  \n  \n    BRVM - DISTRIBUTION \n    346.02 \n    345.33 \n    -0.20 \n    0.69 \n  \n  \n    BRVM - FINANCE \n    74.53 \n    75.03 \n    0.67 \n    -0.66 \n  \n  \n    BRVM - INDUSTRY \n    98.33 \n    98.10 \n    -0.23 \n    0.92 \n  \n  \n    BRVM - PRESTIGE \n    102.61 \n    102.56 \n    -0.05 \n    0.00 \n  \n  \n    BRVM - PRINCIPAL \n    94.56 \n    94.62 \n    0.06 \n    0.00 \n  \n  \n    BRVM - PUBLIC SERVICES \n    480.97 \n    479.60 \n    -0.28 \n    2.23 \n  \n  \n    BRVM - TRANSPORT \n    345.28 \n    341.70 \n    -1.04 \n    0.35"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_get.symbol-.from-.to-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_get.symbol-.from-.to-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_get(“.symbol”, “.from”, “.to”) function",
    "text": "The BRVM_get(“.symbol”, “.from”, “.to”) function\nThis function will get the data of the companies listed on the BVRM stock exchange in Rich Bourse website. The function takes a single parameter, .symbol (which represents the “Ticker”). The function will automatically format tickers you enter in uppercase using toupper() and then ensure that the passed ticker is in a Google spreadsheet of allowed tickers.\n\n.symbol : A vector of symbols, like: c(“BICC”,“XOM”,“SlbC”) ;\n.from : A quoted start date, ie. “2020-01-01” or “2020/01/01”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD” ;\n.to : A quoted end date, ie. “2022-01-31” or “2022/01/31”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”.\n\n\n#' Displaying data of SONATEL Senegal stock\nBRVM_get(.symbol = \"snts\")\n\n[1] \"SNTS\"\n\n\n# A tibble: 251 × 6\n   Date        Open  High   Low Close Volume\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1 2022-04-06 15800 15895 15750 15800   7436\n 2 2022-04-07 15800 15900 15750 15900   1265\n 3 2022-04-08 15900 15995 15800 15900   1164\n 4 2022-04-11 15895 15900 15800 15800   4252\n 5 2022-04-12 15800 15800 15780 15800   6561\n 6 2022-04-13 15800 15865 15795 15850   5409\n 7 2022-04-14 15855 15900 15850 15900  16957\n 8 2022-04-15 15995 15995 15900 15900    791\n 9 2022-04-19 15900 15995 15895 15900  31217\n10 2022-04-20 15900 15995 15895 15990  32322\n# ℹ 241 more rows\n\nsymbols <- c(\"BiCc\",\"XOM\",\"SlbC\")   # We use here three tickers\ndata_tbl <- BRVM_get(.symbol = symbols, .from = \"2020-01-01\", .to = Sys.Date() - 1)\n\n[1] \"BICC\" \"SLBC\"\n\n# Display the first twenty observations of the table\nhead(data_tbl, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2020-01-10  6500  6500  6500  6500     24 BICC  \n 2 2020-01-13  6370  6500  6370  6500     29 BICC  \n 3 2020-01-14  6495  6495  6495  6495     10 BICC  \n 4 2020-01-29  6010  6010  6010  6010     24 BICC  \n 5 2020-01-30  6000  6000  6000  6000     50 BICC  \n 6 2020-02-04  5800  5800  5800  5800     12 BICC  \n 7 2020-02-07  5650  5650  5650  5650      5 BICC  \n 8 2020-02-10  5500  5500  5500  5500      5 BICC  \n 9 2020-02-14  5300  5300  5300  5300      9 BICC  \n10 2020-02-17  4910  4910  4910  4910    210 BICC  \n11 2020-02-18  4910  4910  4910  4910     50 BICC  \n12 2020-02-20  4895  4895  4895  4895      5 BICC  \n13 2020-02-21  4895  4895  4890  4890     13 BICC  \n14 2020-02-25  4525  4525  4525  4525     16 BICC  \n15 2020-02-26  4435  4435  4430  4430     21 BICC  \n16 2020-02-27  4345  4760  4335  4760   1809 BICC  \n17 2020-03-03  4745  4750  4745  4750     11 BICC  \n18 2020-03-05  4700  4700  4700  4700      5 BICC  \n19 2020-03-06  4695  4695  4695  4695      6 BICC  \n20 2020-03-11  4345  4450  4345  4450    135 BICC  \n\n# Display the last twenty elements of the table\ntail(data_tbl, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2023-02-15 80000 80000 79000 79000      2 SLBC  \n 2 2023-02-17 78000 78000 78000 78000      5 SLBC  \n 3 2023-02-21 80000 80000 80000 80000      5 SLBC  \n 4 2023-02-23 80000 80000 80000 80000     18 SLBC  \n 5 2023-02-24 80000 80000 80000 80000      6 SLBC  \n 6 2023-02-27 80000 80000 80000 80000     98 SLBC  \n 7 2023-02-28 80000 80000 80000 80000     11 SLBC  \n 8 2023-03-02 80000 80000 80000 80000     11 SLBC  \n 9 2023-03-08 80000 80000 80000 80000      2 SLBC  \n10 2023-03-09 80000 80000 80000 80000      2 SLBC  \n11 2023-03-13 80005 80005 80000 80000     12 SLBC  \n12 2023-03-14 80000 80000 80000 80000      1 SLBC  \n13 2023-03-20 80000 80000 80000 80000      3 SLBC  \n14 2023-03-21 80000 80000 80000 80000      4 SLBC  \n15 2023-03-27 78000 80000 78000 80000    169 SLBC  \n16 2023-03-28 80000 80000 80000 80000    435 SLBC  \n17 2023-03-30 80000 80000 80000 80000      3 SLBC  \n18 2023-03-31 80000 80000 80000 80000      1 SLBC  \n19 2023-04-04 80000 86000 80000 86000      3 SLBC  \n20 2023-04-05 85950 86000 85950 86000      6 SLBC"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_get1ticker-period-from-to-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_get1ticker-period-from-to-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_get1(“ticker”, “Period”, “from”, “to”) function",
    "text": "The BRVM_get1(“ticker”, “Period”, “from”, “to”) function\nThis function will get data of the companies listed on the BVRM stock exchange through the sikafinance site. The function takes in a single parameter of ticker and will auto-format the tickers you input into all upper case by using toupper()\n\nticker : A vector of ticker, like: c(“BICC”,“XOM”,“SlbC”, “BRvm10”);\nPeriod : Numeric number indicating time period. Valid entries are 0, 1, 5, 30, 91, and 365 representing respectively ‘daily’, ‘one year’, ‘weekly’, ‘monthly’, ‘quarterly’ and ‘yearly’;\nfrom : A quoted start date, ie. “2020-01-01” or “2020/01/01”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”;\nto : A quoted end date, ie. “2022-01-31” or “2022/01/31”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”\n\n** NB : There is a small difference between the BRVM_get and BRVM_get1 functions. * With BRVM_get it is only possible to download tickers’ daily data. * But with BRVM_get1, you can download daily, weekly, monthly, annual tickers’ data, indices and even market capitalization.\n\n#' Displaying data of SONATEL Senegal stock\nBRVM_get1(\"snts\")\n\n[1] \"Make sure you have an active internet connection\"\n\n# Get daily data of all indexes\nall_ind <- BRVM_get1(\"ALL INDEXES\", Period = 0, from = \"2020-01-04\", to = \"2023-03-24\") \n\n[1] \"We obtained BRVM10 data from 2019-12-26 to 2023-01-04\"\n[1] \"We obtained BRVMAG data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMC data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMAS data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMDI data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMFI data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMIN data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMSP data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMTR data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMPR data from 2023-01-01 to 2023-03-24\"\n[1] \"We obtained BRVMPA data from 2023-01-04 to 2023-03-24\"\n[1] \"We obtained BRVM30 data from 2023-01-01 to 2023-03-24\"\n[1] \"We obtained CAPIB data from 2020-01-02 to 2023-03-24\"\n\n# display the first two tens elements of the table\nhead(all_ind, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2022-12-26  169.  169.  169.  169.      0 BRVM10\n 2 2022-12-27  169.  169.  169.  169.      0 BRVM10\n 3 2022-12-28  167.  167.  167.  167.      0 BRVM10\n 4 2022-12-29  167.  167.  167.  167.      0 BRVM10\n 5 2022-12-30  166.  166.  166.  166.      0 BRVM10\n 6 2023-01-02  166.  166.  166.  166.      0 BRVM10\n 7 2023-01-03  166.  166.  166.  166.      0 BRVM10\n 8 2023-01-04  166.  166.  166.  166.      0 BRVM10\n 9 2022-09-26  163.  163.  163.  163.      0 BRVM10\n10 2022-09-27  162.  162.  162.  162.      0 BRVM10\n11 2022-09-28  162.  162.  162.  162.      0 BRVM10\n12 2022-09-29  163.  163.  163.  163.      0 BRVM10\n13 2022-09-30  164.  164.  164.  164.      0 BRVM10\n14 2022-10-03  162.  162.  162.  162.      0 BRVM10\n15 2022-10-04  162.  162.  162.  162.      0 BRVM10\n16 2022-10-05  161.  161.  161.  161.      0 BRVM10\n17 2022-10-06  161.  161.  161.  161.      0 BRVM10\n18 2022-10-07  161.  161.  161.  161.      0 BRVM10\n19 2022-10-10  160.  160.  160.  160.      0 BRVM10\n20 2022-10-11  160.  160.  160.  160.      0 BRVM10\n\n# display the two tens of the last elements of the table\ntail(all_ind, 20)\n\n# A tibble: 20 × 7\n   Date          Open    High     Low   Close Volume Ticker\n   <date>       <dbl>   <dbl>   <dbl>   <dbl>  <dbl> <chr> \n 1 2020-02-26 4281311 4281311 4281311 4281311      0 CAPIB \n 2 2020-02-27 4314933 4314933 4314933 4314933      0 CAPIB \n 3 2020-02-28 4346515 4346515 4346515 4346515      0 CAPIB \n 4 2020-03-02 4424073 4424073 4424073 4424073      0 CAPIB \n 5 2020-03-03 4379647 4379647 4379647 4379647      0 CAPIB \n 6 2020-03-04 4369550 4369550 4369550 4369550      0 CAPIB \n 7 2020-03-05 4342229 4342229 4342229 4342229      0 CAPIB \n 8 2020-03-06 4359879 4359879 4359879 4359879      0 CAPIB \n 9 2020-03-09 4338293 4338293 4338293 4338293      0 CAPIB \n10 2020-03-10 4357221 4357221 4357221 4357221      0 CAPIB \n11 2020-03-11 4332656 4332656 4332656 4332656      0 CAPIB \n12 2020-03-12 4318096 4318096 4318096 4318096      0 CAPIB \n13 2020-03-13 4318112 4318112 4318112 4318112      0 CAPIB \n14 2020-03-16 4285184 4285184 4285184 4285184      0 CAPIB \n15 2020-03-17 4301727 4301727 4301727 4301727      0 CAPIB \n16 2020-03-18 4288582 4288582 4288582 4288582      0 CAPIB \n17 2020-03-19 4207231 4207231 4207231 4207231      0 CAPIB \n18 2020-03-20 4209788 4209788 4209788 4209788      0 CAPIB \n19 2020-03-23 4154445 4154445 4154445 4154445      0 CAPIB \n20 2020-03-24 4144325 4144325 4144325 4144325      0 CAPIB \n\n# To get yearly data\nyearly_data <- BRVM_get1(c(\"brvmtr\", \"BiCc\", \"BOAS\"), Period = 365 ) \n# display the first two tens elements of the table\nhead(yearly_data, 20) \n\n# A tibble: 20 × 6\n   Date         Open   High    Low  Close Ticker\n   <date>      <dbl>  <dbl>  <dbl>  <dbl> <chr> \n 1 2003-04-11   74.0   88.6   73.6   88.6 BRVMTR\n 2 2004-01-02   88.6   89.2   72.9   89.2 BRVMTR\n 3 2005-01-03   89.2  107.    70.7  104.  BRVMTR\n 4 2006-01-02  104.   158.   104.   153.  BRVMTR\n 5 2007-01-02  153.   275.   149.   249.  BRVMTR\n 6 2008-01-02  249.   386.   226.   296.  BRVMTR\n 7 2009-01-02  275.   296.   227.   236.  BRVMTR\n 8 2010-01-04  236.   259.   224.   238.  BRVMTR\n 9 2011-01-03  238.   249.   204.   239   BRVMTR\n10 2012-01-02  239    349.   201.   349.  BRVMTR\n11 2013-01-02  349.   794.   339.   789.  BRVMTR\n12 2014-01-02  789.  1213.   601.  1213.  BRVMTR\n13 2015-01-02 1213.  1525.   653.  1525.  BRVMTR\n14 2016-01-04 1525.  1525.  1216.  1432.  BRVMTR\n15 2017-01-02 1432.  1433.   764.  1203.  BRVMTR\n16 2018-01-02 1114.  1193.   966.   966.  BRVMTR\n17 2019-06-03  403.   429.   311.   367.  BRVMTR\n18 2020-01-01  367.   475.   292.   379.  BRVMTR\n19 2021-01-04  376.   622.   325    622.  BRVMTR\n20 2022-01-03  667.   667.   295.   342.  BRVMTR\n\n# display the two tens of the last elements of the table\ntail(yearly_data, 20) \n\n# A tibble: 20 × 6\n   Date        Open  High   Low Close Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl> <chr> \n 1 2014-01-02  5650  7848  5650  7800 BICC  \n 2 2015-01-02  8385 10750  7800 10100 BICC  \n 3 2016-01-04 10000 10700  8566  9890 BICC  \n 4 2017-01-05  9750 10000  6440  8490 BICC  \n 5 2018-01-02  8700  8750  3795  7900 BICC  \n 6 2019-01-04  7550  7550  3710  6800 BICC  \n 7 2020-01-01  6800  6890  2855  6680 BICC  \n 8 2021-01-04  6680  7525  4280  7400 BICC  \n 9 2022-01-03  7250  7250  5550  6850 BICC  \n10 2023-01-02  6500  6850  5785  6275 BICC  \n11 2014-12-10  1613  3225  1613  3225 BOAS  \n12 2015-01-02  3370  4300  2900  3950 BOAS  \n13 2016-01-04  3700  4101  2000  2350 BOAS  \n14 2017-01-02  2325  3875  2035  2500 BOAS  \n15 2018-01-02  2400  3250  1700  2020 BOAS  \n16 2019-01-02  1900  2000  1500  1545 BOAS  \n17 2020-01-01  1550  1700  1295  1495 BOAS  \n18 2021-01-04  1480  2750  1340  2350 BOAS  \n19 2022-01-03  2350  2780  2200  2450 BOAS  \n20 2023-01-02  2580  2585  2175  2265 BOAS"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm.index-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm.index-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM.index() function :",
    "text": "The BRVM.index() function :\nIt receives no argument and returns the name of all indexes available on BRVM Stock Exchange.\n\nBRVM.index()\n\n [1] \"BRVMAG\" \"BRVMC\"  \"BRVMAS\" \"BRVMDI\" \"BRVMFI\" \"BRVMIN\" \"BRVMSP\" \"BRVMTR\"\n [9] \"BRVMPR\" \"BRVMPA\" \"BRVM30\"\n\n\nAuthors : \n\nKoffi Frederic Sessie (koffisessie@gmail.com),\nAbdoul Oudouss Diakité (abdouloudoussdiakite@gmail.com),\nSanderson Steven(spsanderson@gmail.com)\n\nCreator : Koffi Frederic Sessie \ncph (Copyright Holder) : Koffi Frederic Sessie \nLicense : MIT 2023, BRVM authors. All rights reserved."
  },
  {
    "objectID": "posts/rtip-2023-04-07/index.html",
    "href": "posts/rtip-2023-04-07/index.html",
    "title": "Reading in Multiple Excel Sheets with lapply and {readxl}",
    "section": "",
    "text": "Intruduction\nReading in an Excel file with multiple sheets can be a daunting task, especially for users who are not familiar with the process. In this blog post, we will walk through a sample function that can be used to read in an Excel file with multiple sheets using the R programming language.\n\n\nFunction\nThe function we will be using is called excel_sheet_reader(). This function takes one argument: filename, which is the name of the Excel file we want to read in. This function, since it is using the {readxl} package will automatically read that data to a tibble.\n\n\nExample\nHere is the function:\n\nexcel_sheet_reader <- function(filename) {\n  sheets <- excel_sheets(filename)\n  x <- lapply(sheets, function(X) read_excel(filename, sheet = X))\n  names(x) <- sheets\n  x\n}\n\nThe first thing the excel_sheet_reader() function does is to determine the names of all the sheets in the Excel file using the excel_sheets function from the readxl package. This function returns a character vector containing the names of all the sheets in the Excel file.\n\nsheets <- excel_sheets(filename)\n\nNext, the function uses the lapply function to loop through all the sheet names and read in each sheet using the read_excel() function, also from the readxl package. This function takes two arguments: filename, which is the name of the Excel file, and sheet, which is the name of the sheet we want to read in. The lapply function returns a list containing all the sheets.\n\nx <- lapply(sheets, function(X) read_excel(filename, sheet = X))\n\nFinally, the function uses the names function to assign the sheet names to the list of sheets and returns the list.\n\nnames(x) <- sheets\nx\n\nNow that we have explained the excel_sheet_reader() function, let’s use it to read in the iris and mtcars datasets.\n\nlibrary(healthyR)\nlibrary(dplyr)\nlibrary(writexl)\nlibrary(readxl)\n\niris |>\n  named_item_list(Species) |>\n  write_xlsx(path = \"iris.xlsx\")\n\nmtcars |>\n  named_item_list(cyl) |>\n  write_xlsx(path = \"mtcars.xlsx\")\n\niris_sheets <- excel_sheet_reader(\"iris.xlsx\")\nmtcars_sheets <- excel_sheet_reader(\"mtcars.xlsx\")\n\nNow lets see the structure of each file.\n\niris_sheets\n\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <chr>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <chr>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# ℹ 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <chr>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# ℹ 40 more rows\n\n\nNow mtcars_sheets\n\nmtcars_sheets\n\n$`4`\n# A tibble: 11 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  22.8     4 108      93  3.85  2.32  18.6     1     1     4     1\n 2  24.4     4 147.     62  3.69  3.19  20       1     0     4     2\n 3  22.8     4 141.     95  3.92  3.15  22.9     1     0     4     2\n 4  32.4     4  78.7    66  4.08  2.2   19.5     1     1     4     1\n 5  30.4     4  75.7    52  4.93  1.62  18.5     1     1     4     2\n 6  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n 7  21.5     4 120.     97  3.7   2.46  20.0     1     0     3     1\n 8  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1\n 9  26       4 120.     91  4.43  2.14  16.7     0     1     5     2\n10  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n11  21.4     4 121     109  4.11  2.78  18.6     1     1     4     2\n\n$`6`\n# A tibble: 7 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n3  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n4  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n5  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n6  17.8     6  168.   123  3.92  3.44  18.9     1     0     4     4\n7  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6\n\n$`8`\n# A tibble: 14 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 2  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 3  16.4     8  276.   180  3.07  4.07  17.4     0     0     3     3\n 4  17.3     8  276.   180  3.07  3.73  17.6     0     0     3     3\n 5  15.2     8  276.   180  3.07  3.78  18       0     0     3     3\n 6  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n 7  10.4     8  460    215  3     5.42  17.8     0     0     3     4\n 8  14.7     8  440    230  3.23  5.34  17.4     0     0     3     4\n 9  15.5     8  318    150  2.76  3.52  16.9     0     0     3     2\n10  15.2     8  304    150  3.15  3.44  17.3     0     0     3     2\n11  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4\n12  19.2     8  400    175  3.08  3.84  17.0     0     0     3     2\n13  15.8     8  351    264  4.22  3.17  14.5     0     1     5     4\n14  15       8  301    335  3.54  3.57  14.6     0     1     5     8\n\n\nAnd that’s it! Hope this has been helpful!"
  },
  {
    "objectID": "posts/rtip-2023-04-10/index.html",
    "href": "posts/rtip-2023-04-10/index.html",
    "title": "Styling Tables for Excel with {styledTables}",
    "section": "",
    "text": "Introduction\nIn the analytics realm whether some like it or not, Excel is huge and maybe King. This is due to the fact of the shear volume of people using it. Microsoft has positioned Excel well in this situation, but, that does not mean we cannot extend Excel with R. In fact we can do just that. I will be focusing new posts on this topic as I gear up to collaborate on a new project focusing on this issue.\nFor this post we are going to discuss the {styledTable} R package that can be installed from GitHub. Here are a few ways in which the styledTable package can help.\n\nCreating visually appealing tables: Excel is a powerful tool for data analysis and visualization, but it can be limited in terms of formatting options. With the ‘styledtable’ package, users can create tables with a wide range of formatting options, such as bold text, colored cells, and borders. This can make the tables more visually appealing and easier to read, which can be helpful when presenting data to others.\nAutomating data analysis: The ‘styledtable’ package can be used in combination with other R packages to automate data analysis tasks. For example, users can use R to clean and transform data, and then use the ‘styledtable’ package to create formatted tables for reporting or sharing with others. This can save time and reduce errors associated with manual data entry and formatting.\nIntegrating with other R packages: R has a large ecosystem of packages for data analysis, visualization, and reporting. The ‘styledtable’ package can be used in conjunction with other R packages to extend the functionality of Excel. For example, users can use R to perform statistical analysis on data, and then use the ‘styledtable’ package to create formatted tables for reporting the results in Excel.\nFacilitating collaboration: Sharing Excel files can be challenging when working with multiple users or teams. With the ‘styledtable’ package, users can export styled tables to Excel format, which can be shared with others. This can facilitate collaboration and streamline the process of sharing data and analysis results.\n\nThe styledtable package in R, which allows users to create styled tables in R Markdown documents. The package can help to create tables with various formatting options such as bold text, colored cells, and borders. It also has functionality on how to port these to Excel itself.\nThe package offers a simple syntax that allows users to specify formatting options using HTML and CSS. The resulting table can be customized by changing the CSS file or by using the ‘styler’ function to apply custom styles to individual cells or rows.\nOverall, the styledtable package provides a useful tool for creating visually appealing tables in R Markdown documents, and the ability to export these tables to Excel format makes it easier to share and analyze data with others.\n\n\nFunctions\n\n\nExamples\n\n# Install development version from GitHub\ndevtools::install_github('R-package/styledTables', build_vignettes = TRUE)"
  },
  {
    "objectID": "posts/rtip-2023-04-11/index.html",
    "href": "posts/rtip-2023-04-11/index.html",
    "title": "Styling Tables for Excel with {styledTables}",
    "section": "",
    "text": "Introduction\nIn the analytics realm whether some like it or not, Excel is huge and maybe King. This is due to the fact of the shear volume of people using it. Microsoft has positioned Excel well in this situation, but, that does not mean we cannot extend Excel with R. In fact we can do just that. I will be focusing new posts on this topic as I gear up to collaborate on a new project focusing on this issue.\nFor this post we are going to discuss the {styledTable} R package that can be installed from GitHub. Here are a few ways in which the styledTable package can help.\n\nCreating visually appealing tables: Excel is a powerful tool for data analysis and visualization, but it can be limited in terms of formatting options. With the ‘styledtable’ package, users can create tables with a wide range of formatting options, such as bold text, colored cells, and borders. This can make the tables more visually appealing and easier to read, which can be helpful when presenting data to others.\nAutomating data analysis: The ‘styledtable’ package can be used in combination with other R packages to automate data analysis tasks. For example, users can use R to clean and transform data, and then use the ‘styledtable’ package to create formatted tables for reporting or sharing with others. This can save time and reduce errors associated with manual data entry and formatting.\nIntegrating with other R packages: R has a large ecosystem of packages for data analysis, visualization, and reporting. The ‘styledtable’ package can be used in conjunction with other R packages to extend the functionality of Excel. For example, users can use R to perform statistical analysis on data, and then use the ‘styledtable’ package to create formatted tables for reporting the results in Excel.\nFacilitating collaboration: Sharing Excel files can be challenging when working with multiple users or teams. With the ‘styledtable’ package, users can export styled tables to Excel format, which can be shared with others. This can facilitate collaboration and streamline the process of sharing data and analysis results.\n\nThe styledtable package in R, which allows users to create styled tables in R Markdown documents. The package can help to create tables with various formatting options such as bold text, colored cells, and borders. It also has functionality on how to port these to Excel itself.\nThe package offers a simple syntax that allows users to specify formatting options using HTML and CSS. The resulting table can be customized by changing the CSS file or by using the ‘styler’ function to apply custom styles to individual cells or rows.\nOverall, the styledtable package provides a useful tool for creating visually appealing tables in R Markdown documents, and the ability to export these tables to Excel format makes it easier to share and analyze data with others.\n\n\nExamples\n\n# Install development version from GitHub\ndevtools::install_github('R-package/styledTables', build_vignettes = TRUE)\n\n\nlibrary(styledTables)\nlibrary(dplyr)\nlibrary(xlsx)\n\ndf <- mtcars |>\n  select(mpg, cyl, am)\n\ndf\n\n                     mpg cyl am\nMazda RX4           21.0   6  1\nMazda RX4 Wag       21.0   6  1\nDatsun 710          22.8   4  1\nHornet 4 Drive      21.4   6  0\nHornet Sportabout   18.7   8  0\nValiant             18.1   6  0\nDuster 360          14.3   8  0\nMerc 240D           24.4   4  0\nMerc 230            22.8   4  0\nMerc 280            19.2   6  0\nMerc 280C           17.8   6  0\nMerc 450SE          16.4   8  0\nMerc 450SL          17.3   8  0\nMerc 450SLC         15.2   8  0\nCadillac Fleetwood  10.4   8  0\nLincoln Continental 10.4   8  0\nChrysler Imperial   14.7   8  0\nFiat 128            32.4   4  1\nHonda Civic         30.4   4  1\nToyota Corolla      33.9   4  1\nToyota Corona       21.5   4  0\nDodge Challenger    15.5   8  0\nAMC Javelin         15.2   8  0\nCamaro Z28          13.3   8  0\nPontiac Firebird    19.2   8  0\nFiat X1-9           27.3   4  1\nPorsche 914-2       26.0   4  1\nLotus Europa        30.4   4  1\nFord Pantera L      15.8   8  1\nFerrari Dino        19.7   6  1\nMaserati Bora       15.0   8  1\nVolvo 142E          21.4   4  1\n\n\nOk, now we have our data that we are going to work with, so let’s check out some features.\nFirst we will just apply the styled_table() function and inspect the output.\n\nstl_df <- df |>\n  styled_table(keep_header = TRUE)\n\nclass(stl_df)\n\n[1] \"StyledTable\"\nattr(,\"package\")\n[1] \"styledTables\"\n\n\nNow let’s apply some simple formatting.\n\nstl_df <- stl_df |>\n  set_border_position(\"all\", row_id = 1) |>\n  set_bold(row_id = 1) |>\n  set_fill_color(\"#00FF00\", col_id = 2, condition = X == \"6\")\n\nWrite out to excel.\n\nwb <- createWorkbook()\nsheet <- createSheet(wb, \"mtcars_tbl\")\n\n# Insert table\nwrite_excel(sheet, stl_df)\n\n# Save workbook\nsaveWorkbook(wb, \"test.xlsx\")\n\nHere is the test output:\n\n\n\nTest Output"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html",
    "href": "posts/rtip-2023-04-18/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "",
    "text": "Shiny is an R package that allows you to build interactive web applications using R code. TidyDensity is an R package that provides a tidyverse-style interface for working with probability density functions. In this tutorial, we’ll use these two packages to build a Shiny app that allows users to interact with TidyDensity functions."
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#required-packages",
    "href": "posts/rtip-2023-04-18/index.html#required-packages",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "Required Packages",
    "text": "Required Packages\nBefore we dive into the code, let’s go over the packages that we’ll be using in this app:\n\nShiny: As mentioned earlier, Shiny is an R package for building interactive web applications. It provides a variety of input controls and output elements that allow you to create user interfaces for your R code.\nTidyDensity: TidyDensity is an R package that provides a tidyverse-style interface for working with probability density functions. It provides a set of functions for generating density functions, as well as a tidy_autoplot() function for creating visualizations.\ntidyverse: Tidyverse is a collection of R packages designed for data science. It includes many popular packages such as ggplot2, dplyr, and tidyr. We’ll be using some functions from the tidyverse packages in our Shiny app.\nDT: DT is an R package for creating interactive tables in RMarkdown documents, Shiny apps, and RStudio. We’ll be using the DT::datatable() function to create a table of output data in our app.\n\nLoad them up!\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(tidyverse)\nlibrary(TidyDensity)"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#the-ui-object",
    "href": "posts/rtip-2023-04-18/index.html#the-ui-object",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "The UI Object",
    "text": "The UI Object\nThe UI object is the first argument of the shinyApp() function, and it defines the layout and appearance of the app. In our TidyDensity Shiny app, we’ll use a sidebar layout with two input controls and two output elements:\n\nSelect Function Input: A selectInput() control that allows users to select one of four TidyDensity functions: tidy_normal(), tidy_bernoulli(), tidy_beta(), and tidy_gamma().\nNumber of Simulations Input: A numericInput() control that allows users to specify the number of simulations to use in the TidyDensity function.\nSample Size Input: A numericInput() control that allows users to specify the sample size to use in the TidyDensity function.\nDensity Plot Output: A plotOutput() element that displays the density plot generated by tidy_autoplot().\nData Table Output: A dataTableOutput() element that displays the output data from the TidyDensity function.\n\nHere’s the code for the UI object:\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"function\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                    )\n                  ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200)\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      dataTableOutput(\"data_table\")\n    )\n  )\n)"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#the-server-object",
    "href": "posts/rtip-2023-04-18/index.html#the-server-object",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "The Server Object",
    "text": "The Server Object\nThe server object is the second argument of the shinyApp() function, and it defines the behavior and output of the app. In our TidyDensity Shiny app, the server object consists of two reactive expressions that generate the output elements based on the user inputs:\n\nData Reactive Expression: A reactive expression that generates the output data for the selected TidyDensity function based on the user inputs. We use match.fun() to convert the selected function name into an R function, and we pass the num_sims and n arguments from the input controls.\nDensity Plot Reactive Expression: A reactive expression that generates the density plot using tidy_autoplot() and the output data from the data reactive expression.\nData Table Output: We use DT::renderDataTable() to generate the data table output element based on the output data from the data reactive expression.\n\nHere’s the code for the server object:\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$function)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() %>%\n      tidy_autoplot()\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n}"
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html",
    "href": "posts/rtip-2023-04-19/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "",
    "text": "Shiny is an R package that allows you to create interactive web applications from R code. In this blog post, we’ll explore the different components of a Shiny application and show how they work together to create an interactive data visualization app. This is a part 2 with a small enhancement."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-ui",
    "href": "posts/rtip-2023-04-19/index.html#the-ui",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The UI",
    "text": "The UI\nThe user interface (UI) is the visual part of the app that the user interacts with. In our app, the UI is defined using the fluidPage() function from the shiny package. It consists of a title panel, a sidebar layout, and a main panel.\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      )\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\nThe title panel displays the app name, while the sidebar layout contains the input controls for the user. In this case, we have four input elements:\n\nselectInput() allows the user to choose a statistical distribution to generate data from.\nnumericInput() allows the user to set the number of simulations.\nAnother numericInput() allows the user to set the sample size.\nselectInput() allows the user to choose the type of plot to display.\n\nThe main panel contains the output elements for the app, in this case a plot and a table."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-server",
    "href": "posts/rtip-2023-04-19/index.html#the-server",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The Server",
    "text": "The Server\nThe server is the backend of the app that handles the logic and generates the output based on user input. In our app, the server is defined using the server() function from the shiny package.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n}\n\nThe server() function takes two arguments, input and output. These arguments allow the server to interact with the user interface.\nFirst, we create a reactive data object data, which takes in the user’s input for the function, number of simulations, and sample size, and passes it to the appropriate function using match.fun().\nNext, we create the density_plot output. We use the renderPlot() function to create a reactive plot of the data using the tidy_autoplot() function from the {TidyDensity} package. The tidy_autoplot() function allows the user to choose from several plot types, including density, quantile, probability, qq, and mcmc. We then print the plot using the print() function.\nFinally, we create the data_table output using the DT::renderDataTable() function. This output displays the reactive data as a table using the DT::datatable() function."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-shiny-app",
    "href": "posts/rtip-2023-04-19/index.html#the-shiny-app",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The Shiny App",
    "text": "The Shiny App\nFinally, we run the Shiny app using the shinyApp() function, which takes the ui and server functions as arguments:\n\nshinyApp(ui = ui, server = server)\n\nThis launches the app and displays the user interface. The user can interact with the app by selecting a function, specifying the number of simulations and sample size, and viewing the resulting density plot and data table. The app provides a simple and interactive way to explore the TidyDensity package and its functionality."
  },
  {
    "objectID": "posts/rtip-2023-04-20/index.html",
    "href": "posts/rtip-2023-04-20/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 3",
    "section": "",
    "text": "Introduction\nIn the previous post we allowed users to choose a distribution and a plot type. Now, we want to allow users to download a .csv file of the data that is generated.\nIn the UI, we added a downloadButton with outputId = \"download_data\" and label = \"Download Data\". In the server, we added a downloadHandler that takes a filename and content function. The filename function returns the name of the file to be downloaded (in this case, we used the selected function name as the file name with “.csv” extension). The content function writes the reactive data to a CSV file using the write.csv function. The downloadHandler returns the file to be downloaded when the button is clicked.\nSee here: \n\n\nUI Section\nHere is the update to the UI Section\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      # Download the data\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n\n\nServer Section\nHere is the update to the Server section.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      paste0(input$functions, \".csv\")\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\n\n\nConclusion\nWith these changes, the user can now export the data to a .csv file by clicking the “Export Data” button and selecting where to save the file.\nI hope this update to the TidyDensity app will make it more useful for your data analysis needs. If you have any questions or feedback, please feel free to let me know, and as usual…Steal this Code!! Modify for yourself and see what you come up with.\nHere is the entire script:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      paste0(input$functions, \".csv\")\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-21/index.html",
    "href": "posts/rtip-2023-04-21/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 4",
    "section": "",
    "text": "Introduction\nIf you’re new to data science or statistics, you may have heard about probability distributions. Probability distributions are mathematical functions that help us understand the probability of a random variable taking on a certain value. For example, if we’re rolling a fair six-sided die, we know that each number has an equal chance of being rolled (1/6 or about 17% chance). We can represent this using a probability distribution, specifically a discrete uniform distribution.\nHowever, not all probability distributions are as simple as a uniform distribution. Many real-world phenomena, such as the heights of people, the number of cars passing through a toll booth in a day, or the amount of rainfall in a particular area, are continuous and can’t be represented using a discrete distribution. Instead, we use continuous probability distributions, which describe the probability of a continuous variable taking on a range of values.\nThere are many different types of continuous probability distributions, each with their own properties and use cases. For example, the normal distribution, also known as the bell curve, is commonly used to model many natural phenomena, such as human heights and weights. The beta distribution is used to model proportions or percentages, such as the proportion of voters who support a particular candidate. The gamma distribution is used to model the time between events in a Poisson process, such as the time between customers arriving at a store.\nThe sample TidyDensity App is a tool that helps us explore and visualize these different types of probability distributions. It’s a web application built using the R programming language and the Shiny framework, which allows us to create interactive web applications with R.\nLet’s break down the different components of the TidyDensity App.\n\n\nUser Interface\nThe user interface, or UI for short, is what the user sees and interacts with when they use the app. It’s built using HTML, CSS, and JavaScript, and it’s the first thing the user sees when they open the app.\nThe TidyDensity App has a simple UI that allows the user to select from four different probability distributions: normal, Bernoulli, beta, and gamma. Each of these distributions has its own properties and use cases, and the user can select which one they want to explore using a dropdown menu.\nIn addition, the user can specify the number of simulations they want to run, which determines how many times the probability distribution is sampled to generate data. They can also specify the sample size, which determines how many data points are generated in each simulation.\nFinally, the user can select which type of plot they want to see, such as a density plot, a quantile plot, a probability plot, or a QQ plot. Each of these plots shows a different aspect of the data generated from the probability distribution, and the user can choose which one to explore.\nHere is the code:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n\n\n\nHere is the new addition to the UI\n\n\n\n\nServer\nThe server is the back-end of the TidyDensity App. It’s responsible for generating the data based on the user’s inputs, and for creating the plots and tables that the user sees on the UI.\nThe server is written in R, and it uses several R packages to generate the data and create the plots. For example, the TidyDensity package is used to generate data from the selected probability distribution, and the ggplot2 package is used to create the plots.\nThe server is also responsible for handling user inputs, such as which probability distribution to use, how many simulations to run, and which plot type to show. It then generates the appropriate data and plot based on these inputs and sends them back to the UI for display.\nThe first thing we do is create a reactive variable data that will store the output of the match.fun() function, which is called with the arguments .num_sims and .n obtained from the user interface. We use the reactive variable because it will update automatically whenever the inputs are changed.\nThe output$density_plot object is created with renderPlot(), which takes the reactive variable data() and passes it to tidy_autoplot() with the plot type selected by the user in the input$plot_type object. The resulting plot is then printed to the user interface.\nThe output$data_table object is created with DT::renderDataTable(), which takes the reactive variable data() and returns a table to the user interface using the DT::datatable() function.\nFinally, the output$download_data object is created using downloadHandler(), which creates a download button for the user to download a .csv file of the data. The filename argument specifies the name of the file, and the content argument writes the data to a .csv file.\nHere is the code:\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n\n\nData Table\nThe data table is a table that shows the data generated from the probability distribution. It’s displayed on.\nOverall, this app is designed to allow users to generate various types of probability density plots and accompanying data tables based on user input. By allowing users to select different functions, sample sizes, and plot types, this app provides a flexible and customizable tool for exploring and visualizing probability distributions.\n\n\nFull Shiny App\nHere is the full script:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-24/index.html",
    "href": "posts/rtip-2023-04-24/index.html",
    "title": "Exploring Distributions with {shiny}, {TidyDensity} and {plotly} Part 5",
    "section": "",
    "text": "Introduction\nI have been writing about using the {TidyDensity} package with shiny for the last few posts, and this one is the last. This post will go over the app and discuss how to change the output of the graph from a ggplot2 object into a plotly object. So we will end up with something like this in the menu panel:\n\n\n\nPlotly Output\n\n\nAnd here is the difference between the plots, first the ggplot2 plot: \nAnd the plotly_plot: \nFirst, the required libraries are loaded: shiny, TidyDensity, tidyverse, DT, and plotly.\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(plotly)\n\n\n\nUI\nThe user interface (UI) is defined using the fluidPage() function from the shiny package. The UI consists of a title panel, a sidebar panel, and a main panel. The title panel simply displays the title of the app, while the sidebar panel contains user input elements such as radio buttons, text inputs, and numeric inputs. The main panel displays the plot, data table, and download button.\n\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      # user input elements\n    ),\n    mainPanel(\n      # plot, data table, and download button\n    )\n  )\n)\n\nNext, the server is defined using the server() function from the shiny package. The server is responsible for generating the output based on the user inputs. The first step is to create reactive data using the reactive() function. The reactive data is created based on the user inputs for the distribution function or the entered data. The match.fun() function is used to match the selected function with the corresponding function in the TidyDensity package. The tidy_empirical() function is used if the user entered their own data.\n\n\nServer\n\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n\nAfter the reactive data is created, the output is generated. The output consists of the density plot, data table, and download button. The renderPlot() and renderPlotly() functions are used to generate the plot output. The renderDataTable() function is used to generate the data table output. The downloadHandler() function is used to generate the download button.\n\n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n\nNext, we define the server function, which contains the code that will run in response to user input. We start by creating a reactive data object called data. This object will store the data that will be used to generate the plots and tables in the app.\nThe data that data stores depends on the user’s input. If the user selects “Enter Data” in the sidebar, then data will be set to a tidy_empirical() object generated from the user-entered data. Otherwise, if the user selects “Select Function”, then data will be set to a tidy_ function object generated using the user’s choices for number of simulations and sample size.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  ...\n}\n\nThe tidy_empirical() function is used to generate a density plot of the empirical distribution of the user-entered data. This function takes the user-entered data as input and returns a tidy data frame that can be used to create a density plot.\nThe tidy_ functions are used to simulate data from various distributions and generate plots based on that data. These functions take the number of simulations and sample size as input and return a tidy data frame that can be used to create various types of plots.\nNext, we define the code for generating the density plot. This code uses the data object that was created earlier to generate a plot. The tidy_autoplot() function is used to generate the plot based on the user’s selected plot type. If the user selects the “Use Plotly” option, then the plot is generated using the ggplotly() function from the plotly package.\n\n # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n\nThe ggplotly() function is used to generate an interactive version of the plot that can be zoomed in and out of and hovered over to see details about specific data points.\nNext, we define the code for generating the data table. This code simply displays the data object as a table using the datatable() function from the DT package.\n\n# Create data table\noutput$data_table <- DT::renderDataTable({\n  # Return reactive data as a data table\n  if (!is.null(data())) {\n    DT::datatable(data())\n  }\n})\n\nFinally, we define the code for downloading the data as a CSV file. This code uses the downloadHandler() function to generate a file download link that, when clicked, will download the data as a CSV file. The name of the CSV file depends on the user’s input.\n\n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n\nFinally, here is the script in it’s entirety, steal it and see what you can come up with!!\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(plotly)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      selectInput(inputId = \"plotly_option\",\n                  label = \"Use Plotly\",\n                  choices = c(\"TRUE\", \"FALSE\"),\n                  selected = \"FALSE\"\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      conditionalPanel(\n        condition = \"input.plotly_option == 'TRUE'\",\n        plotlyOutput(\"density_plotly\")\n      ),\n      conditionalPanel(\n        condition = \"input.plotly_option == 'FALSE'\",\n        plotOutput(\"density_plot\")\n      ),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-25/index.html",
    "href": "posts/rtip-2023-04-25/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 1",
    "section": "",
    "text": "Introduction\nWelcome to the {tidyAML} Model Builder, a Shiny web application that allows you to build predictive models using the tidyAML and Parsnip packages in R.\nLet’s dive into the code to understand how it works!\n\n\nLoad Libraries\nFirst, we load the necessary packages:\n\nshiny\ntidyAML\nrecipes\nDT\nglmnet.\n\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\n\n\n\nUI\nNext, we define the user interface (UI) of the Shiny app using the fluidPage() function from the shiny package. The UI consists of a title panel, a sidebar panel, and a main panel.\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\n        \"dataset\", \n        \"Choose a built-in dataset:\", \n        choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\n        \"predictor_col\", \n        \"Select the predictor column:\", \n        choices = NULL\n      ),\n      selectInput(\n        \"model_type\", \n        \"Select a model type:\", \n        choices = c(\"regression\", \"classification\")\n      ),\n      selectInput(\n        \"model_fn\", \n        \"Select a model function:\", \n         choices = c(\"lm\", \"glm\", \"glmnet\")\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\")\n    )\n  )\n)\n\nThe sidebarPanel() contains several input elements that allow the user to specify the dataset, the predictor column, the type of model, and the model function. There is also an input element that allows the user to upload their own data file. The actionButton() is used to trigger the model building process. Finally, the verbatimTextOutput() element is used to display the output of the model building process.\nThe mainPanel() contains a single verbatimTextOutput() element that displays the output of the model building process.\nNext, we define the server function, which is responsible for handling the user inputs and building the predictive models. The server function takes three arguments:input, output, and session.\n\nserver <- function(input, output, session){\n  ...\n}\n\nWe start by defining a reactive expression called data. This expression reads in the user-specified dataset or data file and updates the predictor_col select input with the names of the columns of the dataset.\n\n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n\nThe first reactive expression, data, reads in the data file uploaded by the user or selects a built-in dataset, depending on which option the user chooses. If the user uploads a file, the read.csv() function is used to read the data file into a data frame. If the user selects a built-in dataset, the get() function is used to retrieve the data frame associated with that dataset. In both cases, the column names of the data frame are used to update the choices in the predictor_col select input, so that the user can select which column to use as the predictor variable.\nThe next reactive expression, recipe_obj, creates a recipe object based on thepredictor_col selected by the user and the data frame returned by data(). The as.formula() function is used to create a formula that specifies the predictor column as the response variable and all other columns as the predictors. The resulting formula is passed to the recipe() function, along with the data frame. The step_normalize() function is then used to standardize all numeric predictors (except for the outcome variable) to have a mean of 0 and a standard deviation of 1. The resulting recipe object is returned by the reactive expression.\n\n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    \n    return(rec)\n  })\n\nThe model_fn reactive expression uses a switch() statement to determine which model function to use based on the model_fn select input. The available options are \"lm\" (for linear regression), \"glm\" (for generalized linear models), and \"glmnet\" (for regularized linear models).\n\n  model_fn <- reactive({\n    switch(\n      input$model_fn,\n      \"lm\" = \"lm\",\n      \"glm\" = \"glm\",\n      \"glmnet\" = \"glmnet\"\n    )\n  })\n\nThe last reactive expression, model, uses the fast_regression() or fast_classification() functions from the tidyAML package to build a regression or classification model based on the data, recipe, and model function selected by the user. The resulting model object is returned by the reactive expression.\n\n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(\n        .data = data(),\n        .rec_obj = recipe_obj(),\n        .parsnip_eng = model_fn()\n      )\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(\n        .data = data(),\n        .rec_obj = recipe_obj(),\n        .parsnip_eng = model_fn()\n      )\n    }\n    return(mod)\n  })\n\nFinally we output the summary of the recipe_obj and print the resulting tibble of model(s) to the screen.\n\n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n\nAnd of course, we cannot serve our app until we run the following line:\n\nshinyApp(ui = ui, server = server)\n\nI hope you have enjoyed this post. Please steal this code and see what you can do with it. I am trying to figure out how to print the tibble using the DT package so maybe in another post.\n\n\nFull Shiny App\nHere are some pictures \n\n\n\nMaking a recipe change\n\n\n\n\n\nSingle Model Output\n\n\n\n\n\nTwo Model Output with one successful failure\n\n\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n                  ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n                  ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_fn\", \"Select a model function:\", \n                  choices = c(\"lm\", \"glm\", \"glmnet\")\n                  ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n                  ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_fn <- reactive({\n    switch(input$model_fn,\n           \"lm\" = \"lm\",\n           \"glm\" = \"glm\",\n           \"glmnet\" = \"glmnet\")\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_fn())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_fn())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-26/index.html",
    "href": "posts/rtip-2023-04-26/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 2",
    "section": "",
    "text": "Introduction\nYesterday I spoke about building tidymodels models using my package {tidyAML} and {shiny}. I have made an update to it, and will continue to make updates to it this week.\nI have added all of the supported engines for regression problems only, NOT classification yet, that will be tomorrow’s work. I will then add a drop down for users to pick which backend function they want to use from {parsnp} like linear_reg().\nHere are some pictures of the udpates.\n\n\n\nNew Drop Down Additions\n\n\n\n\n\nreactable Error, not sure on how to fix yet\n\n\n\n\n\nreactable output\n\n\nHere is the full application, please steal this code and modify for yourself, you never know what you might come up with!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n                  ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n                  ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_fn\", \"Select a model function:\", \n                  choices = c(\"all\",\"lm\",\"brulee\",\"gee\",\"glm\",\n                              \"glmer\",\"glmnet\",\"gls\",\"lme\",\n                              \"lmer\",\"stan\",\"stan_glmer\",\n                              \"Cubist\",\"hurdle\",\"zeroinfl\",\"earth\",\n                              \"rpart\",\"dbarts\",\"xgboost\",\"lightgbm\",\n                              \"partykit\",\"mgcv\",\"nnet\",\"kknn\",\"ranger\",\n                              \"randomForest\",\"xrf\",\"LiblineaR\",\"kernlab\"\n                            )\n                  ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n                  ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_fn <- reactive({\n    switch(input$model_fn,\n           \"all\" = \"all\",\n           \"lm\" = \"lm\",\n           \"brulee\" = \"brulee\",\n           \"gee\" = \"gee\",\n           \"glm\" = \"glm\",\n           \"glmer\" = \"glmer\",\n           \"glmnet\" = \"glmnet\",\n           \"gls\" = \"gls\",\n           \"lme\" = \"lme\",\n           \"lmer\" = \"lmer\",\n           \"stan\" = \"stan\",\n           \"stan_glmer\" = \"stan_glmer\",\n           \"Cubist\" = \"Cubist\",\n           \"hurdle\" = \"hurdle\",\n           \"zeroinfl\" = \"zeroinfl\",\n           \"earth\" = \"earth\",\n           \"rpart\" = \"rpart\",\n           \"dbarts\" = \"dbarts\",\n           \"xgboost\" = \"xgboost\"          ,\n           \"lightgbm\" = \"lightgbm\",\n           \"partykit\" = \"partykit\",\n           \"mgcv\" = \"mgcv\",\n           \"nnet\" = \"nnet\",\n           \"kknn\" = \"kknn\",\n           \"ranger\" = \"ranger\",\n           \"randomForest\" = \"randomForest\",\n           \"xrf\" = \"xrf\",\n           \"LiblineaR\" = \"LiblineaR\",\n           \"kernlab = kernlab\")\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_fn())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_fn())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/rtip-2023-04-27/index.html",
    "href": "posts/rtip-2023-04-27/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 3",
    "section": "",
    "text": "Introduction\nAs data science continues to be a sought-after field, creating a reliable and accurate model is essential. While there are various machine learning algorithms available, the process of selecting the correct algorithm can be complex. The {tidyAML} package, part of the tidymodels suite, offers an easy-to-use, consistent interface for building machine learning models. In this post, we will explore a Shiny application that utilizes tidyAML to build a machine learning model.\nToday I have updated the tidyAML shiny app to include the ability to set the parameter of the fast_regression() function .parsnip_fns and this is things like linear_reg.\nHere is a full list of what is available:\n\nlibrary(tidyAML)\nlibrary(dplyr)\n\nc(\"all\",\n  make_regression_base_tbl() |> \n    pull(.parsnip_fns) |> \n    unique()\n  )\n\n [1] \"all\"              \"linear_reg\"       \"cubist_rules\"     \"poisson_reg\"     \n [5] \"bag_mars\"         \"bag_tree\"         \"bart\"             \"boost_tree\"      \n [9] \"decision_tree\"    \"gen_additive_mod\" \"mars\"             \"mlp\"             \n[13] \"nearest_neighbor\" \"rand_forest\"      \"rule_fit\"         \"svm_linear\"      \n[17] \"svm_poly\"         \"svm_rbf\"         \n\n\nI have updated the UI to reflect using that method as well. Here is the UI changes:\n\n      selectInput(\"model_engine\", \"Select a model engine:\", \n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_engine) |> \n                                unique()\n                  )\n      ),\n      selectInput(\"model_fns\", \"Select a model function:\",\n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_fns) |> \n                                unique()\n                              )\n\nHere are some pictures showing the changes:\n\n\n\nUI Change\n\n\n\n\n\nUI Change 2\n\n\n\n\n\nOutput\n\n\nSo what this means is that we can just pick a function like parsnip::linear_reg() and leave the engine set to \"all\" and it will build models for all engines supported that work with linear_reg().\n\n\nThe Shiny Application\nThe Shiny application is a graphical user interface (GUI) that allows users to select a dataset, predictor column, model type, and engine, and then build a machine learning model. The user can upload a CSV or TXT file or choose one of two built-in datasets: “mtcars” or “iris”. The user can select the predictor column, which is the variable used to predict the outcome, and then choose the model type, either “regression” or “classification”. Next, the user can select a model engine and a model function to use in building the model. Once the user has made all the selections, they can click the “Build Model” button to create the model.\nThe code for the Shiny application can be broken down into two parts, the User Interface (UI) and the Server. Let’s take a closer look at each of these parts.\n\n\nThe UI\nThe UI is created using the fluidPage() function from the shiny package. The titlePanel() function creates the title of the application. The sidebarLayout() function creates the sidebar and main panel. The sidebar contains input controls such as file input, select input, and an action button. The main panel displays the outputs generated by the model.\nThe fileInput() function creates a widget that allows the user to upload a data file. The selectInput() function creates dropdown menus for the user to select the dataset, predictor column, model type, model engine, and model function. The actionButton() function creates a button that the user clicks to build the model. The verbatimTextOutput() function and reactableOutput() function display the output generated by the model.\n\n\nThe Server\nThe Server is where the input data is processed, the model is built, and the output is generated. The Server is created using the server() function from the shiny package.\nThe reactive() function is used to create a reactive object called data that reads in the data file or built-in dataset selected by the user. The eventReactive() function is used to create a reactive object called recipe_obj that creates a recipe for preprocessing the data. The recipe includes steps to normalize the numeric variables and remove the outcome variable from the recipe.\nTwo other reactive objects, model_engine and model_fns, are created using the switch() function. These objects contain a list of available engines and model functions for the user to choose from.\nFinally, the eventReactive() function is used to create a reactive object called model that builds the machine learning model. The fast_regression() and fast_classification() functions from the tidyAML package are used to build the regression and classification models, respectively.\n\n\nConclusion\nIn this post, we explored a Shiny application that uses tidyAML to build a machine learning model. The application allows users to select a dataset, predictor column, model type, engine, and function to build a machine learning model. The Shiny application is an excellent tool for those who are new to machine learning or those who want to streamline the rapid prototyping process.\n\n\nFull Application\nThis is a work in progress, and I want you to steal this code and see what you can come up with!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_engine\", \"Select a model engine:\", \n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_engine) |> \n                                unique()\n                  )\n      ),\n      selectInput(\"model_fns\", \"Select a model function:\",\n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_fns) |> \n                                unique()\n                              )\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n      )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n    ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_engine <- reactive({\n    switch(input$model_engine,\n           \"all\" = \"all\",\n           \"lm\" = \"lm\",\n           \"brulee\" = \"brulee\",\n           \"gee\" = \"gee\",\n           \"glm\" = \"glm\",\n           \"glmer\" = \"glmer\",\n           \"glmnet\" = \"glmnet\",\n           \"gls\" = \"gls\",\n           \"lme\" = \"lme\",\n           \"lmer\" = \"lmer\",\n           \"stan\" = \"stan\",\n           \"stan_glmer\" = \"stan_glmer\",\n           \"Cubist\" = \"Cubist\",\n           \"hurdle\" = \"hurdle\",\n           \"zeroinfl\" = \"zeroinfl\",\n           \"earth\" = \"earth\",\n           \"rpart\" = \"rpart\",\n           \"dbarts\" = \"dbarts\",\n           \"xgboost\" = \"xgboost\"          ,\n           \"lightgbm\" = \"lightgbm\",\n           \"partykit\" = \"partykit\",\n           \"mgcv\" = \"mgcv\",\n           \"nnet\" = \"nnet\",\n           \"kknn\" = \"kknn\",\n           \"ranger\" = \"ranger\",\n           \"randomForest\" = \"randomForest\",\n           \"xrf\" = \"xrf\",\n           \"LiblineaR\" = \"LiblineaR\",\n           \"kernlab = kernlab\")\n  })\n  \n  model_fns <- reactive({\n    switch(input$model_fns,\n           \"all\" = \"all\",\n           \"linear_reg\" = \"linear_reg\",\n           \"cubist_rules\" = \"cubist_rules\",\n           \"poisson_reg\" = \"poisson_reg\",\n           \"bag_mars\" = \"bag_mars\",\n           \"bag_tree\" = \"bag_tree\",\n           \"bart\" = \"bart\",\n           \"boost_tree\" = \"boost_tree\",\n           \"decision_tree\" = \"decision_tree\",\n           \"gen_additive_mod\" = \"gen_additive_mod\",\n           \"mars\" = \"mars\",\n           \"mlp\" = \"mlp\",\n           \"nearest_neighbor\" = \"nearest_neighbor\",\n           \"rand_forest\" = \"rand_forest\",\n           \"rule_fit\" = \"rule_fit\",\n           \"svm_linear\" = \"svm_linear\",\n           \"svm_poly\" = \"svm_poly\",\n           \"svm_rbf\" = \"svm_rbf\"\n    )\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_engine(),\n                             .parsnip_fns = model_fns())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_engine(),\n                                 .parsnip_fns = model_fns())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-29/index.html",
    "href": "posts/rtip-2023-04-29/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 4",
    "section": "",
    "text": "Introduction\nThis is a Shiny app for building models using the {tidyAML} which is based on the tidymodels package in R. The app allows you to upload your own data or choose from one of two built-in datasets (mtcars or iris) and select the type of model you want to build (regression or classification).\nLet’s take a closer look at the code.\nFirst, the necessary packages are loaded:\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\nThe tidymodels_prefer() function is called to set some default options for the tidymodels package, and load_deps() from tidyAML is called to make sure all the necessary packages are loaded, you can also separately run install_deps() to make sure they all get installed.\n\ntidymodels_prefer()\nload_deps()\n\nNext, the user interface (UI) is defined using the fluidPage() function. The UI consists of a title panel and a sidebar layout with various input elements, such as file input and select input. There are also two conditional panels that are shown depending on the selected model type (regression or classification). The UI also includes an action button and some output elements, such as verbatimTextOutput and reactableOutput.\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\",\n                  \"Select the predictor column:\",\n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")\n      ),\n      conditionalPanel(\n        condition = \"input.model_type == 'regression'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique()))\n      ),\n      \n      conditionalPanel(\n        condition = \"input.model_type == 'classification'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique())),\n        checkboxInput(\"predictor_factor\",\n                      \"Convert predictor column to factor?\",\n                      value = TRUE)\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nAfter defining the UI, the server function is defined. The server function handles the reactive behavior of the app.\nThe first reactive element is data, which reads in the data file if one is uploaded or loads the selected built-in dataset if one is chosen. It also converts the predictor column to a factor if the classification model type is selected.\nIn the server function, we first define a reactive expression data() that will read the data file uploaded by the user or one of the built-in datasets (mtcars or iris). If the user has uploaded a file, the function read.csv is used to read the data, and if it’s a classification problem, the predictor column is converted to a factor variable. The updateSelectInput function is then called to update the predictor_col select input with the names of the columns in the data. If the user has chosen one of the built-in datasets, it is loaded using the get function, and the same preprocessing is performed.\nNext, we define an event reactive recipe_obj() that creates a recipes object based on the selected predictor column and normalizes the numeric variables in the data. The step_normalize function standardizes all numeric variables (except the outcome variable) to have mean 0 and standard deviation 1. This is a common preprocessing step in machine learning pipelines that can improve model performance.\nTwo reactive expressions, model_engine() and model_fns(), are then defined to generate the available model engines and functions based on the selected model type. For regression models, the make_regression_base_tbl functions are used, and for classification models, the make_classification_base_tbl functions are used. These functions return a table with information about the available model engines and functions for a given problem type. The pull function is used to extract the relevant columns from the table, and unique is used to remove duplicate values. The c function is used to concatenate the “all” choice with the available model engines or functions.\nFinally, an event reactive model() is defined that builds the model based on the selected parameters. If the model type is regression, the fast_regression function from the tidyAML package is used, and if the model type is classification, the fast_classification function is used. These functions take as inputs the data, the recipes object, the selected model engine and function, and any additional model parameters.\nThere are three output functions defined in the server: output$recipe_output, output$model_table, and output$model_reactable. The first output function output$recipe_output renders a summary of the recipes object created by recipe_obj() if the predictor_col input is not null. The second output function output$model_table prints the model object returned by model() if the build_model button has been clicked. The third output function output$model_reactable renders a reactive table using the reactable function from the reactable package if the build_model button has been clicked. This table displays the tidyaml_model_tbl.\nOverall, this code creates a Shiny web application that allows users to build machine learning models using the tidymodels framework via {tidyAML}. Users can upload their own data or use one of the built-in datasets, select a predictor column, choose a model type, select a model engine and function, and build the model. The output is displayed in a table that provides insights into the model’s performance and coefficients. This code is useful for data scientists and analysts who want to quickly build and evaluate machine learning models without having to write code from scratch.\n\n\nFull Application\nAs usual, steal this code and make it your own! See what you can do too!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\",\n                  \"Select the predictor column:\",\n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")\n      ),\n      conditionalPanel(\n        condition = \"input.model_type == 'regression'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique()))\n      ),\n      \n      conditionalPanel(\n        condition = \"input.model_type == 'classification'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique())),\n        checkboxInput(\"predictor_factor\",\n                      \"Convert predictor column to factor?\",\n                      value = TRUE)\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n      )\n      if (input$model_type == \"classification\") {\n        df[[input$predictor_col]] <- as.factor(df[[input$predictor_col]])\n      }\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      if (input$model_type == \"classification\") {\n        df[[input$predictor_col]] <- as.factor(df[[input$predictor_col]])\n      }\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n    ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_engine <- reactive({\n    if (input$model_type == \"regression\") {\n      c(\"all\", \n        make_regression_base_tbl() %>% \n          pull(.parsnip_engine) %>% \n          unique())\n    } else if (input$model_type == \"classification\") {\n      c(\"all\", \n        make_classification_base_tbl() %>% \n          pull(.parsnip_engine) %>% \n          unique())\n    }\n  })\n  \n  model_fns <- reactive({\n    if (input$model_type == \"regression\") {\n      c(\"all\", \n        make_regression_base_tbl() %>% \n          pull(.parsnip_fns) %>% \n          unique())\n    } else if (input$model_type == \"classification\") {\n      c(\"all\", \n        make_classification_base_tbl() %>% \n          pull(.parsnip_fns) %>% \n          unique())\n    }\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_engine(),\n                             .parsnip_fns = model_fns())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_engine(),\n                                 .parsnip_fns = model_fns())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-01/index.html",
    "href": "posts/rtip-2023-05-01/index.html",
    "title": "Extracting a model call from a fitted workflow in {tidymodels}",
    "section": "",
    "text": "Introduction\nIn this post, we are using a package called tidymodels, which provides a suite of tools for modeling and machine learning.\nNow, let’s take a closer look at the code itself and how we extract a model call from a fitted workflow object.\n\nlibrary(tidymodels)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nrec_obj\n\nThe first line loads the tidymodels package. Then, we create a “recipe” object called rec_obj using the recipe() function. A recipe is a set of instructions for preparing data for modeling. In this case, we are telling the recipe to use the mpg variable as the outcome or dependent variable, and all other variables in the mtcars dataset as the predictors or independent variables.\n\nmodel_spec <- linear_reg(mode = \"regression\", engine = \"lm\")\nmodel_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNext, we create a “model specification” object called model_spec using the linear_reg() function. This specifies the type of model we want to use, which is a linear regression model in this case. We also specify that the model is a regression (i.e., we are predicting a continuous outcome variable) and that the model engine is “lm”, which stands for “linear model”.\n\nwflw <- workflow() |>\n  add_recipe(rec_obj) |>\n  add_model(model_spec)\nwflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nIn the next section of code, we create a “workflow” object called wflw using the workflow() function. A workflow is a way of organizing the steps involved in building a machine learning model. In this case, we are using a “pipe” (|>) to sequentially add the recipe and model specification to the workflow. This means that we first add the recipe to the workflow using the add_recipe() function, and then add the model specification using the add_model() function.\n\nwflw_fit <- fit(wflw, data = mtcars)\nwflw_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   12.30337     -0.11144      0.01334     -0.02148      0.78711     -3.71530  \n       qsec           vs           am         gear         carb  \n    0.82104      0.31776      2.52023      0.65541     -0.19942  \n\n\nFinally, we fit the workflow to the data using the fit() function, which takes the workflow object (wflw) and the data (mtcars) as input. This creates a new object called wflw_fit, which is the fitted model object. This object contains various pieces of information about the fitted model, such as the model coefficients and the R-squared value.\n\nwflw_fit$fit$fit$fit$call\n\nstats::lm(formula = ..y ~ ., data = data)\n\n\nThe last line of code extracts the actual function call that was used to fit the model. This can be useful for reproducing the analysis later on.\nOverall, the code you shared shows how to build a simple linear regression model using the tidymodels package in R. We start by creating a recipe that specifies the outcome variable and predictor variables, then create a model specification for a linear regression model, and finally combine these into a workflow and fit the model to the data."
  },
  {
    "objectID": "posts/rtip-2023-05-03/index.html",
    "href": "posts/rtip-2023-05-03/index.html",
    "title": "How to Download a File from the Internet using download.file()",
    "section": "",
    "text": "Introduction\nThe download.file() function in R is used to download files from the internet and save them onto your computer. Here’s a simple explanation of how to use it:\n\nSpecify the URL of the file you want to download.\nSpecify the file name and the location where you want to save the file on your computer.\nCall the download.file() function, passing in the URL and file name/location as arguments.\n\nHere’s an example:\n\n# Specify the URL of the file you want to download\nurl <- \"https://example.com/data.csv\"\n\n# Specify the file name and location where you want to save the file on your computer\nfile_name <- \"my_data.csv\"\nfile_path <- \"/path/to/save/folder/\"\n\n# Call the download.file() function, passing in the URL and file name/location as arguments\ndownload.file(url, paste(file_path, file_name, sep = \"\"), mode = \"wb\")\n\nIn this example, we’re downloading a CSV file from “https://example.com/data.csv”, and saving it as “my_data.csv” in the “/path/to/save/folder/” directory on our computer.\nThe mode = “wb” argument specifies that we want to download the file in binary mode.\nOnce you run this code, the file will be downloaded from the URL and saved to your specified file location.\nLet’s try a working example.\n\n\nExample\nWe are going to download the Measure Dates file from the following location: {https://data.cms.gov/provider-data/dataset/4j6d-yzce}\n\nurl <- \"https://data.cms.gov/provider-data/sites/default/files/resources/49244993de5a948bcb0d69bf5cc778bd_1681445112/Measure_Dates.csv\"\n\nfile_name <- \"measure_dates.csv\"\nfile_path <- \"C:\\\\Downloads\\\\\"\n\ndownload.file(url = url, destfile = paste0(file_path, file_name, sep = \"\"))\n\nNow let’s read in the file in order to make sure it actually downloaded.\n\nmeasure_dates_df <- read.csv(file = paste0(file_path, file_name))\n\ndplyr::glimpse(measure_dates_df)\n\nRows: 170\nColumns: 6\n$ Measure.ID            <chr> \"ASC_11\", \"ASC_12\", \"ASC_13\", \"ASC_14\", \"ASC_17\"…\n$ Measure.Name          <chr> \"Percentage of patients who had cataract surgery…\n$ Measure.Start.Quarter <chr> \"1Q2021\", \"1Q2019\", \"1Q2021\", \"1Q2021\", \"3Q2020\"…\n$ Start.Date            <chr> \"01/01/2021\", \"01/01/2019\", \"01/01/2021\", \"01/01…\n$ Measure.End.Quarter   <chr> \"4Q2021\", \"4Q2021\", \"4Q2021\", \"4Q2021\", \"4Q2021\"…\n$ End.Date              <chr> \"12/31/2021\", \"12/31/2021\", \"12/31/2021\", \"12/31…\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-11/index.html",
    "href": "posts/rtip-2023-01-11/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2022, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2023!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2022\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nfp <- \"linkedin_content.xlsx\"\n\nengagement_tbl <- read_excel(fp, sheet = \"ENGAGEMENT\") %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ntop_posts_tbl <- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %>%\n  clean_names()\n\nfollowers_tbl <- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ndemographics_tbl <- read_excel(fp, sheet = \"DEMOGRAPHICS\") %>%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 362\nColumns: 4\n$ date              <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 202…\n$ impressions       <dbl> 3088, 3911, 3303, 3134, 1118, 799, 3068, 1954, 2663,…\n$ engagements       <dbl> 31, 56, 51, 42, 8, 4, 43, 20, 33, 43, 14, 41, 5, 17,…\n$ `Engagement Rate` <dbl> 1.0038860, 1.4318589, 1.5440509, 1.3401404, 0.715563…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 5\n$ post_url_1  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ engagements <dbl> 241, 136, 123, 117, 117, 115, 107, 106, 104, 104, 95, 81, …\n$ x3          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_4  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ impressions <dbl> 52300, 33903, 30752, 29887, 25953, 24139, 23769, 18522, 18…\n\nglimpse(followers_tbl)\n\nRows: 362\nColumns: 2\n$ date          <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 2022-01…\n$ new_followers <dbl> 10, 10, 12, 5, 12, 13, 9, 8, 11, 4, 9, 6, 7, 9, 10, 11, …\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics <chr> \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            <chr> \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       <chr> \"0.054587073624134064\", \"0.035217467695474625\", \"0.02…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %>%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\n\nYou will notice that I placed a blue line where I started my telegram channel @steveondata and a red line where I started this blog. So far, not bad, it looks like the telegram channel helped a little bit but writing on the blog seems to maybe been helping the most.\nLet’s look at a cumulative view of things.\n\nengagement_tbl %>%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %>%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %>%\n  slice(1:12) %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %>%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %>%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %>%\n  slice(1:12) %>%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html",
    "href": "posts/rtip-2023-05-04/index.html",
    "title": "Maps with {shiny}",
    "section": "",
    "text": "The code is used to create a Shiny app that allows the user to search for a type of amenity (such as a pharmacy) in a particular city, state, and country, and then display the results on a map. Here is a step-by-step explanation of how the code works."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#concatenating-the-address",
    "href": "posts/rtip-2023-05-04/index.html#concatenating-the-address",
    "title": "Maps with {shiny}",
    "section": "Concatenating the Address",
    "text": "Concatenating the Address\nThe first thing that the observeEvent function does is concatenate the user inputs for city, state, and country into a single string. This is done using the paste function. The sep argument specifies that the words should be separated by a comma and space. The resulting string is the address that will be used to search for the specified amenity."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#obtaining-the-bounding-box",
    "href": "posts/rtip-2023-05-04/index.html#obtaining-the-bounding-box",
    "title": "Maps with {shiny}",
    "section": "Obtaining the Bounding Box",
    "text": "Obtaining the Bounding Box\nNext, the code uses the getbb function from the osmdata library to obtain the bounding box for the specified address. A bounding box is a rectangle that contains the entire area of interest (in this case, the specified city, state, and country). The bounding box is necessary to limit the search for the specified amenity to only the specified area."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#creating-the-query",
    "href": "posts/rtip-2023-05-04/index.html#creating-the-query",
    "title": "Maps with {shiny}",
    "section": "Creating the Query",
    "text": "Creating the Query\nThe code then creates a query object using the opq function from the osmdata library. The bbox argument specifies the bounding box that was obtained in the previous step. The add_osm_feature function is then used to specify the amenity that the user is searching for. The key argument specifies that we are searching for an “amenity”, and the value argument specifies the specific type of amenity that the user entered (e.g., pharmacy)."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#obtaining-the-results",
    "href": "posts/rtip-2023-05-04/index.html#obtaining-the-results",
    "title": "Maps with {shiny}",
    "section": "Obtaining the Results",
    "text": "Obtaining the Results\nThe osmdata_sf function is used to retrieve the results of the query. This function returns a sf object that contains the spatial data for the points that match the specified amenity. The resulting sf object is then passed to the mapview function from the mapview library, which creates an interactive map of the results."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#displaying-the-map",
    "href": "posts/rtip-2023-05-04/index.html#displaying-the-map",
    "title": "Maps with {shiny}",
    "section": "Displaying the Map",
    "text": "Displaying the Map\nFinally, the renderLeaflet function is used to display the map in the UI. The m@map argument specifies that we want to display the map that was created by the mapview function. The resulting map is displayed in the leafletOutput that was defined in the UI."
  },
  {
    "objectID": "posts/rtip-2023-05-05/index.html",
    "href": "posts/rtip-2023-05-05/index.html",
    "title": "Maps with {shiny} Pt 2",
    "section": "",
    "text": "Introduction\nThe code provided at the end of this post is an example of how to create a simple Shiny app in R that utilizes the OpenStreetMap (OSM) API to create a map of amenities in a specific location.\nThe app has two main parts: the user interface (UI) and the server.\nThe UI section is defined using the fluidPage function from the Shiny library, which creates a responsive, fluid layout for the app. It includes a title panel, a sidebar panel with text input fields for the city, state, country, and amenity type, and a submit button. The main panel of the UI includes a leafletOutput object, which will display the map of amenities.\nThe server section is defined using the server function from the Shiny library. This function is responsible for processing the inputs from the UI, performing any necessary calculations, and rendering the output.\nThe observeEvent function is used to capture the click event of the submit button. When the button is clicked, the function getbb from the osmdata library is used to retrieve the bounding box (bbox) for the specified location.\nNext, the opq function from the osmdata library is used to create a query object that searches for amenities of the specified type (input$amenity) within the retrieved bbox.\nThe assign function is used to set a variable has_internet_via_proxy to TRUE in the curl environment. This is necessary to ensure that the osmdata_sf function, which downloads the OSM data, works properly.\nThe osmdata_sf function is then called with the created query object as its argument. This function downloads the OSM data and converts it to an sf object. The resulting sf object contains a data frame with information about the amenities found in the specified location.\nA mapview object is then created using the osm_points part of the sf object. This object is assigned to the variable m.\nFinally, the renderLeaflet function is used to display the resulting map. The mapview object m is accessed and its @map attribute is used as the input to the renderLeaflet function. This displays the map of amenities in the specified location.\nThere is also some commented out code in the server section that provides an alternative way to display the map using the leaflet library instead of the mapview library. This code creates a leaflet object, adds tiles to the map, and then adds circle markers to represent the amenities found in the specified location. The popup argument specifies what information is displayed in the popups that appear when the user clicks on a marker.\nOverall, this code demonstrates how to use the Shiny library to create an interactive web application that utilizes the OSM API to display maps of amenities in specific locations.\n\n\nFull Application\nAs usual, here is the full code. Please take it and see what you can do with it.\n\nlibrary(shiny)\nlibrary(osmdata)\nlibrary(mapview)\nlibrary(leaflet)\nlibrary(htmltools)\n\nui <- fluidPage(\n  titlePanel(\"Mapping with Shiny\"),\n  sidebarLayout(\n    sidebarPanel(\n      textInput(\"city\", \"City\", placeholder = \"e.g. Queens\"),\n      textInput(\"state\", \"State\", placeholder = \"e.g. New York\"),\n      textInput(\"country\", \"Country\", placeholder = \"e.g. USA\"),\n      textInput(\"amenity\", \"Amenity Type\", placeholder = \"e.g. pharmacy\"),\n      actionButton(\"submit\", \"Submit\")\n    ),\n    mainPanel(\n      leafletOutput(\"map\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  observeEvent(input$submit, {\n    # Concatenate city, state, and country inputs into a single string\n    address <- paste(input$city, input$state, input$country, sep = \", \")\n    \n    bbox <- getbb(address)\n    \n    query <- opq(bbox = bbox) |>\n      add_osm_feature(key = \"amenity\", value = input$amenity)\n    \n    assign(\"has_internet_via_proxy\", TRUE, environment(curl::has_internet))\n    sf_obj <- osmdata_sf(query)\n    \n    m <- mapview(sf_obj$osm_points)\n    output$map <- renderLeaflet({\n      m@map\n    })\n    \n    # output$map <- renderLeaflet({\n    #   leaflet(sf_obj$osm_points) |>\n    #     addTiles() |>\n    #     addCircleMarkers(\n    #       radius = 3, \n    #       popup = ~as.character(\n    #         paste(\n    #           \"Name: \", name, \"<br/>\",\n    #           \"OSM ID: \", osm_id, \"<br/>\"\n    #         )\n    #       ),\n    #       opacity = 0.3\n    #     )\n    # })\n  })\n}\n\nshinyApp(ui, server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-08/index.html",
    "href": "posts/rtip-2023-05-08/index.html",
    "title": "Updates to {healthyR.data}",
    "section": "",
    "text": "Introduction\nIntroducing the Updated {healthyR.data} Package: Your Ultimate Health Data Companion\nIf you’re a healthcare professional or a data enthusiast, you’re probably familiar with the healthyR.data package. This R package has been an invaluable resource for accessing and analyzing public health data. With its latest release, version 1.0.3, the package has undergone some significant changes, including the addition of several new functions and a requirement for R version 3.4.0. In this post, we’ll take a closer look at the updates and how they can help you work with health data more efficiently.\n\n\nBreaking Changes\nIn keeping with tidyverse practices, healthyR.data now requires R version 3.4.0. This change may affect some users who haven’t updated their R version recently, but it’s an important step to keep the package up-to-date and compatible with other tidyverse packages.\n\n\nNew Functions\nOne of the main highlights of the new version is the addition of several new functions. Let’s take a look at each one and how it can help you work with health data:\n\ndl_hosp_data_dict(): This function downloads the data dictionary for the Hospital Compare dataset. This information can be crucial when working with health data, as it provides a clear understanding of the variables and their definitions.\ncurrent_hosp_data(): This function retrieves the most recent Hospital Compare dataset, which includes information on hospital quality, patient experience, and more.\ncurrent_asc_data(): This function retrieves the most recent Ambulatory Surgical Center (ASC) dataset, which includes information on ASC quality measures.\ncurrent_asc_oas_cahps_data(): This function retrieves the most recent ASC Outpatient and Ambulatory Surgery Consumer Assessment of Healthcare Providers and Systems (OAS CAHPS) dataset, which includes patient experience measures for ASCs.\ncurrent_comp_death_data(): This function retrieves the most recent data on hospital mortality rates for conditions such as heart attack, pneumonia, and stroke.\ncurrent_hai_data(): This function retrieves the most recent Healthcare-Associated Infections (HAI) dataset, which includes information on infections acquired during hospitalization.\ncurrent_hcahps_data(): This function retrieves the most recent Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS) dataset, which includes patient experience measures for hospitals.\ncurrent_hvbp_data(): This function retrieves the most recent Hospital Value-Based Purchasing (HVBP) dataset, which includes information on hospital quality and payment incentives.\ncurrent_ipfqr_data(): This function retrieves the most recent Inpatient Psychiatric Facility Quality Reporting (IPFQR) dataset, which includes information on psychiatric facility quality measures.\ncurrent_maternal_data(): This function retrieves the most recent Maternal and Infant Health Care Quality dataset, which includes information on maternal and infant health outcomes.\ncurrent_medicare_hospital_spending_data(): This function retrieves the most recent Medicare Hospital Spending by Claim dataset, which includes information on Medicare payments for hospital services.\ncurrent_opqr_data(): This function retrieves the most recent Outpatient Prospective Payment System (OPPS) Quality Reporting (OPQR) dataset, which includes information on outpatient facility quality measures.\ncurrent_imaging_efficiency_data(): This function retrieves the most recent Radiology Imaging Efficiency (RIE) dataset, which includes information on the appropriateness of imaging studies.\ncurrent_unplanned_hospital_visits_data(): This function retrieves the most recent Unplanned Hospital Visits dataset, which includes information on hospital readmissions and emergency department visits.\ncurrent_payments_data(): This function retrieves the most recent Provider-Level Payments dataset, which includes information on payments to healthcare providers.\ncurrent_pch_hcahps_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) dataset.\ncurrent_pch_hai_hospital_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Healthcare-Associated Infections (HAI) Hospital dataset, which includes information on healthcare-associated infections in PCMH hospitals.\ncurrent_pch_oncology_measures_hospital_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Oncology Measures Hospital dataset, which includes information on oncology measures in PCMH hospitals.\ncurrent_pch_outcomes_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Outcomes dataset, which includes information on outcomes for PCMH practices.\ncurrent_timely_and_effective_care_data(): This function retrieves the most recent Timely and Effective Care dataset, which includes information on hospital performance on timely and effective care measures.\ncurrent_va_data(): This function retrieves the most recent Veterans Affairs (VA) dataset, which includes information on VA hospital quality measures.\n\nAll of these functions provide valuable access to important health data, allowing users to perform detailed analyses and gain insights into various aspects of healthcare quality and outcomes.\n\n\nOther Improvements\nIn addition to the new functions, healthyR.data version 1.0.3 also includes several bug fixes and improvements. For example, the logic in the current_hosp_data() function has been confirmed by user feedback.\n\n\nConclusion\nThe healthyR.data package has long been a valuable resource for anyone working with health data. With the latest release, version 1.0.3, the package has become even more powerful and versatile, thanks to the addition of many new functions and improvements. If you’re a healthcare professional, researcher, or data enthusiast, healthyR.data is a must-have tool in your arsenal. Give it a try and see how it can help you gain new insights into the world of healthcare quality and outcomes."
  },
  {
    "objectID": "posts/rtip-2023-05-09/index.html",
    "href": "posts/rtip-2023-05-09/index.html",
    "title": "VBA to R and Back Again: Running R from VBA",
    "section": "",
    "text": "Introduction\nToday I am going to briefly go over an extremely simple example of running some R code via Excel VBA.\nLet’s start by discussing each line of code one by one:\nSub CallRnorm()\nThis line defines a subroutine called “CallRnorm”. A subroutine is a block of code that can be executed repeatedly from any part of the code, and it starts with the “Sub” keyword followed by the subroutine name and any arguments in parentheses.\nDim R As Variant\nDim result As Variant\nThese two lines declare two variables named “R” and “result” as “Variant” data type. “Variant” is a data type that can store any type of data.\nColumns(\"A\").Delete\nThis line deletes the entire column A from the active worksheet.\nR = \"library(stats);rnorm(10) |&gt; as.data.frame()\"\nThis line assigns a string of R code to the variable “R”. The code will load the “stats” package and generate 10 random numbers from a normal distribution using the “rnorm()” function, and then convert the result to a data frame using the pipe operator “|&gt;” and the “as.data.frame()” function.\nresult = VBA.CreateObject(\"WScript.Shell\").Exec(\"C:\\Program Files\\R\\R-4.1.2\\bin\\x64\\Rscript.exe -e \"\"\" & R & \"\"\"\").StdOut.ReadAll\nThis line uses the “CreateObject” method to create a new object of the “WScript.Shell” class, which allows us to execute commands in the Windows command shell. It then uses the “Exec” method to execute the R code stored in the “R” variable using the “Rscript.exe” command-line tool, which runs R scripts from the command line. The result of the command is stored in the “result” variable by reading the output of the command using the “StdOut” property of the “Exec” object and the “ReadAll” method.\nresult = Split(result, vbCrLf)\nFor i = 0 To UBound(result)\n    ActiveSheet.Range(\"A1\").Offset(i, 0).Value = result(i)\nNext i\nThese two lines split the result of the R code execution into an array of strings using the “Split” function and the newline character (vbCrLf) as the delimiter. It then loops through the array using a “For” loop and assigns each element to a cell in the active worksheet, starting from cell A1 and offsetting each cell by one row using the “Offset” method.\nSo, in summary, this VBA code creates a subroutine that deletes column A from the active worksheet, executes a block of R code that generates 10 random numbers from a normal distribution and converts the result to a data frame, captures the output of the R code execution, splits the output into an array of strings, and pastes the result into column A of the active worksheet.\n\n\n\nVBA to R and Back Again"
  },
  {
    "objectID": "posts/rtip-2023-05-10/index.html",
    "href": "posts/rtip-2023-05-10/index.html",
    "title": "VBA to R and Back Again: Running R from VBA Pt 2",
    "section": "",
    "text": "Introduction\nYesterday I posted on using VBA to execute R code that is written inside of the VBA script. So today, I will go over a simple example on executing an R script from VBA. So let’s get into the code and what it does.\nFirst, let’s look at the Function called “Run_R_Script”. This function takes four arguments, where the first two are mandatory, and the last two are optional.\n\nsRApplicationPath - This is the path to the R application that you want to use to run your script. It is a required argument, and you need to provide the full path to the Rscript.exe file on your machine.\nsRFilePath - This is the path to the R script file that you want to execute. It is also a required argument, and you need to provide the full path to your R script file.\niStyle - This is an optional argument that specifies how the script will be executed. By default, it is set to 1, which means that the script will run in a minimized window.\nbWaitTillComplete - This is another optional argument that specifies whether the function should wait until the script has finished running before returning control to the caller. By default, it is set to True, which means that the function will not return control until the script has completed execution.\n\nThe first line inside the Function defines two variables: sPath and shell.\n\nsPath - This variable will hold the path to the Rscript.exe file and the path to the R script file, which will be used later to run the script.\nshell - This variable is used to create an instance of the WScript.Shell object.\n\nNext, we wrap the R path with double quotations to avoid any issues with spaces in the path.\nAfter that, the script deletes Column A.\nThen, instead of using the “shell.Run” function, the code uses the “shell.Exec” function to execute the R script. This function returns an object that has a “StdOut” property, which contains the output of the script.\nThe output is then read using the “ReadAll” method, and the resulting string is split into an array using the “Split” function. The array is then iterated using a “For” loop, and each element of the array is written to Column A, starting at cell A1.\nFinally, the Function returns an Integer value, which is the result of the “shell.Run” function.\nThe Subroutine called “Demo” just demonstrates how to use the “Run_R_Script” function by calling it with the appropriate parameters.\n\n\nFull Code\nHere is the R Script\n\ndata.frame(\n    x = 1:10,\n    y = rnorm(10)\n)\n\nlist(\n    data.frame(\n        x = 1:10,\n        y = rnorm(10)\n    ),\n    data.frame(\n        x = 1:10,\n        y = rnorm(10)\n    )\n)\n\nFull VBA\nFunction Run_R_Script(sRApplicationPath As String, _\n                        sRFilePath As String, _\n                        Optional iStyle As Integer = 1, _\n                        Optional bWaitTillComplete As Boolean = True) As Integer\n\n    Dim sPath As String\n    Dim shell As Object\n\n    'Define shell object\n    Set shell = VBA.CreateObject(\"WScript.Shell\")\n\n    'Wrap the R path with double quotations\n    sPath = \"\"\"\" & sRApplicationPath & \"\"\"\"\n    sPath = sPath & \" \"\n    sPath = sPath & sRFilePath\n\n    'Delete Coumn A\n    Columns(\"A\").Delete\n    \n    'Get Result\n    result = shell.Exec(sPath).StdOut.ReadAll\n    result = Split(result, vbCrLf)\n    For i = 0 To UBound(result)\n        ActiveSheet.Range(\"A1\").Offset(i, 0).Value = result(i)\n    Next i\n    \nEnd Function\n\nSub Demo()\n    Dim iEerrorCode As Integer\n    iEerrorCode = Run_R_Script(\"C:\\Program Files\\R\\R-4.1.2\\bin\\x64\\Rscript.exe\", \"C:\\Users\\ssanders\\Desktop\\test.R\")\nEnd Sub\n\n\nPicture\n\n\n\nExample Output, VBA, and R\n\n\n\n\nReference\nhttps://stackoverflow.com/a/54816881"
  },
  {
    "objectID": "posts/rtip-2023-05-12/index.html",
    "href": "posts/rtip-2023-05-12/index.html",
    "title": "Working with Dates and Times Pt 1",
    "section": "",
    "text": "Introduction\nIn this post, we will cover the basics of handling dates and times in R using the as.Date, as.POSIXct, and as.POSIXlt functions. We will use the example code below to explain each line in simple terms. Let’s get started!\nHere is the script we are going to look at:\n\n# the date object\nSteve_online &lt;- as.Date(\"1981-02-25\")\n\nstr(Steve_online) #Date[1:1], format: \"1981-02-25\n\n Date[1:1], format: \"1981-02-25\"\n\nclass(Steve_online) #Date\n\n[1] \"Date\"\n\nas.numeric(Steve_online) # stored as number of days since 1970-01-01\n\n[1] 4073\n\nas.numeric(as.Date(\"1970-01-01\")) # equals zero\n\n[1] 0\n\nas.Date(as.Date(\"1970-01-01\") + 4073) # produces 1981-02-25 -- our original date\n\n[1] \"1981-02-25\"\n\n# vectors can contain multiple dates\nSteve_online &lt;- as.Date(c(\"1981-02-25\", \"1997-01-12\"))\nstr(Steve_online)\n\n Date[1:2], format: \"1981-02-25\" \"1997-01-12\"\n\nSteve_online[2]\n\n[1] \"1997-01-12\"\n\n# what about POSIX?\n# POSIXct stores date time as integer == # seconds since 1970-01-01 UTC\nSteve_online &lt;- as.POSIXct(\"1981-02-25 02:25:00\", tz = \"US/Mountain\")\nas.integer(`Steve_online`)\n\n[1] 351941100\n\n# POSIXlt stores date time as list:sec, min, hour, mday, mon, year, wday, yday, isdst, zone, gmtoff\nSteve_online &lt;- as.POSIXlt(\"1981-02-25 02:25:00\", tz = \"US/Mountain\")\nas.integer(Steve_online) # no longer an integer\n\nWarning: NAs introduced by coercion\n\n\n [1]  0 25  2 25  1 81  3 55  0 NA NA\n\nunclass(Steve_online) # this shows the components of the list\n\n$sec\n[1] 0\n\n$min\n[1] 25\n\n$hour\n[1] 2\n\n$mday\n[1] 25\n\n$mon\n[1] 1\n\n$year\n[1] 81\n\n$wday\n[1] 3\n\n$yday\n[1] 55\n\n$isdst\n[1] 0\n\n$zone\n[1] \"MST\"\n\n$gmtoff\n[1] NA\n\nattr(,\"tzone\")\n[1] \"US/Mountain\"\n\nmonth.name[Steve_online$mon + 1] # equals February\n\n[1] \"February\"\n\n\nThe first line of code creates a date object called Steve_online with the value of February 25, 1981, using the as.Date function. This function is used to convert a character string to a date object. The str function is then used to show the structure of the Steve_online object, which is of class Date.\nThe as.numeric function is used to convert the Steve_online object to the number of days since January 1, 1970 (known as the Unix epoch). This is a common way of representing dates in programming languages, and is useful for calculations involving dates. We also demonstrate that as.numeric(as.Date(\"1970-01-01\")) returns zero, since this is the starting point of the Unix epoch.\nWe then show how to add or subtract days from a date object by adding or subtracting the desired number of days (as an integer) to the as.Date function with the reference date of January 1, 1970. In this case, we add 4073 days to January 1, 1970, resulting in the date of February 25, 1981 (our original date).\nNext, we demonstrate how to create a vector of date objects by passing a character vector of dates to the as.Date function. The str function is used again to show the structure of the Steve_online object, which is now a vector of two date objects. We then show how to access the second element of the vector using indexing (Steve_online[2]).\nMoving on to POSIX objects, we introduce the as.POSIXct function, which creates a POSIXct object that stores date time as an integer equal to the number of seconds since January 1,"
  },
  {
    "objectID": "posts/rtip-2023-05-15/index.html",
    "href": "posts/rtip-2023-05-15/index.html",
    "title": "Working with Dates and Times Pt 2: Finding the Next Mothers Day with Simplicity",
    "section": "",
    "text": "Introduction\nMother’s Day is a special occasion to honor and appreciate the incredible women in our lives. As programmers, we can use our coding skills to make our lives easier when it comes to important dates like Mother’s Day. In this blog post, we’ll walk through a simple and engaging R code that helps us find the next Mother’s Day. So grab your coding hats, and let’s get started!\n\n# if you aren't using times, use the Date class; it's simpler\nNextMothersDay &lt;- as.Date(\n  c(\n    startMothersDay = \"2024-05-14\", \n    endMothersDay =\"2024-05-14\"\n    )\n  )\n\nNextMothersDay\n\nstartMothersDay   endMothersDay \n   \"2024-05-14\"    \"2024-05-14\" \n\n\nIn the first part of our code, we use the as.Date() function to find the next Mother’s Day. Since we don’t need to consider specific times, we can simply use the Date class, which simplifies the process. We create a vector with two elements: startMothersDay and endMothersDay, both set to “2024-05-14”. This represents the range of Mother’s Day for the year 2024. Finally, we store the result in the variable NextMothersDay and print it to the console. Voilà! We have the next Mother’s Day date.\n\n# if you have times, then use POSIX.\nNextMothersDay_ct &lt;- as.POSIXct(\n  c(\n    startMothersDay = \"2024-05-15 10:00\", # Let Mommy Sleep!\n    endMothersDay =\"2024-05-15 23:59\"\n    ),\n  tz = \"GMT\"\n  )\n\nNextMothersDay_ct\n\n          startMothersDay             endMothersDay \n\"2024-05-15 10:00:00 GMT\" \"2024-05-15 23:59:00 GMT\" \n\n\nNow, let’s say we want to consider specific times for Mother’s Day celebrations. We can use the as.POSIXct() function to handle dates and times together. We create another vector with two elements: startMothersDay and endMothersDay, but this time with specific times. The start time is set to “2024-05-15 10:00” (because let’s let Mommy sleep in!) and the end time is set to “2024-05-15 23:59”. We also specify the time zone as “GMT” using the tz argument. The result is stored in the variable NextMothersDay_ct, and when we print it, we get the range of Mother’s Day with times included.\n\n# converting from one POSIX to another is easy\nNextMothersDay_lt &lt;- as.POSIXlt(NextMothersDay_ct)\nunclass(NextMothersDay_lt)\n\n$sec\n[1] 0 0\n\n$min\n[1]  0 59\n\n$hour\n[1] 10 23\n\n$mday\n[1] 15 15\n\n$mon\n[1] 4 4\n\n$year\nstartMothersDay   endMothersDay \n            124             124 \n\n$wday\n[1] 3 3\n\n$yday\n[1] 135 135\n\n$isdst\n[1] 0 0\n\nattr(,\"tzone\")\n[1] \"GMT\"\n\n\nNow, let’s explore how to convert a POSIXct object to a POSIXlt object. We use the as.POSIXlt() function to convert NextMothersDay_ct into a POSIXlt object. This conversion allows us to access more detailed components of the date and time, such as the day of the week, hour, minute, and second. Finally, we use the unclass() function to remove the class attributes from the object and print the result to the console.\n\n\nConclusion\nWith just a few lines of code, we have learned how to find the next Mother’s Day using R. Whether you need a simple date or a specific time range, R provides us with convenient functions to handle both scenarios. So the next time you want to plan a special surprise for your mom, you can rely on your coding skills to\n\n\nFull Script\n\n# if you aren't using times, use the Date class it's simpler\nNextMothersDay &lt;- as.Date(\n  c(\n    startMothersDay = \"2024-05-14\", \n    endMothersDay =\"2024-05-14\"\n    )\n  )\n\nNextMothersDay\n\n# if you have times, then use POSIX.\nNextMothersDay_ct &lt;- as.POSIXct(\n  c(\n    startMothersDay = \"2024-05-15 10:00\", # Let Mommy Sleep!\n    endMothersDay =\"2024-05-15 23:59\"\n    ),\n  tz = \"GMT\"\n  )\n\nNextMothersDay_ct\n\n# converting from one POSIX to another is easy\nNextMothersDay_lt &lt;- as.POSIXlt(NextMothersDay_ct)\nunclass(NextMothersDay_lt)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-16/index.html",
    "href": "posts/rtip-2023-05-16/index.html",
    "title": "Working with Dates and Times Pt 3",
    "section": "",
    "text": "Introduction\nDates and times are essential components in many programming tasks, and R provides various functions and packages to handle them effectively. In this post, we’ll explore some common operations using both the base R functions and the lubridate package, comparing their simplicity and ease of understanding.\nLet’s dive right in!\n\n# What class does as.Date() produce?\nclass(as.Date(\"1881/10/25\"))\n\n[1] \"Date\"\n\n# be sure lubridate \n# install.packages(\"lubridate\")\nlibrary(lubridate)\n\n# Which do you find easier to understand? base or lubridate?\ntoday() # today() = Sys.Date()\n\n[1] \"2023-05-16\"\n\nnow() # now() = Sys.time()\n\n[1] \"2023-05-16 08:27:52 EDT\"\n\n# as_date and as.Date produce the same class\nclass(as_date(\"1881/10/25\")) # lubridate\n\n[1] \"Date\"\n\nclass(as.Date(\"1881/10/25\")) # base\n\n[1] \"Date\"\n\n# simpler strptime\nstrptime(\"2014-07-13 16:00:00 -0300\", \"%Y-%m-%d %H:%M:%S %z\") # time zone is messed up\n\n[1] \"2014-07-13 15:00:00\"\n\nparse_date_time(\"2014-07-13 16:00:00 -0300\", \"ymd HMS z\") # time zone works\n\n[1] \"2014-07-13 19:00:00 UTC\"\n\n# lubridate takes it one step further\nymd(\"2014-07-13 16:00:00 -0300\")\n\n[1] NA\n\nymd_hms(\"2014-07-13 16:00:00 -0300\")\n\n[1] \"2014-07-13 19:00:00 UTC\"\n\nmdy_hm(\"July 13, 2014 4:00 pm\")\n\n[1] \"2014-07-13 16:00:00 UTC\"\n\n\n1️⃣ Determining the Class of a Date: The first line of code checks the class produced by the as.Date() function when given the input “1881/10/25.” By using the class() function, we can identify that the output is of class “Date.” This means that the as.Date() function converts the input into a date format.\n2️⃣ Base R vs. lubridate: Before we proceed further, we need to ensure that the lubridate package is installed. If not, the code installs it using the install.packages() function. We then load the package using the library() function.\nNext, we compare the ease of use between base R and lubridate for working with dates and times.\n\nToday’s Date and Current Time: The today() function, equivalent to Sys.Date(), gives you the current date. Similarly, now() returns the current date and time using Sys.time(). These functions make it straightforward to obtain the current date or date and time in R.\nClass Comparison: We compare the classes of dates produced by as_date() from lubridate and as.Date() from base R. Using the class() function on each result, we observe that both functions produce the same “Date” class output. Hence, both methods are equivalent in this regard.\n\n3️⃣ Simplifying Date and Time Parsing: Parsing date and time strings can sometimes be tricky, especially when dealing with time zones. However, lubridate provides simplified functions to handle such scenarios.\n\nBase R’s strptime(): The strptime() function is a base R function that parses a date and time string based on a given format. In this case, we try to parse “2014-07-13 16:00:00 -0300” with the format “%Y-%m-%d %H:%M:%S %z.” However, we encounter a problem with the time zone, as it does not parse correctly.\nlubridate’s parse_date_time(): To overcome the time zone issue, lubridate offers the parse_date_time() function. We provide the same date and time string along with the format “ymd HMS z.” This time, the time zone is parsed correctly, resulting in a valid date and time object.\n\n4️⃣ Going the Extra Mile with lubridate: lubridate takes date and time manipulation a step further with its intuitive functions.\n\nymd(): The ymd() function converts a character string of the form “2014-07-13 16:00:00 -0300” into a date object. It handles various date formats and automatically infers the year, month, and day information.\nymd_hms(): Similar to ymd(), the ymd_hms() function converts a character string into a date-time object, considering the year, month, day, hour, minute, and second components.\nmdy_hm(): The mdy_hm() function allows us to parse a character string like “July 13, 2014 4:00 pm” into\n\na date-time object. It handles different date formats and automatically extracts the month, day, year, hour, and minute information.\nBy leveraging these functions, lubridate simplifies the process of working with dates and times, offering a more intuitive and concise syntax compared to base R.\nIn conclusion, understanding how to handle dates and times in R is crucial for many programming tasks. While base R provides essential functions, the lubridate package offers additional capabilities and a more straightforward syntax, making it an attractive choice for working with dates and times in R."
  },
  {
    "objectID": "posts/rtip-2023-05-17/index.html",
    "href": "posts/rtip-2023-05-17/index.html",
    "title": "Working with Dates and Times Pt 4",
    "section": "",
    "text": "Introduction\nFormatting dates is an essential task in data analysis and programming. In R, there are various ways to manipulate and present dates according to specific requirements. In this blog post, we will explore the world of date formatting in R, uncovering the power of the strftime() function. We will walk through practical examples using the provided code snippet, demonstrating how to format dates in a clear and concise manner. So, let’s dive in and uncover the secrets of date formatting in R!\n#Understanding the strftime() Function:\nIn R, the strftime() function allows us to format dates and times based on a set of predefined modifiers. These modifiers act as placeholders for different components of the date and time. By using these modifiers, we can customize the output format to suit our needs.\nLet’s analyze the code snippet provided to gain a better understanding of the strftime() function and its capabilities.\n\n# all of the modifiers\nfor (formatter in sort(c(letters, LETTERS))) {\n  modifier &lt;- paste0(\"%\", formatter)\n  print(\n    paste0(\n      modifier, \n      \" used on: \",\n      RightNow,\n      \" will give: \",\n      strftime(RightNow, modifier)\n    )\n  )\n}\n\nThe code snippet above iterates through a set of modifiers, both lowercase and uppercase letters, and applies each modifier to the RightNow variable. It then prints the modifier, the original RightNow value, and the formatted output. This allows us to see the effect of each modifier on the date and time representation.\n\n\nModifier Showcase:\nLet’s explore some commonly used modifiers and their corresponding output formats:\n%a - Abbreviated weekday name (e.g., \"Mon\").\n%A - Full weekday name (e.g., \"Monday\").\n%b - Abbreviated month name (e.g., \"Jan\").\n%B - Full month name (e.g., \"January\").\n%d - Day of the month (01-31).\n%H - Hour in 24-hour format (00-23).\n%I - Hour in 12-hour format (01-12).\n%m - Month (01-12).\n%M - Minute (00-59).\n%p - AM/PM indicator.\n%S - Second (00-59).\n%Y - Year with century (e.g., \"2023\").\n%y - Year without century (e.g., \"23\").\nFeel free to experiment with different modifiers and observe the changes in the output format.\nHere is a full example\n\nRightNow &lt;- Sys.time()\n\n# all of the modifiers\nfor (formatter in sort(c(letters, LETTERS))) {\n  modifier &lt;- paste0(\"%\", formatter)\n  print(\n    paste0(\n      modifier, \n      \" used on: \",\n      RightNow,\n      \" will give: \",\n      strftime(RightNow, modifier)\n    )\n  )\n}\n\n[1] \"%a used on: 2023-05-17 09:34:47 will give: Wed\"\n[1] \"%A used on: 2023-05-17 09:34:47 will give: Wednesday\"\n[1] \"%b used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%B used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%c used on: 2023-05-17 09:34:47 will give: Wed May 17 09:34:47 2023\"\n[1] \"%C used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%d used on: 2023-05-17 09:34:47 will give: 17\"\n[1] \"%D used on: 2023-05-17 09:34:47 will give: 05/17/23\"\n[1] \"%e used on: 2023-05-17 09:34:47 will give: 17\"\n[1] \"%E used on: 2023-05-17 09:34:47 will give: E\"\n[1] \"%f used on: 2023-05-17 09:34:47 will give: f\"\n[1] \"%F used on: 2023-05-17 09:34:47 will give: 2023-05-17\"\n[1] \"%g used on: 2023-05-17 09:34:47 will give: 23\"\n[1] \"%G used on: 2023-05-17 09:34:47 will give: 2023\"\n[1] \"%h used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%H used on: 2023-05-17 09:34:47 will give: 09\"\n[1] \"%i used on: 2023-05-17 09:34:47 will give: i\"\n[1] \"%I used on: 2023-05-17 09:34:47 will give: 09\"\n[1] \"%j used on: 2023-05-17 09:34:47 will give: 137\"\n[1] \"%J used on: 2023-05-17 09:34:47 will give: J\"\n[1] \"%k used on: 2023-05-17 09:34:47 will give:  9\"\n[1] \"%K used on: 2023-05-17 09:34:47 will give: K\"\n[1] \"%l used on: 2023-05-17 09:34:47 will give:  9\"\n[1] \"%L used on: 2023-05-17 09:34:47 will give: L\"\n[1] \"%m used on: 2023-05-17 09:34:47 will give: 05\"\n[1] \"%M used on: 2023-05-17 09:34:47 will give: 34\"\n[1] \"%n used on: 2023-05-17 09:34:47 will give: \\n\"\n[1] \"%N used on: 2023-05-17 09:34:47 will give: N\"\n[1] \"%o used on: 2023-05-17 09:34:47 will give: o\"\n[1] \"%O used on: 2023-05-17 09:34:47 will give: O\"\n[1] \"%p used on: 2023-05-17 09:34:47 will give: AM\"\n[1] \"%P used on: 2023-05-17 09:34:47 will give: am\"\n[1] \"%q used on: 2023-05-17 09:34:47 will give: q\"\n[1] \"%Q used on: 2023-05-17 09:34:47 will give: Q\"\n[1] \"%r used on: 2023-05-17 09:34:47 will give: 09:34:47 AM\"\n[1] \"%R used on: 2023-05-17 09:34:47 will give: 09:34\"\n[1] \"%s used on: 2023-05-17 09:34:47 will give: 1684330487\"\n[1] \"%S used on: 2023-05-17 09:34:47 will give: 47\"\n[1] \"%t used on: 2023-05-17 09:34:47 will give: \\t\"\n[1] \"%T used on: 2023-05-17 09:34:47 will give: 09:34:47\"\n[1] \"%u used on: 2023-05-17 09:34:47 will give: 3\"\n[1] \"%U used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%v used on: 2023-05-17 09:34:47 will give: 17-May-2023\"\n[1] \"%V used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%w used on: 2023-05-17 09:34:47 will give: 3\"\n[1] \"%W used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%x used on: 2023-05-17 09:34:47 will give: 5/17/2023\"\n[1] \"%X used on: 2023-05-17 09:34:47 will give: 9:34:47 AM\"\n[1] \"%y used on: 2023-05-17 09:34:47 will give: 23\"\n[1] \"%Y used on: 2023-05-17 09:34:47 will give: 2023\"\n[1] \"%z used on: 2023-05-17 09:34:47 will give: -0400\"\n[1] \"%Z used on: 2023-05-17 09:34:47 will give: EDT\"\n\n\n\n\nConclusion\nIn this blog post, we explored the strftime() function in R, which provides powerful capabilities for formatting dates. By using the various modifiers available, we can easily customize the representation of dates and times to meet our specific requirements. Understanding date formatting is crucial for effective data analysis, visualization, and reporting.\nRemember to refer to the R documentation for strftime() to discover additional modifiers and advanced formatting options. With the knowledge gained from this blog post, you are now equipped to master date formatting in R and handle dates with confidence in your programming endeavors.\nHappy coding with R and may your dates always be formatted to perfection!"
  },
  {
    "objectID": "posts/rtip-2023-05-18/index.html",
    "href": "posts/rtip-2023-05-18/index.html",
    "title": "The which() Function in R",
    "section": "",
    "text": "Introduction:\nAs a programmer, one of the most important tasks is to extract valuable insights from data. To make this process efficient, it is crucial to have a reliable tool at your disposal. Enter the which() function in R. This versatile function allows you to locate specific elements within a vector or a data frame, helping you filter and analyze data with ease. In this blog post, we’ll explore the ins and outs of the which() function, discussing its syntax, common use cases, and providing practical examples to solidify your understanding.\n\n\nUnderstanding the Syntax:\nBefore diving into real-world examples, let’s grasp the basic syntax of the which() function. The general structure of the function is as follows:\n\nwhich(logical_vector, arr.ind = FALSE)\n\nThe logical_vector parameter represents the condition or logical expression you want to evaluate. It can be any expression that returns a logical vector, such as a comparison or logical operation. The optional arr.ind parameter, when set to TRUE, returns the result in array indices instead of a vector, that is to say that the which() function will return a vector of integers that correspond to the positions of the elements in the vector that satisfy the condition.\n\n\nExample 1: Locating Elements in a Numeric Vector\nSuppose we have a numeric vector called scores, representing test scores of students. We want to find the indices of scores greater than or equal to 90. Here’s how we can accomplish that using the which() function:\n\nscores &lt;- c(85, 92, 88, 94, 79, 91, 87, 98, 84, 90)\nindices &lt;- which(scores &gt;= 90)\nindices\n\n[1]  2  4  6  8 10\n\n\nIn this example, the which() function evaluates the logical expression scores &gt;= 90 and returns the indices of the elements satisfying the condition. The resulting indices vector will contain [2, 4, 6, 8, 10], indicating the positions of the scores that meet the criteria.\n\n\nExample 2: Filtering Data Frames\nData frames are widely used in data analysis. The which() function can be incredibly useful when working with data frames to filter rows based on specific conditions. Consider the following example:\n\ndata &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Charlie\", \"Dave\"),\n                   Age = c(25, 32, 28, 30),\n                   City = c(\"New York\", \"London\", \"Paris\", \"Sydney\"))\n\nselected_rows &lt;- which(data$Age &gt;= 30)\nfiltered_data &lt;- data[selected_rows, ]\nfiltered_data\n\n  Name Age   City\n2  Bob  32 London\n4 Dave  30 Sydney\n\n\nIn this case, we use the which() function to find the rows where the Age column is greater than or equal to 30. The selected_rows vector will hold the indices [2, 4], which we subsequently use to filter the original data frame. The resulting filtered_data will contain the rows corresponding to the selected indices, in this case, rows for Bob and Dave.\n\n\nExample 3: Using the arr.ind Parameter\nThe arr.ind parameter of the which() function comes in handy when working with multi-dimensional arrays. It allows you to obtain the indices as an array instead of a vector. Let’s illustrate this with an example:\n\nmatrix_data &lt;- matrix(1:12, nrow = 3, ncol = 4)\nselected_indices &lt;- which(matrix_data %% 3 == 0, arr.ind = TRUE)\nselected_indices\n\n     row col\n[1,]   3   1\n[2,]   3   2\n[3,]   3   3\n[4,]   3   4\n\n\nIn this example, we create a matrix called matrix_data and use the which() function to find the indices where the matrix elements are divisible by 3. By setting arr.ind = TRUE, we obtain a matrix of indices, where each row represents the position of an element satisfying the condition.\n\n\nConclusion:\nThe which() function in R proves to be an invaluable tool for data exploration and filtering. By allowing you to locate specific elements in vectors or data frames, it simplifies the process of extracting relevant information from your data. Throughout this blog post, we explored the syntax and various practical examples of using the which() function. Armed with this knowledge, you can now confidently apply the which() function to your own data analysis tasks in R, boosting your productivity and uncovering hidden insights with ease."
  },
  {
    "objectID": "posts/2023-05-19/index.html",
    "href": "posts/2023-05-19/index.html",
    "title": "Mastering File Manipulation with R’s list.files() Function",
    "section": "",
    "text": "Introduction\nWhen it comes to working with files in R, having a powerful tool at your disposal can make a world of difference. Enter the list.files() function, a versatile and handy utility that allows you to effortlessly navigate through directories, retrieve file names, and perform various file-related operations. In this blog post, we will delve into the intricacies of list.files() and explore real-world examples to help you harness its full potential.\n\nlist.files(\n  path, \n  all.files = FALSE, \n  full.names = FALSE, \n  recursive = FALSE, \n  pattern = NULL\n)\n\n\npath is a character vector specifying the directory to list. If no path is specified, the current working directory is used.\nall.files is a logical value specifying whether all files should be listed, including hidden files. The default value is FALSE, which only lists visible files.\nfull.names is a logical value specifying whether the full paths to the files should be returned. The default value is FALSE, which only returns the file names.\nrecursive is a logical value specifying whether subdirectories should be searched. The default value is FALSE, which only lists files in the specified directory.\npattern is a regular expression that can be used to filter the files that are listed. If no pattern is specified, all files are listed.\n\n\n\nUnderstanding the Basics\nBefore diving into the practical examples, let’s familiarize ourselves with the fundamental aspects of the list.files() function. In its simplest form, list.files() retrieves a character vector containing the names of files and directories within a specified directory. It takes in several optional arguments that provide flexibility and control over the file selection process.\n\n\nExample 1: Listing Files in a Directory\n\n# List all files in the current working directory\nfile_names &lt;- list.files()\nprint(file_names)\n\n[1] \"blank.txt\"       \"index.qmd\"       \"index.rmarkdown\"\n\n\nIn this example, the list.files() function is called without any arguments, resulting in the retrieval of all file names within the current working directory. The file_names variable will store the obtained character vector, which can then be printed or further processed.\n\n\nExample 2: Specifying a Directory\n\n# List all files in a specific directory\ndirectory &lt;- \"../rtip-2022-10-24/\"\nfile_names &lt;- list.files(path = directory)\nprint(file_names)\n\n[1] \"index.qmd\"\n\n\nHere, by setting the path argument to the desired directory, you can obtain the list of file names within that particular location. Remember to provide the appropriate path to the directory you wish to explore.\n\n\nExample 3: Selecting Files with a Pattern\n\n# List only files with a specific extension\npattern &lt;- \"\\\\.txt$\"\nfile_names &lt;- list.files(pattern = pattern)\nprint(file_names)\n\n[1] \"blank.txt\"\n\n\nIn this case, the pattern argument is used to filter the file names based on a regular expression. The example showcases the retrieval of only those files with a “.txt” extension. Customize the pattern as per your requirements, utilizing the power of regular expressions.\n\n\nExample 4: Recursive File Listing\n\n# List files recursively within a directory and its subdirectories\ndirectory &lt;- \"../rtip-2023-02-14/R/box/\"\nfile_names &lt;- list.files(path = directory, recursive = TRUE)\nprint(file_names)\n\n[1] \"global_options/global_options.R\" \"io/exports.R\"                   \n[3] \"io/imports.R\"                    \"mod/mod.R\"                      \n\n\nBy setting the recursive argument to TRUE, you can instruct list.files() to search for files not only in the specified directory but also in its subdirectories. This feature is particularly useful when dealing with nested file structures.\n\n\nExample 5: Excluding Directories\n\n# List only files and exclude directories\ndirectory &lt;- \"../rtip-2023-02-14/R/box/\"\nfile_names &lt;- list.files(path = directory, include.dirs = FALSE)\nprint(file_names)\n\n[1] \"global_options\" \"io\"             \"mod\"           \n\n\nIn scenarios where you only want to retrieve files and exclude directories, set the include.dirs argument to FALSE. This ensures that only the file names are included in the result, omitting any directory names.\nHere are some more examples:\n\n# List all files in the current working directory\nlist.files()\n\n# List all files in the current working directory, including hidden files\nlist.files(all.files = TRUE)\n\n# List all files in the current working directory with the .csv extension\nlist.files(pattern = \"\\\\.csv$\")\n\n# List all files in the /data directory\nlist.files(\"/data\")\n\n# List all files in the /data directory, including subdirectories\nlist.files(\"/data\", recursive = TRUE)\n\n\n\nConclusion\nThe list.files() function in R is an invaluable tool for file manipulation, enabling you to effortlessly retrieve file names, filter based on patterns, explore nested directories, and more. By mastering this function, you gain greater control over your file-handling tasks and can efficiently process and analyze data stored in files.\nRemember to consult R’s documentation for additional details on the various optional arguments and explore the wide range of possibilities offered by list.files(). With practice and experimentation, you’ll become a proficient file explorer in no time!\nHappy coding!"
  },
  {
    "objectID": "posts/2023-05-22/index.html",
    "href": "posts/2023-05-22/index.html",
    "title": "Update to {TidyDensity}",
    "section": "",
    "text": "Introduction\nTo effectively extract insights and communicate findings, you need powerful tools that simplify the process and present data in an engaging manner. If you’re a programmer with a penchant for data analysis, you’re in luck! The latest version of {TidyDensity}, the popular R package, has just been released, bringing you exciting new features and enhancements. In this blog post, we’ll explore the highlights of TidyDensity 1.2.5 and why you should download it today.\n\n\nNew Feature: Introducing util_burr_param_estimate()\nTidyDensity 1.2.5 introduces a new function: util_burr_param_estimate(). This function enables you to estimate parameters using the Burr distribution, expanding the possibilities of your data analysis. Whether you’re working with survival analysis, reliability modeling, or extreme value theory, util_burr_param_estimate() equips you with a powerful tool to tackle complex scenarios with ease. Say goodbye to manual calculations and embrace the simplicity and accuracy of TidyDensity.\n\n\nMinor Fixes and Improvements for Enhanced Workflow\nIn addition to the groundbreaking new feature, TidyDensity 1.2.5 addresses user feedback and provides several minor fixes and improvements. Let’s take a look at a couple of them:\n\nImproved Parameter Rounding: With the new version, you now have more control over the rounding of parameter estimates. The updated function tidy_distribution_comparison() includes a parameter called .round_to_place, allowing you to precisely control the rounding behavior of the parameter estimates passed to their corresponding distribution parameters. This enhancement ensures that your analysis remains accurate and aligned with your specific requirements.\n\n\n\nWhy Upgrade to TidyDensity 1.2.5?\n\nStay Ahead of the Curve: The world of data analysis is constantly evolving, and staying up to date with the latest tools and features is crucial to remain competitive. TidyDensity 1.2.5 empowers you with advanced capabilities, enabling you to analyze and visualize data more effectively than ever before.\nSimplify Complex Analysis: With the new util_burr_param_estimate() function, TidyDensity 1.2.5 simplifies complex data analysis tasks. Whether you’re a seasoned data scientist or a beginner, this feature allows you to explore a wider range of statistical distributions and unlock deeper insights from your data.\nFine-Tuned Precision: The improved parameter rounding in tidy_distribution_comparison() ensures that your analysis is not only powerful but also precise. This level of control over rounding provides you with the flexibility to align your analysis with your specific requirements.\n\n\n\nConclusion\nTidyDensity 1.2.5 is a significant update that brings you exciting new features and enhancements. From the introduction of util_burr_param_estimate() to the fine-tuned parameter rounding and polished visuals, this version is designed to empower you in your data analysis journey. By downloading TidyDensity1.2.5, you can stay at the forefront of data analysis, simplify complex tasks, and elevate the precision and user experience of your projects. Upgrade to TidyDensity 1.2.5 today!."
  },
  {
    "objectID": "posts/2023-05-23/index.html",
    "href": "posts/2023-05-23/index.html",
    "title": "What is the sink() function? Capturing Output to External Files",
    "section": "",
    "text": "Introduction\nThe sink() function in R is used to divert R output to an external connection. This can be useful for a variety of purposes, such as exporting data to a file, logging R output, or debugging R code.\nIn this blog post, we will explore the inner workings of the sink() function, understand its purpose, and provide practical examples using the popular datasets mtcars and iris.\nThe sink() function takes four arguments:\n\nfile: The name of the file to which R output will be diverted. If file is NULL, then R output will be diverted to the console.\nappend: A logical value indicating whether R output should be appended to the file (TRUE) or overwritten (FALSE). The default value is FALSE.\ntype: A character string. Either the output stream or the messages stream. The name will be partially match so can be abbreviated.\nsplit: logical: if TRUE, output will be sent to the new sink and the current output stream, like the Unix program tee.\n\n\n\nExamples\nHere are some examples of how to use the sink() function. To export the mtcars dataset to a file called “mtcars.csv”, you would use the following code:\n\nsink(\"mtcars.csv\")\nprint(mtcars)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nsink()\n\nTo log R output to a file called “r_output.log”, you would use the following code:\n\nsink(\"r_output.log\")\n# Your R code goes here\nsink()\n\nTo debug R code, you can use the sink() function to divert R output to a file. This can be helpful for tracking down errors in your code. For example, if you are trying to debug a function called my_function(), you could use the following code:\n\nsink(\"my_function.log\")\nmy_function()\nsink()\n\n\n\nCapturing Summary Statistics of mtcars Dataset\n\nsink(\"summary_output.txt\")  # Redirect output to the file\n\nsummary(mtcars)  # Generate summary statistics\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\nsink()  # Turn off redirection\n\nIn this example, the output of the summary(mtcars) command will be saved in the “summary_output.txt” file. We can later open the file to review the summary statistics of the mtcars dataset.\n\n\nSaving Regression Results of iris Dataset\n\nsink(\"regression_results.txt\")  # Redirect output to the file\n\nfit &lt;- lm(Sepal.Length ~ Sepal.Width, data = iris)  # Perform linear regression\n\nsummary(fit)  # Display regression summary\n\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5561 -0.6333 -0.1120  0.5579  2.2226 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.5262     0.4789   13.63   &lt;2e-16 ***\nSepal.Width  -0.2234     0.1551   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8251 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\nsink()  # Turn off redirection\n\nIn this example, the output of the summary(fit) command will be saved in the “regression_results.txt” file. By redirecting the output, we can analyze the regression results in detail without cluttering the console.\n\n\nAppending Output to a File\nBy default, calling sink() with a file name will overwrite any existing content in the file. However, if we want to append output to an existing file, we can pass the append = TRUE argument to sink().\n\nsink(\"output.txt\", append = TRUE)  # Append output to the existing file\n\ncat(\"Additional text\\n\")  # Append custom text\n\nAdditional text\n\nsink()  # Turn off redirection\n\nIn this example, the string “Additional text” will be appended to the “output.txt” file. This feature is useful when we want to continuously update a log file or add multiple output sections to a single file.\n\n\nConclusion\nThe sink() function is a handy tool in R that allows us to redirect output to external files. By using this function, we can save and review the output generated during data analysis, statistical modeling, or any other R programming tasks. In this blog post, we explored the basic usage of sink() and provided practical examples using the mtcars and iris datasets. By mastering sink(), you can efficiently manage your R output and ensure a more organized workflow."
  },
  {
    "objectID": "posts/2023-05-24/index.html",
    "href": "posts/2023-05-24/index.html",
    "title": "Exploring Data with TidyDensity: A Guide to Using tidy_empirical() and tidy_four_autoplot() in R",
    "section": "",
    "text": "Introduction\nYesterday I had the need to see data that had a grouping column in it. I wanted to use the tidy_four_autoplot() function on it from the {TidyDensity} library on it. This post will explain how I did it. The data in my session was called df_tbl. In this blog post, we will explore the steps involved in using the tidy_empirical() and tidy_four_autoplot() functions from the R library TidyDensity. These functions are incredibly useful when working with data, as they allow us to analyze and visualize empirical distributions efficiently. We will walk through a code snippet that demonstrates how to use these functions within a map() function, enabling us to analyze multiple subsets of data simultaneously.\n#Prerequisites\nTo follow along with this tutorial, it is assumed that you have a basic understanding of the R programming language, as well as familiarity with the dplyr, purrr, and TidyDensity libraries. Make sure you have these packages installed and loaded before proceeding.\nHere is the code that I used, the explanation will follow:\n\nlibrary(dplyr) # to use group_split()\nlibrary(purrr) # to use map()\nlibrary(TidyDensity) # to use tidy_empirical() and tidy_four_plot()\n\ndf_tbl |&gt;\n  group_split(SP_NAME) |&gt;\n  map(\\(run_time) pull(run_time) |&gt;\n        tidy_empirical() |&gt;\n        tidy_four_autoplot()\n      )\n\n\n\nCode Explanation\nLet’s break down the code step by step:\nImporting Required Libraries:\n\nTo access the necessary functions, we need to load the required libraries. In this case, we use library(dplyr) to utilize the group_split() function from the dplyr package, library(purrr) to use the map() function from the purrr package, and library(TidyDensity) to access the tidy_empirical() and tidy_four_autoplot() functions from the TidyDensity package.\n\nGrouping and Splitting the Data:\n\nThe first line of the code snippet takes a dataframe named df_tbl and uses the group_split() function from the dplyr library to split it into multiple subsets based on a variable called SP_NAME. This creates a list of dataframes, each representing a unique group based on SP_NAME.\n\nApplying Functions to Each Subset using map():\n\nThe second line of code utilizes the map() function from the purrr library to iterate over each subset of data created in the previous step. The map() function takes two arguments: the object to iterate over (in this case, the list of dataframes) and a function to apply to each element.\n\nAnonymous Function Inside map():\n\nWithin the map() function, an anonymous function (denoted by (run_time)) is defined. This function takes a single argument named run_time, representing each individual subset of data. The purpose of this anonymous function is to perform the necessary computations and visualizations on each subset of data.\n\nData Manipulation and Visualization:\n\nInside the anonymous function, the pull(run_time) function is used to extract the run_time column from each subset of data. This column is then passed to the tidy_empirical() function from the TidyDensity library, which calculates the empirical distribution of the data. The result is a tidy dataframe that contains information about the empirical distribution.\n\nTidy Four Autoplot:\n\nThe output of tidy_empirical() is then piped (|&gt;) into the tidy_four_autoplot() function from the TidyDensity library. This function generates a visualization called a “Tidy Four Plot,” which consists of four individual plots: empirical density, empirical cumulative density, QQ plot, and histogram.\n\nFinal Output:\n\nThe result of the tidy_four_autoplot() function is the final output of the anonymous function within map(). This output represents the visualization of the empirical distribution for each subset of data.\n\nHappy Coding!"
  },
  {
    "objectID": "posts/2023-05-25/index.html",
    "href": "posts/2023-05-25/index.html",
    "title": "Comparing R Packages for Writing Excel Files: An Analysis of writexl, openxlsx, and xlsx in R",
    "section": "",
    "text": "Introduction\nIn the realm of data analysis and manipulation, R has become a popular programming language due to its extensive collection of packages and libraries. One common task is exporting data to Excel files, which allows for easy sharing and presentation of results. In this blog post, we will explore three popular R packages for writing Excel files: writexl, openxlsx, and xlsx. We will compare their performance using the benchmarking package and analyze the results. So let’s dive in!\n\n\nSetting up the Environment\nBefore we proceed, make sure you have the necessary packages installed. We will be using the rbenchmark, nycflights13, and dplyr packages. The nycflights13 package provides a dataset named “flights,” which we will use for our benchmarking tests.\n\nlibrary(rbenchmark)\nlibrary(nycflights13)\nlibrary(dplyr)\n\n#Defining the Number of Replications\nTo ensure reliable performance measurements, we will repeat each test multiple times. The variable n represents the number of replications, and you can adjust its value depending on your requirements.\n\nn &lt;- 5\n\n\n\nBenchmarking the Packages\nNow, let’s move on to the actual benchmarking process. We will use the benchmark() function from the rbenchmark package to compare the performance of writexl, openxlsx, and xlsx.\n\nbenchmark(\n  \"writexl\" = {\n    writexl::write_xlsx(flights, tempfile())\n  },\n  \"openxlsx\" = {\n    openxlsx::write.xlsx(flights, tempfile())\n  },\n  \"xlsx\" = {\n    xlsx::write.xlsx(flights, paste0(tempfile(),\".xlsx\"))\n  },\n  replications = n,\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n)\n\nIn the code snippet above, we define three tests, each representing one package. We provide the code to execute for each test. For example, in the “writexl” test, we use the write_xlsx() function from the writexl package to write the “flights” dataset to a temporary Excel file.\nThe replications parameter specifies the number of times each test should be repeated. In our case, we set it to n, which we defined earlier as 5.\nThe columns parameter defines the columns to include in the benchmarking results. We specify “test” for the test name, “replications” for the number of replications, “elapsed” for the total time taken, “relative” for the relative performance compared to the fastest test, “user.self” for the CPU time used in user code, and “sys.self” for the CPU time used in system code.\n\n\nPrettifying the Results\nTo make the results more readable, we can use the arrange() function from the dplyr package to sort the results by the “relative” column in ascending order.\n\narrange(relative)\n\nThis will arrange the benchmarking results in ascending order of relative performance, allowing us to easily identify the most efficient package.\n\n\nBenchmark Output\n\ntest replications elapsed relative user.self sys.self\n1 writexl       5   0.034   1.000000   0.024   0.010\n2 openxlsx       5   0.055   1.617647   0.044   0.011\n3 xlsx          5   0.101   2.941176   0.078   0.023\n\n\n\nInterpretation of the Results\nThe results of the benchmark show that writexl is the fastest package for writing to Excel, followed by openxlsx and xlsx. The difference in performance between the three packages is not significant, but writexl is consistently faster than the other two packages.\n\n\nConclusion\nIn this blog post, we compared the performance of three R packages, writexl, openxlsx, and xlsx, for writing Excel files. We used the rbenchmark package to benchmark the packages, considering the number of replications, elapsed time, relative performance, user CPU time, and system CPU time. By arranging the results using the dplyr package, we obtained a sorted view of the relative performance. This analysis can help you choose the most suitable package for your specific needs, considering both performance and functionality.\nRemember, benchmarking can vary depending on the dataset and system specifications. So, it’s always a good idea to run your own benchmarks and evaluate the results in your specific context. Happy coding!"
  },
  {
    "objectID": "posts/2023-05-26/index.html",
    "href": "posts/2023-05-26/index.html",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "",
    "text": "When working with data, it is important to be aware of the file size of the data you are working with. This is especially true when you are working with large datasets, as the file size can have a significant impact on the performance of your code.\nIn R, there are a number of different ways to write data to files. Each method has its own advantages and disadvantages, and the file size of the output can vary depending on the method you use.\nIn this blog post, we will discuss why it is a good idea to check the file size output for different methods. We will also provide three examples of how to check the file size output using the R libraries writexl, openxlsx, and xlsx."
  },
  {
    "objectID": "posts/2023-05-26/index.html#writexl",
    "href": "posts/2023-05-26/index.html#writexl",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "writexl",
    "text": "writexl\nTo check the file size output of the writexl::write_xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the\n\nlibrary(writexl)\n\nwrite_xlsx(iris, tmp1 &lt;- tempfile())\n\nfile.info(tmp1)$size\n\n[1] 8497"
  },
  {
    "objectID": "posts/2023-05-26/index.html#openxlsx",
    "href": "posts/2023-05-26/index.html#openxlsx",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "openxlsx",
    "text": "openxlsx\nTo check the file size output of the openxlsx::write.xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the output:\n\nlibrary(openxlsx)\n\nwrite.xlsx(iris, tmp2 &lt;- tempfile())\n\nfile.info(tmp2)$size\n\n[1] 9631"
  },
  {
    "objectID": "posts/2023-05-26/index.html#xlsx",
    "href": "posts/2023-05-26/index.html#xlsx",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "xlsx",
    "text": "xlsx\nTo check the file size output of the xlsx::write.xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the output:\n\nlibrary(xlsx)\n\nwrite.xlsx(iris, tmp3 &lt;- paste0(tempfile(), \".xlsx\"))\n\nfile.info(tmp3)$size\n\n[1] 7905"
  },
  {
    "objectID": "posts/2023-05-30/index.html",
    "href": "posts/2023-05-30/index.html",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "",
    "text": "Programming is often about making decisions based on certain conditions. In the world of R, there are numerous functions that can help us simplify our code and make it more efficient. One such function is any(). In this blog post, we’ll explore the any() function and learn how it can be used to streamline our logical operations. Whether you’re a beginner or an experienced programmer, this post aims to make the concept accessible to everyone. So, let’s dive in!"
  },
  {
    "objectID": "posts/2023-05-30/index.html#basic-examples",
    "href": "posts/2023-05-30/index.html#basic-examples",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Basic Examples",
    "text": "Basic Examples\nNow, let’s see some basic examples on how to use the any() function.\n\nx &lt;- c(1, 2, 3, 4, 5)\n\nany(x &gt; 10)\n\n[1] FALSE\n\n\n\nx &lt;- c(1, 2, NA, 4, 5)\n\nany(x &gt; 10)\n\n[1] NA\n\nany(x == 5)\n\n[1] TRUE\n\n\nNow, let’s explore some examples to see how any() can be utilized in various scenarios:"
  },
  {
    "objectID": "posts/2023-05-30/index.html#checking-for-the-presence-of-a-specific-value",
    "href": "posts/2023-05-30/index.html#checking-for-the-presence-of-a-specific-value",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Checking for the Presence of a Specific Value:",
    "text": "Checking for the Presence of a Specific Value:\nSuppose we have a vector of numbers, and we want to check if any of them are divisible by 5. We can use the any() function to accomplish this as follows:\n\nnumbers &lt;- c(2, 7, 12, 15, 21)\nis_divisible_by_5 &lt;- any(numbers %% 5 == 0)\n\nif (is_divisible_by_5) {\n  print(\"At least one number is divisible by 5.\")\n} else {\n  print(\"None of the numbers are divisible by 5.\")\n}\n\n[1] \"At least one number is divisible by 5.\"\n\n\nIn this example, we use the modulus operator (%%) to check if each number in the vector has a remainder of 0 when divided by 5. The any() function then returns TRUE if any such element is found, indicating the presence of at least one number divisible by 5."
  },
  {
    "objectID": "posts/2023-05-30/index.html#validating-user-input",
    "href": "posts/2023-05-30/index.html#validating-user-input",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Validating User Input:",
    "text": "Validating User Input:\nLet’s say we are building a program that requires the user to input a positive number. We can use the any() function to validate the input as shown below:\n\nuser_input &lt;- as.numeric(readline(prompt = \"Enter a positive number: \"))\n\nEnter a positive number: \n\n# Dummy input\nuser_input &lt;- 5\nis_positive &lt;- any(user_input &gt; 0)\n\nif (is_positive) {\n  print(\"Input is a positive number.\")\n} else {\n  print(\"Input is not a positive number.\")\n}\n\n[1] \"Input is a positive number.\"\n\n\nHere, we convert the user input to a numeric value using as.numeric() and then check if it is greater than zero. The any() function returns TRUE if any element satisfies this condition, confirming that the input is indeed a positive number."
  },
  {
    "objectID": "posts/2023-05-31/index.html",
    "href": "posts/2023-05-31/index.html",
    "title": "Demystifying Regular Expressions: A Programmer’s Guide for Beginners",
    "section": "",
    "text": "Introduction\nRegular expressions, often abbreviated as regex, are powerful tools used in programming to match and manipulate text patterns. While they might seem intimidating at first, regular expressions are incredibly useful for tasks like data validation, text parsing, and pattern matching. In this blog post, we’ll explore regular expressions in the context of R programming, breaking down the concepts step by step and providing practical examples along the way. By the end, you’ll have a solid understanding of regular expressions and be ready to apply them to your own projects.\n\n\nWhat are Regular Expressions?\nAt its core, a regular expression is a sequence of characters that define a search pattern. It allows you to search, extract, and manipulate text based on specific patterns of characters. Regular expressions are supported in many programming languages, including R, and they provide a concise and flexible way to work with text.\n\n\nHow do regular expressions work?\nRegular expressions work by matching patterns of characters in text. The basic syntax of a regular expression is a sequence of characters enclosed in delimiters, such as slashes (/). The characters in the regular expression can be literal characters, special characters, or character classes.\nLiteral characters are characters that match themselves. For example, the regular expression /a/ matches the letter a.\nSpecial characters are characters that have special meaning in regular expressions. For example, the special character . matches any character.\nCharacter classes are a way to specify a set of characters. For example, the character class [a-z] matches any lowercase letter.\n\n\nHow to use regular expressions in R\nRegular expressions can be used in R to search for, extract, and replace text. To use regular expressions in R, you can use the grep(), grepl(), sub(), and gsub() functions.\nThe grep() function is used to search for text that matches a regular expression. The grepl() function is similar to grep(), but it returns a logical vector indicating whether each element of a vector matches the regular expression. The sub() function is used to replace text that matches a regular expression. The gsub() function is similar to sub(), but it replaces all occurrences of the text that matches the regular expression.\n\n\nBasic Characters\n\n. | Matches any single character except a newline character.\n[] | Matches any character within the brackets. For example, [a-z] matches any lowercase letter.\n* | Matches zero or more occurrences of the preceding character. For example, a* matches any number of a characters, including zero.\n+ | Matches one or more occurrences of the preceding character. For example, a+ matches one or more a characters.\n? | Matches zero or one occurrences of the preceding character. For example, a? matches either one or zero a characters.\n^ | Matches the beginning of the string.\n$ | Matches the end of the string.\n\n\n\nSpecial Characters\nThe following are the special characters used in regular expressions:\n\n\\d | Matches a digit.\n\\s | Matches a whitespace character.\n\\w | Matches a word character (alphanumeric character or underscore).\n\\W | Matches a non-word character.\n\\n | Matches a newline character.\n\\r | Matches a carriage return character.\n\\t | Matches a tab character.\n\n\n\nExamples of regular expressions in R\nHere are some examples of regular expressions in R:\n\nTo search for all occurrences of the word “hello” in a string, you would use the following code:\n\n\ngrep(\"hello\", \"This is a string that contains the word 'hello'\")\n\n[1] 1\n\n\n\nTo extract all of the email addresses from a string, you would use the following code:\n\ngrepl(\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}\"), “This is a string that contains some email addresses”)\n\nTo replace all of the spaces in a string with underscores, you would use the following code:\n\n\nsub(\" \", \"_\", \"This is a string with some spaces\")\n\n[1] \"This_is a string with some spaces\"\n\n\n\nTo replace all of the occurrences of the word “hello” with the word “goodbye” in a string, you would use the following code:\n\n\ngsub(\"hello\", \"goodbye\", \"This is a string that contains the word 'hello'\")\n\n[1] \"This is a string that contains the word 'goodbye'\"\n\n\n\n\nMatching a Simple Pattern\nLet’s start with a simple example in R. Suppose we have a character vector called fruits that contains various fruit names:\n\nfruits &lt;- c(\"apple\", \"banana\", \"orange\", \"kiwi\", \"mango\")\n\nWe can use a regular expression to find all the fruits that start with the letter “a”. In R, the grep() function allows us to perform pattern matching. Here’s how we can achieve this:\n\npattern &lt;- \"^a\"  # ^ denotes the start of the line\nmatching_fruits &lt;- grep(pattern, fruits, value = TRUE)\nprint(matching_fruits)\n\n[1] \"apple\"\n\n\nThe output will be “apple”.\nIn this example, the pattern “^a” specifies that we want to match any fruit that starts with the letter “a”. The grep() function returns the matching fruit names, and we set value = TRUE to obtain the matched values instead of their indices.\n\n\nExtracting Digits from a String\nRegular expressions can be used to extract specific information from a string. Suppose we have a character vector called sentences containing sentences with numbers:\n\nsentences &lt;- c(\"I have 10 apples.\", \"The recipe calls for 2 cups of sugar.\", \"You are the 3rd winner.\")\n\nTo extract the digits from each sentence, we can use the gsub() function, which replaces specific patterns within a string:\n\npattern &lt;- \"\\\\D\"  # \\\\D matches any non-digit character\ndigits &lt;- gsub(pattern, \"\", sentences)\nprint(digits)\n\n[1] \"10\" \"2\"  \"3\" \n\n\nThe output will be “10” “2” “3”\nIn this example, the pattern “\\D” matches any non-digit character. By replacing these characters with an empty string, we effectively extract the digits from each sentence.\n\n\nConclusion\nRegular expressions are an invaluable tool for working with text patterns in programming. While they may seem daunting at first, breaking down the concepts and understanding their building blocks can help demystify them. In this blog post, we explored the basics of regular expressions in R, showcasing practical examples along the way. Armed with this knowledge, you can now confidently incorporate regular expressions into your programming projects, allowing you to manipulate and extract information from text efficiently.\nRemember, practice makes perfect when it comes to regular expressions. Experiment with different patterns, explore the rich set of metacharacters and operators available, and refer to the R documentation for more in-depth information. Regular expressions open up a whole new world of possibilities in text manipulation, so embrace their power and have fun exploring the endless patterns you can match!"
  },
  {
    "objectID": "posts/2023-06-01/index.html",
    "href": "posts/2023-06-01/index.html",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "",
    "text": "As a programmer, you’re always on the lookout for tools that can enhance your productivity and make your code more efficient. In the world of R programming, the do.call() function is one such gem. This often-overlooked function is a powerful tool that allows you to dynamically call other functions, opening up a world of possibilities for code organization, reusability, and flexibility. In this blog post, we will demystify the do.call() function in simple terms and provide you with practical examples that showcase its versatility."
  },
  {
    "objectID": "posts/2023-06-01/index.html#example-1-combining-multiple-vectors-with-rbind",
    "href": "posts/2023-06-01/index.html#example-1-combining-multiple-vectors-with-rbind",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "Example 1: Combining Multiple Vectors with rbind()",
    "text": "Example 1: Combining Multiple Vectors with rbind()\nLet’s say you have a list of vectors, and you want to combine them into a single matrix using the rbind() function. Instead of manually specifying the vectors one by one, you can leverage do.call() to dynamically generate the function call:\n\nvectors &lt;- list(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9))\ncombined_matrix &lt;- do.call(rbind, vectors)\ncombined_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nIn this example, do.call() dynamically constructs the function call rbind(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9)), resulting in a matrix that combines the vectors."
  },
  {
    "objectID": "posts/2023-06-01/index.html#example-2-applying-a-function-to-multiple-data-frames-with-lapply",
    "href": "posts/2023-06-01/index.html#example-2-applying-a-function-to-multiple-data-frames-with-lapply",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "Example 2: Applying a Function to Multiple Data Frames with lapply()",
    "text": "Example 2: Applying a Function to Multiple Data Frames with lapply()\nSuppose you have a list of data frames, and you want to apply a specific function to each of them, such as summarizing the mean of a column. Instead of writing repetitive code, you can use do.call() to apply the desired function dynamically:\n\ndata_frames &lt;- list(\n  data.frame(a = 1:3), \n  data.frame(a = 4:6), \n  data.frame(a = 7:9)\n  )\nmean_results &lt;- do.call(\n  rbind, \n  lapply(data_frames, function(df) mean(df$a))\n  )\nmean_results\n\n     [,1]\n[1,]    2\n[2,]    5\n[3,]    8\n\n\nIn this example, do.call() combines the results of applying the mean function to each data frame’s ‘a’ column into a single matrix."
  },
  {
    "objectID": "posts/2023-06-02/index.html",
    "href": "posts/2023-06-02/index.html",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "",
    "text": "In the realm of data analysis and programming, organizing and sorting data efficiently is crucial. In R, a programming language renowned for its data manipulation capabilities, we have three powerful functions at our disposal: order(), sort(), and rank(). In this blog post, we will delve into the intricacies of these functions, explore their applications, and understand their parameters. These R functions are all used to sort data, however, they each have different purposes and use different methods to sort the data."
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-order",
    "href": "posts/2023-06-02/index.html#parameters-of-order",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of order():",
    "text": "Parameters of order():\n\n... - Specify the vectors to be sorted."
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-sort",
    "href": "posts/2023-06-02/index.html#parameters-of-sort",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of sort():",
    "text": "Parameters of sort():\n\nx - The vector or matrix to be sorted.\ndecreasing - A logical value indicating whether the sorting should be in descending order. (Default is FALSE)"
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-rank",
    "href": "posts/2023-06-02/index.html#parameters-of-rank",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of rank():",
    "text": "Parameters of rank():\n\nx - The vector to be ranked.\nties.method - A string specifying the method to handle ties in ranking. (Options: “average”, “first”, “last”, “random”, “max”, “min”) (Default is “average”)"
  },
  {
    "objectID": "posts/2023-06-06/index.html",
    "href": "posts/2023-06-06/index.html",
    "title": "Simplifying Data Transformation with pivot_longer() in R’s tidyr Library",
    "section": "",
    "text": "Introduction\nIn the world of data analysis and manipulation, tidying and reshaping data is often an essential step. R’s tidyr library provides powerful tools to efficiently transform and reshape data. One such function is pivot_longer(). In this blog post, we’ll explore how pivot_longer() works and demonstrate its usage through several examples. By the end, you’ll have a solid understanding of how to use this function to make your data more manageable and insightful.\nThe tidyr library holds the function, so we are going to have to load it first.\n\nlibrary(tidyr)\n\n\n\nUnderstanding pivot_longer()\nThe pivot_longer() function is designed to reshape data from a wider format to a longer format. It takes columns that represent different variables and consolidates them into key-value pairs, making it easier to analyze and visualize the data.\nSyntax: The basic syntax of pivot_longer() is as follows:\npivot_longer(data, cols, names_to, values_to)\n\ndata: The data frame or tibble to be reshaped.\ncols: The columns to be transformed.\nnames_to: The name of the new column that will hold the variable names.\nvalues_to: The name of the new column that will hold the corresponding values.\n\n\n\nExample 1: Reshaping Wide Data to Long Data\nLet’s start with a simple example to demonstrate the usage of pivot_longer(). Suppose we have a data frame called students with columns representing subjects and their respective scores:\n\nstudents &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  math = c(90, 85, 92),\n  science = c(95, 88, 91),\n  history = c(87, 92, 78)\n)\n\nTo reshape this data from a wider format to a longer format, we can use pivot_longer() as follows:\n\nstudents_long &lt;- pivot_longer(\n  students, \n  cols = -name, \n  names_to = \"subject\", \n  values_to = \"score\"\n  )\n\nstudents_long\n\n# A tibble: 9 × 3\n  name    subject score\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 Alice   math       90\n2 Alice   science    95\n3 Alice   history    87\n4 Bob     math       85\n5 Bob     science    88\n6 Bob     history    92\n7 Charlie math       92\n8 Charlie science    91\n9 Charlie history    78\n\n\nThe resulting students_long data frame will have three columns: name, subject, and score, where each row represents a student’s score in a specific subject.\nExample 2: Handling Multiple Variables In many cases, data frames contain multiple variables that need to be pivoted simultaneously. Consider a data frame called sales with columns representing sales figures for different products in different regions:\n\nsales &lt;- data.frame(\n  region = c(\"North\", \"South\", \"East\"),\n  product_A = c(100, 120, 150),\n  product_B = c(80, 90, 110),\n  product_C = c(60, 70, 80)\n)\n\nTo reshape this data, we can specify multiple columns to pivot using pivot_longer():\n\nsales_long &lt;- pivot_longer(\n  sales, \n  cols = starts_with(\"product\"), \n  names_to = \"product\", \n  values_to = \"sales\"\n  )\n\nsales_long\n\n# A tibble: 9 × 3\n  region product   sales\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 North  product_A   100\n2 North  product_B    80\n3 North  product_C    60\n4 South  product_A   120\n5 South  product_B    90\n6 South  product_C    70\n7 East   product_A   150\n8 East   product_B   110\n9 East   product_C    80\n\n\nThe resulting sales_long data frame will have three columns: region, product, and sales, where each row represents the sales figure of a specific product in a particular region.\n\n\nExample 3: Handling Irregular Data\nSometimes, data frames contain irregular structures, such as missing values or uneven numbers of columns. pivot_longer() can handle such scenarios gracefully. Consider a data frame called measurements with columns representing different measurement types and their respective values:\n\nmeasurements &lt;- data.frame(\n  timestamp = c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"),\n  temperature = c(25.3, 27.1, 24.8),\n  humidity = c(65.2, NA, 68.5),\n  pressure = c(1013, 1012, NA)\n)\n\nTo reshape this data, we can use pivot_longer() and handle the missing values:\n\nmeasurements_long &lt;- pivot_longer(\n  measurements, \n  cols = -timestamp, \n  names_to = \"measurement\", \n  values_to = \"value\", \n  values_drop_na = TRUE\n  )\n\nmeasurements_long\n\n# A tibble: 7 × 3\n  timestamp  measurement  value\n  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n1 2022-01-01 temperature   25.3\n2 2022-01-01 humidity      65.2\n3 2022-01-01 pressure    1013  \n4 2022-01-02 temperature   27.1\n5 2022-01-02 pressure    1012  \n6 2022-01-03 temperature   24.8\n7 2022-01-03 humidity      68.5\n\n\nThe resulting measurements_long data frame will have three columns: timestamp, measurement, and value, where each row represents a specific measurement at a particular timestamp. The values_drop_na argument ensures that rows with missing values are dropped.\n\n\nConclusion\nIn this blog post, we explored the pivot_longer() function from the tidyr library, which allows us to reshape data from a wider format to a longer format. We covered the syntax and provided several examples to illustrate its usage. By mastering pivot_longer(), you’ll be equipped to tidy your data and unleash its true potential for analysis and visualization."
  },
  {
    "objectID": "posts/2023-06-08/index.html",
    "href": "posts/2023-06-08/index.html",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "",
    "text": "In R, the file.info() function is a useful tool for retrieving file information, such as file attributes and metadata. It allows programmers to gather details about files, including their size, permissions, and timestamps. In this post, we will explore the file.info() function and demonstrate how it can be used to list files by date.\n\n\nThe file.info() function returns a data frame with file information as its columns. Each row corresponds to a file, and the columns contain attributes such as the file size, permissions, and timestamps. This function accepts one or more file paths as its argument, providing flexibility in examining multiple files simultaneously. The following columns are returned in the data.frame that results from file.info():\n\nname: The name of the file.\nsize: The size of the file in bytes.\nmode: The mode of the file, which can be used to determine the file’s permissions.\nmtime: The modification time of the file.\nctime: The creation time of the file.\natime: The last access time of the file.\n\nIn order to get some data to work with, we will save the iris dataset as an excel file four times in a for loop, waiting 10 seconds between each save.\nlibrary(writexl)\n\n# Generate file names\nfile_prefix &lt;- \"iris\"\nfile_extension &lt;- \".xlsx\"\nnum_files &lt;- 4\n\n# Save iris dataset as Excel files\nfor (i in 1:num_files) {\n  file_name &lt;- paste0(file_prefix, \"_\", i, file_extension)\n  write_xlsx(iris, file_name)\n  cat(\"File\", file_name, \"saved successfully.\\n\")\n  Sys.sleep(10) # Sleep for 10 seconds then go again\n}"
  },
  {
    "objectID": "posts/2023-06-08/index.html#example-1-retrieving-file-information",
    "href": "posts/2023-06-08/index.html#example-1-retrieving-file-information",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "Example 1: Retrieving File Information",
    "text": "Example 1: Retrieving File Information\nLet’s begin by retrieving information about a single file. we have a file named “iris_1.xlsx” located in our working directory. We can use the file.info() function to obtain its attributes:\n\nfile_info &lt;- file.info(\"iris_1.xlsx\")\nprint(file_info)\n\n            size isdir mode               mtime               ctime\niris_1.xlsx 8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\n                          atime exe\niris_1.xlsx 2023-06-08 07:58:19  no\n\n\nThe output will display a data frame with the attributes of the “iris_1.xlsx” file, including the file size, permissions, and timestamps. This information can be valuable for tasks such as file management and quality control."
  },
  {
    "objectID": "posts/2023-06-08/index.html#example-2-listing-files-by-date",
    "href": "posts/2023-06-08/index.html#example-2-listing-files-by-date",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "Example 2: Listing Files by Date",
    "text": "Example 2: Listing Files by Date\nNow, let’s dive into listing files based on their dates. To achieve this, we will combine the file.info() function with other functions to extract and manipulate the timestamp information.\n\n# Obtain file information for all files in a directory\nfiles &lt;- list.files(full.names = TRUE, pattern = \"*.xlsx$\")\nfile_info &lt;- file.info(files)\nfile_info$file_name &lt;- rownames(file_info)\n\n# Sort files by modification date in ascending order\nsorted_files &lt;- files[order(file_info$mtime)]\n\n# Display the sorted file list\nprint(sorted_files)\n\n[1] \"./iris_1.xlsx\" \"./iris_2.xlsx\" \"./iris_3.xlsx\" \"./iris_4.xlsx\"\n\nfile_info[order(file_info$mtime), ]\n\n              size isdir mode               mtime               ctime\n./iris_1.xlsx 8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\n./iris_2.xlsx 8497 FALSE  666 2023-06-08 07:34:41 2023-06-08 07:34:41\n./iris_3.xlsx 8497 FALSE  666 2023-06-08 07:34:52 2023-06-08 07:34:52\n./iris_4.xlsx 8497 FALSE  666 2023-06-08 07:35:05 2023-06-08 07:35:05\n                            atime exe     file_name\n./iris_1.xlsx 2023-06-08 07:59:30  no ./iris_1.xlsx\n./iris_2.xlsx 2023-06-08 07:58:19  no ./iris_2.xlsx\n./iris_3.xlsx 2023-06-08 07:58:19  no ./iris_3.xlsx\n./iris_4.xlsx 2023-06-08 07:58:19  no ./iris_4.xlsx\n\n\nIn this example, we first specify the directory path where our target files are located. By using list.files(), we obtain a vector of file names within that directory. Setting full.names = TRUE ensures that the file paths include the directory path. We also used the pattern parameter to ensure that we only grab the Excel files.\nNext, we use file.info() on the vector of file names to retrieve the file information for all files in the directory. The resulting data frame, file_info, contains details about each file, including the modification timestamp (mtime).\nTo list the files by date, we sort the file names vector based on the modification timestamp, using order(file_info$mtime). The resulting sorted_files vector contains the file paths sorted in ascending order based on the modification date.\nFinally, we print the sorted file list to the console, providing an easy way to visualize the files listed by their modification date.\nLet’s go over some more examples. How about you want to see the files that were created in the last 24 hours, well, you could then do the following:\n\nfiles &lt;- file.info(list.files(), full.names = TRUE)\nfiles &lt;- files[files$mtime &gt;= Sys.time() - 24 * 60 * 60, ]\nprint(files)\n\n                size isdir mode               mtime               ctime\nindex.qmd       5161 FALSE  666 2023-06-08 07:59:28 2023-06-07 08:14:49\nindex.rmarkdown 5281 FALSE  666 2023-06-08 07:59:30 2023-06-08 07:59:30\niris_1.xlsx     8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\niris_2.xlsx     8497 FALSE  666 2023-06-08 07:34:41 2023-06-08 07:34:41\niris_3.xlsx     8497 FALSE  666 2023-06-08 07:34:52 2023-06-08 07:34:52\niris_4.xlsx     8497 FALSE  666 2023-06-08 07:35:05 2023-06-08 07:35:05\nNA                NA    NA &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n                              atime  exe\nindex.qmd       2023-06-08 07:59:29   no\nindex.rmarkdown 2023-06-08 07:59:30   no\niris_1.xlsx     2023-06-08 07:59:30   no\niris_2.xlsx     2023-06-08 07:59:30   no\niris_3.xlsx     2023-06-08 07:59:30   no\niris_4.xlsx     2023-06-08 07:59:30   no\nNA                             &lt;NA&gt; &lt;NA&gt;\n\n\nThe file.infor() function can also be used to filter files by other criteria such as size. Lets say we want to find all files that are larger than 100MB, well we could do the following:\n\nfiles &lt;- file.info(list.files(), full.name = TRUE)\nfiles &lt;- files[files$size &gt; 100 * 1024^2, ]\nprint(files)\n\n   size isdir mode mtime ctime atime  exe\nNA   NA    NA &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n\n\nWe can see that we had no files greater than 100MB in the current directory."
  },
  {
    "objectID": "posts/2023-06-13/index.html",
    "href": "posts/2023-06-13/index.html",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "",
    "text": "As a programmer, you may come across various scenarios where you need to create complex model formulas in R. However, constructing these formulas can often be challenging and time-consuming. This is where the ‘reformulate()’ function comes to the rescue! In this blog post, we will explore the purpose and usage of the reformulate() function in R, and provide you with simple examples to help you grasp its power."
  },
  {
    "objectID": "posts/2023-06-13/index.html#example-1-linear-regression",
    "href": "posts/2023-06-13/index.html#example-1-linear-regression",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "Example 1: Linear Regression",
    "text": "Example 1: Linear Regression\nLet’s say we want to use the mtcars dataset containing information about cars, including their hp and number of cylinders. We want to perform a linear regression to predict the mpg of the car based upon hp and cyl. Here’s how we can use ‘reformulate()’ for this purpose:\n\nlibrary(stats)\n\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"hp\", \"cyl\"), response = \"mpg\")\n\n# Fitting a linear regression model\nmodel &lt;- lm(formula, data = mtcars)\n\nformula\n\nmpg ~ hp + cyl\n\nmodel\n\n\nCall:\nlm(formula = formula, data = mtcars)\n\nCoefficients:\n(Intercept)           hp          cyl  \n   36.90833     -0.01912     -2.26469  \n\n\nIn this example, the ‘reformulate()’ function creates a formula object that specifies the relationship between the response variable “mpg” and the predictor variables “hp” and “cyl”. This formula is then passed to the ‘lm()’ function for fitting a linear regression model."
  },
  {
    "objectID": "posts/2023-06-13/index.html#example-2-logistic-regression",
    "href": "posts/2023-06-13/index.html#example-2-logistic-regression",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "Example 2: Logistic Regression",
    "text": "Example 2: Logistic Regression\nConsider a scenario where we use the mtcars dataset. We use the mpg, hp, and disp variables, and whether the car is an automatic or manual. We want to perform a logistic regression to predict the probability of passing based on the mpg, hp, and disp. Here’s how ‘reformulate()’ can help us:\n\nlibrary(stats)\n\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"mpg\", \"hp\", \"disp\"), response = \"am\")\n\n# Fitting a logistic regression model\nmodel &lt;- glm(formula, data = mtcars, family = \"binomial\")\n\nformula\n\nam ~ mpg + hp + disp\n\nmodel\n\n\nCall:  glm(formula = formula, family = \"binomial\", data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           hp         disp  \n  -33.81283      1.28498      0.14936     -0.06545  \n\nDegrees of Freedom: 31 Total (i.e. Null);  28 Residual\nNull Deviance:      43.23 \nResidual Deviance: 10.15    AIC: 18.15\n\n\nIn this example, the ‘reformulate()’ function constructs a formula that defines the relationship between the response variable “am” and the predictor variables “mpg”, “hp”, and “disp”. The resulting formula is then passed to the glm() function for fitting a logistic regression model."
  },
  {
    "objectID": "posts/2023-06-14/index.html",
    "href": "posts/2023-06-14/index.html",
    "title": "Pulling a formula from a recipe object",
    "section": "",
    "text": "Introduction\nThe formula() function in R is a generic function that is used to create and manipulate formulas. Formulas are used to specify the relationship between variables in statistical models. The basic syntax for a formula is:\nresponse ~ predictors\nThe response is the variable that you are trying to predict, and the predictors are the variables that you are using to predict the response. You can use multiple predictors by separating them with + signs. For example, the following formula predicts the mpg (miles per gallon) of a car based on the wt (weight) and hp (horsepower) of the car:\nmpg ~ wt + hp\nThe formula() function can be used to create formulas from scratch, or it can be used to extract formulas from existing objects. For example, the following code creates a formula object called my_formula that predicts the mpg of a car based on the wt and hp of the car:\n\nmy_formula &lt;- formula(mpg ~ wt + hp)\nmy_formula\n\nmpg ~ wt + hp\n\n\nThe formula() function can also be used to manipulate formulas. For example, the following code adds a new predictor called drat (drive ratio) to the my_formula formula:\n\nmy_formula &lt;- update(my_formula, mpg ~ wt + hp + drat)\nmy_formula\n\nmpg ~ wt + hp + drat\n\n\nThe formula() function is a powerful tool that can be used to create, manipulate, and analyze formulas in R.\nHere are some additional things to know about the formula() function:\n\nFormulas are objects in R, and they have a number of methods that can be used to manipulate them. For example, you can use the summary() method to get a summary of a formula, or you can use the plot() method to plot a formula.\nFormulas can be used with a variety of statistical functions in R. For example, you can use the lm() function to fit a linear model to a formula, or you can use the glm() function to fit a generalized linear model to a formula.\nFormulas are a powerful tool for statistical analysis, and they can be used to solve a wide variety of problems. If you are working with data in R, it is important to understand how to use formulas.\n\nNow that we have a decent understanding of the function, I want to shift focus a little bit and show how we can use the generics function formula() in order to extract a formula from a recipe object.\nHere is the full code that we are going to look at:\n\nlibrary(recipes)\n\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\n\nsummary(rec_obj)\n\n# A tibble: 11 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 cyl      &lt;chr [2]&gt; predictor original\n 2 disp     &lt;chr [2]&gt; predictor original\n 3 hp       &lt;chr [2]&gt; predictor original\n 4 drat     &lt;chr [2]&gt; predictor original\n 5 wt       &lt;chr [2]&gt; predictor original\n 6 qsec     &lt;chr [2]&gt; predictor original\n 7 vs       &lt;chr [2]&gt; predictor original\n 8 am       &lt;chr [2]&gt; predictor original\n 9 gear     &lt;chr [2]&gt; predictor original\n10 carb     &lt;chr [2]&gt; predictor original\n11 mpg      &lt;chr [2]&gt; outcome   original\n\n# Get formula\nrec_obj |&gt; prep() |&gt; formula()\n\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n&lt;environment: 0x0000013e3255d2f0&gt;\n\n\nLet’s break down each line and understand what it does:\nlibrary(recipes)\nThe first line imports the recipes package, which is a powerful tool for preparing and preprocessing data in a structured and reproducible manner.\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\nHere, we create a recipe object named rec_obj. This object represents a set of instructions for data transformation. In this case, we specify the formula mpg ~ ., which means we want to predict the miles per gallon (mpg) using all other variables in the mtcars dataset.\nrec_obj |&gt; prep() |&gt; formula()\nThe next line leverages the magrittr pipe operator (|&gt;) to chain multiple operations. Let’s break it down:\n\nrec_obj is passed to the prep() function. This function performs data preparation steps specified in the recipe object, such as handling missing values, feature scaling, or encoding categorical variables.\nThe output of prep() is then piped to the formula() function, which extracts the formula representation from the preprocessed recipe object. The resulting formula can be used in subsequent modeling steps.\n\nThat’s it! With just a few lines of code, we have defined a recipe, prepared the data accordingly, and obtained the formula representation for further modeling.\nNow, let’s dive into a couple more examples to showcase the versatility of the recipes package:\n\nrec_obj &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_normalize(all_predictors())\n\nrec_obj |&gt; prep() |&gt; formula()\n\nSpecies ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\n&lt;environment: 0x0000013e2e8346f0&gt;\n\n\nIn this example, we create a recipe to predict the species (Species) using all other variables in the iris dataset. We then use the step_normalize() function to standardize all predictor variables in the recipe. This step ensures that variables are on a similar scale, which can be beneficial for certain machine learning algorithms.\nrec_obj &lt;- recipe(SalePrice ~ ., data = train_data) |&gt;\n  step_dummy(all_nominal(), -all_outcomes())\nHere, we define a recipe to predict the sale price (SalePrice) using all other variables in the train_data dataset. The step_dummy() function is used to convert all nominal variables in the recipe into dummy variables. The all_nominal() argument specifies that all variables should be considered, while the -all_outcomes() argument ensures that the outcome variable (SalePrice) is not transformed.\nThese examples provide a glimpse into the power and flexibility of the recipes package for data preprocessing in R. It enables you to define a clear and reproducible data transformation pipeline that can greatly simplify your machine learning workflows.\nHappy coding! 🚀"
  },
  {
    "objectID": "posts/2023-06-15/index.html",
    "href": "posts/2023-06-15/index.html",
    "title": "Introduction to Linear Regression in R: Analyzing the mtcars Dataset with lm()",
    "section": "",
    "text": "Introduction\nThe lm() function in R is used for fitting linear regression models. It stands for “linear model,” and it allows you to analyze the relationship between variables and make predictions based on the data.\nLet’s dive into the parameters of the lm() function:\n\nformula: This is the most important parameter, as it specifies the relationship between the variables. It follows a pattern: y ~ x1 + x2 + ..., where y is the response variable, and x1, x2, etc., are the predictor variables. For example, in the mtcars dataset, we can use the formula mpg ~ wt to predict the miles per gallon (mpg) based on the weight (wt) of the cars.\ndata: This parameter refers to the dataset you want to use for the analysis. In our case, we’ll use the mtcars dataset that comes with R.\n\nNow, let’s see some examples using the mtcars dataset\n\n\nExamples\nExample 1: Simple Linear Regression\n\n# Fit a linear regression model to predict mpg based on weight\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nExample 2: Multiple Linear Regression\n\n# Fit a linear regression model to predict mpg based on weight and horsepower\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nExample 3: Include Interaction Term\n\n# Fit a linear regression model to predict mpg based on weight, horsepower, and their interaction\nmodel &lt;- lm(mpg ~ wt + hp + wt:hp, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp + wt:hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0632 -1.6491 -0.7362  1.4211  4.5513 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.80842    3.60516  13.816 5.01e-14 ***\nwt          -8.21662    1.26971  -6.471 5.20e-07 ***\nhp          -0.12010    0.02470  -4.863 4.04e-05 ***\nwt:hp        0.02785    0.00742   3.753 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.153 on 28 degrees of freedom\nMultiple R-squared:  0.8848,    Adjusted R-squared:  0.8724 \nF-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13\n\n\nThese examples demonstrate how to use the lm() function with different sets of predictor variables. After fitting the model, you can use the summary() function to get detailed information about the regression results, including coefficients, p-values, and R-squared values.\nI encourage you to try running these examples and explore different variables in the mtcars dataset. Feel free to modify the formulas and experiment with additional parameters to deepen your understanding of linear regression modeling in R!"
  },
  {
    "objectID": "posts/2023-06-08/index.html#explaining-the-file.info-function",
    "href": "posts/2023-06-08/index.html#explaining-the-file.info-function",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "",
    "text": "The file.info() function returns a data frame with file information as its columns. Each row corresponds to a file, and the columns contain attributes such as the file size, permissions, and timestamps. This function accepts one or more file paths as its argument, providing flexibility in examining multiple files simultaneously. The following columns are returned in the data.frame that results from file.info():\n\nname: The name of the file.\nsize: The size of the file in bytes.\nmode: The mode of the file, which can be used to determine the file’s permissions.\nmtime: The modification time of the file.\nctime: The creation time of the file.\natime: The last access time of the file.\n\nIn order to get some data to work with, we will save the iris dataset as an excel file four times in a for loop, waiting 10 seconds between each save.\nlibrary(writexl)\n\n# Generate file names\nfile_prefix &lt;- \"iris\"\nfile_extension &lt;- \".xlsx\"\nnum_files &lt;- 4\n\n# Save iris dataset as Excel files\nfor (i in 1:num_files) {\n  file_name &lt;- paste0(file_prefix, \"_\", i, file_extension)\n  write_xlsx(iris, file_name)\n  cat(\"File\", file_name, \"saved successfully.\\n\")\n  Sys.sleep(10) # Sleep for 10 seconds then go again\n}"
  },
  {
    "objectID": "posts/2023-06-16/index.html",
    "href": "posts/2023-06-16/index.html",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, it’s crucial to have a deep understanding of the tools at your disposal. In the realm of data analysis and manipulation, R stands as a powerhouse. One function that proves to be invaluable in many scenarios is diff(). In this blog post, we will explore the ins and outs of the diff() function, showcasing its functionality and providing you with practical examples to enhance your programming skills."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-1-simple-vector",
    "href": "posts/2023-06-16/index.html#example-1-simple-vector",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 1: Simple Vector",
    "text": "Example 1: Simple Vector\nLet’s start with a straightforward example using a numeric vector:\n\n# Create a vector\nmy_vector &lt;- c(2, 5, 9, 12, 18)\n\n# Compute differences\ndiff_vector &lt;- diff(my_vector)\n\n# Display the result\ndiff_vector\n\n[1] 3 4 3 6\n\n\nIn this example, the diff() function calculates the differences between consecutive elements in my_vector. The resulting vector, diff_vector, shows the differences [3, 4, 3, 6]."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-2-time-series-data",
    "href": "posts/2023-06-16/index.html#example-2-time-series-data",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 2: Time Series Data",
    "text": "Example 2: Time Series Data\nThe diff() function is particularly handy when working with time series data. Let’s consider a time series dataset representing monthly sales:\n\n# Create a time series\nmonthly_sales &lt;- c(150, 200, 180, 250, 300, 270, 350)\n\n# Compute month-to-month differences\nmonthly_diff &lt;- diff(monthly_sales)\n\n# Display the result\nmonthly_diff\n\n[1]  50 -20  70  50 -30  80\n\n\nHere, the diff() function calculates the changes in sales between consecutive months. The resulting vector, monthly_diff, displays the differences [50, -20, 70, 50, -30, 80]."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-3-advanced-applications",
    "href": "posts/2023-06-16/index.html#example-3-advanced-applications",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 3: Advanced Applications",
    "text": "Example 3: Advanced Applications\nBeyond simple differences, the diff() function can be combined with other R functions to solve more complex problems. Let’s say we have a vector representing the daily closing prices of a stock:\n\n# Create a vector of stock prices\nstock_prices &lt;- c(105.2, 103.9, 105.8, 107.5, 109.1)\n\n# Compute daily price changes as percentages\ndaily_returns &lt;- diff(stock_prices) / stock_prices[-length(stock_prices)] * 100\n\n# Display the result\ndaily_returns\n\n[1] -1.235741  1.828681  1.606805  1.488372\n\n\nIn this example, we calculate the daily returns as a percentage by taking the differences between consecutive closing prices and dividing them by the previous day’s closing price. The resulting vector, daily_returns, represents the daily percentage changes."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-4-miscellaneous-examples",
    "href": "posts/2023-06-16/index.html#example-4-miscellaneous-examples",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 4: Miscellaneous Examples",
    "text": "Example 4: Miscellaneous Examples\n\nx &lt;- rnorm(10)\n\n# Calculate the first-order difference of a vector\ndiff(x)\n\n[1] -0.5814577  0.5824454  1.0677214 -0.7505515  0.9924554 -2.0034078  0.5492343\n[8] -1.8906742  1.1942760\n\n# Calculate the second-order difference of a vector\ndiff(x, differences=2)\n\n[1]  1.163903  0.485276 -1.818273  1.743007 -2.995863  2.552642 -2.439908\n[8]  3.084950\n\n# Calculate the first-order difference of a matrix\ndiff(x, lag=1, differences=1)\n\n[1] -0.5814577  0.5824454  1.0677214 -0.7505515  0.9924554 -2.0034078  0.5492343\n[8] -1.8906742  1.1942760\n\n# Calculate the second-order difference of a matrix\ndiff(x, lag=1, differences=2)\n\n[1]  1.163903  0.485276 -1.818273  1.743007 -2.995863  2.552642 -2.439908\n[8]  3.084950"
  },
  {
    "objectID": "posts/2023-06-20/index.html",
    "href": "posts/2023-06-20/index.html",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "",
    "text": "As a programmer, you’re constantly faced with the task of organizing and analyzing data. One powerful tool in your R arsenal is the xtabs() function. In this blog post, we’ll explore the versatility and simplicity of xtabs() for aggregating data. We’ll use the mtcars dataset and the healthyR.data::healthyR_data dataset to illustrate its functionality. Get ready to dive into the world of data aggregation with xtabs()!"
  },
  {
    "objectID": "posts/2023-06-20/index.html#example-1-analyzing-car-performance-with-mtcars-dataset",
    "href": "posts/2023-06-20/index.html#example-1-analyzing-car-performance-with-mtcars-dataset",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "Example 1: Analyzing Car Performance with mtcars Dataset",
    "text": "Example 1: Analyzing Car Performance with mtcars Dataset\nLet’s start with the mtcars dataset, which contains information about various car models. Suppose we want to understand the distribution of cars based on the number of cylinders and the transmission type. We can use xtabs() to accomplish this:\n\n# Create a contingency table using xtabs()\ntable_cars &lt;- xtabs(~ cyl + am, data = mtcars)\n\n# View the resulting table\ntable_cars\n\n   am\ncyl  0  1\n  4  3  8\n  6  4  3\n  8 12  2\n\n\nIn this example, the formula ~ cyl + am specifies that we want to cross-tabulate the “cyl” (number of cylinders) variable with the “am” (transmission type) variable. The resulting table provides a clear breakdown of car counts based on these two factors.\nThe xtabs() function also allows you to specify the order of the variables in the formula. For example, the following formula would create the same contingency table as the previous formula, but the rows of the table would be ordered by the number of cylinders in the car:\n\nxtabs(~am + cyl, data = mtcars)\n\n   cyl\nam   4  6  8\n  0  3  4 12\n  1  8  3  2"
  },
  {
    "objectID": "posts/2023-06-20/index.html#example-2-analyzing-health-data-with-healthyr.data",
    "href": "posts/2023-06-20/index.html#example-2-analyzing-health-data-with-healthyr.data",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "Example 2: Analyzing Health Data with healthyR.data",
    "text": "Example 2: Analyzing Health Data with healthyR.data\nLet’s now explore the healthyR.data::healthyR_data dataset, which is a simulated administrative dataset. Suppose we’re interested in analyzing the distribution of patients’ insurance type based on their type of stay. Here’s how we can use xtabs() for this analysis:\n\n# Load the dataset\nlibrary(healthyR.data)\n\n# Create a contingency table using xtabs()\ntable_health &lt;- xtabs(~ payer_grouping + ip_op_flag, data = healthyR_data)\n\n# View the resulting table\ntable_health\n\n                ip_op_flag\npayer_grouping       I     O\n  ?                  1     0\n  Blue Cross     10797 13560\n  Commercial      3328  3239\n  Compensation     787  1715\n  Exchange Plans  1206  1194\n  HMO             8113  9331\n  Medicaid        7131  1646\n  Medicaid HMO   15466 10018\n  Medicare A     52621     1\n  Medicare B       293 22270\n  Medicare HMO   13572  5425\n  No Fault        1713   645\n  Self Pay        2089  1560\n\n\nIn this example, the formula ~ payer_grouping + ip_op_flag specifies that we want to cross-tabulate the “payer_grouping” variable with the “ip_op_flag” variable. By using xtabs(), we obtain a comprehensive summary of patients’ insurance type and their stay type."
  },
  {
    "objectID": "posts/2023-06-21/index.html",
    "href": "posts/2023-06-21/index.html",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "",
    "text": "Sampling is a fundamental technique in data analysis and statistical modeling. It allows us to draw meaningful insights and make inferences about a larger population based on a representative subset. In the world of R programming, the sample() function stands as a versatile tool that enables us to create random samples efficiently. In this post, we will explore the sample() function and its various applications through a series of plain English examples.\nFirst, let’s take a look at the syntax:\nsample(x, size, replace = FALSE, prob = NULL)\nwhere:\n\nx is the dataset or vector from which to take the sample\nsize is the number of elements to include in the sample\nreplace is a logical value that indicates whether or not to allow sampling with replacement (the default is FALSE)\nprob is a vector of probabilities that can be used to weight the sample (the default is NULL)"
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-1-simple-random-sampling",
    "href": "posts/2023-06-21/index.html#example-1-simple-random-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 1: Simple Random Sampling",
    "text": "Example 1: Simple Random Sampling\nLet’s say we have a dataset containing the ages of 100 people. To create a random sample of 10 individuals, we can use the sample() function as follows:\n\nages &lt;- 1:100\nrandom_sample &lt;- sample(ages, size = 10)\nrandom_sample\n\n [1] 53 13 84 50 55  9 12 38 79 15\n\n\nThe sample() function randomly selects 10 values from the ages vector, without replacement, resulting in a new vector named random_sample. This technique represents simple random sampling, where each individual in the population has an equal chance of being included in the sample."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-2-sampling-with-replacement",
    "href": "posts/2023-06-21/index.html#example-2-sampling-with-replacement",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 2: Sampling with Replacement",
    "text": "Example 2: Sampling with Replacement\nIn some scenarios, we might want to allow repeated selections from the population. Let’s say we have a bag with colored balls, and we want to simulate drawing 5 balls with replacement. Here’s how we can achieve it:\n\ncolors &lt;- c(\"red\", \"blue\", \"green\", \"yellow\")\nsample_with_replacement &lt;- sample(colors, size = 5, replace = TRUE)\nsample_with_replacement\n\n[1] \"yellow\" \"yellow\" \"green\"  \"green\"  \"red\"   \n\n\nThe sample() function, with the replace = TRUE argument, enables us to randomly select 5 colors from the colors vector, allowing duplicates. This approach represents sampling with replacement, where each selection is independent of the previous ones."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-3-weighted-sampling",
    "href": "posts/2023-06-21/index.html#example-3-weighted-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 3: Weighted Sampling",
    "text": "Example 3: Weighted Sampling\nIn certain situations, we may want to assign different probabilities to elements in the population. Let’s assume we have a list of items and corresponding weights denoting their probabilities of being selected. We can use the sample() function with the prob parameter to achieve weighted sampling. Consider the following example:\n\nlibrary(dplyr)\n\nitems &lt;- c(\"apple\", \"banana\", \"orange\")\nweights &lt;- c(0.4, 0.2, 0.4)\nweighted_sample &lt;- sample(items, size = 1, prob = weights)\nweighted_sample\n\n[1] \"apple\"\n\ntibble(x = 1:10) |&gt; \n  group_by(x) |&gt; \n  mutate(rs = sample(items, size = 1, prob = weights)) |&gt;\n  ungroup()\n\n# A tibble: 10 × 2\n       x rs    \n   &lt;int&gt; &lt;chr&gt; \n 1     1 orange\n 2     2 apple \n 3     3 apple \n 4     4 apple \n 5     5 apple \n 6     6 orange\n 7     7 orange\n 8     8 orange\n 9     9 apple \n10    10 orange\n\n\nBy specifying the prob argument with the corresponding weights, the sample() function randomly selects a single item from the items vector. The probability of each item being chosen is proportional to its weight. In this case, “apple” and “orange” have a higher chance (40% each) of being selected compared to “banana” (20%)."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-4-stratified-sampling",
    "href": "posts/2023-06-21/index.html#example-4-stratified-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 4: Stratified Sampling",
    "text": "Example 4: Stratified Sampling\nStratified sampling involves dividing the population into subgroups or strata and then sampling from each stratum proportionally. Let’s assume we have a dataset of students’ grades in different subjects, and we want to select a sample that maintains the proportion of students from each subject. We can achieve this using the sample() function along with additional parameters. Consider the following example:\n\nsubjects &lt;- c(\"Math\", \"Science\", \"English\", \"History\")\ngrades &lt;- c(80, 90, 85, 70, 75, 95, 60, 92, 88, 83, 78, 91)\nstrata &lt;- factor(subjects)\nstratified_sample &lt;- unlist(\n  by(\n    grades, \n    rep(strata, 3), \n    FUN = function(x) sample(x, size = 2)\n    )\n  )\nstratified_sample\n\nEnglish1 English2 History1 History2    Math1    Math2 Science1 Science2 \n      78       60       92       91       80       75       90       95 \n\n\nIn this example, we use the by() function to group the grades by subject (strata). Then, we apply the sample() function to each subgroup (subject) using the FUN argument. The result is a stratified sample of two grades from each subject, maintaining the relative proportions of students in the final sample."
  },
  {
    "objectID": "posts/2023-06-22/index.html",
    "href": "posts/2023-06-22/index.html",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, you’re constantly faced with the need to repeat tasks efficiently. Repetition is a fundamental concept in programming, and R provides a powerful tool to accomplish this: the rep() function. In this blog post, we will explore the syntax of the rep() function and delve into several examples to showcase its versatility and practical applications. Whether you’re working with data manipulation, generating sequences, or creating repeated patterns, rep() will become your go-to function for mastering repetition in R."
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-1-repeating-a-single-value",
    "href": "posts/2023-06-22/index.html#example-1-repeating-a-single-value",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 1: Repeating a Single Value",
    "text": "Example 1: Repeating a Single Value\nLet’s start with a simple example. Suppose we want to repeat the value 5 three times. We can achieve this using the following code:\n\nresult &lt;- rep(5, times = 3)\nprint(result)\n\n[1] 5 5 5"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-2-replicating-a-vector",
    "href": "posts/2023-06-22/index.html#example-2-replicating-a-vector",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 2: Replicating a Vector",
    "text": "Example 2: Replicating a Vector\nThe rep() function can also replicate entire vectors. Consider the following example where we replicate the vector c(1, 2, 3) four times:\n\nvector &lt;- c(1, 2, 3)\nresult &lt;- rep(vector, times = 4)\nprint(result)\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-3-repeating-elements-using-each",
    "href": "posts/2023-06-22/index.html#example-3-repeating-elements-using-each",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 3: Repeating Elements Using ‘each’",
    "text": "Example 3: Repeating Elements Using ‘each’\nThe each argument allows us to repeat each element of a vector a specific number of times. Let’s illustrate this with the following example:\n\nvector &lt;- c(1, 2, 3)\nresult &lt;- rep(vector, times = 2, each = 2)\nprint(result)\n\n [1] 1 1 2 2 3 3 1 1 2 2 3 3"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-4-creating-repeated-patterns",
    "href": "posts/2023-06-22/index.html#example-4-creating-repeated-patterns",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 4: Creating Repeated Patterns",
    "text": "Example 4: Creating Repeated Patterns\nOne interesting use case of the rep() function is to create repeated patterns. Consider this example, where we want to generate a pattern of “ABABAB” ten times:\n\npattern &lt;- rep(c(\"A\", \"B\"), times = 10)\nresult &lt;- paste(pattern, collapse = \"\")\nprint(result)\n\n[1] \"ABABABABABABABABABAB\""
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-5-expanding-factors-or-categories",
    "href": "posts/2023-06-22/index.html#example-5-expanding-factors-or-categories",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 5: Expanding Factors or Categories",
    "text": "Example 5: Expanding Factors or Categories\nThe rep() function is useful for expanding factors or categories. Let’s say we have a factor with three levels, and we want to replicate each level four times:\n\nfactor &lt;- factor(c(\"low\", \"medium\", \"high\"))\nresult &lt;- rep(factor, times = 4)\nprint(result)\n\n [1] low    medium high   low    medium high   low    medium high   low   \n[11] medium high  \nLevels: high low medium"
  },
  {
    "objectID": "posts/2023-06-23/index.html",
    "href": "posts/2023-06-23/index.html",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "",
    "text": "Bootstrap resampling is a powerful technique used in statistics and data analysis to estimate the uncertainty of a statistic by repeatedly sampling from the original data. In R, we can easily implement a bootstrap function using the lapply, rep, and sample functions. In this blog post, we will explore how to write a bootstrap function in R and provide an example using the “mpg” column from the popular “mtcars” dataset."
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-1-load-the-required-dataset",
    "href": "posts/2023-06-23/index.html#step-1-load-the-required-dataset",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 1: Load the required dataset",
    "text": "Step 1: Load the required dataset\nLet’s begin by loading the “mtcars” dataset, which is included in the base R package:\n\ndata(mtcars)"
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-2-define-the-bootstrap-function",
    "href": "posts/2023-06-23/index.html#step-2-define-the-bootstrap-function",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 2: Define the bootstrap function",
    "text": "Step 2: Define the bootstrap function\nWe’ll define a function called bootstrap() that takes two arguments: data (the input data vector) and n (the number of bootstrap iterations).\n\nbootstrap &lt;- function(data, n) {\n  resampled_data &lt;- lapply(1:n, function(i) {\n    resample &lt;- sample(data, replace = TRUE)\n    # Perform desired operations on the resampled data, e.g., compute a statistic\n    # and return the result\n  })\n  return(resampled_data)\n}\n\nbootstrapped_samples &lt;- bootstrap(mtcars$mpg, 5)\nbootstrapped_samples\n\n[[1]]\n [1] 21.0 18.1 33.9 21.4 17.3 19.2 19.2 15.8 16.4 30.4 18.1 14.3 32.4 10.4 15.0\n[16] 16.4 30.4 17.8 21.4 19.2 17.3 22.8 14.3 22.8 30.4 18.7 13.3 13.3 15.2 10.4\n[31] 15.0 13.3\n\n[[2]]\n [1] 18.7 32.4 21.0 10.4 15.0 14.7 24.4 10.4 32.4 10.4 21.0 19.7 21.4 10.4 30.4\n[16] 17.3 10.4 22.8 15.2 15.2 21.4 15.8 21.4 33.9 24.4 15.2 18.1 19.2 21.0 24.4\n[31] 15.5 21.0\n\n[[3]]\n [1] 15.5 30.4 21.0 22.8 27.3 18.1 21.0 13.3 15.2 17.3 15.8 21.0 18.1 14.3 17.8\n[16] 15.8 21.0 18.1 19.2 24.4 19.2 22.8 18.7 14.3 26.0 21.4 22.8 32.4 14.7 15.2\n[31] 15.2 14.3\n\n[[4]]\n [1] 13.3 21.0 13.3 15.0 19.2 18.1 18.1 19.2 22.8 18.7 26.0 21.4 14.7 14.3 17.8\n[16] 22.8 19.7 21.4 30.4 30.4 18.7 17.3 16.4 21.5 18.1 21.0 17.8 21.4 14.3 19.7\n[31] 32.4 18.7\n\n[[5]]\n [1] 15.0 21.4 21.5 26.0 17.3 30.4 18.1 17.8 17.3 30.4 24.4 32.4 21.0 17.8 33.9\n[16] 32.4 19.2 22.8 19.7 16.4 17.8 22.8 14.3 33.9 21.5 10.4 21.4 26.0 33.9 14.7\n[31] 21.5 18.1\n\n\nIn the above code, we use lapply to generate a list of n resampled datasets. Inside the lapply function, we use the sample function to randomly sample from the original data with replacement (replace = TRUE). This ensures that each resampled dataset has the same length as the original dataset."
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-3-perform-desired-operations-on-resampled-data",
    "href": "posts/2023-06-23/index.html#step-3-perform-desired-operations-on-resampled-data",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 3: Perform desired operations on resampled data",
    "text": "Step 3: Perform desired operations on resampled data\nWithin the lapply function, you can perform any desired operations on the resampled data. This could involve calculating statistics, fitting models, or conducting hypothesis tests. Customize the code within the lapply function to suit your specific needs.\nExample: Bootstrapping the “mpg” column in mtcars: Let’s illustrate the usage of our bootstrap function by resampling the “mpg” column from the “mtcars” dataset. We will calculate the mean of the resampled datasets.\n\n# Step 1: Load the dataset\ndata(mtcars)\n\n# Step 2: Define the bootstrap function\nbootstrap &lt;- function(data, n) {\n  resampled_data &lt;- lapply(1:n, function(i) {\n    resample &lt;- sample(data, replace = TRUE)\n    mean(resample)  # Calculate the mean of each resampled dataset\n  })\n  return(resampled_data)\n}\n\n# Step 3: Perform the bootstrap resampling\nbootstrapped_means &lt;- bootstrap(mtcars$mpg, n = 1000)\n\n# Display the first few resampled means\nhead(bootstrapped_means)\n\n[[1]]\n[1] 20.21562\n\n[[2]]\n[1] 20.09375\n\n[[3]]\n[1] 19.59375\n\n[[4]]\n[1] 20.13437\n\n[[5]]\n[1] 21.17813\n\n[[6]]\n[1] 21.5375\n\n\nIn the above example, we resample the “mpg” column of the “mtcars” dataset 1000 times. The bootstrap() function calculates the mean of each resampled dataset and returns a list of resampled means. The head() function is then used to display the first few resampled means.\nOf course we do not have to specify a statistic function in the bootstrap, we can choose to just return bootstrap samples and then perform some sort of statistic on it. Look at the following example using the above bootstrapped_samples data.\n\nquantile(unlist(bootstrapped_samples), \n         probs = c(0.025, 0.25, 0.5, 0.75, 0.975))\n\n  2.5%    25%    50%    75%  97.5% \n10.400 15.725 19.200 22.800 33.900 \n\nmean(unlist(bootstrapped_samples))\n\n[1] 20.06625\n\nsd(unlist(bootstrapped_samples))\n\n[1] 5.827239"
  },
  {
    "objectID": "posts/2023-06-26/index.html",
    "href": "posts/2023-06-26/index.html",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "",
    "text": "Welcome to the world of data visualization in R! In this blog post, we will explore the abline() function, a versatile tool that allows you to add straight lines to your plots effortlessly. Whether you’re a beginner or an experienced R programmer, mastering abline() will empower you to create more informative and visually appealing graphs. So, let’s dive in!"
  },
  {
    "objectID": "posts/2023-06-26/index.html#example-1.-simple-linear-regression-line",
    "href": "posts/2023-06-26/index.html#example-1.-simple-linear-regression-line",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Example 1. Simple Linear Regression Line:",
    "text": "Example 1. Simple Linear Regression Line:\nLet’s start with a classic example of drawing a linear regression line on a scatter plot. Consider the following data:\n\nx &lt;- 1:10\ny &lt;- c(2, 3, 5, 7, 9, 10, 13, 15, 17, 19)\n\nTo visualize the relationship between x and y, we can plot the points and add a regression line using abline():\n\nplot(x, y, main = \"Linear Regression Example\", xlab = \"x\", ylab = \"y\")\nabline(lm(y ~ x), col = \"red\")"
  },
  {
    "objectID": "posts/2023-06-26/index.html#examle-2.-custom-slope-and-intercept",
    "href": "posts/2023-06-26/index.html#examle-2.-custom-slope-and-intercept",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Examle 2. Custom Slope and Intercept",
    "text": "Examle 2. Custom Slope and Intercept\nThe abline() function allows you to specify custom slope and intercept values. Suppose you have a dataset where y increases by 3 for every unit increase in x. We can draw a line with a slope of 3 and an intercept of 0 using the following code:\n\nplot(x, y, main = \"Custom Slope and Intercept\", xlab = \"x\", ylab = \"y\")\nabline(a = 0, b = 3, col = \"blue\")"
  },
  {
    "objectID": "posts/2023-06-26/index.html#example-3.-vertical-and-horizontal-lines",
    "href": "posts/2023-06-26/index.html#example-3.-vertical-and-horizontal-lines",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Example 3. Vertical and Horizontal Lines:",
    "text": "Example 3. Vertical and Horizontal Lines:\nabline() isn’t limited to just diagonal lines; you can also draw vertical and horizontal lines. For instance, let’s draw a vertical line at x = 5 and a horizontal line at y = 12:\n\nplot(x, y, main = \"Vertical and Horizontal Lines\", xlab = \"x\", ylab = \"y\")\nabline(v = 5, col = \"green\") # Vertical line\nabline(h = 12, col = \"orange\") # Horizontal line"
  },
  {
    "objectID": "posts/2023-06-26/index.html#encouragement-to-try-it-yourself",
    "href": "posts/2023-06-26/index.html#encouragement-to-try-it-yourself",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Encouragement to Try It Yourself",
    "text": "Encouragement to Try It Yourself\nNow that you’ve seen a few examples of what the abline() function can do, I encourage you to unleash your creativity and explore its full potential. Experiment with different datasets, slopes, intercepts, and line styles. The more you practice, the more comfortable you will become with this powerful visualization tool."
  },
  {
    "objectID": "posts/2023-06-26/index.html#conclusion",
    "href": "posts/2023-06-26/index.html#conclusion",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we delved into the abline() function in R, exploring its capabilities for adding straight lines to plots. We covered simple linear regression lines, custom slopes and intercepts, as well as vertical and horizontal lines. Armed with this knowledge, you can enhance your data visualizations, making them more informative and engaging. So, go ahead, give abline() a try, and unlock a whole new world of possibilities in R programming! Happy coding!"
  },
  {
    "objectID": "posts/2023-06-27/index.html",
    "href": "posts/2023-06-27/index.html",
    "title": "The ave() Function in R",
    "section": "",
    "text": "In the world of data analysis and statistics, grouping data based on certain criteria is a common task. Whether you’re working with large datasets or analyzing trends within smaller subsets, having a reliable and efficient tool for data grouping can make your life as a programmer much easier. In this blog post, we’ll dive into the R function ave() and explore how it can help you achieve seamless data grouping and computation."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-1-computing-average-sales-by-region",
    "href": "posts/2023-06-27/index.html#example-1-computing-average-sales-by-region",
    "title": "The ave() Function in R",
    "section": "Example 1: Computing Average Sales by Region",
    "text": "Example 1: Computing Average Sales by Region\nLet’s consider a dataset containing sales data for different regions. We’ll use ave() to calculate the average sales for each region.\n\nsales &lt;- data.frame(\n  region = c(\"North\", \"South\", \"North\", \"East\", \"South\", \"East\"),\n  sales = c(500, 700, 600, 450, 800, 550)\n)\n\nsales$avg_sales &lt;- ave(sales$sales, sales$region)\nsales[order(sales$region),]\n\n  region sales avg_sales\n4   East   450       500\n6   East   550       500\n1  North   500       550\n3  North   600       550\n2  South   700       750\n5  South   800       750\n\n\nIn this example, we create a new column called avg_sales and assign the output of ave() to it. The resulting dataset will include the average sales for each region, as computed by ave()."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-2-calculating-median-age-by-gender",
    "href": "posts/2023-06-27/index.html#example-2-calculating-median-age-by-gender",
    "title": "The ave() Function in R",
    "section": "Example 2: Calculating Median Age by Gender",
    "text": "Example 2: Calculating Median Age by Gender\nLet’s explore another scenario where we have a dataset containing information about individuals’ ages and genders. We’ll use ave() to calculate the median age for each gender category.\n\npeople &lt;- data.frame(\n  age = c(32, 28, 35, 40, 26, 30),\n  gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\")\n)\n\npeople$median_age &lt;- ave(people$age, people$gender, FUN = median)\npeople[order(people$gender),]\n\n  age gender median_age\n2  28 Female         30\n4  40 Female         30\n6  30 Female         30\n1  32   Male         32\n3  35   Male         32\n5  26   Male         32\n\n\nIn this example, we introduce the FUN argument to specify the median() function. ave() will compute the median age for each gender category and assign the values to the new column median_age."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-3-finding-maximum-temperature-by-month",
    "href": "posts/2023-06-27/index.html#example-3-finding-maximum-temperature-by-month",
    "title": "The ave() Function in R",
    "section": "Example 3: Finding Maximum Temperature by Month",
    "text": "Example 3: Finding Maximum Temperature by Month\nLet’s say we have a weather dataset containing temperature readings for different months. We can use ave() to calculate the maximum temperature recorded for each month.\n\nweather &lt;- data.frame(\n  month = rep(c(\"Jan\", \"Feb\", \"Mar\"), each = 4),\n  temperature = c(15, 18, 20, 14, 16, 22, 25, 23, 19, 21, 24, 20)\n)\n\nweather$max_temp &lt;- ave(weather$temperature, weather$month, FUN = max)\nweather\n\n   month temperature max_temp\n1    Jan          15       20\n2    Jan          18       20\n3    Jan          20       20\n4    Jan          14       20\n5    Feb          16       25\n6    Feb          22       25\n7    Feb          25       25\n8    Feb          23       25\n9    Mar          19       24\n10   Mar          21       24\n11   Mar          24       24\n12   Mar          20       24\n\n\nIn this example, we use ave() to compute the maximum temperature for each month, and the resulting values are assigned to the new column max_temp."
  },
  {
    "objectID": "posts/2023-06-28/index.html",
    "href": "posts/2023-06-28/index.html",
    "title": "Exploring Rolling Correlation with the rollapply Function: A Powerful Tool for Analyzing Time-Series Data",
    "section": "",
    "text": "Introduction\nIn the world of data analysis, time-series data is a common sight. Whether it’s stock prices, weather patterns, or website traffic, understanding the relationship between variables over time is crucial. One valuable technique in this domain is calculating rolling correlation, which allows us to examine the evolving correlation between two variables as our data moves through time. In this blog post, we will delve into the rollapply function and its capabilities, exploring its applications through a series of practical examples. So, let’s get started!\n\n\nUnderstanding Rolling Correlation\nBefore we jump into the technical details, let’s quickly recap what correlation means. In simple terms, correlation measures the strength and direction of the linear relationship between two variables. It ranges between -1 and 1, where -1 represents a perfect negative correlation, 1 represents a perfect positive correlation, and 0 represents no correlation.\nRolling correlation takes this concept further by calculating correlation values over a moving window of observations. By doing so, we can observe how the correlation between two variables changes over time, gaining insights into trends, seasonality, or other patterns in our data.\n\n\nIntroducing the rollapply Function\nIn R programming, the rollapply function, available in the zoo package, is a powerful tool for calculating rolling correlation. It enables us to apply a function, such as correlation, to a rolling window of our data. The general syntax for using rollapply is as follows:\nrollapply(data, width, FUN, ...)\nHere’s what each parameter represents: - data: The time-series data we want to analyze. - width: The size of the rolling window, indicating how many observations should be included in each correlation calculation. - FUN: The function we want to apply to each rolling window. In this case, we will use the cor function to calculate correlation. - ...: Additional arguments that can be passed to the correlation function or any other function used with rollapply.\nNow, let’s dive into some practical examples to see the rollapply function in action.\n\n\nExample\nImagine we have a dataset containing daily stock prices for two companies, A and B. Our goal is to explore the rolling correlation between the returns of these two stocks over a 30-day window.\n\nlibrary(zoo)\nlibrary(tidyquant)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndf &lt;- FANG |&gt; \n  filter(symbol %in% c(\"FB\", \"AMZN\")) |&gt; \n  select(symbol, adjusted) |&gt; \n  pivot_wider(values_from = adjusted, names_from = symbol) |&gt;\n  unnest()\n\nfb_rets &lt;- diff(log(df$FB))\namzn_rets &lt;- diff(log(df$AMZN))\ndf_rets &lt;- cbind(fb_rets, amzn_rets)\ncorrelation &lt;- rollapply(\n  df_rets, \n  width = 5, \n  function(x) cor(x[,1], x[,2]), \n  by.column = FALSE\n  )\n\nplot(correlation, type=\"l\")\n\n\n\n\nIn this example, we calculate the logarithmic returns of FB and AMZN using the diff function. Then, we apply the cor function to the rolling window of returns, with a width of 5. The by.column = FALSE parameter ensures that the correlation is computed across rows instead of columns, and the fill = NA parameter fills any incomplete windows with NA values.\n\n\nConclusion\nIn this blog post, we explored the concept of rolling correlation and its significance in analyzing time-series data. We learned how to harness the power of the rollapply function from the zoo package to calculate rolling correlation effortlessly. By utilizing rollapply, we can observe the dynamic nature of correlation, uncover trends, and gain valuable insights from our time-dependent datasets.\nRemember, rolling correlation is just one of the many applications of the rollapply function. Its versatility empowers us to explore various other statistics, such as moving averages, standard deviations, and more. So, dive into the world of time-series analysis with rollapply and unlock the hidden patterns in your data!\nHappy coding and happy analyzing!"
  },
  {
    "objectID": "posts/2023-06-29/index.html",
    "href": "posts/2023-06-29/index.html",
    "title": "How to Use a Windows .bat File to Execute an R Script",
    "section": "",
    "text": "Introduction\nUsing a Windows .bat file to execute an R script can be a convenient way to automate tasks and streamline your workflow. In this blog post, we will explain each line of a sample .bat file and its corresponding R script, along with a simple explanation of what each section does.\n\nThe .bat File:\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\nNow, let’s break down each line:\n\n@echo off: This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem Set the path to the Rscript executable: The rem keyword denotes a comment in a batch file. This line sets the path to the Rscript executable, which is the command-line interface for executing R scripts.\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\": This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nrem Set the path to the R script to execute: This line is another comment, specifying that the next line sets the path to the R script that will be executed.\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\": Here, the path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE%: This line executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nrem Pause so the user can see the output: This comment suggests that the script should pause after execution so that the user can view the output before the command prompt window closes.\nexit: This command exits the batch file and closes the command prompt window.\n\n\n\nThe R Script:\nThe R script contains several sections. Here is the full script and then I will give an explanation of each section:\n# Library Load\nlibrary(DBI)\nlibrary(odbc)\nlibrary(dplyr)\nlibrary(writexl)\nlibrary(stringr)\nlibrary(Microsoft365R)\nlibrary(glue)\nlibrary(blastula)\n\n# Source SSMS Connection Functions \nsource(\"C:/Path/to/SQL_Connection_Functions.r\")\n\n# Connect to SSMS\ndbc &lt;- db_connect()\n\n# Query SSMS\nquery &lt;- DBI::dbGetQuery(\n  conn = dbc,\n  statement = paste0(\n    \"\n    select encounter,\n        pt_no \n    from dbo.c_xfer_fac_tbl \n    where encounter in \n        (\n        select distinct encounter\n        from DBO.c_xfer_fac_tbl \n        group by encounter, file_name \n        having Count(Distinct pt_no) &gt; 1\n        ) \n        and INSERT_DATETIME = \n        (\n        select Max(INSERT_DATETIME) \n        from dbo.c_xfer_fac_tbl\n        ) \n    group by encounter, pt_no \n    order by encounter\n    \"\n  )\n)\n\ndb_disconnect(dbc)\n\n# Save file to disk\npath &lt;- \"C:/Path/to/files/encounter_duplicates/\"\nf_name &lt;- \"Encounter_Duplicates_\"\nf_date &lt;- Sys.time() |&gt; \n  str_replace_all(pattern = \"[-|:]\",\"\") |&gt;\n  str_replace(pattern = \"[ ]\", \"_\")\nfull_file_name &lt;- paste0(f_name, f_date, \".xlsx\")\nfpn &lt;- paste0(path, full_file_name)\n\nwrite_xlsx(\n  x = query,\n  path = fpn\n)\n\n# Compose Email ----\n# Open Outlook\nOutlook &lt;- get_business_outlook()\n\nemail_body &lt;- md(glue(\n\"\n  ## Important!\n  \n  Please see attached file {full_file_name}\n  \n  The file attached contains a list of accounts from Hospital B\n  that have two or more Hospital A account numbers associated with them. We therefore\n  cannot process these accounts.\n  \n  Thank you,\n\n  The Team\n  \"\n))\n\nemail_template &lt;- compose_email(\n  body = email_body,\n  footer = md(\"sent via Microsoft365R and The Team\")\n)\n\n# Create Email\nOutlook$create_email(email_template)$\n  #set_body(email_body, content_type=\"html\")$\n  set_recipients(to=c(\"email1@email.com\", \"email2@email.com\"))$\n  set_subject(\"Encounter Duplicates\")$\n  add_attachment(fpn)$\n  send()\n\n# Archive File after it has been sent\narchive_path &lt;- \"C:/Path/to/Encounter_Duplicate_Files/Sent/\"\nmove_to_path &lt;- paste0(archive_path, full_file_name)\nfile.rename(\n  from = fpn,\n  to = move_to_path\n)\n\n# Clear the Session\nrm(list = ls())\n\nLibrary Load: This section loads various R libraries needed for the script’s functionality, such as database connections, data manipulation, and email composition.\nSource SSMS Connection Functions: Here, a separate R script file (SQL_Connection_Functions.r) is sourced. This file likely contains custom functions related to connecting to and querying a SQL Server Management System (SSMS) database.\nConnect to SSMS: This line establishes a connection to the SSMS database using the db_connect() function.\nQuery SSMS: The script executes a SQL query against the SSMS database using the dbGetQuery() function. The result of the query is assigned to the query variable.\nSave file to disk: The script saves the query result (query) to an Excel file on the local disk using the write_xlsx() function.\nCompose Email: This section composes an email using the blastula package, preparing the email body and setting the recipients, subject, and\n\nattachments.\n\nCreate Email: The composed email is created using the create_email() function from the Microsoft365R package. The body, recipients, subject, and attachment are set.\nSend Email: The email is sent using the send() function, which relies on a connection to Microsoft Outlook. The email body, recipients, subject, and attachment are all included in the email.\nArchive File after it has been sent: The script moves the Excel file to an archive folder after sending the email, using the file.rename() function.\nClear the Session: The rm() function is used to clear the current R session, removing any remaining objects from memory.\n\n\n\n\nConclusion\nUsing a Windows .bat file to execute an R script allows for easy automation and integration of R scripts into your workflow. By understanding each line of the .bat file and the corresponding R script sections, you can customize and adapt the process to suit your specific needs."
  },
  {
    "objectID": "posts/2023-06-30/index.html",
    "href": "posts/2023-06-30/index.html",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "",
    "text": "Managing files is an essential task for any programmer, and when working with R, the file.rename() function can become your best friend. In this blog post, we’ll explore the ins and outs of file.rename(), discuss its syntax, provide real-life examples, and share some best practices to empower you in your file management endeavors. So grab a cup of coffee and let’s dive into the world of file.rename()!"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-1-renaming-a-single-fil",
    "href": "posts/2023-06-30/index.html#example-1-renaming-a-single-fil",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 1: Renaming a Single Fil",
    "text": "Example 1: Renaming a Single Fil\nSuppose you have a file named “old_file.txt,” and you want to rename it to “new_file.txt”. Here’s how you can accomplish this with file.rename():\nfile.rename(from = \"old_file.txt\", to = \"new_file.txt\")"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-2-renaming-multiple-file",
    "href": "posts/2023-06-30/index.html#example-2-renaming-multiple-file",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 2: Renaming Multiple File",
    "text": "Example 2: Renaming Multiple File\nImagine you have a folder with several files that need to be renamed simultaneously. Let’s say you want to change the file extensions from “.doc” to “.docx”. Here’s how you can achieve this using file.rename():\nfiles &lt;- list.files(path = \"path/to/folder\", pattern = \"*.doc\", full.names = TRUE)\nnew_names &lt;- sub(pattern = \".doc$\", replacement = \".docx\", x = files)\nfile.rename(from = files, to = new_names)"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-1-renaming-a-single-file",
    "href": "posts/2023-06-30/index.html#example-1-renaming-a-single-file",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 1: Renaming a Single File",
    "text": "Example 1: Renaming a Single File\nSuppose you have a file named “old_file.txt,” and you want to rename it to “new_file.txt”. Here’s how you can accomplish this with file.rename():\nfile.rename(from = \"old_file.txt\", to = \"new_file.txt\")"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-2-renaming-multiple-files",
    "href": "posts/2023-06-30/index.html#example-2-renaming-multiple-files",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 2: Renaming Multiple Files",
    "text": "Example 2: Renaming Multiple Files\nImagine you have a folder with several files that need to be renamed simultaneously. Let’s say you want to change the file extensions from “.doc” to “.docx”. Here’s how you can achieve this using file.rename():\nfiles &lt;- list.files(path = \"path/to/folder\", pattern = \"*.doc\", full.names = TRUE)\nnew_names &lt;- sub(pattern = \".doc$\", replacement = \".docx\", x = files)\nfile.rename(from = files, to = new_names)"
  },
  {
    "objectID": "posts/2023-07-11/index.html",
    "href": "posts/2023-07-11/index.html",
    "title": "A Closer Look at the R Function identical()",
    "section": "",
    "text": "Introduction\nIn the realm of programming, R is a widely-used language for statistical computing and data analysis. Within R, there exists a powerful function called identical() that allows programmers to compare objects for exact equality. In this blog post, we will delve into the syntax and usage of the identical() function, providing clear explanations and practical examples along the way.\n\n\nSyntax of identical()\nThe identical() function in R has the following simple syntax:\nidentical(x, y)\nHere, x and y are the objects that we want to compare. The function returns a logical value of either TRUE or FALSE, indicating whether x and y are exactly identical.\n\n\nExamples\n\nComparing Numeric Values: Let’s start with a simple example comparing two numeric values:\n\n\na &lt;- 5\nb &lt;- 5\nidentical(a, b)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE since both a and b have the same numeric value of 5.\n\nComparing Character Strings: Now, let’s consider an example with character strings:\n\n\nname1 &lt;- \"John\"\nname2 &lt;- \"John\"\nidentical(name1, name2)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE as both name1 and name2 contain the same string “John”.\n\nComparing Vectors: The identical() function can also compare vectors. Let’s see an example:\n\n\nvec1 &lt;- c(1, 2, 3)\nvec2 &lt;- c(1, 2, 3)\nidentical(vec1, vec2)\n\n[1] TRUE\n\n\nHere, the identical() function will return TRUE since vec1 and vec2 have the same values in the same order.\n\nComparing Data Frames: Data frames are a fundamental data structure in R. Let’s compare two data frames using identical():\n\n\ndf1 &lt;- data.frame(a = 1:3, b = c(\"A\", \"B\", \"C\"))\ndf2 &lt;- data.frame(a = 1:3, b = c(\"A\", \"B\", \"C\"))\nidentical(df1, df2)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE as both df1 and df2 have the same column names, column types, and corresponding values.\n\nHandling Inexact Equality: The identical() function is particularly useful when we want to ensure that two objects are precisely the same. However, it does not handle cases where inexact equality is expected. For example:\n\n\nx &lt;- sqrt(2) * sqrt(2)\ny &lt;- 2\nidentical(x, y)\n\n[1] FALSE\n\n\nSurprisingly, the identical() function will return FALSE in this case. This occurs because sqrt(2) introduces a slight rounding error, resulting in x and y being slightly different despite representing the same mathematical value.\n\n\nConclusion\nIn this blog post, we explored the syntax and various use cases of the identical() function in R. By leveraging this function, you can determine whether two objects are exactly identical, whether they are numbers, strings, vectors, or even complex data structures like data frames. Remember that identical() is designed for exact equality, so if you require inexact comparisons, you may need to explore alternative approaches. Happy coding with R!"
  },
  {
    "objectID": "posts/2023-07-12/index.html",
    "href": "posts/2023-07-12/index.html",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, working with data is a crucial aspect of our work. In R, there are numerous functions available that simplify data analysis tasks. One such function is colMeans(), which allows us to calculate the mean of columns in a matrix or data frame. In this blog post, we will delve into the colMeans() function, understand its usage, and explore various examples to see how it can help us gain valuable insights from our data."
  },
  {
    "objectID": "posts/2023-07-12/index.html#example-1-calculating-column-means-in-a-matri",
    "href": "posts/2023-07-12/index.html#example-1-calculating-column-means-in-a-matri",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "Example 1: Calculating column means in a matri",
    "text": "Example 1: Calculating column means in a matri\n\n# Create a matrix\nmy_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\n\n# Calculate column means\ncol_means &lt;- colMeans(my_matrix)\n\n# Print the result\nprint(col_means)\n\n[1] 1.5 3.5 5.5\n\n\nIn this example, we created a 2x3 matrix called ‘my_matrix’ and used colMeans() to calculate the means for each column. The resulting vector ‘col_means’ contains the mean values of columns [1 3 5], [2 3 6], which are [1.5, 3.5, 5.5] respectively."
  },
  {
    "objectID": "posts/2023-07-12/index.html#example-2-handling-missing-values",
    "href": "posts/2023-07-12/index.html#example-2-handling-missing-values",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "Example 2: Handling missing values",
    "text": "Example 2: Handling missing values\n\n# Create a matrix with missing values\nmy_matrix &lt;- matrix(c(1, 2, NA, 4, 5, 6), nrow = 2, ncol = 3)\n\n# Calculate column means with missing values removed\ncol_means &lt;- colMeans(my_matrix, na.rm = TRUE)\n\n# Print the result\nprint(col_means)\n\n[1] 1.5 4.0 5.5\n\n\nIn this example, our matrix ‘my_matrix’ contains a missing value (NA). By setting the ‘na.rm’ argument to TRUE, colMeans() excludes the missing value while calculating the means. As a result, we obtain the column means [1.5 4.0 5.5]"
  },
  {
    "objectID": "posts/2023-07-13/index.html",
    "href": "posts/2023-07-13/index.html",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "",
    "text": "As a programmer, you’ll often come across situations where you need to check whether a file exists before performing any operations on it. Thankfully, the R programming language provides a handy function called file.exists() that allows you to easily determine the existence of a file. In this blog post, we’ll explore the syntax and usage of file.exists() and provide you with practical examples to encourage you to try it out for yourself."
  },
  {
    "objectID": "posts/2023-07-13/index.html#example-1-checking-the-existence-of-a-file",
    "href": "posts/2023-07-13/index.html#example-1-checking-the-existence-of-a-file",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "Example 1: Checking the Existence of a File",
    "text": "Example 1: Checking the Existence of a File\nSuppose you want to check whether a file named “data.csv” exists in the current working directory. You can use the following code:\n\nfile_path &lt;- \"data.csv\"\nif (file.exists(file_path)) {\n  print(\"The file exists!\")\n} else {\n  print(\"The file does not exist.\")\n}\n\n[1] \"The file does not exist.\"\n\n\nIn this example, we assign the file path to the variable file_path and then use file.exists() to check if the file exists. If the condition is met, it will print “The file exists!” Otherwise, it will print “The file does not exist.”"
  },
  {
    "objectID": "posts/2023-07-13/index.html#example-2-conditional-operations-with-file.exists",
    "href": "posts/2023-07-13/index.html#example-2-conditional-operations-with-file.exists",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "Example 2: Conditional Operations with file.exists()",
    "text": "Example 2: Conditional Operations with file.exists()\nLet’s imagine you want to perform different actions based on the existence of multiple files. Consider the following code snippet:\n\nfile1 &lt;- \"data1.csv\"\nfile2 &lt;- \"data2.csv\"\n\nif (file.exists(file1)) {\n  # Perform an operation if file1 exists\n  print(\"Performing operation on file1...\")\n} else {\n  # Perform a different operation if file1 doesn't exist\n  print(\"File1 does not exist.\")\n}\n\n[1] \"File1 does not exist.\"\n\nif (file.exists(file2)) {\n  # Perform an operation if file2 exists\n  print(\"Performing operation on file2...\")\n} else {\n  # Perform a different operation if file2 doesn't exist\n  print(\"File2 does not exist.\")\n}\n\n[1] \"File2 does not exist.\"\n\n\nIn this example, we check the existence of two files, data1.csv and data2.csv, and perform different actions based on their availability. You can modify the code according to your specific needs and perform any desired operations."
  },
  {
    "objectID": "posts/2023-07-14/index.html",
    "href": "posts/2023-07-14/index.html",
    "title": "Covariance in R with the cov() Function",
    "section": "",
    "text": "In the world of data analysis, understanding the relationship between variables is crucial. One powerful tool for measuring this relationship is the covariance. Today, we’ll explore the cov() function in R and delve into the fascinating world of covariance. Whether you’re a beginner or an experienced programmer, this blog post will equip you with the knowledge to harness the potential of cov() in your data analysis projects."
  },
  {
    "objectID": "posts/2023-07-14/index.html#example-1-calculating-covariance-between-two-variables",
    "href": "posts/2023-07-14/index.html#example-1-calculating-covariance-between-two-variables",
    "title": "Covariance in R with the cov() Function",
    "section": "Example 1: Calculating Covariance between Two Variables",
    "text": "Example 1: Calculating Covariance between Two Variables\nSuppose we have two vectors, x and y, representing the number of hours studied and the corresponding test scores, respectively, for a group of students. We want to measure the covariance between these two variables.\n\n# Create example vectors\nx &lt;- c(5, 7, 3, 6, 8)\ny &lt;- c(65, 80, 50, 70, 90)\n\n# Calculate covariance\ncovariance &lt;- cov(x, y)\n\ncovariance\n\n[1] 29\n\n\nIn this example, the cov() function takes the vectors x and y as inputs and returns the covariance between the two variables. The resulting covariance value will help us understand the relationship between the hours studied and the corresponding test scores. What this is particular example is saying is that for every unit increase in x there is a 29 unit increase in y."
  },
  {
    "objectID": "posts/2023-07-14/index.html#example-2-calculating-covariance-matrix",
    "href": "posts/2023-07-14/index.html#example-2-calculating-covariance-matrix",
    "title": "Covariance in R with the cov() Function",
    "section": "Example 2: Calculating Covariance Matrix",
    "text": "Example 2: Calculating Covariance Matrix\nNow let’s consider a scenario where we have multiple variables, and we want to calculate the covariance matrix to gain insights into their relationships.\n\n# Create example vectors\nx &lt;- c(5, 7, 3, 6, 8)\ny &lt;- c(65, 80, 50, 70, 90)\nz &lt;- c(150, 200, 100, 180, 220)\n\n# Combine vectors into a matrix\ndata &lt;- cbind(x, y, z)\n\n# Calculate covariance matrix\ncov_matrix &lt;- cov(data)\ncov_matrix\n\n     x   y    z\nx  3.7  29   90\ny 29.0 230  700\nz 90.0 700 2200\n\n\nIn this example, we have three variables, x, y, and z, representing hours studied, test scores, and total marks, respectively. We use the cbind() function to combine the vectors into a matrix called data. By applying the cov() function to this matrix, we obtain a covariance matrix that reveals the relationships between all the variables."
  },
  {
    "objectID": "posts/2023-07-17/index.html",
    "href": "posts/2023-07-17/index.html",
    "title": "Finding Duplicate Values in a Data Frame in R: A Guide Using Base R and dplyr",
    "section": "",
    "text": "Introduction\nIn data analysis and programming, it’s common to encounter situations where you need to identify duplicate values within a dataset. Whether you’re a beginner or an experienced programmer, knowing how to find duplicate values is a fundamental skill. In this blog post, we will explore two different approaches to accomplish this task using base R functions and the dplyr package in R. By the end, you’ll have a clear understanding of how to detect and manage duplicate values in your own datasets.\n\n\nUsing Base R Functions\nR provides a variety of functions for data manipulation and analysis, including those specifically designed for identifying duplicate values. Let’s consider a simple data frame to demonstrate this approach:\n\n# Creating a sample data frame\ndf &lt;- data.frame(\n  ID = c(1, 2, 3, 3, 4, 5),\n  Name = c(\"John\", \"Jane\", \"Mark\", \"Mark\", \"Luke\", \"Kate\"),\n  Age = c(25, 30, 35, 35, 40, 45)\n)\n\nTo find duplicate values in this data frame using base R functions, we can utilize the duplicated() and table() functions:\n\n# Using base R functions to find duplicate values\nduplicates &lt;- df[duplicated(df), ]\nduplicate_counts &lt;- table(df[duplicated(df), ])\n\nduplicates\n\n  ID Name Age\n4  3 Mark  35\n\nduplicate_counts\n\n, , Age = 35\n\n   Name\nID  Mark\n  3    1\n\n\nThe duplicated() function identifies the duplicate rows in the data frame, while the table() function creates a frequency table of the duplicate values. By combining these two functions, we can detect and examine the duplicate entries in the data frame.\n\n\nUsing dplyr\nThe dplyr package provides a powerful set of tools for data manipulation and analysis. Let’s see how we can accomplish the same task of finding duplicate values using dplyr functions:\n\n# loading the dplyr package\nlibrary(dplyr)\n\n# Using dplyr to find duplicate values\nduplicates &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\nduplicate_counts &lt;- df |&gt;\n  add_count(ID, Name, Age) |&gt;\n  filter(n &gt; 1) |&gt;\n  distinct()\n\nduplicates\n\n# A tibble: 2 × 3\n     ID Name    Age\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     3 Mark     35\n2     3 Mark     35\n\nduplicate_counts\n\n  ID Name Age n\n1  3 Mark  35 2\n\n\nLet’s break the first one down step by step:\nduplicates &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\ndf refers to a data frame in R.\ngroup_by_all() groups the data frame by all columns. This means that the subsequent operations will consider duplicate values across all columns.\nfilter(n() &gt; 1) filters the grouped data frame to only keep rows where the count (n()) of observations is greater than 1. In other words, it keeps only the rows that have duplicates.\nungroup() removes the grouping, ensuring that the resulting data frame is not grouped anymore.\nThe resulting data frame with duplicate rows is assigned to the variable duplicates.\n\nNow, let’s move on to the second part:\nduplicate_counts &lt;- df |&gt;\n  add_count(ID, Name, Age) |&gt;\n  filter(n &gt; 1) |&gt;\n  distinct()\n\nadd_count(ID, Name, Age) adds a new column called “n” to the data frame, which represents the count of observations for each combination of ID, Name, and Age.\nfilter(n &gt; 1) keeps only the rows where the count (“n”) is greater than 1. This retains only the rows that have duplicates based on the specified columns.\ndistinct() removes any duplicate rows that may still exist after the previous steps, keeping only unique rows.\nThe resulting data frame with duplicate counts and unique rows is assigned to the variable duplicate_counts.\n\nIn simple terms, the code first identifies and extracts the duplicate rows from the original data frame (df) and assigns them to duplicates. Then, it calculates the counts of duplicates based on specific columns (ID, Name, and Age) and stores the results, along with unique rows, in duplicate_counts.\nThese operations allow you to conveniently find duplicate rows and examine their counts within a data frame using both base R functions and some simple dplyr code.\n\n\nConclusion\nDetecting and managing duplicate values is an essential task in data analysis and programming. In this blog post, we explored two different approaches to find duplicate values in a data frame using base R functions and the dplyr package. By leveraging these techniques, you can efficiently identify and handle duplicate entries in your own datasets.\nI encourage you to practice using these methods on your own datasets. Familiarize yourself with the functions, experiment with different data frames, and explore various scenarios. This hands-on experience will deepen your understanding and improve your data analysis skills.\nRemember, the ability to identify and manage duplicate values is crucial for ensuring data integrity and obtaining accurate results in your data analysis projects. So go ahead, give it a try, and unlock the power of duplicate value detection in R!"
  },
  {
    "objectID": "posts/2023-07-18/index.html",
    "href": "posts/2023-07-18/index.html",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "",
    "text": "In data analysis and manipulation tasks, it’s common to encounter situations where we need to identify and handle duplicate rows in a dataset. In this blog post, we will explore three different approaches to finding duplicate rows in R: the base R method, the dplyr package, and the data.table package. We’ll compare their performance using the benchmark function and provide insights on when to use each approach. So, grab your coding gear, and let’s dive in!"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-1-base-rs-duplicated-function",
    "href": "posts/2023-07-18/index.html#approach-1-base-rs-duplicated-function",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 1: Base R’s duplicated Function",
    "text": "Approach 1: Base R’s duplicated Function\nThe simplest approach to finding duplicate rows is to use the duplicated function from base R. This function returns a logical vector indicating which rows are duplicates. We can apply it directly to our data frame df.\n\nduplicated_rows_base &lt;- duplicated(df)"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-2-dplyrs-concise-data-manipulation",
    "href": "posts/2023-07-18/index.html#approach-2-dplyrs-concise-data-manipulation",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 2: dplyr’s Concise Data Manipulation",
    "text": "Approach 2: dplyr’s Concise Data Manipulation\nThe dplyr package provides an intuitive and concise way to manipulate data frames. We can leverage its chaining syntax to filter the duplicated rows. The group_by_all function groups the data frame by all columns, and filter(n() &gt; 1) keeps only those rows with more than one occurrence within each group. Finally, ungroup removes the grouping information.\n\nduplicated_rows_dplyr &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-3-efficient-duplicate-detection-with-data.table",
    "href": "posts/2023-07-18/index.html#approach-3-efficient-duplicate-detection-with-data.table",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 3: Efficient Duplicate Detection with data.table",
    "text": "Approach 3: Efficient Duplicate Detection with data.table\nIf performance is a crucial factor, the data.table package offers highly optimized operations on large datasets. Converting our data frame to a data.table object allows us to utilize the efficient duplicated function from data.table.\n\ndtdf &lt;- data.table(df)\nduplicated_rows_datatable &lt;- duplicated(dtdf)\n\nBenchmarking and Performance Comparison: To evaluate the performance of the three approaches, we will use the benchmark function from the rbenchmark package. We’ll execute each approach ten times and collect information such as execution time (elapsed), relative performance, and CPU times (user.self and sys.self).\n\nbenchmark(\n  duplicated_rows_base = duplicated(df),\n  duplicated_rows_dplyr = df |&gt; \n    group_by_all() |&gt; \n    filter(n() &gt; 1) |&gt;\n    ungroup(),\n  duplicated_rows_datatable = duplicated(dtdf),\n  replications = 10,\n  columns = c(\"test\",\"replications\",\"elapsed\",\n              \"relative\",\"user.self\",\"sys.self\")\n) |&gt;\n  arrange(relative)\n\n                       test replications elapsed relative user.self sys.self\n1 duplicated_rows_datatable           10    0.05      1.0      0.01     0.01\n2     duplicated_rows_dplyr           10    0.29      5.8      0.27     0.02\n3      duplicated_rows_base           10    3.53     70.6      3.45     0.08"
  },
  {
    "objectID": "posts/2023-07-19/index.html",
    "href": "posts/2023-07-19/index.html",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "If you’re an aspiring data scientist or R programmer, you must be familiar with the powerful data structure called “lists.” Lists in R are collections of elements that can contain various data types such as vectors, matrices, data frames, or even other lists. They offer great flexibility and are widely used in many real-world scenarios.\nIn this blog post, we will explore one of the essential skills in working with lists: subsetting. Subsetting allows you to extract specific elements or portions of a list, helping you access and manipulate data efficiently. So, let’s dive into the world of list subsetting and learn some useful techniques along the way!\n\n\nBefore we start subsetting, let’s review how to access elements within a list. In R, you can access elements of a list using square brackets “[]” you can also use double square brackets “[[ ]]” or the dollar sign “$”. The double square brackets are used when you know the exact position of the element you want to extract, while the dollar sign is used when you know the name of the element.\n\n# Create a sample list\nmy_list &lt;- list(name = \"John\", age = 30, scores = c(85, 90, 78))\n\n# Access elements using double square brackets\nname &lt;- my_list[[1]]\nage &lt;- my_list[[2]]\nscores &lt;- my_list[[3]]\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n# Access elements using dollar sign\nname &lt;- my_list$name\nage &lt;- my_list$age\nscores &lt;- my_list$scores\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n\n\n\n\n\n\nSubsetting by position allows you to extract specific elements based on their index within the list. Remember, R uses 1-based indexing, so the first element is at position 1, the second at position 2, and so on.\n\n# Subsetting by position\nelement_1 &lt;- my_list[[1]]      # Extract the first element\nelement_2 &lt;- my_list[[2]]      # Extract the second element\nelement_last &lt;- my_list[[3]]   # Extract the last element\n\nelement_1\n\n[1] \"John\"\n\nelement_2\n\n[1] 30\n\nelement_last\n\n[1] 85 90 78\n\n# You can also use negative values to exclude elements\nexcluding_last &lt;- my_list[-3] # Exclude the last element\nexcluding_last\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\n\n\n\n\nSubsetting by name is particularly useful when you want to access elements using their names. It provides a more intuitive way to extract specific elements from a list.\n\n# Subsetting by name\nname &lt;- my_list[[\"name\"]]      # Extract the element with the name \"name\"\nscores &lt;- my_list[[\"scores\"]]  # Extract the element with the name \"scores\"\n\n# You can also use the dollar sign notation for name-based subsetting\nage &lt;- my_list$age\n\nname\n\n[1] \"John\"\n\nscores\n\n[1] 85 90 78\n\nage\n\n[1] 30\n\n\n\n\n\nYou can subset multiple elements at once using numeric or character vectors for positions or names, respectively.\n\n# Subsetting multiple elements by position\nelements_1_2 &lt;- my_list[c(1, 2)] # Extract the first and second elements\nelements_1_2\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\nelements_first_last &lt;- my_list[c(1, 3)] # Extract the first and last elements\nelements_first_last\n\n$name\n[1] \"John\"\n\n$scores\n[1] 85 90 78\n\n# Subsetting multiple elements by name\nelements_age_scores &lt;- my_list[c(\"age\", \"scores\")] # Extract elements with names \"age\" \n                                                   # and \"scores\"\nelements_age_scores\n\n$age\n[1] 30\n\n$scores\n[1] 85 90 78\n\n\n\n\n\nLists can contain other lists, creating a nested structure. To access elements within nested lists, you can use multiple “[[ ]]” or “$” operators.\n\n# Create a nested list\nnested_list &lt;- list(personal_info = my_list, hobbies = c(\"Reading\", \"Painting\"))\n\nnested_list\n\n$personal_info\n$personal_info$name\n[1] \"John\"\n\n$personal_info$age\n[1] 30\n\n$personal_info$scores\n[1] 85 90 78\n\n\n$hobbies\n[1] \"Reading\"  \"Painting\"\n\n# Access elements within nested lists\nname &lt;- nested_list[[\"personal_info\"]][[\"name\"]] # Extract the name from the nested list\nname\n\n[1] \"John\"\n\nsecond_hobby &lt;- nested_list[[\"hobbies\"]][[2]] # Extract the second \n                                              # hobby from the nested list\nsecond_hobby\n\n[1] \"Painting\"\n\n\n\n\n\n\nSubsetting lists in R is a fundamental skill that will prove invaluable in your data manipulation tasks. I encourage you to practice these techniques with your own data and explore more advanced subsetting methods, such as using logical conditions or applying functions to subsets.\nBy mastering list subsetting, you’ll unlock the true potential of R for data analysis and gain the confidence to handle complex data structures efficiently.\nSo, don’t hesitate! Dive into the world of list subsetting and enhance your R programming skills today. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-19/index.html#accessing-elements-in-a-list",
    "href": "posts/2023-07-19/index.html#accessing-elements-in-a-list",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Before we start subsetting, let’s review how to access elements within a list. In R, you can access elements of a list using square brackets “[]” you can also use double square brackets “[[ ]]” or the dollar sign “$”. The double square brackets are used when you know the exact position of the element you want to extract, while the dollar sign is used when you know the name of the element.\n\n# Create a sample list\nmy_list &lt;- list(name = \"John\", age = 30, scores = c(85, 90, 78))\n\n# Access elements using double square brackets\nname &lt;- my_list[[1]]\nage &lt;- my_list[[2]]\nscores &lt;- my_list[[3]]\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n# Access elements using dollar sign\nname &lt;- my_list$name\nage &lt;- my_list$age\nscores &lt;- my_list$scores\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78"
  },
  {
    "objectID": "posts/2023-07-19/index.html#subsetting-list-elements",
    "href": "posts/2023-07-19/index.html#subsetting-list-elements",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Subsetting by position allows you to extract specific elements based on their index within the list. Remember, R uses 1-based indexing, so the first element is at position 1, the second at position 2, and so on.\n\n# Subsetting by position\nelement_1 &lt;- my_list[[1]]      # Extract the first element\nelement_2 &lt;- my_list[[2]]      # Extract the second element\nelement_last &lt;- my_list[[3]]   # Extract the last element\n\nelement_1\n\n[1] \"John\"\n\nelement_2\n\n[1] 30\n\nelement_last\n\n[1] 85 90 78\n\n# You can also use negative values to exclude elements\nexcluding_last &lt;- my_list[-3] # Exclude the last element\nexcluding_last\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\n\n\n\n\nSubsetting by name is particularly useful when you want to access elements using their names. It provides a more intuitive way to extract specific elements from a list.\n\n# Subsetting by name\nname &lt;- my_list[[\"name\"]]      # Extract the element with the name \"name\"\nscores &lt;- my_list[[\"scores\"]]  # Extract the element with the name \"scores\"\n\n# You can also use the dollar sign notation for name-based subsetting\nage &lt;- my_list$age\n\nname\n\n[1] \"John\"\n\nscores\n\n[1] 85 90 78\n\nage\n\n[1] 30\n\n\n\n\n\nYou can subset multiple elements at once using numeric or character vectors for positions or names, respectively.\n\n# Subsetting multiple elements by position\nelements_1_2 &lt;- my_list[c(1, 2)] # Extract the first and second elements\nelements_1_2\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\nelements_first_last &lt;- my_list[c(1, 3)] # Extract the first and last elements\nelements_first_last\n\n$name\n[1] \"John\"\n\n$scores\n[1] 85 90 78\n\n# Subsetting multiple elements by name\nelements_age_scores &lt;- my_list[c(\"age\", \"scores\")] # Extract elements with names \"age\" \n                                                   # and \"scores\"\nelements_age_scores\n\n$age\n[1] 30\n\n$scores\n[1] 85 90 78\n\n\n\n\n\nLists can contain other lists, creating a nested structure. To access elements within nested lists, you can use multiple “[[ ]]” or “$” operators.\n\n# Create a nested list\nnested_list &lt;- list(personal_info = my_list, hobbies = c(\"Reading\", \"Painting\"))\n\nnested_list\n\n$personal_info\n$personal_info$name\n[1] \"John\"\n\n$personal_info$age\n[1] 30\n\n$personal_info$scores\n[1] 85 90 78\n\n\n$hobbies\n[1] \"Reading\"  \"Painting\"\n\n# Access elements within nested lists\nname &lt;- nested_list[[\"personal_info\"]][[\"name\"]] # Extract the name from the nested list\nname\n\n[1] \"John\"\n\nsecond_hobby &lt;- nested_list[[\"hobbies\"]][[2]] # Extract the second \n                                              # hobby from the nested list\nsecond_hobby\n\n[1] \"Painting\""
  },
  {
    "objectID": "posts/2023-07-19/index.html#explore-further",
    "href": "posts/2023-07-19/index.html#explore-further",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Subsetting lists in R is a fundamental skill that will prove invaluable in your data manipulation tasks. I encourage you to practice these techniques with your own data and explore more advanced subsetting methods, such as using logical conditions or applying functions to subsets.\nBy mastering list subsetting, you’ll unlock the true potential of R for data analysis and gain the confidence to handle complex data structures efficiently.\nSo, don’t hesitate! Dive into the world of list subsetting and enhance your R programming skills today. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-20/index.html",
    "href": "posts/2023-07-20/index.html",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "",
    "text": "If you’ve been working with R for some time, you might have come across situations where your code becomes cumbersome due to repetitive references to data frames or list elements. Luckily, R provides two powerful functions, with() and within(), to help you streamline your code and make it more readable. These functions offer a simple and elegant solution for manipulating data frames and lists. In this blog post, we’ll explore the syntax of these functions and provide several real-world examples to demonstrate their usefulness. So, let’s dive in and discover how with() and within() can become your new best friends in R programming!"
  },
  {
    "objectID": "posts/2023-07-20/index.html#with-syntax",
    "href": "posts/2023-07-20/index.html#with-syntax",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "with() syntax:",
    "text": "with() syntax:\nwith(data, expr)\n\ndata: The data frame or list you want to use as an environment within the expression (expr).\nexpr: The expression where you can refer to data frame/list elements directly, without prefixing them with the data name.\n\n\nwithin(): The within() function is similar to with(), but it modifies the data frame or list in place and returns the modified object."
  },
  {
    "objectID": "posts/2023-07-20/index.html#within-syntax",
    "href": "posts/2023-07-20/index.html#within-syntax",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "within() syntax:",
    "text": "within() syntax:\nwithin(data, expr)\n\ndata: The data frame or list you want to modify within the expression (expr).\nexpr: The expression where you can manipulate data frame/list elements directly, without prefixing them with the data name.\n\nNow that we know the basics, let’s explore some examples to see these functions in action."
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-1-simplifying-data-manipulation-with-with",
    "href": "posts/2023-07-20/index.html#example-1-simplifying-data-manipulation-with-with",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 1: Simplifying Data Manipulation with with()",
    "text": "Example 1: Simplifying Data Manipulation with with()\nSuppose we have a data frame containing information about employees and their salaries:\n\n# Sample data frame\nemployee_data &lt;- data.frame(\n  name = c(\"John\", \"Jane\", \"Michael\", \"Sara\"),\n  age = c(32, 28, 45, 37),\n  salary = c(50000, 60000, 75000, 62000)\n)\n\nemployee_data\n\n     name age salary\n1    John  32  50000\n2    Jane  28  60000\n3 Michael  45  75000\n4    Sara  37  62000\n\n\nWithout with(), calculating the average salary of employees would require repetitive references to the data frame:\n\n# Without with()\navg_salary &lt;- mean(employee_data$salary)\navg_salary\n\n[1] 61750\n\n\nHowever, with the with() function, we can write the same code more concisely:\n\n# With with()\navg_salary &lt;- with(employee_data, mean(salary))\navg_salary\n\n[1] 61750"
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-2-simplifying-data-transformation-with-within",
    "href": "posts/2023-07-20/index.html#example-2-simplifying-data-transformation-with-within",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 2: Simplifying Data Transformation with within()",
    "text": "Example 2: Simplifying Data Transformation with within()\nLet’s consider a scenario where we want to create a new column bonus for employees based on their age:\n\n# Without within()\nemployee_data$bonus &lt;- ifelse(employee_data$age &gt;= 35, 5000, 3000)\nemployee_data\n\n     name age salary bonus\n1    John  32  50000  3000\n2    Jane  28  60000  3000\n3 Michael  45  75000  5000\n4    Sara  37  62000  5000\n\n\nBy using within(), we can modify the data frame directly without repetitive references:\n\n# With within()\nemployee_data &lt;- within(employee_data, bonus &lt;- ifelse(age &gt;= 45, 5000, 3000))\nemployee_data\n\n     name age salary bonus\n1    John  32  50000  3000\n2    Jane  28  60000  3000\n3 Michael  45  75000  5000\n4    Sara  37  62000  3000"
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-3-simplifying-plotting-with-with",
    "href": "posts/2023-07-20/index.html#example-3-simplifying-plotting-with-with",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 3: Simplifying Plotting with with()",
    "text": "Example 3: Simplifying Plotting with with()\nWhen creating visualizations, with() can help you avoid prefixing data frame column names repeatedly. Let’s generate a scatter plot of employee age versus salary:\n\n# Without with()\nplot(\n  employee_data$salary, \n  employee_data$age, \n  xlab = \"Salary\", \n  ylab = \"Age\", \n  main = \"Age vs. Salary\"\n  )\n\n\n\n\nUsing with(), we can eliminate the repetition:\n\n# With with()\nwith(\n  employee_data, \n  plot(\n    salary, \n    age, \n    xlab = \"Salary\", \n    ylab = \"Age\", \n    main = \"Age vs. Salary\"\n    )\n  )\n\n\n\n\nHere are some additional examples of how to use the with() and within() functions. To calculate the mean of the values in the x column of the data data frame, you would use the following code:\nwith(data, mean(x))\nTo create a new data frame that contains the mean of the values in each column, you would use the following code:\nnew_data &lt;- within(data, {\n  for (column in names(data)) {\n    column_mean &lt;- mean(data[[column]])\n    data[[column]] &lt;- column_mean\n  }\n})\n\nnew_data\nTo filter the data data frame to only include rows where the value in the x column is greater than 1, you would use the following code:\nnew_data &lt;- within(data, {\n  new_data &lt;- data[data$x &gt; 1, ]\n})\n\nnew_data"
  },
  {
    "objectID": "posts/2023-07-20/index.html#conclusion",
    "href": "posts/2023-07-20/index.html#conclusion",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored two powerful R functions: with() and within(). These functions provide an elegant way to manipulate data frames and lists by reducing repetitive references and simplifying your code. By leveraging the capabilities of with() and within(), you can write more readable and efficient code. I encourage you to try out these functions in your R projects and experience the benefits firsthand. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-21/index.html",
    "href": "posts/2023-07-21/index.html",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "",
    "text": "As a programmer in R, you’ll often find yourself working with textual data and need to manipulate or display it in various ways. Two essential functions at your disposal for these tasks are paste() and cat(). These functions are powerful tools that allow you to combine and display text easily and efficiently. In this blog post, we’ll explore the syntax, similarities, and differences between these functions and provide you with practical examples to get you started. Let’s dive in!"
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-1-basic-concatenation",
    "href": "posts/2023-07-21/index.html#example-1-basic-concatenation",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 1: Basic Concatenation",
    "text": "Example 1: Basic Concatenation\n\nfruit1 &lt;- \"apple\"\nfruit2 &lt;- \"orange\"\nresult &lt;- paste(fruit1, fruit2)\nprint(result) # Output: \"apple orange\"\n\n[1] \"apple orange\""
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-2-using-different-separator",
    "href": "posts/2023-07-21/index.html#example-2-using-different-separator",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 2: Using Different Separator",
    "text": "Example 2: Using Different Separator\n\nmonths &lt;- c(\"January\", \"February\", \"March\")\nresult &lt;- paste(months, collapse = \", \")\nprint(result) # Output: \"January, February, March\"\n\n[1] \"January, February, March\""
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-1-basic-concatenation-and-display",
    "href": "posts/2023-07-21/index.html#example-1-basic-concatenation-and-display",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 1: Basic Concatenation and Display",
    "text": "Example 1: Basic Concatenation and Display\n\nfruit1 &lt;- \"apple\"\nfruit2 &lt;- \"orange\"\ncat(\"My favorite fruits are\", fruit1, \"and\", fruit2) # Output: \"My favorite fruits are apple and orange\"\n\nMy favorite fruits are apple and orange"
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-2-output-to-file",
    "href": "posts/2023-07-21/index.html#example-2-output-to-file",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 2: Output to File",
    "text": "Example 2: Output to File\nnumbers &lt;- 1:5\nfile_path &lt;- \"numbers.txt\"\ncat(numbers, file = file_path)\n# The content of \"numbers.txt\": 1 2 3 4 5"
  },
  {
    "objectID": "posts/2023-07-24/index.html",
    "href": "posts/2023-07-24/index.html",
    "title": "How to Calculate Percentage by Group in R using Base R, dplyr, and data.table",
    "section": "",
    "text": "Calculating percentages by group is a common task in data analysis. It allows you to understand the distribution of data within different categories. In this blog post, we’ll walk you through the process of calculating percentages by group using three popular R packages: Base R, dplyr, and data.table. To keep things simple, we will use the well-known Iris dataset.\nThe Iris dataset contains information about different species of iris flowers and their measurements, including sepal length, sepal width, petal length, and petal width. We will focus on the ‘Species’ column and calculate the percentage of each species in the dataset."
  },
  {
    "objectID": "posts/2023-07-24/index.html#example-1-using-base-r",
    "href": "posts/2023-07-24/index.html#example-1-using-base-r",
    "title": "How to Calculate Percentage by Group in R using Base R, dplyr, and data.table",
    "section": "Example 1: Using Base R",
    "text": "Example 1: Using Base R\nStep 1: Load the Iris dataset\n\n# Load the Iris dataset\ndata(iris)\n\nStep 2: Calculate the counts by group\n\n# Use the table() function to get the counts of each species\ngroup_counts &lt;- table(iris$Species)\n\nStep 3: Calculate the total count\n\n# Calculate the total count using the sum() function\ntotal_count &lt;- sum(group_counts)\n\nStep 4: Calculate the percentage by group\n\n# Divide each count by the total count and multiply by 100 to get the percentage\npercentage_by_group &lt;- (group_counts / total_count) * 100\n\nStep 5: Combine group names and percentages into a data frame and display the result\n\n# Combine group names and percentages into a data frame\nresult_base_R &lt;- data.frame(\n  Species = names(percentage_by_group), \n  Percentage = percentage_by_group\n  )\n\n# Print the result\nprint(result_base_R)\n\n     Species Percentage.Var1 Percentage.Freq\n1     setosa          setosa        33.33333\n2 versicolor      versicolor        33.33333\n3  virginica       virginica        33.33333"
  },
  {
    "objectID": "posts/2023-07-24/index.html#example-2-using-dplyr",
    "href": "posts/2023-07-24/index.html#example-2-using-dplyr",
    "title": "How to Calculate Percentage by Group in R using Base R, dplyr, and data.table",
    "section": "Example 2: Using dplyr",
    "text": "Example 2: Using dplyr\nStep 1: Load the necessary library and the Iris dataset\n\n# Load the dplyr library\nlibrary(dplyr)\n\n# Load the Iris dataset\ndata(iris)\n\nStep 2: Calculate the percentage by group using dplyr\n\n# Use the group_by() and summarise() functions to calculate percentages\nresult_dplyr &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(Percentage = n() / nrow(iris) * 100)\n\nStep 3: Display the result\n\n# Print the result\nprint(result_dplyr)\n\n# A tibble: 3 × 2\n  Species    Percentage\n  &lt;fct&gt;           &lt;dbl&gt;\n1 setosa           33.3\n2 versicolor       33.3\n3 virginica        33.3"
  },
  {
    "objectID": "posts/2023-07-24/index.html#example-3-using-data.table",
    "href": "posts/2023-07-24/index.html#example-3-using-data.table",
    "title": "How to Calculate Percentage by Group in R using Base R, dplyr, and data.table",
    "section": "Example 3: Using data.table:",
    "text": "Example 3: Using data.table:\nStep 1: Load the necessary library and the Iris dataset\n\n# Load the data.table library\nlibrary(data.table)\n\n# Convert the Iris dataset to a data.table\niris_dt &lt;- as.data.table(iris)\n\nStep 2: Calculate the percentage by group using data.table\n\n# Use the .N special symbol to calculate counts and by-reference to save memory\nresult_data_table &lt;- iris_dt[, .(Percentage = .N / nrow(iris_dt) * 100), by = Species]\n\nStep 3: Display the result\n\n# Print the result\nprint(result_data_table)\n\n      Species Percentage\n1:     setosa   33.33333\n2: versicolor   33.33333\n3:  virginica   33.33333"
  },
  {
    "objectID": "posts/2023-07-25/index.html",
    "href": "posts/2023-07-25/index.html",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "",
    "text": "As a programmer and data enthusiast, you know that summarizing data is essential to gain insights into its distribution and characteristics. R, being a powerful and versatile programming language for data analysis, offers various functions to aid in this process. One such function that stands out is fivenum(), a hidden gem that computes the five-number summary of a dataset. In this blog post, we will explore the fivenum() function and demonstrate how to leverage it for different scenarios, empowering you to unlock valuable insights from your datasets.\nThe five number summary is a concise way to summarize the distribution of a data set. It consists of the following five values:\n\nThe minimum value\nThe first quartile (Q1)\nThe median\nThe third quartile (Q3)\nThe maximum value\n\nThe minimum value is the smallest value in the data set. The first quartile (Q1) is the value below which 25% of the data points lie. The median is the value below which 50% of the data points lie. The third quartile (Q3) is the value below which 75% of the data points lie. The maximum value is the largest value in the data set.\nThe five number summary can be used to get a quick overview of the distribution of a data set. It can tell us how spread out the data is, whether the data is skewed, and whether there are any outliers."
  },
  {
    "objectID": "posts/2023-07-25/index.html#example-1.-a-vector",
    "href": "posts/2023-07-25/index.html#example-1.-a-vector",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "Example 1. A Vector:",
    "text": "Example 1. A Vector:\nLet’s start with the basics. To compute the five-number summary for a vector in R, all you need is the fivenum() function and your data. For example:\n\n# Sample vector\ndata_vector &lt;- c(12, 24, 36, 48, 60, 72, 84, 96, 108, 120)\n\n# Calculate the five-number summary\nsummary_vector &lt;- fivenum(data_vector)\n\n# Output the results\nprint(summary_vector)\n\n[1]  12  36  66  96 120\n\n\nThe fivenum() function will return the minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum values of the vector. Armed with this information, you can easily visualize the dataset’s distribution using box plots, histograms, or other graphical representations."
  },
  {
    "objectID": "posts/2023-07-25/index.html#example-2.-with-boxplot",
    "href": "posts/2023-07-25/index.html#example-2.-with-boxplot",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "Example 2. With boxplot():",
    "text": "Example 2. With boxplot():\nBox plots, also known as box-and-whisker plots, are a fantastic visualization tool to display the distribution and identify outliers in your data. When combined with fivenum(), you can create insightful box plots with minimal effort. Consider this example:\n\n# Sample vector\ndata_vector &lt;- c(12, 24, 36, 48, 60, 72, 84, 96, 108, 120)\n\n# Create a box plot\nboxplot(data_vector)\n\n\n\n# Calculate the five-number summary and print the results\nsummary_vector &lt;- fivenum(data_vector)\nprint(summary_vector)\n\n[1]  12  36  66  96 120\n\n\nBy incorporating the fivenum() function, you can see the minimum, lower hinge (Q1), median (Q2), upper hinge (Q3), and maximum, represented in the box plot. This graphical representation helps in visualizing the spread of the data, presence of outliers, and skewness."
  },
  {
    "objectID": "posts/2023-07-25/index.html#example-3.-on-a-column-in-a-data.frame",
    "href": "posts/2023-07-25/index.html#example-3.-on-a-column-in-a-data.frame",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "Example 3. On a Column in a Data.frame:",
    "text": "Example 3. On a Column in a Data.frame:\nOften, data is stored in data.frames, which are highly efficient for handling and analyzing datasets. To apply fivenum() on a specific column within a data.frame, use the $ operator to access the desired column. Consider the following example:\n\n# Sample data.frame\ndata_df &lt;- data.frame(ID = 1:5,\n                      Age = c(25, 30, 22, 28, 35))\n\n# Calculate the five-number summary for the \"Age\" column\nsummary_age &lt;- fivenum(data_df$Age)\n\n# Output the results\nprint(summary_age)\n\n[1] 22 25 28 30 35\n\n\nBy applying fivenum() on the “Age” column, you obtain the five-number summary, which reveals valuable information about the age distribution of the dataset."
  },
  {
    "objectID": "posts/2023-07-25/index.html#example-4.-across-multiple-columns-of-a-data.frame-using-sapply",
    "href": "posts/2023-07-25/index.html#example-4.-across-multiple-columns-of-a-data.frame-using-sapply",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "Example 4. Across Multiple Columns of a Data.frame Using sapply():",
    "text": "Example 4. Across Multiple Columns of a Data.frame Using sapply():\nTo elevate your data analysis game, you’ll often need to summarize multiple columns simultaneously. In this case, sapply() comes in handy, allowing you to apply fivenum() across several columns at once. Let’s take a look at an example:\n\n# Sample data.frame\ndata_df &lt;- data.frame(ID = 1:5,\n                      Age = c(25, 30, 22, 28, 35),\n                      Salary = c(50000, 60000, 45000, 55000, 70000))\n\n# Apply fivenum() on all numeric columns\nsummary_all_columns &lt;- sapply(data_df[, 2:3], fivenum)\n\n# Output the results\nprint(summary_all_columns)\n\n     Age Salary\n[1,]  22  45000\n[2,]  25  50000\n[3,]  28  55000\n[4,]  30  60000\n[5,]  35  70000\n\n\nIn this example, sapply() is used to calculate the five-number summary for the “Age” and “Salary” columns simultaneously. The output provides a comprehensive summary of these columns, enabling you to quickly assess the distribution of each."
  },
  {
    "objectID": "posts/2023-07-26/index.html",
    "href": "posts/2023-07-26/index.html",
    "title": "Summarizing Data in R: tapply() vs. group_by() and summarize()",
    "section": "",
    "text": "Are you tired of manually calculating summary statistics for your data in R? Look no further! In this blog post, we will explore two powerful ways to summarize data: using the tapply() function and the group_by() and summarize() functions from the dplyr package. Both methods are incredibly useful and can save you time and effort in your data analysis projects."
  },
  {
    "objectID": "posts/2023-07-26/index.html#example-1-summarizing-a-numeric-vector-with-tapply",
    "href": "posts/2023-07-26/index.html#example-1-summarizing-a-numeric-vector-with-tapply",
    "title": "Summarizing Data in R: tapply() vs. group_by() and summarize()",
    "section": "Example 1: Summarizing a Numeric Vector with tapply()",
    "text": "Example 1: Summarizing a Numeric Vector with tapply()\nSuppose you have a dataset with students’ exam scores and their corresponding grades. You want to calculate the average score for each grade.\n\n# Sample data\nscores &lt;- c(85, 90, 78, 92, 88, 76, 84, 92, 95, 89)\ngrades &lt;- c(\"A\", \"A\", \"B\", \"A\", \"B\", \"C\", \"B\", \"A\", \"A\", \"B\")\n\n# Using tapply() to calculate the average score for each grade\navg_scores &lt;- tapply(scores, grades, mean)\n\nprint(avg_scores)\n\n    A     B     C \n90.80 84.75 76.00 \n\n\nOr using the built in iris dataset:\n\nmean_width_by_species &lt;- tapply(iris$Sepal.Width, iris$Species, mean)\n\nprint(mean_width_by_species)\n\n    setosa versicolor  virginica \n     3.428      2.770      2.974 \n\n\nIn this example, tapply() splits the scores vector based on the different grades in the grades vector and calculates the average score for each grade. The same type of thing is done with the second example, splitting the data by Species."
  },
  {
    "objectID": "posts/2023-07-26/index.html#example-2-summarizing-a-data-frame-with-group_by-and-summarize",
    "href": "posts/2023-07-26/index.html#example-2-summarizing-a-data-frame-with-group_by-and-summarize",
    "title": "Summarizing Data in R: tapply() vs. group_by() and summarize()",
    "section": "Example 2: Summarizing a Data Frame with group_by() and summarize()",
    "text": "Example 2: Summarizing a Data Frame with group_by() and summarize()\nSuppose you have a dataset with information about employees, including their department, salary, and years of experience. You want to find the average salary and the maximum years of experience for each department.\nThe group_by() and summarize() functions from the dplyr package provide a more concise way to summarize data. The syntax for these functions is as follows:\ndata %&gt;%\n  group_by(INDEX) %&gt;%\n  summarize(FUN(...))\nWhere:\n\ndata is the data frame that you want to summarize.\nINDEX is the vector that you want to group by.\nFUN is the function that you want to apply to data.\n... are additional arguments that you want to pass to FUN.\n\n\n# Assuming you have already installed and loaded the 'dplyr' package\nlibrary(dplyr)\n\n# Sample data frame\nemployees &lt;- data.frame(\n  department = c(\"HR\", \"Engineering\", \"HR\", \"Engineering\", \"Marketing\", \"Marketing\"),\n  salary = c(50000, 65000, 48000, 70000, 55000, 60000),\n  experience = c(3, 5, 2, 7, 4, 6)\n)\n\n# Using group_by() and summarize() to calculate average salary \n# and max experience by department\nsummary_data &lt;- employees %&gt;%\n  group_by(department) %&gt;%\n  summarize(\n    avg_salary = mean(salary), \n    max_experience = max(experience)\n  )\n\nprint(summary_data)\n\n# A tibble: 3 × 3\n  department  avg_salary max_experience\n  &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Engineering      67500              7\n2 HR               49000              3\n3 Marketing        57500              6\n\n\nThe group_by() function groups the data by the department variable, and then summarize() calculates the average salary and maximum years of experience for each group.\nNow let’s also see how the functions can produce the same results and what it looks like side by side:\n\ntapply(iris$Sepal.Width, iris$Species, mean)\n\n    setosa versicolor  virginica \n     3.428      2.770      2.974 \n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(mean_width = mean(Sepal.Width))\n\n# A tibble: 3 × 2\n  Species    mean_width\n  &lt;fct&gt;           &lt;dbl&gt;\n1 setosa           3.43\n2 versicolor       2.77\n3 virginica        2.97"
  },
  {
    "objectID": "posts/2023-07-27/index.html",
    "href": "posts/2023-07-27/index.html",
    "title": "Unleashing the Power of Cumulative Mean in R: A Step-by-Step Guide",
    "section": "",
    "text": "As data-driven decision-making continues to shape our world, the need for insightful statistical analysis becomes ever more apparent. One crucial tool in a programmer’s arsenal is the “cumulative mean,” a statistical measure that allows us to understand the average value of a dataset as it evolves over time. In this blog post, we will delve into what a cumulative mean is, explore its applications, and equip you with the knowledge to unleash its potential using base R."
  },
  {
    "objectID": "posts/2023-07-27/index.html#example-1-finding-the-cumulative-mean-of-a-simple-vector",
    "href": "posts/2023-07-27/index.html#example-1-finding-the-cumulative-mean-of-a-simple-vector",
    "title": "Unleashing the Power of Cumulative Mean in R: A Step-by-Step Guide",
    "section": "Example 1: Finding the Cumulative Mean of a Simple Vector",
    "text": "Example 1: Finding the Cumulative Mean of a Simple Vector\n\n# Step 1: Create the 'data' vector\ndata &lt;- c(2, 4, 6, 8, 10)\n\n# Step 2: Calculate the cumulative sum\ncumulative_sum &lt;- cumsum(data)\n\n# Step 3: Calculate the cumulative mean\ncumulative_mean &lt;- cumulative_sum / seq_along(data)\n\n# Display the result\ncumulative_mean\n\n[1] 2 3 4 5 6"
  },
  {
    "objectID": "posts/2023-07-27/index.html#example-2-applying-cumulative-mean-to-real-world-data",
    "href": "posts/2023-07-27/index.html#example-2-applying-cumulative-mean-to-real-world-data",
    "title": "Unleashing the Power of Cumulative Mean in R: A Step-by-Step Guide",
    "section": "Example 2: Applying Cumulative Mean to Real-World Data",
    "text": "Example 2: Applying Cumulative Mean to Real-World Data\nLet’s use the cumulative mean to analyze monthly website traffic data.\n\n# Step 1: Create the 'monthly_traffic' vector\nmonthly_traffic &lt;- c(100, 200, 300, 400, 500)\n\n# Step 2: Calculate the cumulative sum\ncumulative_sum &lt;- cumsum(monthly_traffic)\n\n# Step 3: Calculate the cumulative mean\ncumulative_mean &lt;- cumulative_sum / seq_along(monthly_traffic)\n\n# Display the result\ncumulative_mean\n\n[1] 100 150 200 250 300\n\n\nHere are some more examples of how you might want to use a cumulative mean in R:\n\nTo track the average stock price over time\nTo track the average temperature over a period of days\nTo track the average number of visitors to a website over a period of weeks"
  },
  {
    "objectID": "posts/2023-07-28/index.html",
    "href": "posts/2023-07-28/index.html",
    "title": "The intersect() function in R",
    "section": "",
    "text": "Welcome to another exciting blog post where we delve into the world of R programming. Today, we’ll be discussing the intersect() function, a handy tool that helps us find the common elements shared between two or more vectors in R. Whether you’re a seasoned R programmer or just starting your journey, this function is sure to become a valuable addition to your toolkit."
  },
  {
    "objectID": "posts/2023-07-28/index.html#example-1-finding-common-elements",
    "href": "posts/2023-07-28/index.html#example-1-finding-common-elements",
    "title": "The intersect() function in R",
    "section": "Example 1: Finding Common Elements",
    "text": "Example 1: Finding Common Elements\nSuppose we have two vectors, vec1 and vec2, with some elements in common. We want to find those common elements:\n\nvec1 &lt;- c(1, 3, 5, 7, 9)\nvec2 &lt;- c(2, 4, 6, 8, 5, 10)\n\ncommon_elements &lt;- intersect(vec1, vec2)\ncommon_elements\n\n[1] 5\n\n\n\nExplanation\nIn this example, we have two vectors, vec1 and vec2. The intersect() function takes these two vectors as input and identifies the common element between them, which is 5. The function returns a new vector with only the common element."
  },
  {
    "objectID": "posts/2023-07-28/index.html#example-2-removing-duplicates",
    "href": "posts/2023-07-28/index.html#example-2-removing-duplicates",
    "title": "The intersect() function in R",
    "section": "Example 2: Removing Duplicates",
    "text": "Example 2: Removing Duplicates\nThe intersect() function can also be used to remove duplicates from a single vector:\n\nrepeated_vec &lt;- c(1, 2, 3, 4, 1, 2, 5, 6)\n\nunique_elements &lt;- intersect(repeated_vec, repeated_vec)\nunique_elements\n\n[1] 1 2 3 4 5 6\n\n\n\nExplanation\nIn this example, we have a vector repeated_vec with some duplicate elements. By using intersect() with the same vector twice, the function effectively removes all duplicates, giving us a new vector with only unique elements."
  },
  {
    "objectID": "posts/2023-07-28/index.html#example-3-empty-intersection",
    "href": "posts/2023-07-28/index.html#example-3-empty-intersection",
    "title": "The intersect() function in R",
    "section": "Example 3: Empty Intersection",
    "text": "Example 3: Empty Intersection\nIf the input vectors have no common elements, the intersect() function will return an empty vector:\n\nvec3 &lt;- c(11, 22, 33)\nvec4 &lt;- c(44, 55, 66)\n\nempty_intersection &lt;- intersect(vec3, vec4)\nempty_intersection\n\nnumeric(0)\n\n\n\nExplanation\nIn this case, vec3 and vec4 have no elements in common. Thus, the intersect() function returns an empty numeric vector (numeric(0))."
  },
  {
    "objectID": "posts/2023-07-28/index.html#example-4-using-strings",
    "href": "posts/2023-07-28/index.html#example-4-using-strings",
    "title": "The intersect() function in R",
    "section": "Example 4: Using Strings",
    "text": "Example 4: Using Strings\nHere is a final example using strings:\n\nintersect(c(\"apple\", \"banana\", \"cherry\"), c(\"banana\", \"cherry\", \"grape\"))\n\n[1] \"banana\" \"cherry\""
  },
  {
    "objectID": "posts/2023-07-31/index.html",
    "href": "posts/2023-07-31/index.html",
    "title": "The replicate() function in R",
    "section": "",
    "text": "As a programmer, you must have encountered situations where you need to repeat a task multiple times. Repetitive tasks are not only tedious but also prone to errors. What if I tell you there’s an elegant solution to this problem in R? Enter the replicate() function, your ultimate ally when it comes to replicating tasks effortlessly and efficiently."
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-1-generating-random-data",
    "href": "posts/2023-07-31/index.html#example-1-generating-random-data",
    "title": "The replicate() function in R",
    "section": "Example 1: Generating Random Data",
    "text": "Example 1: Generating Random Data\nSuppose you need to simulate a dataset for testing purposes or to understand the behavior of a statistical model. You can easily create 10 random samples, each containing 5 values, from a standard normal distribution using replicate():\n\n# Generate 10 random samples with 5 values each\nrandom_samples &lt;- replicate(10, rnorm(5))\nrandom_samples\n\n           [,1]       [,2]        [,3]       [,4]       [,5]       [,6]\n[1,]  0.5268093 -0.5237928  0.85010590  0.7289362  0.8444399  0.4592547\n[2,] -0.6796813  1.3037502 -1.18353409  0.5008129  0.2064732 -1.2990195\n[3,] -2.0398061 -1.2456373 -0.21356106 -0.3625780  0.1002410 -0.2273825\n[4,]  1.1870052  0.7734783 -0.32729379  2.0315941 -1.1789518 -0.2668686\n[5,] -1.1664056 -2.1379542  0.02431003  1.4414115 -0.7040298  0.7619186\n            [,7]       [,8]       [,9]      [,10]\n[1,] -0.89963532  0.4878779  0.4429697 -2.2369269\n[2,]  1.23571839  1.0790711 -1.4933201  1.4367740\n[3,]  0.06225175  0.1443591 -1.1423172 -0.6037171\n[4,] -1.37931063 -2.0674399 -1.8445978 -0.8033205\n[5,]  1.01821474  1.2571034  0.4151621  1.0140082\n\n\nIn this example, rnorm(5) generates five random values from a standard normal distribution, and replicate(10, ...) repeats this process 10 times, resulting in a matrix with 10 columns and 5 rows."
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-2-rolling-dice-with-replicate",
    "href": "posts/2023-07-31/index.html#example-2-rolling-dice-with-replicate",
    "title": "The replicate() function in R",
    "section": "Example 2: Rolling Dice with Replicate",
    "text": "Example 2: Rolling Dice with Replicate\nLet’s say you want to simulate rolling a fair six-sided die 20 times. With replicate(), you can easily simulate the rolls and get the results in a single line of code:\n\n# Simulate rolling a die 20 times\ndie_rolls &lt;- replicate(20, sample(1:6, 1, replace = TRUE))\ndie_rolls\n\n [1] 2 2 3 3 2 2 6 4 1 4 4 1 1 2 6 2 3 5 2 2\n\n\nIn this case, sample(1:6, 1, replace = TRUE) randomly selects one value from the sequence 1 to 6, simulating a single die roll. replicate(20, ...) repeats this simulation 20 times, giving you a vector of 20 die roll results."
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-3-evaluating-an-expression-multiple-times",
    "href": "posts/2023-07-31/index.html#example-3-evaluating-an-expression-multiple-times",
    "title": "The replicate() function in R",
    "section": "Example 3: Evaluating an Expression Multiple Times",
    "text": "Example 3: Evaluating an Expression Multiple Times\nConsider a scenario where you want to calculate the sum of squares for the numbers 1 to 5. Instead of manually typing out the expression five times, you can use replicate() to handle the repetition for you:\n\n# Calculate sum of squares for numbers 1 to 5\nsum_of_squares &lt;- replicate(5, sum((1:5)^2))\nsum_of_squares\n\n[1] 55 55 55 55 55\n\n\nHere, (1:5)^2 squares each number from 1 to 5, and sum(...) calculates the sum of these squared values. replicate(5, ...) repeats this process five times, giving you the sum of squares for each repetition."
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-4-generate-100-samples-of-a-binomial-distribution",
    "href": "posts/2023-07-31/index.html#example-4-generate-100-samples-of-a-binomial-distribution",
    "title": "The replicate() function in R",
    "section": "Example 4: Generate 100 samples of a binomial distribution",
    "text": "Example 4: Generate 100 samples of a binomial distribution\nTo generate 10 samples of size 100 from a binomial distribution with probability 0.5, you could use the following code:\n\nreplicate(10, sample(0:1, 100, replace = TRUE, prob = c(0.5, 0.5))) |&gt;\n  head(10)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    0    0    0    1    1    0    1    1    1     1\n [2,]    1    0    1    1    1    0    1    1    1     1\n [3,]    1    1    1    1    0    0    1    0    1     1\n [4,]    0    1    0    0    1    0    1    1    1     0\n [5,]    1    0    0    1    0    1    1    0    0     0\n [6,]    0    1    0    1    0    0    1    1    0     1\n [7,]    1    1    0    1    1    1    1    1    1     1\n [8,]    1    1    0    1    1    1    0    1    0     0\n [9,]    1    0    0    1    1    1    0    1    1     1\n[10,]    1    0    0    0    0    0    1    1    1     0"
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-5-calcluatel-meand-and-standard-deviation-of-normal-distribution",
    "href": "posts/2023-07-31/index.html#example-5-calcluatel-meand-and-standard-deviation-of-normal-distribution",
    "title": "The replicate() function in R",
    "section": "Example 5: Calcluatel Meand and Standard Deviation of Normal Distribution",
    "text": "Example 5: Calcluatel Meand and Standard Deviation of Normal Distribution\nTo calculate the mean and standard deviation of a normal distribution with mean 10 and standard deviation 5, you could use the following code:\n\nmeans &lt;- replicate(1000, mean(rnorm(100, 10, 5)))\nsds &lt;- replicate(1000, sd(rnorm(100, 10, 5)))\n\nhead(means)\n\n[1] 10.899256 10.613631 10.243307  9.977991  9.255811  9.440273\n\nhead(sds)\n\n[1] 4.908458 5.442502 4.883132 5.057129 4.953904 4.530682\n\nmean(means)\n\n[1] 9.997757\n\nsd(sds)\n\n[1] 0.3503813"
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-5-calcluatel-mean-and-standard-deviation-of-normal-distribution",
    "href": "posts/2023-07-31/index.html#example-5-calcluatel-mean-and-standard-deviation-of-normal-distribution",
    "title": "The replicate() function in R",
    "section": "Example 5: Calcluatel Mean and Standard Deviation of Normal Distribution",
    "text": "Example 5: Calcluatel Mean and Standard Deviation of Normal Distribution\nTo calculate the mean and standard deviation of a normal distribution with mean 10 and standard deviation 5, you could use the following code:\n\nmeans &lt;- replicate(1000, mean(rnorm(100, 10, 5)))\nsds &lt;- replicate(1000, sd(rnorm(100, 10, 5)))\n\nhead(means)\n\n[1] 10.541465  9.613299  9.204776 10.254969  9.695199 10.398412\n\nhead(sds)\n\n[1] 4.693568 5.696749 6.103586 5.760336 5.045217 5.156735\n\nmean(means)\n\n[1] 9.99477\n\nsd(sds)\n\n[1] 0.352739"
  },
  {
    "objectID": "posts/2023-08-01/index.html",
    "href": "posts/2023-08-01/index.html",
    "title": "R Functions for Getting Objects",
    "section": "",
    "text": "Welcome, fellow programmers, to this exciting journey into the world of R functions! Today, we’ll explore four powerful functions: get(), get0(), dynGet(), and mget(). These functions may sound mysterious, but fear not; we’ll demystify them together and see how they can be incredibly handy tools in your R toolkit. So, let’s dive in!"
  },
  {
    "objectID": "posts/2023-08-01/index.html#example-get",
    "href": "posts/2023-08-01/index.html#example-get",
    "title": "R Functions for Getting Objects",
    "section": "Example get()",
    "text": "Example get()\nLet’s say you have a variable named my_variable stored somewhere in your R environment, and you want to access its value using the get() function:\n\n# Sample variable in the environment\nmy_variable &lt;- 42\n\n# Using get() to retrieve the value\nresult &lt;- get(\"my_variable\")\nresult\n\n[1] 42"
  },
  {
    "objectID": "posts/2023-08-01/index.html#explanation-of-get",
    "href": "posts/2023-08-01/index.html#explanation-of-get",
    "title": "R Functions for Getting Objects",
    "section": "Explanation of get()",
    "text": "Explanation of get()\nIn the example above, we used get(\"my_variable\") to access the value of the variable my_variable. The function returned the value 42, which was stored in the variable."
  },
  {
    "objectID": "posts/2023-08-01/index.html#example-get0",
    "href": "posts/2023-08-01/index.html#example-get0",
    "title": "R Functions for Getting Objects",
    "section": "Example get0()",
    "text": "Example get0()\nLet’s use the same variable my_variable as before and see the difference between get() and get0():\n\n# Sample variable in the environment\nmy_variable &lt;- 42\n\n# Using get0() to retrieve the variable itself\nresult &lt;- get0(\"my_var\", ifnotfound = \"Does Not Exist\")\nresult\n\n[1] \"Does Not Exist\""
  },
  {
    "objectID": "posts/2023-08-01/index.html#explanation-of-get0",
    "href": "posts/2023-08-01/index.html#explanation-of-get0",
    "title": "R Functions for Getting Objects",
    "section": "Explanation of get0()",
    "text": "Explanation of get0()\nIn this example, get0(\"my_var\") returned an error message as the variable was not found."
  },
  {
    "objectID": "posts/2023-08-01/index.html#example-dynget",
    "href": "posts/2023-08-01/index.html#example-dynget",
    "title": "R Functions for Getting Objects",
    "section": "Example dynGet()",
    "text": "Example dynGet()\nConsider a scenario where you have a variable named num inside a custom environment, and you want to access it using dynGet():\n\n# Create a new environment\ncustom_env &lt;- new.env()\n\n# Assign a variable inside the custom environment\ncustom_env$num &lt;- 99\n\n# Using dynGet() to retrieve the value\nresult_env &lt;- dynGet(\"num\", custom_env)\nresult_env\n\n&lt;environment: 0x0000019c769ff050&gt;\n\nresult_num &lt;- dynGet(\"num\", custom_env$num)\nresult_num\n\n[1] 99"
  },
  {
    "objectID": "posts/2023-08-01/index.html#explanation-of-dynget",
    "href": "posts/2023-08-01/index.html#explanation-of-dynget",
    "title": "R Functions for Getting Objects",
    "section": "Explanation of dynGet()",
    "text": "Explanation of dynGet()\nIn this example, we used dynGet(\"num\", custom_env$num) to access the value of the variable num from the specified custom_env environment. The function successfully retrieved the value 99."
  },
  {
    "objectID": "posts/2023-08-01/index.html#example-of-mget",
    "href": "posts/2023-08-01/index.html#example-of-mget",
    "title": "R Functions for Getting Objects",
    "section": "Example of mget()",
    "text": "Example of mget()\nLet’s say we have two variables, x and y, and we want to retrieve their values using mget():\n\n# Sample variables in the environment\nx &lt;- 10\ny &lt;- 20\n\n# Using mget() to retrieve the values of multiple variables\nresult &lt;- mget(c(\"x\", \"y\"))\nresult # Output: a named list with values: $x [1] 10, $y [1] 20\n\n$x\n[1] 10\n\n$y\n[1] 20"
  },
  {
    "objectID": "posts/2023-08-01/index.html#explanation-of-mget",
    "href": "posts/2023-08-01/index.html#explanation-of-mget",
    "title": "R Functions for Getting Objects",
    "section": "Explanation of mget()",
    "text": "Explanation of mget()\nIn this example, we provided the vector c(\"x\", \"y\") to mget(), and it returned a named list with the values of both variables x and y."
  },
  {
    "objectID": "posts/2023-08-02/index.html",
    "href": "posts/2023-08-02/index.html",
    "title": "The unlist() Function in R",
    "section": "",
    "text": "Hey fellow R enthusiasts!\nToday, we’re diving deep into the incredible world of R programming to explore the often-overlooked but extremely handy unlist() function. If you’ve ever found yourself dealing with complex nested lists or vectors, this little gem can be a lifesaver. The unlist() function is like a magician that simplifies your data structures, making them more manageable and easier to work with. Let’s unlock its magic together!"
  },
  {
    "objectID": "posts/2023-08-02/index.html#example-1-flattening-a-simple-list",
    "href": "posts/2023-08-02/index.html#example-1-flattening-a-simple-list",
    "title": "The unlist() Function in R",
    "section": "Example 1: Flattening a Simple List",
    "text": "Example 1: Flattening a Simple List\nLet’s start with a straightforward example of a list containing some numeric values:\n\n# Create a simple list\nmy_list &lt;- list(1, 2, 3, 4, 5)\nmy_list\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n# Flatten the list\nflattened_vector &lt;- unlist(my_list)\nflattened_vector\n\n[1] 1 2 3 4 5\n\n\nIn this example, we had a list containing five numeric elements, and unlist() transformed it into a flat atomic vector."
  },
  {
    "objectID": "posts/2023-08-02/index.html#example-2-flattening-a-nested-list",
    "href": "posts/2023-08-02/index.html#example-2-flattening-a-nested-list",
    "title": "The unlist() Function in R",
    "section": "Example 2: Flattening a Nested List",
    "text": "Example 2: Flattening a Nested List\nThe real magic of unlist() shines when dealing with nested lists. Let’s consider a nested list:\n\n# Create a nested list\nnested_list &lt;- list(1, 2, list(3, 4), 5)\nnested_list\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[[3]][[1]]\n[1] 3\n\n[[3]][[2]]\n[1] 4\n\n\n[[4]]\n[1] 5\n\n# Flatten the nested list\nflattened_vector &lt;- unlist(nested_list)\nflattened_vector\n\n[1] 1 2 3 4 5\n\n\nThe unlist() function works recursively by default, so it will dive into the nested list and create a single vector containing all elements."
  },
  {
    "objectID": "posts/2023-08-02/index.html#example-3-removing-names-from-the-result",
    "href": "posts/2023-08-02/index.html#example-3-removing-names-from-the-result",
    "title": "The unlist() Function in R",
    "section": "Example 3: Removing Names from the Result",
    "text": "Example 3: Removing Names from the Result\nSometimes, you might prefer to discard the names of elements in the resulting vector to keep things simple and clean. You can achieve this using the use.names parameter:\n\n# Create a named list\nnamed_list &lt;- list(a = 10, b = 20, c = 30)\nnamed_list\n\n$a\n[1] 10\n\n$b\n[1] 20\n\n$c\n[1] 30\n\n# Flatten the list without preserving names\nflattened_vector &lt;- unlist(named_list, use.names = TRUE)\nflattened_vector\n\n a  b  c \n10 20 30"
  },
  {
    "objectID": "posts/2023-08-03/index.html",
    "href": "posts/2023-08-03/index.html",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "",
    "text": "Welcome, data enthusiasts! If you’re diving into the realm of data analysis with R, one function you’ll undoubtedly encounter is read.delim(). It’s an essential tool that allows you to read tabular data from a delimited text file and load it into R for further analysis. But fret not, dear reader, as I’ll walk you through this function in simple terms, with plenty of examples to guide you along the way.\n\n\nread.delim() is an R function used to read data from a text file where columns are separated by a delimiter. The default delimiter is a tab character (\\t), but you can customize it to match your data’s format.\nHere’s the basic syntax of read.delim():\nread.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\", ...)\n\nfile is the name of the file to be read.\nheader is a logical value that indicates whether the first line of the file contains the column names. The default value is TRUE.\nsep is the character that separates the columns in the file. The default value is a tab (.\nquote is the character that is used to quote strings in the file. The default value is a double quote (“).\n… are additional arguments that can be passed to the function."
  },
  {
    "objectID": "posts/2023-08-03/index.html#what-is-read.delim-and-its-syntax",
    "href": "posts/2023-08-03/index.html#what-is-read.delim-and-its-syntax",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "",
    "text": "read.delim() is an R function used to read data from a text file where columns are separated by a delimiter. The default delimiter is a tab character (\\t), but you can customize it to match your data’s format.\nHere’s the basic syntax of read.delim():\nread.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\", ...)\n\nfile is the name of the file to be read.\nheader is a logical value that indicates whether the first line of the file contains the column names. The default value is TRUE.\nsep is the character that separates the columns in the file. The default value is a tab (.\nquote is the character that is used to quote strings in the file. The default value is a double quote (“).\n… are additional arguments that can be passed to the function."
  },
  {
    "objectID": "posts/2023-08-03/index.html#example-1-basic-usage",
    "href": "posts/2023-08-03/index.html#example-1-basic-usage",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "Example 1: Basic Usage",
    "text": "Example 1: Basic Usage\nImagine we have a file named data.txt that looks like this:\nName,Age,Country\nJohn,25,USA\nJane,30,Canada\nLet’s make the file:\ncat(\"Name,Age,Country\\nJohn,25,USA\\nJane,30,Canada\\n\", \n    file = \"posts/2023-08-03/data.txt\")\nTo load this data into R:\n\n# Assuming the file is in the current working directory\nread.delim(\"data.txt\")\n\n  Name.Age.Country\n1      John,25,USA\n2   Jane,30,Canada\n\n\nIn this case, read.delim() will automatically detect the tab delimiter and consider the first row as column names. You will notice that it did not separate based upon the delimiter, as this file was not actually tab delimited."
  },
  {
    "objectID": "posts/2023-08-03/index.html#example-2-custom-delimiter",
    "href": "posts/2023-08-03/index.html#example-2-custom-delimiter",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "Example 2: Custom Delimiter",
    "text": "Example 2: Custom Delimiter\nNow, let’s read in that same file but change the sep argument to ',':\n\nread.delim(\"data.txt\", sep = \",\")\n\n  Name Age Country\n1 John  25     USA\n2 Jane  30  Canada"
  },
  {
    "objectID": "posts/2023-08-03/index.html#example-3-file-without-header",
    "href": "posts/2023-08-03/index.html#example-3-file-without-header",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "Example 3: File Without Header",
    "text": "Example 3: File Without Header\nIn some cases, your file might not have a header row. Let’s consider data_no_header.txt:\nJohn,25,USA\nJane,30,Canada\nYou can handle this by setting header = FALSE:\n\nread.delim(\"data_no_header.txt\",sep = \",\",header = FALSE)\n\n    V1 V2     V3\n1 John 25    USA\n2 Jane 30 Canada"
  },
  {
    "objectID": "posts/2023-08-04/index.html",
    "href": "posts/2023-08-04/index.html",
    "title": "Exploring R’s Versatile str() Function: Unraveling Your Data with Ease!",
    "section": "",
    "text": "Welcome, fellow data enthusiasts, to another exciting blog post! Today, we’re diving deep into R’s invaluable str() function – a powerful tool for gaining insight into your datasets. Whether you’re a seasoned data scientist or just starting with R, str() will undoubtedly become your go-to function for data exploration. Let’s embark on this journey together and unleash the full potential of str()!"
  },
  {
    "objectID": "posts/2023-08-04/index.html#example-1-basic-usage-with-a-vector",
    "href": "posts/2023-08-04/index.html#example-1-basic-usage-with-a-vector",
    "title": "Exploring R’s Versatile str() Function: Unraveling Your Data with Ease!",
    "section": "Example 1: Basic Usage with a Vector",
    "text": "Example 1: Basic Usage with a Vector\nLet’s begin with a simple example. Suppose we have a numeric vector named “ages,” representing the ages of individuals in a survey:\n\nages &lt;- c(25, 30, 22, 40, 35)\nstr(ages)\n\n num [1:5] 25 30 22 40 35\n\n\nHere, the output reveals that “ages” is a numeric vector with five elements, ranging from 25 to 35. It helps us quickly confirm the data type and size."
  },
  {
    "objectID": "posts/2023-08-04/index.html#example-2-investigating-a-data-frame",
    "href": "posts/2023-08-04/index.html#example-2-investigating-a-data-frame",
    "title": "Exploring R’s Versatile str() Function: Unraveling Your Data with Ease!",
    "section": "Example 2: Investigating a Data Frame",
    "text": "Example 2: Investigating a Data Frame\nNow, let’s explore a more complex scenario. We have a data frame named “students,” containing information about students’ names, ages, and grades:\n\nstudents &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(22, 23, 21),\n  grade = c(\"A\", \"B\", \"A-\")\n)\nstr(students)\n\n'data.frame':   3 obs. of  3 variables:\n $ name : chr  \"Alice\" \"Bob\" \"Charlie\"\n $ age  : num  22 23 21\n $ grade: chr  \"A\" \"B\" \"A-\"\n\n\nThe output informs us that “students” is a data frame with three observations (rows) and three variables (columns). It also lists the data types for each column, with “chr” representing character and “num” representing numeric."
  },
  {
    "objectID": "posts/2023-08-04/index.html#example-3-checking-nested-data-structures",
    "href": "posts/2023-08-04/index.html#example-3-checking-nested-data-structures",
    "title": "Exploring R’s Versatile str() Function: Unraveling Your Data with Ease!",
    "section": "Example 3: Checking Nested Data Structures",
    "text": "Example 3: Checking Nested Data Structures\nstr() handles nested data structures effortlessly. Let’s consider a list called “nested_data” containing a data frame and a character vector:\n\nnested_data &lt;- list(\n  data_frame = data.frame(x = 1:3, y = 4:6),\n  character_vector = c(\"hello\", \"world\", \"R\")\n)\nstr(nested_data)\n\nList of 2\n $ data_frame      :'data.frame':   3 obs. of  2 variables:\n  ..$ x: int [1:3] 1 2 3\n  ..$ y: int [1:3] 4 5 6\n $ character_vector: chr [1:3] \"hello\" \"world\" \"R\"\n\n\nThe output provides a comprehensive breakdown of the nested_data list. It consists of two components: a data frame with two variables, “x” and “y,” and a character vector.\nHere are some additional examples of how to use the str() function:\nTo display the structure of a list, you would use the following code:\n\nstr(list(a = 1, b = \"hello\", c = list(1, 2, 3)))\n\nList of 3\n $ a: num 1\n $ b: chr \"hello\"\n $ c:List of 3\n  ..$ : num 1\n  ..$ : num 2\n  ..$ : num 3\n\n\nTo display the structure of a function, you would use the following code:\n\nstr(function(x) x^2)\n\nfunction (x)  \n - attr(*, \"srcref\")= 'srcref' int [1:8] 1 5 1 19 5 19 1 1\n  ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' &lt;environment: 0x000002c3e62abd70&gt; \n\n\nIf you want to see the options that are available to be set to the str() function, then just run the below code:\n\noptions()$str\n\n$strict.width\n[1] \"no\"\n\n$digits.d\n[1] 3\n\n$vec.len\n[1] 4\n\n$list.len\n[1] 99\n\n$deparse.lines\nNULL\n\n$drop.deparse.attr\n[1] TRUE\n\n$formatNum\nfunction (x, ...) \nformat(x, trim = TRUE, drop0trailing = TRUE, ...)\n&lt;environment: 0x000002c3e214ad28&gt;"
  },
  {
    "objectID": "posts/2023-08-07/index.html",
    "href": "posts/2023-08-07/index.html",
    "title": "Enhance Your Plots with the text() Function in R",
    "section": "",
    "text": "As a programmer, you’re well aware of the importance of data visualization. A well-crafted plot can convey complex information with clarity and impact. In R, creating stunning plots is a breeze, especially when you’re armed with the versatile text() function. This little gem allows you to add custom text to your plots, enabling you to annotate and highlight essential details. Let’s dive into the world of text() and uncover its syntax and potential through some hands-on examples."
  },
  {
    "objectID": "posts/2023-08-07/index.html#example-1-simple-annotation",
    "href": "posts/2023-08-07/index.html#example-1-simple-annotation",
    "title": "Enhance Your Plots with the text() Function in R",
    "section": "Example 1: Simple Annotation",
    "text": "Example 1: Simple Annotation\nLet’s start with a basic scatter plot representing the relationship between two variables.\n\n# Create sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(5, 9, 6, 8, 12)\nplot(x, y, main = \"Scatter Plot Example\")\n\n# Add text annotation\ntext(3, 8, \"Key Point\", col = \"blue\", cex = 1.2)\n\n\n\n\nIn this example, we’ve created a scatter plot and used text() to add an annotation (“Key Point”) at the coordinates (3, 8). We’ve also adjusted the text color and size for emphasis."
  },
  {
    "objectID": "posts/2023-08-07/index.html#example-2-annotating-multiple-points",
    "href": "posts/2023-08-07/index.html#example-2-annotating-multiple-points",
    "title": "Enhance Your Plots with the text() Function in R",
    "section": "Example 2: Annotating Multiple Points",
    "text": "Example 2: Annotating Multiple Points\nWhat if you want to annotate multiple points on your plot? No worries, the text() function can handle that too!\n\n# Continue from previous example\npoints &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\")\nplot(x, y)\ntext(x, y, labels = points, pos = 3, col = \"green\")\n\n\n\n\nHere, we’ve added labels “A” through “E” to their respective data points. The pos parameter ensures that the text appears above the points, making the plot more informative."
  },
  {
    "objectID": "posts/2023-08-07/index.html#example-3-mathematical-expressions",
    "href": "posts/2023-08-07/index.html#example-3-mathematical-expressions",
    "title": "Enhance Your Plots with the text() Function in R",
    "section": "Example 3: Mathematical Expressions",
    "text": "Example 3: Mathematical Expressions\nMathematical annotations can elevate your plots, making them more informative.\n\n# Create a sine wave plot\nx &lt;- seq(0, 2 * pi, length.out = 100)\ny &lt;- sin(x)\nplot(x, y, type = \"l\", col = \"red\")\n\n# Add equation using mathematical notation\ntext(pi/2, 1, expression(sin(theta)), col = \"purple\", cex = 1.2)\n\n\n\n\nIn this case, we’ve drawn a sine wave and used an expression to annotate the maximum point with the equation “sin(θ)”."
  },
  {
    "objectID": "posts/2023-08-08/index.html",
    "href": "posts/2023-08-08/index.html",
    "title": "Mastering Data Transformation with the scale() Function in R",
    "section": "",
    "text": "Data analysis often requires preprocessing and transforming data to make it more suitable for analysis. In R, the scale() function is a powerful tool that allows you to standardize or normalize your data, helping you unlock deeper insights. In this blog post, we’ll dive into the syntax of the scale() function, provide real-world examples, and encourage you to explore this function on your own. The scale() function can be used to center and scale the columns of a numeric matrix, or to scale a vector. This can be useful for a variety of tasks, such as:\n\nComparing data that is measured in different units\nImproving the performance of machine learning algorithms\nMaking data more interpretable"
  },
  {
    "objectID": "posts/2023-08-08/index.html#example-1-centering-and-scaling",
    "href": "posts/2023-08-08/index.html#example-1-centering-and-scaling",
    "title": "Mastering Data Transformation with the scale() Function in R",
    "section": "Example 1: Centering and Scaling",
    "text": "Example 1: Centering and Scaling\nLet’s say you have a dataset height_weight with columns ‘Height’ and ‘Weight’, and you want to center and scale the data:\n\n# Sample data\nheight_weight &lt;- data.frame(Height = c(160, 175, 150, 180),\n                             Weight = c(60, 70, 55, 75))\n\n# Centering and scaling\nscaled_data &lt;- scale(height_weight, center = TRUE, scale = TRUE)\nscaled_data\n\n         Height     Weight\n[1,] -0.4539206 -0.5477226\n[2,]  0.6354889  0.5477226\n[3,] -1.1801937 -1.0954451\n[4,]  0.9986254  1.0954451\nattr(,\"scaled:center\")\nHeight Weight \n166.25  65.00 \nattr(,\"scaled:scale\")\n   Height    Weight \n13.768926  9.128709 \n\n\nIn this example, the scale() function calculates the mean and standard deviation for each column. It then subtracts the mean and divides by the standard deviation, giving you centered and scaled data."
  },
  {
    "objectID": "posts/2023-08-08/index.html#example-2-centering-only",
    "href": "posts/2023-08-08/index.html#example-2-centering-only",
    "title": "Mastering Data Transformation with the scale() Function in R",
    "section": "Example 2: Centering Only",
    "text": "Example 2: Centering Only\nLet’s consider a scenario where you want to center the data but not scale it:\n\n# Sample data\ntemperatures &lt;- c(25, 30, 28, 33, 22)\n\n# Centering without scaling\nscaled_temps &lt;- scale(temperatures, center = TRUE, scale = FALSE)\nscaled_temps\n\n     [,1]\n[1,] -2.6\n[2,]  2.4\n[3,]  0.4\n[4,]  5.4\n[5,] -5.6\nattr(,\"scaled:center\")\n[1] 27.6\n\n\nIn this case, the scale() function only centers the data by subtracting the mean, maintaining the original range of values."
  },
  {
    "objectID": "posts/2023-08-08/index.html#example-3-scaling-a-matrix",
    "href": "posts/2023-08-08/index.html#example-3-scaling-a-matrix",
    "title": "Mastering Data Transformation with the scale() Function in R",
    "section": "Example 3: Scaling a Matrix",
    "text": "Example 3: Scaling a Matrix\nHere is an example of how to use the scale() function to scale the columns of a matrix:\n\nm &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3)\nscaled_m &lt;- scale(m)\n\nscaled_m\n\n     [,1] [,2] [,3]\n[1,]   -1   -1   -1\n[2,]    0    0    0\n[3,]    1    1    1\nattr(,\"scaled:center\")\n[1] 2 5 8\nattr(,\"scaled:scale\")\n[1] 1 1 1"
  },
  {
    "objectID": "posts/2023-08-09/index.html",
    "href": "posts/2023-08-09/index.html",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "",
    "text": "When it comes to data visualization in R, the par() function is an indispensable tool that often goes overlooked. This function allows you to control various graphical parameters, unleashing a world of customization possibilities for your plots. In this blog post, we’ll demystify the par() function, break down its syntax, and provide you with hands-on examples to help you create stunning visualizations."
  },
  {
    "objectID": "posts/2023-08-09/index.html#example-1-adjusting-plot-margins",
    "href": "posts/2023-08-09/index.html#example-1-adjusting-plot-margins",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "Example 1: Adjusting Plot Margins",
    "text": "Example 1: Adjusting Plot Margins\n\npar(mar = c(5, 4, 4, 2) + 0.1)\nplot(1:10)\n\n\n\n\nIn this example, we’re using the mar parameter to control the margins of the plot. The vector c(5, 4, 4, 2) + 0.1 specifies the bottom, left, top, and right margins, respectively. Increasing the margins gives more space for titles, labels, and annotations."
  },
  {
    "objectID": "posts/2023-08-09/index.html#example-2-changing-plot-colors",
    "href": "posts/2023-08-09/index.html#example-2-changing-plot-colors",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "Example 2: Changing Plot Colors",
    "text": "Example 2: Changing Plot Colors\n\npar(col.main = \"blue\", col.axis = \"red\")\nplot(1:10, main = \"Custom Colors\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n\n\n\nHere, we’re utilizing col.main and col.axis to change the color of the main title and axis labels. This adds a touch of vibrancy to your plots and enhances readability."
  },
  {
    "objectID": "posts/2023-08-09/index.html#example-3-adjusting-font-size",
    "href": "posts/2023-08-09/index.html#example-3-adjusting-font-size",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "Example 3: Adjusting Font Size:",
    "text": "Example 3: Adjusting Font Size:\n\npar(cex.main = 1.5, cex.axis = 0.8)\nplot(1:10, main = \"Bigger Title, Smaller Labels\")\n\n\n\n\nWith cex.main and cex.axis, you can control the size of the main title and axis labels, respectively. This allows you to emphasize important information and fine-tune the presentation."
  },
  {
    "objectID": "posts/2023-08-09/index.html#example-4-controlling-axis-type",
    "href": "posts/2023-08-09/index.html#example-4-controlling-axis-type",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "Example 4: Controlling Axis Type",
    "text": "Example 4: Controlling Axis Type\n\npar(log = \"y\")\n\nWarning in par(log = \"y\"): \"log\" is not a graphical parameter\n\nplot(1:10, log = \"y\", main = \"Logarithmic Y-axis\")\n\n\n\n\nBy setting log = \"y\", you’re instructing R to use a logarithmic scale for the y-axis. This is particularly useful when dealing with data that spans several orders of magnitude."
  },
  {
    "objectID": "posts/2023-08-10/index.html",
    "href": "posts/2023-08-10/index.html",
    "title": "Mastering Grouped Counting in R: A Comprehensive Guide",
    "section": "",
    "text": "Introduction\nAs data-driven decision-making becomes more critical in various fields, the ability to extract valuable insights from datasets has never been more important. One common task is to calculate counts by group, which can shed light on trends and patterns within your data. In this guide, we’ll explore three different approaches to achieve this using the powerful R programming language. So, let’s dive into the world of grouped counting with the help of the classic mtcars dataset!\n\n\nThe aggregate() Function: A Solid Foundation\nTo kick things off, let’s start with the aggregate() function available in base R. This function is a versatile tool for aggregating data based on grouping variables. Here’s how you can use it to calculate counts by group using the mtcars dataset:\n\n# Load the dataset\ndata(\"mtcars\")\n\n# Calculate counts by group using aggregate()\ngroup_counts &lt;- aggregate(\n  data = mtcars, \n  carb ~ cyl, \n  FUN = function(x) length(unique(x))\n  )\ngroup_counts\n\n  cyl carb\n1   4    2\n2   6    3\n3   8    4\n\n\nIn this example, we’re counting the number of cars in each cylinder group. The aggregate() function groups the data by the ‘cyl’ variable and applies the length() and unique() functions to count the number of distinct carb per cyl group.\n\n\nHarnessing the Power of dplyr Library\nMoving on, the dplyr package is a staple in data manipulation and offers an elegant way to work with grouped data. The group_by() and summarise() functions are your go-to tools for such tasks. Let’s see how they can be used with the mtcars dataset:\n\n# Load the required library\nlibrary(dplyr)\n\n# Calculate counts by group using dplyr\ngroup_counts_dplyr &lt;- mtcars |&gt;\n  group_by(cyl) |&gt;\n  summarise(count = n_distinct(carb))\ngroup_counts_dplyr\n\n# A tibble: 3 × 2\n    cyl count\n  &lt;dbl&gt; &lt;int&gt;\n1     4     2\n2     6     3\n3     8     4\n\n\nIn this example, we use the group_by() function to group the data by cylinder count and then use summarise() with n_distinct() to create a ‘count’ column containing the number of distinct carb per cyl group.\n\n\nEfficiency and Speed with data.table\nFor those dealing with larger datasets, the data.table package offers lightning-fast performance. It’s especially handy for tasks involving grouping and aggregation. Here’s how you can use it with the mtcars dataset:\n\n# Load the required library\nlibrary(data.table)\n\n# Convert mtcars to data.table\ndt_mtcars &lt;- as.data.table(mtcars)\n\n# Calculate counts by group using data.table\ngroup_counts_dt &lt;- dt_mtcars[, .(count = length(unique(carb))), by = cyl]\nsetorder(group_counts_dt, cols = \"cyl\")\ngroup_counts_dt\n\n   cyl count\n1:   4     2\n2:   6     3\n3:   8     4\n\n\nIn this example, we convert the mtcars dataset to a data.table using as.data.table(). Then, we use the length(unique(carb)) special symbol to count the number of distinct carb in each cyl group.\n\n\nTry It Yourself!\nNow that you’ve seen three powerful ways to calculate counts by group in R, it’s time to roll up your sleeves and give them a try. Experiment with these methods using your own datasets, and witness how easy it is to uncover valuable insights from your data.\nWhether you opt for the solid foundation of aggregate(), the elegance of dplyr, or the efficiency of data.table, each approach has its unique strengths. As you become more comfortable with these techniques, you’ll be better equipped to tackle complex data analysis tasks and make informed decisions.\nSo, don’t hesitate to put your newfound knowledge into action. Happy coding and happy exploring your data!"
  },
  {
    "objectID": "posts/2023-08-11/index.html",
    "href": "posts/2023-08-11/index.html",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "",
    "text": "Title: Unleashing the Power of pmax() and pmin() Functions in R\nIntroduction: In the realm of data manipulation and analysis, R stands tall as a versatile programming language. Among its plethora of functions, pmax() and pmin() shine as unsung heroes that can greatly simplify your coding experience. These functions allow you to effortlessly find the element-wise maximum and minimum values across vectors in R, providing an elegant solution to a common programming challenge. In this blog post, we’ll dive into the syntax and explore real-world examples that showcase the true potential of pmax() and pmin()."
  },
  {
    "objectID": "posts/2023-08-11/index.html#pmax-function",
    "href": "posts/2023-08-11/index.html#pmax-function",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "pmax() Function:",
    "text": "pmax() Function:\npmax(..., na.rm = FALSE)\n\nThe ellipsis (...) signifies the input vectors. You can pass two or more vectors to compare element-wise.\nThe na.rm parameter (defaulting to FALSE) determines whether to remove NAs before computation."
  },
  {
    "objectID": "posts/2023-08-11/index.html#pmin-function",
    "href": "posts/2023-08-11/index.html#pmin-function",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "pmin() Function:",
    "text": "pmin() Function:\npmin(..., na.rm = FALSE)\n\nSimilar to pmax(), the ellipsis (...) denotes the input vectors for element-wise comparison.\nThe na.rm parameter (defaulting to FALSE) decides whether to exclude NAs before calculation."
  },
  {
    "objectID": "posts/2023-08-11/index.html#example-1-using-pmax-to-find-element-wise-maximum",
    "href": "posts/2023-08-11/index.html#example-1-using-pmax-to-find-element-wise-maximum",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "Example 1: Using pmax() to Find Element-wise Maximum",
    "text": "Example 1: Using pmax() to Find Element-wise Maximum\n\nvec1 &lt;- c(3, 9, 2, 6)\nvec2 &lt;- c(7, 1, 8, 4)\nresult &lt;- pmax(vec1, vec2)\nresult\n\n[1] 7 9 8 6"
  },
  {
    "objectID": "posts/2023-08-11/index.html#example-2-using-pmin-to-find-element-wise-minimum",
    "href": "posts/2023-08-11/index.html#example-2-using-pmin-to-find-element-wise-minimum",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "Example 2: Using pmin() to Find Element-wise Minimum",
    "text": "Example 2: Using pmin() to Find Element-wise Minimum\n\ndata1 &lt;- c(12, 5, 9, 16)\ndata2 &lt;- c(6, 14, 8, 11)\nresult &lt;- pmin(data1, data2)\nresult\n\n[1]  6  5  8 11"
  },
  {
    "objectID": "posts/2023-08-11/index.html#example-3-handling-na-values",
    "href": "posts/2023-08-11/index.html#example-3-handling-na-values",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "Example 3: Handling NA Values",
    "text": "Example 3: Handling NA Values\n\ndata1 &lt;- c(7, 3, NA, 12)\ndata2 &lt;- c(9, NA, 5, 8)\nresult &lt;- pmax(data1, data2, na.rm = TRUE)\nresult\n\n[1]  9  3  5 12"
  },
  {
    "objectID": "posts/2023-08-11/index.html#example-4-scaling-with-multiple-vectors",
    "href": "posts/2023-08-11/index.html#example-4-scaling-with-multiple-vectors",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "Example 4: Scaling with Multiple Vectors",
    "text": "Example 4: Scaling with Multiple Vectors\n\nheights &lt;- c(165, 178, 155, 189)\nweights &lt;- c(68, 82, 61, 76)\nbmi &lt;- pmax(weights / ((heights / 100) ^ 2))\nbmi\n\n[1] 24.97704 25.88057 25.39022 21.27600"
  },
  {
    "objectID": "posts/2023-08-14/index.html",
    "href": "posts/2023-08-14/index.html",
    "title": "The substring() function in R",
    "section": "",
    "text": "The substring() function in R is used to extract a substring from a character vector. The syntax of the function is:\nsubstring(x, start, stop)\nwhere:\n\nx is the character vector from which to extract the substring\nstart is the starting position of the substring\nstop is the ending position of the substring\n\nThe start and stop arguments can be either integers or character strings. If they are integers, they specify the positions of the characters in the string. If they are character strings, they specify the characters that should be used as the starting and ending positions of the substring."
  },
  {
    "objectID": "posts/2023-08-14/index.html#example-1",
    "href": "posts/2023-08-14/index.html#example-1",
    "title": "The substring() function in R",
    "section": "Example 1",
    "text": "Example 1\nFor example, the following code will extract the substring from the string “Hello, world!” that starts at the 5th character and ends at the 8th character:\n\nsubstring(\"Hello, world!\", 8, 12)\n\n[1] \"world\"\n\n\nAs we see this will return the string “world”."
  },
  {
    "objectID": "posts/2023-08-14/index.html#example-2",
    "href": "posts/2023-08-14/index.html#example-2",
    "title": "The substring() function in R",
    "section": "Example 2",
    "text": "Example 2\nThe substring() function can also be used to extract the first N characters of a string, the last N characters of a string, or to replace a substring in a string.\nTo extract the first N characters of a string, you can use the following syntax:\nsubstring(x, 1, N)\nFor example, the following code will extract the first 5 characters of the string “Hello, world!”:\n\nsubstring(\"Hello, world!\", 1, 5)\n\n[1] \"Hello\"\n\n\nAs seen this will return the string “Hello”."
  },
  {
    "objectID": "posts/2023-08-14/index.html#example-3",
    "href": "posts/2023-08-14/index.html#example-3",
    "title": "The substring() function in R",
    "section": "Example 3",
    "text": "Example 3\nTo extract the last N characters of a string, you can use the following syntax:\nsubstring(x, nchar(x) - N + 1, nchar(x))\nwhere nchar(x) is the function that returns the length of the string x.\nFor example, the following code will extract the last 5 characters of the string “Hello, world!”:\n\ns &lt;- \"Hello, world!\"\n\nsubstring(s, nchar(s) - 6 + 1, nchar(s))\n\n[1] \"world!\"\n\n\nThis will return the string “world!”."
  },
  {
    "objectID": "posts/2023-08-14/index.html#example-4",
    "href": "posts/2023-08-14/index.html#example-4",
    "title": "The substring() function in R",
    "section": "Example 4",
    "text": "Example 4\nTo replace a substring in a string, you can use the following syntax:\nsubstring(x, start, stop) &lt;- value\nwhere value is the string that you want to replace the substring with.\nFor example, the following code will replace the substring “world” in the string “Hello, world!” with the string “universe”:\n\ns &lt;- \"Hello, world!\"\nsubstring(s, first = 8) &lt;- \"universe\"\ns\n\n[1] \"Hello, univer\"\n\n\nThis will change the string to “Hello, univer”. You notice that it will not expand the original length of the string.\nIn addition to the substring() function, there are also a few other functions that can be used to extract substrings from strings in R. These functions are:\n\nstr_sub() from the stringr package\nsql_left(), sql_right() and sql_mid() from the healthyR library\n\nThe str_sub() function from the stringr package is a more powerful and flexible function than the substring() function. It supports a wider range of arguments and it can be used to perform more complex string manipulations.\nThe sql_left(), sql_right() and sql_mid() functions from the `{healthyR} library are designed to be similar to the corresponding functions in SQL. They are easy to use and they can be a good choice for users who are familiar with SQL.\nI encourage readers to try things on their own with the substring() function and the other functions mentioned in this blog post. There are many different ways to use these functions to extract substrings from strings in R. Experimenting with different functions and different arguments is a great way to learn how to use them effectively.\nHere is a link to a blog post that shows some examples of how to use the sql_left(), sql_right() and sql_mid() functions: https://www.spsanderson.com/steveondata/posts/rtip-2023-03-01/index.html"
  },
  {
    "objectID": "posts/2023-08-15/index.html",
    "href": "posts/2023-08-15/index.html",
    "title": "Solving Systems of Equations in R using the solve() Function",
    "section": "",
    "text": "In mathematical modeling and data analysis, it is often necessary to solve systems of equations to find the values of unknown variables. R provides the solve() function, which is a powerful tool for solving systems of linear equations. In this blog post, we will explore the purpose of solving systems of equations, explain the syntax of the solve() function, and provide three examples of increasing complexity to demonstrate its usage."
  },
  {
    "objectID": "posts/2023-08-15/index.html#example-1-solving-a-system-of-two-equations",
    "href": "posts/2023-08-15/index.html#example-1-solving-a-system-of-two-equations",
    "title": "Solving Systems of Equations in R using the solve() Function",
    "section": "Example 1 Solving a System of Two Equations",
    "text": "Example 1 Solving a System of Two Equations\nLet’s start with a simple example of solving a system of two equations with two variables. Suppose we have the following system of equations:\n2x + 3y = 10\n4x - 2y = 6\nTo solve this system using the solve() function, we define the coefficient matrix “a” and the constant matrix “b” as follows:\n\na &lt;- matrix(c(2, 3, 4, -2), nrow = 2, byrow = TRUE)\nb &lt;- c(10, 6)\n\nThen, we can use the solve() function to find the values of “x” and “y”:\n\nsolution &lt;- solve(a, b)\nsolution\n\n[1] 2.375 1.750\n\n\nThe solution will be stored in the “solution” variable, which can be accessed to obtain the values of “x” and “y”."
  },
  {
    "objectID": "posts/2023-08-15/index.html#example-2-solving-a-system-of-three-equations",
    "href": "posts/2023-08-15/index.html#example-2-solving-a-system-of-three-equations",
    "title": "Solving Systems of Equations in R using the solve() Function",
    "section": "Example 2 Solving a System of Three Equations",
    "text": "Example 2 Solving a System of Three Equations\nLet’s consider a slightly more complex system of three equations with three variables:\n3x + 2y - z = 7\nx - y + 2z = -1\n2x + 3y + 4z = 12\nTo solve this system, we define the coefficient matrix “a” and the constant matrix “b”:\n\na &lt;- matrix(c(3, 2, -1, 1, -1, 2, 2, 3, 4), nrow = 3, byrow = TRUE)\nb &lt;- c(7, -1, 12)\n\nWe can then use the solve() function to find the values of “x”, “y”, and “z”:\n\nsolution &lt;- solve(a, b)\nsolution\n\n[1] 0.6571429 2.8000000 0.5714286\n\n\nThe solution will be stored in the “solution” variable, and we can access the values of “x”, “y”, and “z” from it."
  },
  {
    "objectID": "posts/2023-08-15/index.html#example-3-solving-a-system-of-equations-with-matrix-coefficients",
    "href": "posts/2023-08-15/index.html#example-3-solving-a-system-of-equations-with-matrix-coefficients",
    "title": "Solving Systems of Equations in R using the solve() Function",
    "section": "Example 3 Solving a System of Equations with Matrix Coefficients",
    "text": "Example 3 Solving a System of Equations with Matrix Coefficients\nIn some cases, the coefficient matrix “a” can be a matrix instead of a vector. For example, consider the following system of equations:\n2x + 3y = 10\n4x - 2y = 6\nWe can represent the coefficient matrix “a” as follows:\n\na &lt;- matrix(c(2, 3, 4, -2), nrow = 2, byrow = TRUE)\n\nThe constant vector “b” remains the same:\n\nb &lt;- c(10, 6)\n\nWe can then use the solve() function to find the values of “x” and “y”:\n\nsolution &lt;- solve(a, b)\nsolution\n\n[1] 2.375 1.750\n\n\nThe solution will be stored in the “solution” variable, and we can access the values of “x” and “y” from it."
  },
  {
    "objectID": "posts/2023-08-16/index.html",
    "href": "posts/2023-08-16/index.html",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "",
    "text": "In the vast world of R programming, there are numerous functions that provide powerful capabilities for data visualization and analysis. One such function that often goes under appreciated is the curve() function. This neat little function allows us to plot mathematical functions and explore their behavior. In this blog post, we will dive into the syntax of the curve() function, provide a couple of examples to demonstrate its usage, and encourage readers to try it on their own."
  },
  {
    "objectID": "posts/2023-08-16/index.html#example-1-plotting-a-simple-line",
    "href": "posts/2023-08-16/index.html#example-1-plotting-a-simple-line",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "Example 1: Plotting a Simple Line",
    "text": "Example 1: Plotting a Simple Line\nLet’s start with a simple example to plot a line. Suppose we want to plot the line y = x. We can achieve this using the curve() function as follows:\n\ncurve((x))\n\n\n\n\nIn this example, we provide the expression (x) to the curve() function. The expression (x) represents the line y = x. By default, the curve() function will plot the curve between 0 and 1. The resulting plot will show a straight line passing through the origin with a slope of 1."
  },
  {
    "objectID": "posts/2023-08-16/index.html#example-2-overlaying-multiple-curves",
    "href": "posts/2023-08-16/index.html#example-2-overlaying-multiple-curves",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "Example 2: Overlaying Multiple Curves",
    "text": "Example 2: Overlaying Multiple Curves\nThe curve() function allows us to overlay multiple curves on the same plot. Let’s consider an example where we plot several curves together:\n\ncurve((x), from = -2, to = 2, lwd = 2)\ncurve(0 * x, add = TRUE, col = \"blue\")\ncurve(0 * x + 1.5, add = TRUE, col = \"green\")\ncurve(x^3, add = TRUE, col = \"red\")\ncurve(-3 * (x + 2), add = TRUE, col = \"orange\")\n\n\n\n\nIn this example, we first plot the line y = x with a thicker line width (lwd = 2). Then, we overlay four additional curves on the same plot: a horizontal line at y = 0 (colored blue), a horizontal line at y = 1.5 (colored green), a cubic curve y = x^3 (colored red), and a linear curve y = -3(x + 2) (colored orange). This example showcases the versatility of the curve() function in visualizing multiple functions simultaneously."
  },
  {
    "objectID": "posts/2023-08-16/index.html#example-3-plotting-a-simple-function",
    "href": "posts/2023-08-16/index.html#example-3-plotting-a-simple-function",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "Example 3: Plotting a Simple Function",
    "text": "Example 3: Plotting a Simple Function\nLet’s start with a simple example. Suppose we want to visualize the curve of the quadratic function f(x) = x^2. Here’s how we can achieve this using the curve() function:\n\ncurve(x^2, from = -5, to = 5, type = \"l\", col = \"blue\",\n      xlab = \"x-axis\", ylab = \"y = x^2\", \n      main = \"Quadratic Curve: y = x^2\")\n\n\n\n\nIn this example, we’ve provided the expression x^2 to the curve() function. We’ve also specified the range of x-values from -5 to 5. The curve type is set to “l” for lines, and we’ve customized the colors, labels, and title of the plot."
  },
  {
    "objectID": "posts/2023-08-16/index.html#example-4-plotting-multiple-functions",
    "href": "posts/2023-08-16/index.html#example-4-plotting-multiple-functions",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "Example 4: Plotting Multiple Functions",
    "text": "Example 4: Plotting Multiple Functions\nNow, let’s take it up a notch and visualize two functions on the same plot. Imagine we want to compare the curves of the sine and cosine functions. Here’s how we can do it:\n\ncurve(sin(x), from = -2 * pi, to = 2 * pi, type = \"l\", col = \"red\",\n      xlab = \"x-axis\", ylab = \"y-axis\", \n      main = \"Sine and Cosine Curves\")\ncurve(cos(x), from = -2 * pi, to = 2 * pi, type = \"l\", col = \"blue\",\n      add = TRUE)\nlegend(\"topright\", legend = c(\"sin(x)\", \"cos(x)\"), \n       col = c(\"red\", \"blue\"), lty = 1)\n\n\n\n\nIn this example, we’ve plotted both the sine and cosine functions on the same plot. By setting add = TRUE in the second curve() call, we overlay the cosine curve on the existing plot. We’ve also added a legend to differentiate between the two curves."
  },
  {
    "objectID": "posts/2023-08-17/index.html",
    "href": "posts/2023-08-17/index.html",
    "title": "Mastering Data Approximation with R’s approx() Function",
    "section": "",
    "text": "Are you tired of dealing with irregularly spaced data points that just don’t seem to fit together? Do you find yourself struggling to interpolate or smooth your data for better analysis? Look no further! In this blog post, we’ll dive deep into the powerful world of data approximation using R’s approx() function. Buckle up, because by the end of this journey, you’ll have a new tool in your R toolkit that can help you tame even the wildest datasets."
  },
  {
    "objectID": "posts/2023-08-17/index.html#examples",
    "href": "posts/2023-08-17/index.html#examples",
    "title": "Mastering Data Approximation with R’s approx() Function",
    "section": "Examples",
    "text": "Examples\n\nExample 1: Basic Linear Interpolation\nSuppose you have a dataset of temperature measurements at irregular intervals and you want to estimate the temperature at a specific time. Here’s how approx() can help:\n\n# Sample data\ntime &lt;- c(0, 2, 5, 8, 10)\ntemperature &lt;- c(20, 25, 30, 28, 22)\n\n# Time point to estimate temperature\ntime_estimate &lt;- 6\n\n# Using approx() for linear interpolation\napproximated_temp &lt;- approx(time, temperature, xout = time_estimate)$y\n\ncat(\"Estimated temperature at time\", time_estimate, \"is\", approximated_temp, \"°C\\n\")\n\nEstimated temperature at time 6 is 29.33333 °C\n\n\n\n\nExample 2: Smoothing Out Noisy Data\nNoisy data can be a nightmare for analysis. Let’s say you have a dataset with some irregularly spaced noisy sine wave points, and you want to create a smoother curve:\n\n# Generating noisy sine wave data\nset.seed(123)\nx &lt;- seq(0, 10, length.out = 20)\ny &lt;- sin(x) + rnorm(length(x), mean = 0, sd = 0.2)\n\n# Smoothing out the curve\nsmoothed &lt;- approx(x, y, xout = seq(0, 10, length.out = 100), f = 0.5)$y\n\n# Plotting the original and smoothed data\nplot(x, y, main = \"Noisy Sine Wave vs. Smoothed\", type = \"p\", col = \"blue\", pch = 16)\nlines(seq(0, 10, length.out = 100), smoothed, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Noisy Data\", \"Smoothed\"), col = c(\"blue\", \"red\"), lwd = 2)"
  },
  {
    "objectID": "posts/2023-08-18/index.html",
    "href": "posts/2023-08-18/index.html",
    "title": "Exploring Data Distribution with Box Plots in R",
    "section": "",
    "text": "Are you ready to dive into the world of data visualization in R? One powerful tool at your disposal is the box plot, also known as a box-and-whisker plot. This versatile chart can help you understand the distribution of your data and identify potential outliers. In this blog post, we’ll walk you through the process of creating box plots using R’s ggplot2 package, using the airquality dataset as an example. Whether you’re a beginner or an experienced R programmer, you’ll find something valuable here."
  },
  {
    "objectID": "posts/2023-08-18/index.html#examples-with-ggplot2",
    "href": "posts/2023-08-18/index.html#examples-with-ggplot2",
    "title": "Exploring Data Distribution with Box Plots in R",
    "section": "Examples with ggplot2",
    "text": "Examples with ggplot2\nBefore we jump into code, let’s get the ggplot2 package loaded and our dataset ready:\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\n\nCreating a Basic Box Plot\nLet’s start with a basic example. Suppose we want to visualize the distribution of ozone levels in the airquality dataset. Here’s how you can create a plain box plot:\n\n# Basic box plot for ozone levels\nbasic_box_plot &lt;- ggplot(airquality, aes(x = factor(1), y = Ozone)) +\n  geom_boxplot() +\n  labs(title = \"Basic Box Plot of Ozone Levels\",\n       x = \"\", y = \"Ozone Levels\") +\n  theme_minimal()\n\nbasic_box_plot\n\nWarning: Removed 37 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nIn this example, we use ggplot() to initiate the plot and specify the x aesthetic as a factor to create a single box plot. The y aesthetic is set to the Ozone variable, and we add the geom_boxplot() layer to create the box plot itself. The labs() function helps us set the title and axis labels.\n\n\nAdding Fill to Box Plots\nIf you want to add more visual depth to your box plots, you can use color to differentiate categories. Let’s create a box plot of ozone levels, grouped by the months:\n\n# Box plot with fill for different months\nfilled_box_plot &lt;- ggplot(\n  airquality, \n  aes(\n    x = factor(Month), \n    y = Ozone, \n    fill = factor(Month)\n    )\n  ) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of Ozone Levels by Month\",\n       x = \"Month\", y = \"Ozone Levels\") +\n  scale_fill_discrete(name = \"Month\") +\n  theme_minimal()\n\nfilled_box_plot\n\nWarning: Removed 37 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nIn this code, we add the fill aesthetic to the aes() function, which creates separate box plots for each month and fills them with different colors based on the Month variable.\n\n\nNotching for Comparing Medians\nA notched box plot can help you compare the medians of different groups. Let’s create a notched box plot to visualize the distribution of ozone levels for different temperatures:\n\n# Notched box plot for ozone levels by temperature\nnotched_box_plot &lt;- ggplot(\n  airquality, \n  aes(\n    x = factor(Temp), \n    y = Ozone, \n    fill = factor(Temp)\n    )\n  ) +\n  geom_boxplot(notch = TRUE) +\n  labs(title = \"Notched Box Plot of Ozone Levels by Temperature\",\n       x = \"Temperature\", y = \"Ozone Levels\") +\n  scale_fill_discrete(name = \"Temperature\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nnotched_box_plot\n\n\n\n\nBy setting notch = TRUE within geom_boxplot(), you add notches to the boxes that provide a rough comparison of medians."
  },
  {
    "objectID": "posts/2023-08-18/index.html#base-r-examples",
    "href": "posts/2023-08-18/index.html#base-r-examples",
    "title": "Exploring Data Distribution with Box Plots in R",
    "section": "Base R Examples",
    "text": "Base R Examples\n\nBase boxplot()\n\n# Create a filled box plot of ozone by month\nboxplot(\n  airquality$Ozone ~ airquality$Month, \n  main = \"Distribution of Ozone by Month\", \n  xlab = \"Month\", \n  ylab = \"Ozone\", \n  col = \"lightblue\"\n  )\n\n\n\n\nExplanation:\n\nIn this example, we use the formula notation (~) to create a filled box plot of the ozone variable (airquality$Ozone) grouped by the month variable (airquality$Month).\nWe provide the same title, x-axis label, and y-axis label as in the previous example.\nAdditionally, we specify the col argument to set the color of the boxes to “lightblue”.\n\n\n\nConclusion\nBox plots are a fantastic tool for quickly understanding the distribution of your data. With the ggplot2 package in R, creating informative and visually appealing box plots is both accessible and customizable. I encourage you to experiment with different aesthetics, variations, and datasets to explore the insights these plots can reveal. So why not grab your R console and embark on your data visualization journey today? Happy plotting!\nRemember, the best way to truly master box plots is by trying them yourself. Copy and paste the code snippets provided here into your R environment, modify them, and observe how the plots change. As you become more comfortable, you can start applying box plots to your own datasets and discover new patterns and trends. Happy coding!"
  },
  {
    "objectID": "posts/2023-08-21/index.html",
    "href": "posts/2023-08-21/index.html",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "",
    "text": "Data visualization is a powerful tool for understanding and interpreting data. In this blog post, we will explore how to create box plots with mean values using both base R and ggplot2. We will use the famous iris dataset as an example. So, grab your coding tools and let’s dive into the world of box plots!"
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-1-box-plots-with-mean-value-in-base-r",
    "href": "posts/2023-08-21/index.html#example-1-box-plots-with-mean-value-in-base-r",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 1: Box Plots with Mean Value in Base R",
    "text": "Example 1: Box Plots with Mean Value in Base R\nTo start, let’s use base R to create box plots with mean values. Here’s the code:\n\n# Calculate the mean for each species\nmean_values &lt;- aggregate(iris$Sepal.Length, by = list(iris$Species), FUN = mean)\n\n# Create a box plot with mean value\nboxplot(iris$Sepal.Length ~ iris$Species, \n        main = \"Box Plot with Mean Value\",\n        xlab = \"Species\", ylab = \"Sepal Length\", \n        col = \"lightblue\")\npoints(mean_values$x ~ mean_values$Group.1, col = \"red\", pch = 19)\n\n\n\n\nIn this code, we first load the iris dataset using the data() function. Then, we calculate the mean value for each species using the aggregate() function. Finally, we create a box plot using boxplot() and add the mean values as red points using points()."
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-2",
    "href": "posts/2023-08-21/index.html#example-2",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 2:",
    "text": "Example 2:\nSection 2: Box Plots with Mean Value in ggplot2 Next, let’s explore how to create box plots with mean values using ggplot2. Here’s the code:\n\n# Create a basic box plot with mean using Base R\nboxplot(iris$Sepal.Length, main=\"Box Plot with Mean (Sepal.Length)\", \n        ylab=\"Sepal Length\", col=\"lightblue\")\nabline(h=mean(iris$Sepal.Length), col=\"red\", lwd=2)\n\n\n\n\nIn this code snippet, we load the Iris dataset and generate a box plot for the Sepal.Length attribute. The abline() function adds a horizontal line at the mean value, highlighted in red. Don’t hesitate to modify attributes like color, line width, or title to customize your plot to your heart’s content!\nConclusion: In this blog post, we explored how to create box plots with mean values using both base R and ggplot2. We used the iris dataset as an example and provided code snippets for each approach. Box plots are a great way to visualize the distribution of data and the addition of mean values provides further insights. We encourage you to try these examples with the iris dataset or apply them to your own data. Happy coding and happy visualizing!\nRemember, data visualization is an art form, so feel free to experiment with different customizations and explore other types of plots. The more you practice, the better you’ll become at creating informative and visually appealing visualizations. So, keep coding and keep exploring the world of data visualization!\nCitations: [1] https://stackoverflow.com/questions/64732557/add-mean-to-grouped-box-plot-in-r-with-ggplot2 [2] https://gexijin.github.io/learnR/visualizing-the-iris-flower-data-set.html [3] https://rgraphs.com/make-a-boxplot-in-r-using-already-computed-statistics/ [4] https://www.kaggle.com/code/susree64/ggplot-basic-data-visualization-on-iris-data [5] https://www.sarfarazalam.com/post/r_ggplot_tutorial_barplot_boxplot/r_tutorial_barplot_boxplot [6] https://rstudio-pubs-static.s3.amazonaws.com/669797_ce311ad305e249c2a7278de2fc1c6aac.html"
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-2-single-boxplot-with-mean-line",
    "href": "posts/2023-08-21/index.html#example-2-single-boxplot-with-mean-line",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 2: Single Boxplot with mean line",
    "text": "Example 2: Single Boxplot with mean line\n\n# Create a basic box plot with mean using Base R\nboxplot(iris$Sepal.Length, main=\"Box Plot with Mean (Sepal.Length)\", \n        ylab=\"Sepal Length\", col=\"lightblue\")\nabline(h=mean(iris$Sepal.Length), col=\"red\", lwd=2)\n\n\n\n\nIn this code snippet, we load the Iris dataset and generate a box plot for the Sepal.Length attribute. The abline() function adds a horizontal line at the mean value, highlighted in red. Don’t hesitate to modify attributes like color, line width, or title to customize your plot to your heart’s content!"
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-3-box-plots-with-mean-value-in-ggplot2",
    "href": "posts/2023-08-21/index.html#example-3-box-plots-with-mean-value-in-ggplot2",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 3: Box Plots with Mean Value in ggplot2",
    "text": "Example 3: Box Plots with Mean Value in ggplot2\nNow let’s use the ggplot2 library.\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create a box plot with mean using ggplot2\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot() +\n  geom_point(data = aggregate(Sepal.Length ~ Species, data = iris, mean),\n             aes(x = Species, y = Sepal.Length), color = \"red\", size = 3) +\n  labs(title = \"Box Plot of Sepal Length by Species\",\n       x = \"Species\",\n       y = \"Sepal Length\") +\n  theme_minimal()\n\n\n\n\n\nWe load the ggplot2 library using library(ggplot2).\nWe use the ggplot() function to create a ggplot object and specify the dataset and aesthetic mappings with the aes() function.\nWe use geom_boxplot() to create the box plot.\nWe use geom_point() to add red points representing the mean values using the aggregate() result.\nlabs() is used to set the plot title and axis labels.\nWe use theme_minimal() to apply a clean and minimal theme to the plot."
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-4-single-boxplot-with-mean-line-ggplot2",
    "href": "posts/2023-08-21/index.html#example-4-single-boxplot-with-mean-line-ggplot2",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 4: Single Boxplot with mean line ggplot2",
    "text": "Example 4: Single Boxplot with mean line ggplot2\n\n# Create a box plot with mean using ggplot2\nggplot(iris, aes(x=\"\", y=Sepal.Length)) +\n  geom_boxplot(fill=\"lightblue\", color=\"black\") +\n  geom_hline(yintercept = mean(iris$Sepal.Length), color=\"red\", linetype=\"dashed\") +\n  labs(title=\"Box Plot with Mean using ggplot2\",\n       y=\"Sepal Length\") +\n  theme_minimal()\n\n\n\n\nHere, we use the ggplot() function to set up the plot structure and aesthetics. The geom_boxplot() function generates the box plot, and the geom_hline() function adds the mean line. Customize the color palette, line types, titles, and themes to make your visualization shine!"
  },
  {
    "objectID": "posts/2023-08-22/index.html",
    "href": "posts/2023-08-22/index.html",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "",
    "text": "Data visualization is a powerful tool that allows us to uncover patterns and insights within datasets. One such tool in the R programming arsenal is the stripchart() function. If you’re looking to reveal distribution patterns in your data with style and simplicity, then this function might just become your new best friend. In this blog post, we’ll dive into the world of stripchart(), exploring its syntax, uses, and providing you with hands-on examples to master its application."
  },
  {
    "objectID": "posts/2023-08-22/index.html#example-1.-comparing-distributions",
    "href": "posts/2023-08-22/index.html#example-1.-comparing-distributions",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "Example 1. Comparing Distributions",
    "text": "Example 1. Comparing Distributions\nLet’s say you have two datasets containing exam scores of students from different schools. You can use stripchart() to visually compare their distributions:\n\n# Sample data\nschool_A &lt;- c(70, 72, 75, 78, 80, 85, 88)\nschool_B &lt;- c(65, 68, 70, 73, 75, 80, 85, 90)\n\n# Create a stripchart\nstripchart(list(School_A = school_A, School_B = school_B),\n           vertical = FALSE, method = \"jitter\",\n           main = \"Exam Score Distributions\",\n           xlab = \"Score\", ylab = \"School\")\n\n\n\n\nIn this example, we’re using the \"jitter\" method to spread out the points along the y-axis, making it easier to see the density of scores."
  },
  {
    "objectID": "posts/2023-08-22/index.html#example-2.-visualizing-data-points",
    "href": "posts/2023-08-22/index.html#example-2.-visualizing-data-points",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "Example 2. Visualizing Data Points",
    "text": "Example 2. Visualizing Data Points\nImagine you have a dataset with the heights of individuals. You can use stripchart() to visualize each individual’s height as a data point:\n\n# Sample data\nheights &lt;- c(160, 170, 175, 155, 180, 165, 172, 158, 185)\n\n# Create a stripchart\nstripchart(heights, method = \"overplot\",\n           main = \"Individual Heights\",\n           xlab = \"Height (cm)\")\n\n\n\n\nIn this case, the \"overplot\" method allows us to see individual data points that might overlap."
  },
  {
    "objectID": "posts/2023-08-22/index.html#example-3.-categorical-data-comparison",
    "href": "posts/2023-08-22/index.html#example-3.-categorical-data-comparison",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "Example 3. Categorical Data Comparison",
    "text": "Example 3. Categorical Data Comparison\nSuppose you have a dataset of employees’ years of service. You can use stripchart() to compare the years of service among different departments:\n\n# Sample data\nhr_dept &lt;- c(2, 3, 4, 2, 5, 3)\ntech_dept &lt;- c(1, 2, 1, 3, 2, 4, 2)\n\n# Create a stripchart\nstripchart(list(HR = hr_dept, Tech = tech_dept),\n           vertical = FALSE, method = \"stack\",\n           main = \"Years of Service by Department\",\n           xlab = \"Years\", ylab = \"Department\")\n\n\n\n\nThe \"divide\" method segments data points based on the provided categories."
  },
  {
    "objectID": "posts/2023-08-22/index.html#example-4.-all-three-methods-in-one",
    "href": "posts/2023-08-22/index.html#example-4.-all-three-methods-in-one",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "Example 4. All three methods in one",
    "text": "Example 4. All three methods in one\nNow let’s see what all three methods show for the same data set. We will place them all on one plot.\n\nx &lt;- rnorm(100)\n\npar(mfrow = c(2, 2))\n# Create a stripchart of the heights of 100 randomly generated people\nstripchart(x, method = \"overplot\", main = \"Overplot\")\n\n# Create a stripchart of the heights of 100 people, jittering the points to prevent overlapping\nstripchart(x, method = \"jitter\", main = \"Jitter\")\n\n# Create a stripchart of the heights of 100 people, stacking the points on top of each other\nstripchart(x, method = \"stack\", main = \"Stack\")\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "posts/2023-08-23/index.html",
    "href": "posts/2023-08-23/index.html",
    "title": "Exploring Data Distribution in R: A Comprehensive Guide",
    "section": "",
    "text": "Understanding the distribution of your data is a fundamental step in any data analysis process. It gives you insights into the spread, central tendency, and overall shape of your data. In this blog post, we’ll explore two popular functions in R for visualizing data distribution: density() and hist(). We’ll use the classic Iris dataset for our examples. Additionally, we will introduce the {TidyDensity} library and show how it can be used to create distribution plots."
  },
  {
    "objectID": "posts/2023-08-23/index.html#example-1.-visualizing-data-distribution-using-density",
    "href": "posts/2023-08-23/index.html#example-1.-visualizing-data-distribution-using-density",
    "title": "Exploring Data Distribution in R: A Comprehensive Guide",
    "section": "Example 1. Visualizing Data Distribution using density()",
    "text": "Example 1. Visualizing Data Distribution using density()\nThe density() function in R is used to estimate the probability density function of a continuous random variable. This function calculates density curve, allowing us to see the underlying distribution of the data with the plot() function.\n\nSyntax:\ndensity(x, ...)\nWhere x is the numeric vector for which the density will be estimated.\n\n\nExample:\n\n# Plot the density distribution of Sepal Length\nplot(\n  density(iris$Sepal.Length), \n  main=\"Density Plot of Sepal Length\",\n  xlab=\"Sepal Length\", ylab=\"Density\"\n  )\n\n\n\n\nIn this example, we load the Iris dataset and plot the density distribution of Sepal Length. The main, xlab, and ylab arguments are used to provide titles and labels to the plot."
  },
  {
    "objectID": "posts/2023-08-23/index.html#example-2.-visualizing-data-distribution-using-hist",
    "href": "posts/2023-08-23/index.html#example-2.-visualizing-data-distribution-using-hist",
    "title": "Exploring Data Distribution in R: A Comprehensive Guide",
    "section": "Example 2. Visualizing Data Distribution using hist()",
    "text": "Example 2. Visualizing Data Distribution using hist()\nThe hist() function is another powerful tool for visualizing the distribution of data. It creates a histogram, which is a graphical representation of the frequency distribution of a dataset.\n\nSyntax:\nhist(x, ...)\nWhere x is the numeric vector for which the histogram will be created.\n\n\nExample:\n\n# Create a histogram of Petal Width\nhist(iris$Petal.Width, main=\"Histogram of Petal Width\",\n     xlab=\"Petal Width\", ylab=\"Frequency\", col=\"skyblue\")\n\n\n\n\nHere, we create a histogram of the Petal Width from the Iris dataset. The main, xlab, ylab, and col arguments allow customization of the plot’s appearance.\n\n\nAdd lines to a histogram\nHere we will combine the density plot and the histogram together. Sometimes this helps.\n\nx &lt;- iris$Sepal.Length\n\nhist(x, prob = TRUE)\nlines(density(x))"
  },
  {
    "objectID": "posts/2023-08-23/index.html#using-tidydensity-for-data-distribution-visualization",
    "href": "posts/2023-08-23/index.html#using-tidydensity-for-data-distribution-visualization",
    "title": "Exploring Data Distribution in R: A Comprehensive Guide",
    "section": "Using TidyDensity for Data Distribution Visualization",
    "text": "Using TidyDensity for Data Distribution Visualization\nThe TidyDensity library is a convenient way to visualize data distributions with a modern and tidy approach. Let’s take a look at how it works.\n\nExample:\n\n# Load the required library\nlibrary(TidyDensity)\n\n# Extract the 'mpg' column\nx &lt;- mtcars$mpg\n\n# Use TidyDensity functions to visualize data distribution\ntidy_empirical(x) |&gt; tidy_autoplot()\n\n\n\n\nIn this example, we load the TidyDensity library and the mtcars dataset. We extract the ‘mpg’ column and then utilize the tidy_empirical() function to compute the empirical density. The tidy_autoplot() function creates a visually appealing distribution plot."
  },
  {
    "objectID": "posts/2023-08-24/index.html",
    "href": "posts/2023-08-24/index.html",
    "title": "Plotting Multiple Lines on a Graph in R: A Step-by-Step Guide",
    "section": "",
    "text": "Graphs are powerful visual tools for analyzing and presenting data. In this blog post, we will explore how to plot multiple lines on a graph using base R. We will cover two methods: matplot() and lines(). These functions provide flexibility and control over the appearance of the lines, allowing you to create informative and visually appealing plots. So, let’s dive in and learn how to plot multiple lines on a graph in R!"
  },
  {
    "objectID": "posts/2023-08-24/index.html#example-1-using-matplot",
    "href": "posts/2023-08-24/index.html#example-1-using-matplot",
    "title": "Plotting Multiple Lines on a Graph in R: A Step-by-Step Guide",
    "section": "Example 1 Using matplot():",
    "text": "Example 1 Using matplot():\nThe matplot() function is a convenient way to plot multiple lines in one chart when you have a dataset in a wide format. Here’s an example:\n\n# Create sample data\nx &lt;- 1:10\ny1 &lt;- c(1, 4, 3, 6, 5, 8, 7, 9, 10, 2)\ny2 &lt;- c(2, 5, 4, 7, 6, 9, 8, 10, 3, 1)\ny3 &lt;- c(3, 6, 5, 8, 7, 10, 9, 2, 4, 1)\n\n# Plot multiple lines using matplot\nmatplot(x, cbind(y1, y2, y3), type = \"l\", lty = 1, \n        col = c(\"red\", \"blue\", \"green\"), xlab = \"X\", \n        ylab = \"Y\", main = \"Multiple Lines Plot\")\nlegend(\"topright\", legend = c(\"Line 1\", \"Line 2\", \"Line 3\"), \n       col = c(\"red\", \"blue\", \"green\"), \n       lty = 1)\n\n\n\n\n\nExplanation:\n\nWe first create sample data for the x-axis (x) and three lines (y1, y2, y3).\nThe matplot() function is then used to plot the lines. We pass the x-axis values (x) and a matrix of y-axis values (cbind(y1, y2, y3)) as input.\nThe type = \"l\" argument specifies that we want to plot lines.\nThe lty = 1 argument sets the line type to solid.\nThe col argument specifies the colors of the lines.\nThe xlab, ylab, and main arguments set the labels for the x-axis, y-axis, and the main title of the plot, respectively.\nFinally, the legend() function is used to add a legend to the plot, indicating the colors and labels of the lines."
  },
  {
    "objectID": "posts/2023-08-24/index.html#example-2-using-lines",
    "href": "posts/2023-08-24/index.html#example-2-using-lines",
    "title": "Plotting Multiple Lines on a Graph in R: A Step-by-Step Guide",
    "section": "Example 2 Using lines():",
    "text": "Example 2 Using lines():\nAnother way to plot multiple lines is to plot them one by one using the points() and lines() functions. Here’s an example:\n\n# Create sample data\nx &lt;- 1:10\ny1 &lt;- c(1, 4, 3, 6, 5, 8, 7, 9, 10, 2)\ny2 &lt;- c(2, 5, 4, 7, 6, 9, 8, 10, 3, 1)\ny3 &lt;- c(3, 6, 5, 8, 7, 10, 9, 2, 4, 1)\n\n# Create an empty plot\nplot(x, y1, type = \"n\", xlim = c(1, 10), ylim = c(0, 10), \n     xlab = \"X\", ylab = \"Y\", main = \"Multiple Lines Plot\")\n\n# Plot each line one by one\nlines(x, y1, type = \"l\", col = \"red\")\nlines(x, y2, type = \"l\", col = \"blue\")\nlines(x, y3, type = \"l\", col = \"green\")\n\n# Add a legend\nlegend(\"topright\", legend = c(\"Line 1\", \"Line 2\", \"Line 3\"), \n       col = c(\"red\", \"blue\", \"green\"), lty = 1)\n\n\n\n\n\nExplanation\n\nWe create the same sample data as in the previous example.\nThe plot() function is used to create an empty plot with appropriate labels and limits.\nWe then use the lines() function to plot each line one by one. The type = \"l\" argument specifies that we want to plot lines, and the col argument sets the color of each line.\nFinally, the legend() function is used to add a legend to the plot."
  },
  {
    "objectID": "posts/2023-08-25/index.html",
    "href": "posts/2023-08-25/index.html",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "",
    "text": "Histograms are a powerful tool for visualizing the distribution of numerical data. They allow us to quickly understand the frequency distribution of values within a dataset. In this tutorial, we’ll explore how to create multiple histograms using two popular R packages: base R and ggplot2. By the end of this guide, you’ll be able to confidently display multiple histograms on a single graph using both methods."
  },
  {
    "objectID": "posts/2023-08-25/index.html#example-1-creating-side-by-side-histograms",
    "href": "posts/2023-08-25/index.html#example-1-creating-side-by-side-histograms",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Example 1: Creating Side-by-Side Histograms",
    "text": "Example 1: Creating Side-by-Side Histograms\nTo plot multiple histograms side by side using base R, you can make use of the par(mfrow) function. This function allows you to specify the number of rows and columns for your layout. Here’s an example:\n\n# Create two example datasets\ndata1 &lt;- rnorm(100, mean = 0, sd = 1)\ndata2 &lt;- rnorm(100, mean = 2, sd = 1)\n\n# Set up a side-by-side layout\npar(mfrow = c(1, 2))\n\n# Create the first histogram\nhist(data1, main = \"Histogram 1\", xlab = \"Value\", ylab = \"Frequency\")\n\n# Create the second histogram\nhist(data2, main = \"Histogram 2\", xlab = \"Value\", ylab = \"Frequency\")\n\n\n\npar(mfrow = c(1, 1))\n\nIn this example, we first generate two example datasets (data1 and data2). Then, we use par(mfrow = c(1, 2)) to set up a side-by-side layout. Finally, we create the histograms for each dataset using the hist() function.\nNow, let’s plot them on the same graph."
  },
  {
    "objectID": "posts/2023-08-25/index.html#example-2-creating-histograms-on-the-same-graph",
    "href": "posts/2023-08-25/index.html#example-2-creating-histograms-on-the-same-graph",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Example 2: Creating Histograms on the same graph",
    "text": "Example 2: Creating Histograms on the same graph\n\n# Create two example datasets\ndata1 &lt;- rnorm(100, mean = 0, sd = 1)\ndata2 &lt;- rnorm(100, mean = 2, sd = 1)\n\nxmin &lt;- min(data1, data2)\nxmax &lt;- max(data1, data2)\n\n# Create the first histogram\nhist(data1, main = \"Histogram 1\", xlab = \"Value\", ylab = \"Frequency\",\n     col = \"powderblue\", xlim = c(xmin, xmax))\n\n# Create the second histogram\nhist(data2, main = \"Histogram 2\", xlab = \"Value\", ylab = \"Frequency\",\n     col = \"pink\", add = TRUE, xlim = c(xmin, xmax))"
  },
  {
    "objectID": "posts/2023-08-25/index.html#using-ggplot2-to-plot-multiple-histograms",
    "href": "posts/2023-08-25/index.html#using-ggplot2-to-plot-multiple-histograms",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Using ggplot2 to Plot Multiple Histograms",
    "text": "Using ggplot2 to Plot Multiple Histograms\nggplot2 is a highly customizable and versatile package for creating complex visualizations. Let’s see how to use ggplot2 to create multiple histograms.\n\nSyntax for Creating a Histogram in ggplot2\nTo create a histogram using ggplot2, you use the ggplot() function and the geom_histogram() layer. The basic syntax is as follows:\nlibrary(ggplot2)\n\nggplot(data, aes(x = variable)) +\n  geom_histogram(binwidth = width, fill = \"color\") +\n  labs(title = \"Histogram Title\", x = \"X-axis Label\", y = \"Frequency\")\n\ndata: The dataset containing the variable you want to plot.\nvariable: The variable for which you want to create a histogram.\nbinwidth: The width of the histogram bins.\ncolor: The fill color of the bars.\n\n\n\nCreating Multiple Histograms\nTo create multiple histograms using ggplot2, you can utilize facets. Facets allow you to split your data into subsets and create separate histograms for each subset. Here’s an example:\nlibrary(ggplot2)\n\n# Create an example dataset\ndata &lt;- data.frame(\n  group = rep(c(\"Group A\", \"Group B\"), each = 100),\n  value = c(rnorm(100, mean = 0, sd = 1), rnorm(100, mean = 2, sd = 1))\n)\n\n# Create multiple histograms using facets\nggplot(data, aes(x = value)) +\n  geom_histogram(binwidth = 0.5, fill = \"steelblue\") +\n  labs(title = \"Multiple Histograms\", x = \"Value\", y = \"Frequency\") +\n  facet_wrap(~ group, nrow = 1)\nIn this example, we first create an example dataset with two groups (Group A and Group B). Then, we use the facet_wrap() function to create separate histograms for each group."
  },
  {
    "objectID": "posts/2023-08-25/index.html#get-hands-on",
    "href": "posts/2023-08-25/index.html#get-hands-on",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Get Hands-On!",
    "text": "Get Hands-On!\nNow that you have a grasp of how to create multiple histograms using both base R and ggplot2, it’s time to put your skills to the test. Pick a dataset you’re interested in, import it into R, and start creating engaging histograms. Experiment with different bin widths, colors, and layouts to find the visualizations that best convey your data’s story.\nRemember, practice makes perfect! The more you experiment and create histograms, the more comfortable you’ll become with the syntax and options offered by both base R and ggplot2. Happy plotting!"
  },
  {
    "objectID": "posts/2023-08-25/index.html#example-3-using-ggplot2-to-plot-multiple-histograms",
    "href": "posts/2023-08-25/index.html#example-3-using-ggplot2-to-plot-multiple-histograms",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Example 3: Using ggplot2 to Plot Multiple Histograms",
    "text": "Example 3: Using ggplot2 to Plot Multiple Histograms\nggplot2 is a highly customizable and versatile package for creating complex visualizations. Let’s see how to use ggplot2 to create multiple histograms."
  },
  {
    "objectID": "posts/2023-08-28/index.html",
    "href": "posts/2023-08-28/index.html",
    "title": "Enhancing Your Histograms in R: Adding Vertical Lines for Better Insights",
    "section": "",
    "text": "Are you tired of looking at plain, vanilla histograms that just show the distribution of your data without any additional context? If so, you’re in for a treat! In this blog post, we’ll explore a simple yet powerful technique to take your histograms to the next level by adding vertical lines that provide valuable insights into your data. We’ll use R, a popular programming language for data analysis and visualization, to demonstrate how to achieve this step by step. Don’t worry if you’re new to R or programming – we’ll break down each code block in easy-to-understand terms."
  },
  {
    "objectID": "posts/2023-08-28/index.html#using-base-r",
    "href": "posts/2023-08-28/index.html#using-base-r",
    "title": "Enhancing Your Histograms in R: Adding Vertical Lines for Better Insights",
    "section": "Using Base R",
    "text": "Using Base R\n\nExample 1: Adding a Solid Vertical Line at a Specific Location\nTo add a solid vertical line at a specific location in a histogram, we can use the abline() function in R. Here’s an example:\n\n# Create a vector of data\ndata &lt;- c(5, 7, 3, 9, 2, 6, 4, 8)\n\n# Create a histogram to visualize the distribution of data\nhist(data)\n\n# Add a vertical line at x = 6\nabline(v = 6)\n\n\n\n\nExplanation:\n\nWe first create a vector of data with some values.\nNext, we create a histogram using the hist() function to visualize the distribution of the data.\nFinally, we use the abline() function with the argument v = 6 to add a vertical line at x = 6 to the histogram.\n\n\n\nExample 2: Adding a Customized Vertical Line at a Specific Location\nIf you want to add a customized vertical line with different colors, line widths, or line types, you can modify the abline() function. Here’s an example:\n\n# Create a vector of data\ndata &lt;- c(5, 7, 3, 9, 2, 6, 4, 8)\n\n# Create a histogram to visualize the distribution of data\nhist(data)\n\n# Add a vertical line at the mean value of the data with a red dashed line\nabline(v = mean(data), col = 'red', lwd = 2, lty = 'dashed')\n\n\n\n\nExplanation:\n\nWe start by creating a vector of data.\nThen, we create a histogram to visualize the distribution of the data.\nFinally, we use the abline() function with the argument v = mean(data) to add a vertical line at the mean value of the data. We also customize the line color to red, line width to 2, and line type to dashed.\n\n\n\nExample 3: Adding Multiple Customized Vertical Lines\nIn some cases, you may want to add multiple customized vertical lines to a histogram. Here’s an example:\n\n# Create a vector of data\ndata &lt;- c(5, 7, 3, 9, 2, 6, 4, 8)\n\n# Create a histogram to visualize the distribution of data\nhist(data)\n\n# Add multiple vertical lines at specific locations with different colors\nabline(v = c(4, 6, 8), col = c('red', 'blue', 'green'), lwd = 2, lty = 'dashed')\n\n\n\n\nExplanation:\n\nWe create a vector of data.\nThen, we create a histogram to visualize the distribution of the data.\nFinally, we use the abline() function with the argument v = c(4, 6, 8) to add multiple vertical lines at specific locations. We customize each line with different colors (red, blue, green), line width (2), and line type (dashed)."
  },
  {
    "objectID": "posts/2023-08-28/index.html#using-ggplot2",
    "href": "posts/2023-08-28/index.html#using-ggplot2",
    "title": "Enhancing Your Histograms in R: Adding Vertical Lines for Better Insights",
    "section": "Using ggplot2",
    "text": "Using ggplot2\n\nExample 1: Marking the Mean\nLet’s start with a simple scenario: you have a dataset of exam scores and you want to visualize the distribution while highlighting the mean score. Here’s how you can do it:\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a sample dataset\ndata &lt;- data.frame(x = c(65, 72, 78, 85, 90, 92, 95, 98, 100))\n\n# Create a histogram with a vertical line for the mean\nggplot(data=data, aes(x=x)) +\n  geom_histogram(binwidth=5, fill=\"blue\", color=\"black\") +\n  geom_vline(aes(xintercept=mean(data)), color=\"red\", linetype=\"dashed\") +\n  labs(title=\"Exam Scores Distribution with Mean Highlighted\", x=\"Scores\", y=\"Frequency\") +\n  theme_minimal()\n\nWarning in mean.default(data): argument is not numeric or logical: returning NA\n\n\nWarning: Removed 9 rows containing missing values (`geom_vline()`).\n\n\n\n\n\nIn this example, we used the ggplot2 library to create a histogram. The geom_vline function adds a vertical line at the position of the mean score. The xintercept argument specifies the position of the line, and we used the color and linetype arguments to style the line.\n\n\nExample 2: Threshold Highlighting\nNow, let’s say you’re analyzing customer purchase data and you want to see how many customers made purchases above a certain threshold. You can add a vertical line to indicate this threshold:\n\n# Create a sample dataset\npurchase_amounts &lt;- data.frame(x= c(20, 30, 45, 50, 55, 60, 70, 80, 90, 100, 110, 130, 150))\n\n# Create a histogram with a vertical line for the threshold\nthreshold &lt;- 70\nggplot(data=data.frame(amount=purchase_amounts), aes(x=x)) +\n  geom_histogram(binwidth=20, fill=\"green\", color=\"black\") +\n  geom_vline(xintercept=threshold, color=\"orange\", linetype=\"dashed\") +\n  labs(title=\"Purchase Amount Distribution with Threshold Highlighted\", x=\"Purchase Amount\", y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\nIn this example, we directly specified the threshold value using the threshold variable. The vertical line is added to the histogram at that threshold value."
  },
  {
    "objectID": "posts/2023-08-28/index.html#i-encourage-you-to-try",
    "href": "posts/2023-08-28/index.html#i-encourage-you-to-try",
    "title": "Enhancing Your Histograms in R: Adding Vertical Lines for Better Insights",
    "section": "I Encourage you to try!",
    "text": "I Encourage you to try!\nAdding vertical lines to histograms in R is a straightforward way to enhance your data visualization. By highlighting specific values or thresholds, you can convey more information to your audience and make your insights clearer. Don’t hesitate to experiment with different datasets, color schemes, and line styles to match your needs and preferences.\nSo, what are you waiting for? Open up R, load your data, and start creating histograms with vertical lines to uncover hidden patterns and insights that may have gone unnoticed. Happy coding and visualizing!\nRemember, practice makes perfect. The more you experiment with these concepts, the more proficient you’ll become at crafting compelling visualizations. Have fun exploring your data in a new light!"
  },
  {
    "objectID": "posts/2023-08-29/index.html",
    "href": "posts/2023-08-29/index.html",
    "title": "Visualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset",
    "section": "",
    "text": "Categorical data is a type of data that represents distinct groups or categories. Visualizing categorical data can provide valuable insights and help in understanding patterns and relationships within the data. In this blog post, we will explore three popular charts for visualizing categorical data in R using the iris dataset: geom_bar() from ggplot2, a grouped boxplot with base R and ggplot2, and a mosaic plot. We will explain each section of code in simple terms and encourage readers to try these charts on their own."
  },
  {
    "objectID": "posts/2023-08-29/index.html#example-1-barplots-with-geom_bar-from-ggplot2",
    "href": "posts/2023-08-29/index.html#example-1-barplots-with-geom_bar-from-ggplot2",
    "title": "Visualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset",
    "section": "Example 1 Barplots with geom_bar() from ggplot2:",
    "text": "Example 1 Barplots with geom_bar() from ggplot2:\nBarplots are a common and effective way to visualize categorical data. We can use the geom_bar() function from the ggplot2 package to create barplots in R. The geom_bar() function accepts a variable for the x-axis and plots the number of times each value of the variable appears in the dataset[3].\n\nlibrary(ggplot2)\n\n# Create a barplot using geom_bar()\nggplot(data = iris, aes(x = Species, fill = factor(Species))) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    title = \"Bar Chart of Species Count\",\n    ylab = \"Count\",\n    fill = \"Species\"\n  )\n\n\n\n\nExplanation: - Load the ggplot2 package using library(ggplot2). - The iris dataset is already available in R, so we can directly use it. - The aes() function specifies the aesthetic mappings, where x represents the variable on the x-axis. - The geom_bar() function creates the barplot.\nTry creating a barplot with the Species variable from the iris dataset using the provided code. Experiment with different variables and datasets to explore the patterns and distributions within your data."
  },
  {
    "objectID": "posts/2023-08-29/index.html#example-2-grouped-boxplot-with-base-r-and-ggplot2",
    "href": "posts/2023-08-29/index.html#example-2-grouped-boxplot-with-base-r-and-ggplot2",
    "title": "Visualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset",
    "section": "Example 2 Grouped Boxplot with base R and ggplot2",
    "text": "Example 2 Grouped Boxplot with base R and ggplot2\nA grouped boxplot is a useful chart for comparing the distribution of a continuous variable across different categories of a categorical variable. We can create a grouped boxplot using both base R and ggplot2.\n\nBase R\n\n# Create a grouped boxplot using  base R\nboxplot(Sepal.Length ~ Species, data = iris)\n\n\n\n\n\n\nggplot2\n\n# Create a grouped boxplot using ggplot2\nggplot(data = iris, aes(x = Species, y = Sepal.Length,\n                        fill = factor(Species))) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(fill = \"Species\")\n\n\n\n\nExplanation: - In base R, we use the boxplot() function to create a grouped boxplot. The formula Sepal.Length ~ Species specifies that the Sepal.Length variable should be plotted against the Species variable[2]. - In ggplot2, we use the geom_boxplot() function to create a grouped boxplot. The aes() function specifies the aesthetic mappings, where x represents the categorical variable and y represents the numeric variable.\nCreate a grouped boxplot with the Sepal.Length variable across different species in the iris dataset using either base R or ggplot2. Compare the distributions of Sepal.Length for each species and observe any differences."
  },
  {
    "objectID": "posts/2023-08-29/index.html#example-3-mosaic-plot",
    "href": "posts/2023-08-29/index.html#example-3-mosaic-plot",
    "title": "Visualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset",
    "section": "Example 3 Mosaic Plot",
    "text": "Example 3 Mosaic Plot\nA mosaic plot is a graphical representation of the relationship between two or more categorical variables. It displays the proportions of each category within the variables and allows for visual comparison.\n\nmosaicplot(table(iris$Species, iris$Petal.Width))\n\n\n\n\nExplanation: - The table() function creates a contingency table of the two variables, Species and Petal.Width, from the iris dataset. - The mosaicplot() function creates the mosaic plot.\nCreate a mosaic plot with the Species and Petal.Width variables from the iris dataset using the provided code. Explore the relationships and proportions between the variables. Experiment with different combinations of variables to gain insights from the mosaic plot."
  },
  {
    "objectID": "posts/2023-08-30/index.html",
    "href": "posts/2023-08-30/index.html",
    "title": "Exploring Relationships with Correlation Heatmaps in R",
    "section": "",
    "text": "Data visualization is a powerful tool for understanding the relationships between variables in a dataset. One of the most common and insightful ways to visualize correlations is through heatmaps. In this blog post, we’ll dive into the world of correlation heatmaps using R, using the mtcars and iris datasets as examples. By the end of this post, you’ll be equipped to create informative correlation heatmaps on your own."
  },
  {
    "objectID": "posts/2023-08-30/index.html#creating-correlation-heatmaps",
    "href": "posts/2023-08-30/index.html#creating-correlation-heatmaps",
    "title": "Exploring Relationships with Correlation Heatmaps in R",
    "section": "Creating Correlation Heatmaps",
    "text": "Creating Correlation Heatmaps\n\nExample 1: mtcars Dataset\nLet’s start by exploring the relationships within the mtcars dataset, which contains information about various car models and their characteristics.\n\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(mtcars)\n\n# Create a basic correlation heatmap using corrplot\ncorrplot(cor_matrix, method = \"color\")\n\n\n\n\nIn this example, we use the cor() function to compute the correlation matrix for the mtcars dataset. The corrplot() function is then used to create the heatmap. The argument method = \"color\" specifies that we want to represent the correlation values using colors.\n\n\nExample 2: iris Dataset\nNow, let’s explore the relationships within the iris dataset, which contains measurements of various iris flowers.\n\n# Calculate the correlation matrix\ncor_matrix_iris &lt;- cor(iris[, 1:4])  # Consider only numeric columns\n\n# Create a more visually appealing heatmap\nggcorrplot(cor_matrix_iris, type = \"lower\", colors = c(\"#6D9EC1\", \"white\", \"#E46726\"))\n\n\n\n\nIn this example, we calculate the correlation matrix for the first four numeric columns of the iris dataset using cor(). We then use the corrplot() function from the ggcorrplot package to create a more aesthetically pleasing heatmap. The type = \"lower\" argument indicates that we want to display only the lower triangle of the correlation matrix. We also customize the color scheme using the colors argument.\nIf you want to check out how to get a correlation heatmap for a time series lagged against itself you can see this article here."
  },
  {
    "objectID": "posts/2023-08-30/index.html#interpreting-the-heatmap",
    "href": "posts/2023-08-30/index.html#interpreting-the-heatmap",
    "title": "Exploring Relationships with Correlation Heatmaps in R",
    "section": "Interpreting the Heatmap",
    "text": "Interpreting the Heatmap\nIn both examples, the heatmap provides a visual representation of the relationships between variables. Darker colors indicate stronger correlations, while lighter colors suggest weaker or no correlations. By analyzing the heatmap, you can quickly identify which variables are positively, negatively, or not correlated with each other."
  },
  {
    "objectID": "posts/2023-08-31/index.html",
    "href": "posts/2023-08-31/index.html",
    "title": "Creating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2",
    "section": "",
    "text": "When it comes to conveying information effectively, data visualization is a powerful tool that can make complex data more accessible and understandable. One captivating type of data visualization is the lollipop chart. Lollipop charts are a great way to showcase and compare data points while adding a touch of elegance to your presentations. In this blog post, we will dive into what lollipop charts are, why they are useful, and how you can create your own stunning lollipop charts using the ggplot2 package in R."
  },
  {
    "objectID": "posts/2023-08-31/index.html#when-to-use-lollipop-charts",
    "href": "posts/2023-08-31/index.html#when-to-use-lollipop-charts",
    "title": "Creating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2",
    "section": "When to Use Lollipop Charts",
    "text": "When to Use Lollipop Charts\nLollipop charts are particularly effective in the following scenarios:\n\n1. Comparing Data Points:\nLollipop charts excel at highlighting individual data points and comparing their values. When you want to showcase the differences between distinct values, a lollipop chart can provide a clear visual representation.\n\n\n2. Showing Distribution:\nLollipop charts can also be used to display the distribution of data points. By placing lollipops along an axis, you can provide insights into the range and distribution of your data.\n\n\n3. Emphasizing Outliers:\nIf your data contains outliers that you want to draw attention to, lollipop charts can be a fantastic choice. Outliers can be visually distinguished from the rest of the data, aiding in spotting anomalies.\n\n\n4. Limited Data Points:\nWhen you’re working with a small dataset, a lollipop chart can be more effective than a bar chart, which might appear overly crowded for a few data points."
  },
  {
    "objectID": "posts/2023-08-31/index.html#examples-of-lollipop-charts",
    "href": "posts/2023-08-31/index.html#examples-of-lollipop-charts",
    "title": "Creating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2",
    "section": "Examples of Lollipop Charts",
    "text": "Examples of Lollipop Charts\n\nExample 1: Top Movies’ IMDb Ratings\nSuppose we have a dataset containing the top-rated movies and their IMDb ratings. We can use a lollipop chart to visualize these ratings:\n\nmovies &lt;- tibble(\n  Movie = c(\"The Shawshank Redemption\", \"The Godfather\", \"The Dark Knight\", \"Pulp Fiction\") |&gt; factor(),\n  Rating = c(9.3, 9.2, 9.0, 8.9)\n)\n\nlollipop_chart(movies, Movie, Rating, \"Top Movies' IMDb Ratings\")\n\n\n\n\n\n\nExample 2: Exam Scores Comparison\nConsider a scenario where we want to compare the scores of students from two different classes. A lollipop chart can effectively illustrate the differences:\n\nexam_scores &lt;- data.frame(\n  Class = rep(c(\"Class A\", \"Class B\"), each = 5),\n  Student = c(\"Alice\", \"Bob\", \"Carol\", \"David\", \"Emma\", \"Frank\", \"Grace\", \"Hannah\", \"Ivan\", \"Jack\") |&gt; factor(),\n  Score = c(85, 78, 92, 67, 75, 88, 82, 95, 70, 79)\n)\n\nlollipop_chart(exam_scores, Student, Score, \"Exam Scores Comparison\")"
  },
  {
    "objectID": "posts/2023-08-31/index.html#try-lollipop-charts-yourself",
    "href": "posts/2023-08-31/index.html#try-lollipop-charts-yourself",
    "title": "Creating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2",
    "section": "Try Lollipop Charts Yourself!",
    "text": "Try Lollipop Charts Yourself!\nLollipop charts provide an engaging way to display and compare data points while highlighting key insights. With the ggplot2 package in R, you have the tools to create stunning lollipop charts for your own datasets. Experiment with different datasets and customize the appearance of your charts to suit your needs. Happy charting!\nIn this blog post, we explored what lollipop charts are, when to use them, and how to create them using the ggplot2 package in R. We provided examples of real-world scenarios where lollipop charts can be valuable and even shared a custom lollipop_chart() function to streamline the chart creation process. Now it’s your turn to apply this knowledge and create captivating lollipop charts with your own data!"
  },
  {
    "objectID": "posts/2023-09-01/index.html",
    "href": "posts/2023-09-01/index.html",
    "title": "Kernel Density Plots in R",
    "section": "",
    "text": "Kernel Density Plots are a type of plot that displays the distribution of values in a dataset using one continuous curve. They are similar to histograms, but they are even better at displaying the shape of a distribution since they aren’t affected by the number of bins used in the histogram. In this blog post, we will discuss what Kernel Density Plots are in simple terms, what they are useful for, and show several examples using both base R and ggplot2."
  },
  {
    "objectID": "posts/2023-09-01/index.html#examples-using-base-r",
    "href": "posts/2023-09-01/index.html#examples-using-base-r",
    "title": "Kernel Density Plots in R",
    "section": "Examples using base R",
    "text": "Examples using base R\nTo create a Kernel Density Plot in base R, we can use the density() function to estimate the density and the plot() function to plot it. Here’s an example:\n\n# Generate data\nset.seed(1234)\nx &lt;- rnorm(500)\n\n# Estimate density\ndens &lt;- density(x)\n\n# Plot density\nplot(dens)\n\n\n\n\nThis will generate a Kernel Density Plot of the x dataset.\nWe can also overlay the density curve over a histogram using the lines() function. Here’s an example:\n\n# Generate data\nset.seed(1234)\nx &lt;- rnorm(500)\n\n# Plot histogram\nhist(x, freq = FALSE)\n\n# Estimate density\ndens &lt;- density(x)\n\n# Overlay density curve\nlines(dens, col = \"red\")\n\n\n\n\nThis will generate a histogram with a Kernel Density Plot overlaid on top."
  },
  {
    "objectID": "posts/2023-09-01/index.html#examples-using-ggplot2",
    "href": "posts/2023-09-01/index.html#examples-using-ggplot2",
    "title": "Kernel Density Plots in R",
    "section": "Examples using ggplot2",
    "text": "Examples using ggplot2\nTo create a Kernel Density Plot in ggplot2, we can use the geom_density() function. Here’s an example:\n\n# Load ggplot2 package\nlibrary(ggplot2)\n\n# Generate data\nset.seed(1234)\nx &lt;- rnorm(500)\n\n# Create data frame\ndf &lt;- data.frame(x = x)\n\n# Plot density\nggplot(df, aes(x = x)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\nThis will generate a Kernel Density Plot of the x dataset using ggplot2.\nWe can also customize the plot by changing the color and fill of the density curve. Here’s an example:\n\n# Generate data\nset.seed(1234)\nx &lt;- rnorm(500)\n\n# Create data frame\ndf &lt;- data.frame(x = x)\n\n# Plot density\nggplot(df, aes(x = x)) +\n  geom_density(color = \"red\", fill = \"blue\", alpha = 0.328) +\n  theme_minimal()\n\n\n\n\nThis will generate a Kernel Density Plot of the x dataset using ggplot2 with a red line, blue fill, and 33% transparency."
  },
  {
    "objectID": "posts/2023-09-01/index.html#conclusion",
    "href": "posts/2023-09-01/index.html#conclusion",
    "title": "Kernel Density Plots in R",
    "section": "Conclusion",
    "text": "Conclusion\nKernel Density Plots are a useful tool for visualizing the distribution of a dataset. They are easy to create in both base R and ggplot2, and can be customized to fit your needs. We encourage readers to try creating their own Kernel Density Plots using the examples provided in this blog post."
  },
  {
    "objectID": "posts/2023-09-01/index.html#example-using-tidydensity",
    "href": "posts/2023-09-01/index.html#example-using-tidydensity",
    "title": "Kernel Density Plots in R",
    "section": "Example using TidyDensity",
    "text": "Example using TidyDensity\nI have posted on it before but TidyDensity can also help in creating density plots for data that use the tidy_ distribution functions with its own autoplot function. Let’s take a look at an example using the same data as above.\n\nlibrary(TidyDensity)\n\nset.seed(1234)\n\ntn &lt;- tidy_normal(.n = 500)\n\ntn |&gt; tidy_autoplot()\n\n\n\n\nNow let’s see it with different means on the same chart.\n\nset.seed(1234)\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 500,\n    .mean = c(-2, 0, 2),\n    .sd = 1,\n    .num_sims = 1\n  )\n) |&gt;\n  tidy_multi_dist_autoplot()\n\n\n\n\nAnd one final one with multiple simulations of each distribution.\n\nset.seed(1234)\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 500,\n    .mean = c(-2,0,2),\n    .sd = 1,\n    .num_sims = 5\n  )\n) |&gt;\n  tidy_multi_dist_autoplot()"
  },
  {
    "objectID": "posts/2023-09-05/index.html",
    "href": "posts/2023-09-05/index.html",
    "title": "When to use Jitter",
    "section": "",
    "text": "As an R programmer, one of the most useful functions to know is the jitter function. The jitter function is used to add random noise to a numeric vector, which can be helpful when visualizing data in a scatterplot. By using the jitter function, we can get a better picture of the true underlying relationship between two variables in a dataset.\n\nWhen to Use Jitter\nScatterplots are excellent for visualizing the relationship between two continuous variables. For example, let’s say we have a dataset of 100 points on the x and y coordinate plane and we want to visualize the relationship between their x and y. We can create a scatterplot using the plot function in R:\n\nx = runif(100, 150, 250)\ny = (x/3) + rnorm(100)\ndata &lt;- data.frame(x, y)\nplot(data$x, data$y, pch = 16, col = 'steelblue')\n\n\n\n\nHowever, if we have a lot of data points that are clustered together, it can be difficult to see the true density of the data. This is where the jitter function comes in. We can add some random noise to the data using the jitter function:\n\nx &lt;- sample(1:10, 200, TRUE)\ny &lt;- 3*x + rnorm(200)\ndata &lt;- data.frame(x, y)\nplot(jitter(data$x, 0.1), jitter(data$y, 0.1), pch = 16, col = 'steelblue')\n\n\n\n\nWe can optionally add a numeric argument to jitter to add even more noise to the data:\n\nplot(jitter(data$x, 0.2), jitter(data$y, 0.2), pch = 16, col = 'steelblue')\n\n\n\n\nWe should be careful not to add too much jitter, though, as this can distort the original data too much:\n\nplot(jitter(data$x, 1), jitter(data$y, 1), pch = 16, col = 'steelblue')\n\n\n\n\n\n\nJittering Provides a Better View of the Data\nAs mentioned before, jittering adds some random noise to data, which can be beneficial when we want to visualize data in a scatterplot. By using the jitter function, we can get a better picture of the true underlying relationship between two variables in a dataset.\nLet’s look at some example data (where the predictor variable is discrete and the outcome is continuous), look at the problems with plotting these kinds of data using R’s defaults, and then look at the jitter function to draw a better scatterplot.\n\nset.seed(1)\nx &lt;- sample(1:10, 200, TRUE)\ny &lt;- 3 * x + rnorm(200, 0, 5)\n\nHere’s what a standard scatterplot of these data looks like:\n\nplot(y ~ x, pch = 15)\n\n\n\n\nscatterplot without jitter\nAs you can see, the data points are stacked on top of each other, making it difficult to see the true density of the data. This is where the jitter function comes in. Let’s add some jitter to the x variable:\n\nplot(y ~ jitter(x), pch = 15)\n\n\n\n\nscatterplot with jitter on x variable\nThis is better, but we can still see some stacking of the data points. Let’s try adding jitter to the y variable:\n\nplot(jitter(y) ~ jitter(x), pch = 15)\n\n\n\n\nscatterplot with jitter on both variables\nThis is much better! We can now see the true density of the data and the underlying relationship between the predictor and outcome variables.\n\n\nConclusion\nThe jitter function is a useful tool for visualizing data in a scatterplot. By adding some random noise to the data, we can get a better picture of the true underlying relationship between two variables in a dataset. However, we should be careful not to add too much jitter, as this can distort the original data too much. I encourage readers to try using the jitter function in their own scatterplots to see how it can improve their visualizations.\n\n\nResources:\n\n[1] https://www.statology.org/jitter-function-r/\n[2] https://www.geeksforgeeks.org/how-to-use-the-jitter-function-in-r-for-scatterplots/\n[3] https://thomasleeper.com/Rcourse/Tutorials/jitter.html\n[4] https://statisticsglobe.com/jitter-r-function-example/\n[5] https://biostats.w.uib.no/creating-a-jitter-plot/\n[6] https://blog.enterprisedna.co/creating-a-jitter-plot-using-ggplot2-in-rstudio/"
  },
  {
    "objectID": "posts/2023-09-06/index.html",
    "href": "posts/2023-09-06/index.html",
    "title": "Exploring Multivariate Data with Principal Component Analysis (PCA) Biplot in R",
    "section": "",
    "text": "Introduction\nWhen it comes to analyzing multivariate data, Principal Component Analysis (PCA) is a powerful technique that can help us uncover hidden patterns, reduce dimensionality, and gain valuable insights. One of the most informative ways to visualize the results of a PCA is by creating a biplot, and in this blog post, we’ll dive into how to do this using the biplot() function in R. To make it more practical, we’ll use the USArrests dataset to demonstrate the process step by step.\n\n\nWhat is a Biplot?\nBefore we get into the details, let’s briefly discuss what a biplot is. A biplot is a graphical representation of a PCA that combines both the scores and loadings into a single plot. The scores represent the data points projected onto the principal components, while the loadings indicate the contribution of each original variable to the principal components. By plotting both, we can see how variables and data points relate to each other in a single chart, making it easier to interpret and analyze the PCA results.\n\n\nGetting Started\nFirst, if you haven’t already, load the necessary R packages. You’ll need the stats package for PCA and the biplot visualization.\n\n# Load required packages\nlibrary(stats)\n\n\n\nPerforming PCA\nNext, let’s perform PCA on the USArrests dataset using the prcomp() function, which is an R function for PCA. We’ll store the PCA results in a variable called pca_result.\n\n# Perform PCA\npca_result &lt;- prcomp(USArrests, scale = TRUE)\n\nIn the code above, we’ve scaled the data (scale = TRUE) to ensure that variables with different scales don’t dominate the PCA.\n\n\nCreating the Biplot\nNow comes the exciting part—creating the biplot! We’ll use the biplot() function to achieve this.\n\n# Create a biplot\nbiplot(pca_result)\n\n\n\n\nWhen you run the biplot() function with your PCA results, R will generate a biplot that combines both the scores and loadings. You’ll see arrows representing the original variables’ contributions to each principal component, and you’ll also see how the data points project onto the components.\n\n\nInterpreting the Biplot\nLet’s break down what you’ll see in the biplot:\n\nData Points: Each point represents a US state in our case, and its position in the biplot indicates how it relates to the principal components.\nArrows: The arrows represent the original variables (in this case, the crime statistics) and show how they contribute to the principal components. Longer arrows indicate stronger contributions.\nPrincipal Components: The biplot will typically show the first two principal components. These components capture the most variation in the data.\n\n\n\nWhat Insights Can You Gain?\nBy examining the biplot, you can draw several conclusions:\n\nClustering: States close to each other on the plot share similar crime profiles.\nVariable Relationships: Variables close to each other on the plot are positively correlated, while those far apart are negatively correlated.\nOutliers: States far from the center may be outliers in terms of their crime statistics.\n\n\n\nTry It Yourself!\nNow that you’ve seen how to create a biplot for PCA using the USArrests dataset, I encourage you to try it with your own data. PCA and biplots are powerful tools for dimensionality reduction and data exploration. They can help you uncover patterns, relationships, and outliers in your data, making it easier to make informed decisions in various fields, from biology to finance.\nIn this tutorial, we’ve barely scratched the surface of what you can do with PCA and biplots. Dive deeper, explore different datasets, and use this knowledge to gain valuable insights into your own multivariate data. Happy analyzing!"
  },
  {
    "objectID": "posts/2023-09-06/index.html#what-insights-can-you-gain",
    "href": "posts/2023-09-06/index.html#what-insights-can-you-gain",
    "title": "Exploring Multivariate Data with Principal Component Analysis (PCA) Biplot in R",
    "section": "What Insights Can You Gain?",
    "text": "What Insights Can You Gain?\nBy examining the biplot, you can draw several conclusions:\n\nClustering: States close to each other on the plot share similar crime profiles.\nVariable Relationships: Variables close to each other on the plot are positively correlated, while those far apart are negatively correlated.\nOutliers: States far from the center may be outliers in terms of their crime statistics."
  },
  {
    "objectID": "posts/2023-09-06/index.html#try-it-yourself",
    "href": "posts/2023-09-06/index.html#try-it-yourself",
    "title": "Exploring Multivariate Data with Principal Component Analysis (PCA) Biplot in R",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nNow that you’ve seen how to create a biplot for PCA using the USArrests dataset, I encourage you to try it with your own data. PCA and biplots are powerful tools for dimensionality reduction and data exploration. They can help you uncover patterns, relationships, and outliers in your data, making it easier to make informed decisions in various fields, from biology to finance.\nIn this tutorial, we’ve barely scratched the surface of what you can do with PCA and biplots. Dive deeper, explore different datasets, and use this knowledge to gain valuable insights into your own multivariate data. Happy analyzing!"
  },
  {
    "objectID": "posts/2023-09-07/index.html",
    "href": "posts/2023-09-07/index.html",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "",
    "text": "Data visualization is a powerful tool for gaining insights from your data. In R, you have a plethora of libraries and functions at your disposal to create stunning and informative plots. One common task is to plot a subset of your data, which allows you to focus on specific aspects or trends within your dataset. In this blog post, we’ll explore various techniques to plot subsets of data in R, and I’ll explain each step in simple terms. Don’t worry if you’re new to R – by the end of this post, you’ll be equipped to create customized plots with ease!\nBefore we start, make sure you have R and RStudio installed on your computer. If not, you can download them from R’s official website and RStudio’s website."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-1-plotting-a-subset-based-on-a-condition",
    "href": "posts/2023-09-07/index.html#example-1-plotting-a-subset-based-on-a-condition",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 1: Plotting a Subset Based on a Condition",
    "text": "Example 1: Plotting a Subset Based on a Condition\nSuppose you have a dataset of monthly sales, and you want to plot only the data points where sales exceeded $10,000. Here’s how you can do it:\n# Load your data (replace 'your_data.csv' with your actual file)\ndata &lt;- read.csv(\"your_data.csv\")\n\n# Create a subset based on the condition\nsubset_data &lt;- data[data$Sales &gt; 10000, ]\n\n# Create a scatter plot\nplot(subset_data$Month, subset_data$Sales, \n     main=\"Monthly Sales &gt; $10,000\", \n     xlab=\"Month\", ylab=\"Sales\")\nExplanation: - We load the data from a CSV file into the ‘data’ variable. - Next, we create a subset of the data using a condition (in this case, sales &gt; $10,000) and store it in ‘subset_data.’ - Finally, we create a scatter plot using the ‘plot’ function, specifying the x-axis (‘Month’) and y-axis (‘Sales’), and adding labels to the plot."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-2-plotting-a-random-subset",
    "href": "posts/2023-09-07/index.html#example-2-plotting-a-random-subset",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 2: Plotting a Random Subset",
    "text": "Example 2: Plotting a Random Subset\nSometimes you might want to plot a random subset of your data. Let’s say you have a large dataset of customer reviews, and you want to visualize a random sample of 100 reviews:\n# Load your data (replace 'your_data.csv' with your actual file)\ndata &lt;- read.csv(\"your_data.csv\")\n\n# Create a random subset\nset.seed(123)  # For reproducibility\nsample_data &lt;- data[sample(nrow(data), 100), ]\n\n# Create a bar plot of review ratings\nbarplot(table(sample_data$Rating), \n        main=\"Random Sample of Customer Reviews\",\n        xlab=\"Rating\", ylab=\"Count\")\nExplanation: - We load the data as before. - Using the sample function, we select 100 random rows from the dataset while setting the seed for reproducibility. - Then, we create a bar plot to visualize the distribution of review ratings."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-3-plotting-data-by-category",
    "href": "posts/2023-09-07/index.html#example-3-plotting-data-by-category",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 3: Plotting Data by Category",
    "text": "Example 3: Plotting Data by Category\nSuppose you have a dataset containing information about various products and you want to plot the sales for each product category. Here’s how you can do it:\n# Load your data (replace 'your_data.csv' with your actual file)\ndata &lt;- read.csv(\"your_data.csv\")\n\n# Create a bar plot of sales by category\nbarplot(tapply(data$Sales, data$Category, sum),\n        main=\"Sales by Product Category\",\n        xlab=\"Category\", ylab=\"Total Sales\")\nExplanation: - We load the data. - Using the tapply function, we group the data by ‘Category’ and calculate the sum of ‘Sales’ for each category. - Finally, we create a bar plot to visualize the total sales for each product category.\nNow for some worked out examples."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-4-using-subset-function",
    "href": "posts/2023-09-07/index.html#example-4-using-subset-function",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 4: Using subset() function",
    "text": "Example 4: Using subset() function\nIn this method, first, a subset of the data is created based on some condition, and then it is plotted using the plot function. Let us first create the subset of the data.\n\ndata_subset &lt;- subset(USArrests, UrbanPop &gt; 70)\nplot(data_subset$Murder, data_subset$Assault)\n\n\n\n\nIn the above code, we have created a subset of the USArrests dataset where UrbanPop is greater than 70. Then we have plotted the Murder and Assault columns of the subset using the plot function."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-5-using-operator",
    "href": "posts/2023-09-07/index.html#example-5-using-operator",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 5: Using [ ] operator",
    "text": "Example 5: Using [ ] operator\nUsing the ‘[ ]’ operator, elements of vectors and observations from data frames can be accessed and subsetted based on some condition.\n\nplot(USArrests$Murder[USArrests$UrbanPop &gt; 70], USArrests$Assault[USArrests$UrbanPop &gt; 70])\n\n\n\n\nIn the above code, we have used the [ ] operator to subset the USArrests dataset where UrbanPop is greater than 70. Then we have plotted the Murder and Assault columns of the subset using the plot function."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-6-using-attributes-for-rows-and-columns",
    "href": "posts/2023-09-07/index.html#example-6-using-attributes-for-rows-and-columns",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 6: Using attributes for rows and columns",
    "text": "Example 6: Using attributes for rows and columns\nIn this method, we pass the row and column attributes to the plot function to plot a subset of the data.\n\nplot(USArrests[USArrests$UrbanPop &gt; 70, c(\"Murder\", \"Assault\")])\n\n\n\n\nIn the above code, we have used the row and column attributes to subset the USArrests dataset where UrbanPop is greater than 70. Then we have plotted the Murder and Assault columns of the subset using the plot function."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-7-using-dplyr-package",
    "href": "posts/2023-09-07/index.html#example-7-using-dplyr-package",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 7: Using dplyr package",
    "text": "Example 7: Using dplyr package\nThe dplyr package provides a simple and efficient way to subset data.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_subset &lt;- USArrests %&gt;% filter(UrbanPop &gt; 70)\nplot(data_subset$Murder, data_subset$Assault)\n\n\n\n\nIn the above code, we have used the filter function from the dplyr package to subset the USArrests dataset where UrbanPop is greater than 70. Then we have plotted the Murder and Assault columns of the subset using the plot function.\nIn conclusion, there are several ways to plot a subset of data in R. We have explored four methods in this blog post. I encourage readers to try these methods on their own and explore other ways to subset and plot data in R."
  },
  {
    "objectID": "posts/2023-09-08/index.html",
    "href": "posts/2023-09-08/index.html",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "",
    "text": "Are you interested in visualizing demographic data in a unique and insightful way? Population pyramids are a fantastic tool for this purpose! They allow you to compare the distribution of populations across age groups for different genders or time periods. In this blog post, we’ll explore how to create population pyramid plots in R using the powerful ggplot2 library. Don’t worry if you’re new to R or ggplot2; we’ll walk you through the process step by step."
  },
  {
    "objectID": "posts/2023-09-08/index.html#step-1-create-a-basic-bar-chart",
    "href": "posts/2023-09-08/index.html#step-1-create-a-basic-bar-chart",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Step 1: Create a Basic Bar Chart",
    "text": "Step 1: Create a Basic Bar Chart\nStart by creating a basic bar chart representing the population distribution for one gender. We’ll use the geom_bar function to do this.\n\n# Create a basic bar chart for one gender\nbasic_plot &lt;-  ggplot(\n    data, \n    aes(\n        x = Age, \n        fill = Gender, \n        y = ifelse(\n            test = Gender == \"Male\", \n            yes = -Population, \n            no = Population\n            )\n        )\n    ) + \ngeom_bar(stat = \"identity\") \n\nIn this code:\n\nWe filter the data to include only one gender (Male) using subset.\nWe use aes to specify the aesthetic mappings. We map Age to the x-axis, -Population to the y-axis (note the negative sign to flip the bars), and Age to the fill color.\ngeom_bar is used to create the bar chart, and stat = \"identity\" ensures that the heights of the bars are determined by the Population variable.\nFinally, coord_flip() is applied to flip the chart horizontally, making it look like a pyramid."
  },
  {
    "objectID": "posts/2023-09-08/index.html#step-2-combine-both-genders",
    "href": "posts/2023-09-08/index.html#step-2-combine-both-genders",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Step 2: Combine Both Genders",
    "text": "Step 2: Combine Both Genders\nTo create a population pyramid, we need to combine both male and female data. We’ll create two separate plots for each gender and then combine them using the + operator.\n\n# Create population pyramids for both genders and combine them\npopulation_pyramid &lt;- basic_plot +\n  scale_y_continuous(\n    labels = abs, \n    limits = max(data$Population) * c(-1,1)\n  ) + \n  coord_flip() + \n  theme_minimal() +\n  labs(\n    x = \"Age\", \n    y = \"Population\", \n    fill = \"Age\", \n    title = \"Population Pyramid\"\n  )\n\nIn this step:\n\nscale_y_continuous(labels = abs, limits = max(data$Population) * c(-1,1)):\nThis part adjusts the y-axis (vertical axis) of the plot.\nlabels = abs means that the labels on the y-axis will show the absolute values (positive numbers) rather than negative values.\nlimits = max(data$Population) * c(-1,1) sets the limits of the y-axis. It ensures that the y-axis extends from the maximum population value (positive) to the minimum (negative) value, creating a symmetrical pyramid shape.\ncoord_flip(): This function flips the coordinate system of the plot. By default, the x-axis (horizontal) represents age, and the y-axis (vertical) represents population. coord_flip() swaps them so that the x-axis represents population and the y-axis represents age, creating the pyramid effect.\ntheme_minimal(): This sets the overall visual theme of the plot to a minimalistic style. It adjusts the background, gridlines, and other visual elements to a simple and clean appearance.\nlabs(x = “Age”, y = “Population”, fill = “Age”, title = “Population Pyramid”): This part labels various elements of the plot:\n\nx = “Age” labels the x-axis as “Age.”\ny = “Population” labels the y-axis as “Population.”\nfill = “Age” specifies that the “Age” variable will be used to fill the bars in the plot.\ntitle = “Population Pyramid” sets the title of the plot as “Population Pyramid.”"
  },
  {
    "objectID": "posts/2023-09-08/index.html#step-3-customize-your-plot",
    "href": "posts/2023-09-08/index.html#step-3-customize-your-plot",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Step 3: Customize Your Plot",
    "text": "Step 3: Customize Your Plot\nFeel free to customize your plot further by adding labels, adjusting colors, or modifying other aesthetics to match your preferences. The ggplot2 library provides extensive customization options."
  },
  {
    "objectID": "posts/2023-09-08/index.html#step-4-visualize-your-population-pyramid",
    "href": "posts/2023-09-08/index.html#step-4-visualize-your-population-pyramid",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Step 4: Visualize Your Population Pyramid",
    "text": "Step 4: Visualize Your Population Pyramid\nTo visualize your population pyramid, simply print the population_pyramid object:\n\npopulation_pyramid\n\n\n\n\nThis will display the population pyramid plot in your R graphics window."
  },
  {
    "objectID": "posts/2023-09-08/index.html#conclusion",
    "href": "posts/2023-09-08/index.html#conclusion",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Conclusion",
    "text": "Conclusion\nCreating population pyramid plots in R using ggplot2 can be a powerful way to visualize demographic data. In this blog post, we walked through the process step by step, from loading libraries and preparing data to constructing and customizing the pyramid plot. Now it’s your turn to give it a try with your own data or explore additional features and customization options in ggplot2. Happy plotting!"
  },
  {
    "objectID": "posts/2023-09-11/index.html",
    "href": "posts/2023-09-11/index.html",
    "title": "Plotting SVM Decision Boundaries with e1071 in R",
    "section": "",
    "text": "Support Vector Machines (SVM) are a powerful tool in the world of machine learning and classification. They excel in finding the optimal decision boundary between different classes of data. However, understanding and visualizing these decision boundaries can be a bit tricky. In this blog post, we’ll explore how to plot an SVM object using the e1071 library in R, making it easier to grasp the magic happening under the hood."
  },
  {
    "objectID": "posts/2023-09-11/index.html#interpreting-the-plot",
    "href": "posts/2023-09-11/index.html#interpreting-the-plot",
    "title": "Plotting SVM Decision Boundaries with e1071 in R",
    "section": "Interpreting the Plot",
    "text": "Interpreting the Plot\nThe resulting plot will display your data points with red dots and blue squares, representing the true class labels. The decision boundary will be shown as a mix of red and blue points, indicating where the SVM has classified the data. The legend on the top-right helps you distinguish between the two classes.\nWe can also more simply plot out the model, see below:\n\nplot(svm_model, data = data)\n\n\n\n# Change the colors\nplot(svm_model, data = data, color.palette = heat.colors)"
  },
  {
    "objectID": "posts/2023-09-11/index.html#try-it-yourself",
    "href": "posts/2023-09-11/index.html#try-it-yourself",
    "title": "Plotting SVM Decision Boundaries with e1071 in R",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nNow that you’ve seen how to plot an SVM decision boundary using the e1071 package, I encourage you to try it with your own datasets and experiment with different kernels (e.g., radial or polynomial) to see how the decision boundary changes.\nSVMs are a versatile tool for classification tasks, and visualizing their decision boundaries can provide valuable insights into your data and model. Happy plotting!"
  },
  {
    "objectID": "posts/2023-09-12/index.html",
    "href": "posts/2023-09-12/index.html",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "",
    "text": "If you’re an R enthusiast looking to take your data visualization to the next level, you’re in for a treat. In this blog post, we’re going to dive into the world of 3D plotting using R’s powerful persp() function. Whether you’re visualizing surfaces, mathematical functions, or complex data, persp() is a versatile tool that can help you create stunning three-dimensional plots."
  },
  {
    "objectID": "posts/2023-09-12/index.html#the-syntax-of-persp",
    "href": "posts/2023-09-12/index.html#the-syntax-of-persp",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "The Syntax of persp()",
    "text": "The Syntax of persp()\nBefore we dive into examples, let’s take a look at the basic syntax of the persp() function:\npersp(x, y, z, theta = 30, phi = 30, col = \"lightblue\",\n      border = \"black\", scale = TRUE, ... )\n\nx, y, and z are the vectors or matrices representing the x, y, and z coordinates of the data points.\ntheta and phi control the orientation of the plot. theta sets the azimuthal angle (rotation around the z-axis), and phi sets the polar angle (rotation from the xy-plane). These angles are in degrees.\ncol and border control the color of the surface and its border, respectively.\nscale is a logical value that determines whether the axes should be scaled to match the data range.\nAdditional parameters can be passed as ... to customize the plot further.\n\nNow, let’s jump into some examples to see how persp() works in action!"
  },
  {
    "objectID": "posts/2023-09-12/index.html#example-1-creating-a-simple-surface-plot",
    "href": "posts/2023-09-12/index.html#example-1-creating-a-simple-surface-plot",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "Example 1: Creating a Simple Surface Plot",
    "text": "Example 1: Creating a Simple Surface Plot\n\n# Create data for a simple surface plot\nx &lt;- seq(-5, 5, length.out = 50)\ny &lt;- seq(-5, 5, length.out = 50)\nz &lt;- outer(x, y, function(x, y) cos(sqrt(x^2 + y^2)))\n\n# Create a 3D surface plot\npersp(x, y, z, col = \"lightblue\", border = \"black\")\n\n\n\n\nIn this example, we generate a grid of x and y values and calculate the corresponding z values based on a mathematical function. The persp() function then creates a 3D surface plot, using the provided x, y, and z data."
  },
  {
    "objectID": "posts/2023-09-12/index.html#example-2-customizing-the-perspective",
    "href": "posts/2023-09-12/index.html#example-2-customizing-the-perspective",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "Example 2: Customizing the Perspective",
    "text": "Example 2: Customizing the Perspective\n\n# Create data for a surface plot\nx &lt;- seq(-10, 10, length.out = 100)\ny &lt;- seq(-10, 10, length.out = 100)\nz &lt;- outer(x, y, function(x, y) 2 * sin(sqrt(x^2 + y^2)) / sqrt(x^2 + y^2))\n\n# Create a customized 3D surface plot\npersp(x, y, z, col = \"lightblue\", border = \"black\", theta = 60, phi = 20)\n\n\n\n\nIn this example, we create a similar surface plot but customize the perspective by changing the theta and phi angles. This gives the plot a different orientation, providing a unique view of the data."
  },
  {
    "objectID": "posts/2023-09-12/index.html#example-3-scaling-the-axes",
    "href": "posts/2023-09-12/index.html#example-3-scaling-the-axes",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "Example 3: Scaling the Axes",
    "text": "Example 3: Scaling the Axes\n\n# Create data for a surface plot\nx &lt;- seq(-2, 2, length.out = 50)\ny &lt;- seq(-2, 2, length.out = 50)\nz &lt;- outer(x, y, function(x, y) exp(-x^2 - y^2))\n\n# Create a 3D surface plot with scaled axes\npersp(x, y, z, col = \"lightblue\", border = \"black\", scale = TRUE)\n\n\n\n\nHere, we enable axis scaling with the scale parameter, which ensures that the x, y, and z axes are scaled to match the data range."
  },
  {
    "objectID": "posts/2023-09-12/index.html#example-4-multiple-plots",
    "href": "posts/2023-09-12/index.html#example-4-multiple-plots",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "Example 4: Multiple Plots",
    "text": "Example 4: Multiple Plots\n\n# Create data\nx &lt;- seq(-10, 10, length.out = 50)\ny &lt;- seq(-10, 10, length.out = 50)\nz1 &lt;- outer(x, y, function(x, y) dnorm(sqrt(x^2 + y^2)))\nz2 &lt;- outer(x, y, function(x, y) dnorm(sqrt((x-2)^2 + (y-2)^2)))\nz3 &lt;- outer(x, y, function(x, y) dnorm(sqrt((x+2)^2 + (y+2)^2)))\n\n# Plot data\npar(mfrow = c(1, 3))\n\npersp(x, y, z1, theta = 30, phi = 30, col = \"lightblue\", border = NA, shade = 0.5, ticktype = \"detailed\", nticks = 5, xlab = \"X\", ylab = \"Y\", zlab = \"Z1\")\npersp(x, y, z2, theta = 30, phi = 30, col = \"lightblue\", border = NA, shade = 0.5, ticktype = \"detailed\", nticks = 5, xlab = \"X\", ylab = \"Y\", zlab = \"Z2\")\npersp(x, y, z3, theta = 30, phi = 30, col = \"lightblue\", border = NA, shade = 0.5, ticktype = \"detailed\", nticks = 5, xlab = \"X\", ylab = \"Y\", zlab = \"Z3\")\n\n\n\npar(mfrow = c(1, 1))\n\nIn this example, we create data for three different Gaussian distributions. We define the x- and y-axes and use the outer() function to calculate the z-values based on the normal distribution. We then use the persp() function to plot the data. We set the color to light blue, the border to NA, and the shading to 0.5. We also set the tick type to detailed and the number of ticks to 5. Finally, we label the x-, y-, and z-axes. We use the par() function to create multiple 3D plots in one figure."
  },
  {
    "objectID": "posts/2023-09-13/index.html",
    "href": "posts/2023-09-13/index.html",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "",
    "text": "Data visualization is a crucial aspect of data analysis. In R, the flexibility and power of its plotting capabilities allow you to create compelling visualizations. One common scenario is the need to display multiple plots on the same graph. In this blog post, we’ll explore three different approaches to achieve this using the same dataset. We’ll use the set.seed(123) and generate data with x and y equal to cumsum(rnorm(25)) for consistency across examples."
  },
  {
    "objectID": "posts/2023-09-13/index.html#example-1-overlaying-multiple-lines-on-the-same-graph",
    "href": "posts/2023-09-13/index.html#example-1-overlaying-multiple-lines-on-the-same-graph",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "Example 1: Overlaying Multiple Lines on the Same Graph",
    "text": "Example 1: Overlaying Multiple Lines on the Same Graph\nIn this example, we will overlay two lines on the same graph. This is a great way to compare trends between two variables in a single plot.\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Generate the data\nx &lt;- 1:25\ny1 &lt;- cumsum(rnorm(25))\ny2 &lt;- cumsum(rnorm(25))\n\n# Create the plot\nplot(x, y1, type = 'l', col = 'blue', ylim = c(min(y1, y2), max(y1, y2)), \n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Overlaying Multiple Lines')\nlines(x, y2, col = 'red')\nlegend('topleft', legend = c('Line 1', 'Line 2'), col = c('blue', 'red'), lty = 1)\n\n\n\n\nIn this code, we first generate the data for y1 and y2. Then, we use the plot() function to create a plot of y1. We specify type = 'l' to create a line plot and set the color to blue. Next, we use the lines() function to overlay y2 on the same plot with a red line. Finally, we add a legend to distinguish the two lines."
  },
  {
    "objectID": "posts/2023-09-13/index.html#example-2-side-by-side-plots",
    "href": "posts/2023-09-13/index.html#example-2-side-by-side-plots",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "Example 2: Side-by-Side Plots",
    "text": "Example 2: Side-by-Side Plots\nSometimes, you might want to display multiple plots side by side to compare different variables. We can achieve this using the par() function and layout options.\n\n# Create a side-by-side layout\npar(mfrow = c(1, 2))\n\n# Create the first plot\nplot(x, y1, type = 'l', col = 'blue', \n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Side-by-Side Plots (1)')\n\n# Create the second plot\nplot(x, y2, type = 'l', col = 'red',\n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Side-by-Side Plots (2)')\n\n\n\n# Reset Par\npar(mfrow = c(1, 1))\n\nIn this example, we use par(mfrow = c(1, 2)) to set up a side-by-side layout. Then, we create two separate plots for y1 and y2."
  },
  {
    "objectID": "posts/2023-09-13/index.html#example-3-stacked-plots",
    "href": "posts/2023-09-13/index.html#example-3-stacked-plots",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "Example 3: Stacked Plots",
    "text": "Example 3: Stacked Plots\nStacked plots are useful when you want to compare the overall trend while preserving the individual patterns of different variables. Here, we stack two line plots on top of each other.\n\npar(mfrow = c(2, 1), mar = c(2, 4, 4, 2))\n\n# Create the first plot\nplot(x, y1, type = 'l', col = 'blue', \n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Stacked Plots')\n\n# Create the second plot\nplot(x, y2, type = 'l', col = 'red',\n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Side-by-Side Plots (2)')\n\n\n\npar(mfrow = c(1, 1))\n\nThe first line of code, par(mfrow = c(2, 1), mar = c(2, 4, 4, 2)), tells R to create a 2x1 (two rows, one column) plot with margins of 2, 4, 4, and 2. This means that the two plots will be stacked on top of each other.\nThe next line of code, plot(x, y1, type = 'l', col = 'blue', xlab = 'X-axis', ylab = 'Y-axis', main = 'Stacked Plots'), create the first plot. The plot() function creates a plot of the data in the vectors x and y1. The type = 'l' argument tells R to create a line plot, the col = ‘blue’ argument tells R to use blue color for the line, and the other arguments set the labels for the axes and the title of the plot.\nThe fourth line of code, plot(x, y2, type = 'l', col = 'red', xlab = 'X-axis', ylab = 'Y-axis', main = 'Side-by-Side Plots (2)'), create the second plot. This plot is similar to the first plot, except that the line is red.\nThe last line of code, par(mfrow = c(1, 1)), resets the plot to a single plot.\nIn summary, this code creates two line plots, one stacked on top of the other. The first plot uses blue lines and the second plot uses red lines. The plots are labeled and titled appropriately."
  },
  {
    "objectID": "posts/2023-09-13/index.html#conclusion",
    "href": "posts/2023-09-13/index.html#conclusion",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored three different techniques for plotting multiple plots on the same graph in R. Whether you need to overlay lines, display plots side by side, or stack them, R offers powerful tools to visualize your data effectively. Try these examples with your own data to harness the full potential of R’s plotting capabilities and create informative visualizations for your analyses. Happy plotting!"
  },
  {
    "objectID": "posts/2023-09-14/index.html",
    "href": "posts/2023-09-14/index.html",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "",
    "text": "Histograms are a fantastic way to visualize the distribution of data. They provide insights into the underlying patterns and help us understand our data better. But what if you want to add some color to your histograms to make them more visually appealing or to highlight specific data points? In this blog post, we’ll explore how to create histograms with different colors in R, and we’ll provide several examples to guide you through the process."
  },
  {
    "objectID": "posts/2023-09-14/index.html#example-1-basic-histogram-with-a-single-color",
    "href": "posts/2023-09-14/index.html#example-1-basic-histogram-with-a-single-color",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "Example 1: Basic Histogram with a Single Color",
    "text": "Example 1: Basic Histogram with a Single Color\nLet’s start with the basics. To create a simple histogram with a single color, we’ll use the built-in hist() function and then customize it with the col parameter:\n\n# Generate some example data\ndata &lt;- rnorm(1000)\n\n# Create a basic histogram with a single color (e.g., blue)\nhist(data, col = \"blue\", main = \"Basic Histogram\")\n\n\n\n\nIn this example, we generated 1000 random data points and created a histogram with blue bars. You can replace \"blue\" with any valid color name or code you prefer."
  },
  {
    "objectID": "posts/2023-09-14/index.html#example-2-customizing-bin-colors",
    "href": "posts/2023-09-14/index.html#example-2-customizing-bin-colors",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "Example 2: Customizing Bin Colors",
    "text": "Example 2: Customizing Bin Colors\nSometimes, you might want to use different colors for individual bins in your histogram. Here’s how you can achieve that:\n\n# Generate example data\ndata &lt;- rnorm(100)\n\n# Define custom colors for each bin\nbin_colors &lt;- c(\"red\", \"green\", \"blue\", \"yellow\", \"purple\")\n\n# Create a histogram with custom bin colors\nhist(data, breaks = 5, col = bin_colors, main = \"Custom Bin Colors\")\n\n\n\n\nIn this example, we’ve specified five custom colors for our histogram’s bins, creating a colorful representation of the data distribution."
  },
  {
    "objectID": "posts/2023-09-14/index.html#example-3-overlaying-multiple-histograms",
    "href": "posts/2023-09-14/index.html#example-3-overlaying-multiple-histograms",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "Example 3: Overlaying Multiple Histograms",
    "text": "Example 3: Overlaying Multiple Histograms\nYou may also want to compare multiple data distributions in a single histogram. To do this, you can overlay histograms with different colors. Here’s an example:\n\n# Generate two sets of example data\ndata1 &lt;- rnorm(1000, mean = 0, sd = 1)\ndata2 &lt;- rnorm(1000, mean = 2, sd = 1)\n\n# Create histograms for each dataset and overlay them\nhist(data1, col = \"blue\", main = \"Overlayed Histograms\")\nhist(data2, col = \"red\", add = TRUE)\nlegend(\"topright\", legend = c(\"Data 1\", \"Data 2\"), fill = c(\"blue\", \"red\"))\n\n\n\n\nIn this example, we generated two datasets and overlaid their histograms with different colors. The alpha parameter controls the transparency of the bars, making it easier to see overlapping areas."
  },
  {
    "objectID": "posts/2023-09-14/index.html#experiment-and-explore",
    "href": "posts/2023-09-14/index.html#experiment-and-explore",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "Experiment and Explore",
    "text": "Experiment and Explore\nNow that you’ve seen how to create histograms with different colors in R, I encourage you to experiment with your own datasets and colors. R provides numerous options for customizing your histograms, so you can tailor them to your specific needs. Play around with colors, transparency, and other graphical parameters to create engaging and informative visualizations.\nRemember, the best way to learn is by doing, so fire up your R environment and start creating colorful histograms today!"
  },
  {
    "objectID": "posts/2023-09-15/index.html",
    "href": "posts/2023-09-15/index.html",
    "title": "Histograms with Two or More Variables in R",
    "section": "",
    "text": "Histograms are powerful tools for visualizing the distribution of a single variable, but what if you want to compare the distributions of two variables side by side? In this blog post, we’ll explore how to create a histogram of two variables in R, a popular programming language for data analysis and visualization.\nWe’ll cover various scenarios, from basic histograms to more advanced techniques, and explain the code step by step in simple terms. So, grab your favorite dataset or generate some random data, and let’s dive into the world of dual-variable histograms!"
  },
  {
    "objectID": "posts/2023-09-15/index.html#basic-dual-variable-histogram",
    "href": "posts/2023-09-15/index.html#basic-dual-variable-histogram",
    "title": "Histograms with Two or More Variables in R",
    "section": "Basic Dual-Variable Histogram",
    "text": "Basic Dual-Variable Histogram\nLet’s begin with the most straightforward scenario: creating a histogram of two variables using the hist() function. We’ll use the built-in mtcars dataset, which contains information about various car models.\n\nx1 &lt;- rnorm(1000)\nx2 &lt;- rnorm(1000, mean = 2)\nminx &lt;- min(x1, x2)\nmaxx &lt;- max(x1, x2)\n\n# Create a basic dual-variable histogram\nhist(x1, main=\"Histogram of rnorm with mean 0 and 2\", xlab=\"\", \n     ylab=\"\", col=\"lightblue\", xlim = c(minx, maxx))\nhist(x2, xlab=\"\", \n     ylab=\"\", col=\"lightgreen\", add=TRUE)\nlegend(\"topright\", legend=c(\"Mean: 0\", \"Mean: 2\"), fill=c(\"lightblue\", \"lightgreen\"))\n\n\n\n\nThe given R code generates a dual-variable histogram in R using the hist() function. The first two lines of code generate two vectors x1 and x2 of 1000 random normal numbers each, with x1 having a mean of 0 and x2 having a mean of 2. The min() and max() functions are then used to find the minimum and maximum values between x1 and x2. These values are used to set the limits of the x-axis of the histogram.\nThe hist() function is then called twice to create two histograms, one for x1 and one for x2. The col argument is used to set the color of each histogram. The add argument is set to TRUE for the second histogram so that it is overlaid on top of the first histogram. Finally, the legend() function is used to add a legend to the plot indicating which histogram corresponds to which variable.\nIn summary, the code generates a dual-variable histogram of two vectors of random normal numbers with different means. The histogram shows the distribution of values for each variable and allows for easy comparison between the two variables."
  },
  {
    "objectID": "posts/2023-09-15/index.html#dual-variable-histogram-with-transparency",
    "href": "posts/2023-09-15/index.html#dual-variable-histogram-with-transparency",
    "title": "Histograms with Two or More Variables in R",
    "section": "Dual-Variable Histogram with Transparency",
    "text": "Dual-Variable Histogram with Transparency\nAdding transparency to the histograms can make the visualization more informative when the bars overlap. We can achieve this by setting the alpha parameter in the col argument. Let’s use the same dataset and create a dual-variable histogram with transparency:\n\n# Create a dual-variable histogram with transparency\nminx &lt;- min(mtcars$mpg, mtcars$hp)\nmaxx &lt;- max(mtcars$mpg, mtcars$hp)\nhist(\n  mtcars$mpg, \n  main=\"Histogram of MPG and Horsepower\", \n  xlab=\"Value\",\n  ylab=\"Frequency\", \n  col=rgb(0, 0, 1, alpha=0.5), \n  xlim=c(minx, maxx))\nhist(\n  mtcars$hp, \n  col=rgb(1, 0, 0, alpha=0.5), \n  add=TRUE\n  )\nlegend(\"topright\", legend=c(\"MPG\", \"Horsepower\"), fill=c(rgb(0, 0, 1, alpha=0.5), rgb(1, 0, 0, alpha=0.5)))\n\n\n\n\nHere, we use the rgb() function to set the color with transparency. The alpha parameter controls the transparency level, with values between 0 (completely transparent) and 1 (completely opaque)."
  },
  {
    "objectID": "posts/2023-09-15/index.html#side-by-side-histograms",
    "href": "posts/2023-09-15/index.html#side-by-side-histograms",
    "title": "Histograms with Two or More Variables in R",
    "section": "Side-by-Side Histograms",
    "text": "Side-by-Side Histograms\nIf you prefer to display the histograms side by side, you can use the par() function to adjust the layout. Here’s an example:\n\n# Set up a side-by-side layout\npar(mfrow=c(1, 2))\n\n# Create side-by-side histograms\nhist(mtcars$mpg, main=\"Histogram of MPG\", xlab=\"Miles Per Gallon\", \n     ylab=\"Frequency\", col=\"lightblue\", xlim=c(10, 35))\nhist(mtcars$hp, main=\"Histogram of Horsepower\", xlab=\"Horsepower\", \n     ylab=\"Frequency\", col=\"lightgreen\")\n\n\n\npar(mfrow=c(1,1))\n\nIn this code, we use par(mfrow=c(1, 2)) to set up a 1x2 layout, which means two plots will appear side by side."
  },
  {
    "objectID": "posts/2023-09-15/index.html#customizing-dual-variable-histograms",
    "href": "posts/2023-09-15/index.html#customizing-dual-variable-histograms",
    "title": "Histograms with Two or More Variables in R",
    "section": "Customizing Dual-Variable Histograms",
    "text": "Customizing Dual-Variable Histograms\nYou can customize your dual-variable histograms further by adjusting various parameters, such as bin width, titles, labels, and colors. Experiment with different settings to create visualizations that best convey your data’s story.\nRemember, the key to effective data visualization is experimentation and exploration. Try different datasets, play with colors and styles, and find the representation that best suits your needs."
  },
  {
    "objectID": "posts/2023-09-15/index.html#conclusion",
    "href": "posts/2023-09-15/index.html#conclusion",
    "title": "Histograms with Two or More Variables in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we’ve explored several ways to create histograms of two variables in R. Whether you’re comparing distributions or just visualizing your data, histograms are a valuable tool in your data analysis toolkit. Experiment with the provided examples and take your data visualization skills to the next level!\nSo, fire up your R environment, load your data, and start creating dual-variable histograms today. Happy coding!"
  },
  {
    "objectID": "posts/2023-09-17/index.html",
    "href": "posts/2023-09-17/index.html",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "",
    "text": "Histograms are a fundamental tool in data analysis and visualization, allowing us to explore the distribution of data quickly and effectively. While creating a histogram in R is straightforward, specifying breaks appropriately can make a world of difference in the insights you can draw from your data. In this blog post, we will delve into the art of specifying breaks in a histogram, providing you with multiple examples and encouraging you to experiment on your own.\nBefore we get started, it’s worth mentioning that this topic has been explored in depth by Steve Sanderson in his previous blog post. If you’re interested in diving even deeper, make sure to check out his article here: Steve’s Blog Post on Optimal Binning. Now, let’s embark on our journey into the fascinating world of histogram breaks in R."
  },
  {
    "objectID": "posts/2023-09-17/index.html#example-1-default-breaks",
    "href": "posts/2023-09-17/index.html#example-1-default-breaks",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "Example 1: Default Breaks",
    "text": "Example 1: Default Breaks\nLet’s start with a simple example using R’s built-in mtcars dataset:\n\n# Create a histogram with default breaks\nhist(mtcars$mpg, main = \"Default Breaks\", xlab = \"Miles per Gallon\")\n\n\n\n\nIn this case, R automatically selects the breaks based on the range of the data. The resulting histogram might not reveal finer details, and it’s essential to understand how to customize breaks to suit your analysis."
  },
  {
    "objectID": "posts/2023-09-17/index.html#example-2-specifying-equal-breaks",
    "href": "posts/2023-09-17/index.html#example-2-specifying-equal-breaks",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "Example 2: Specifying Equal Breaks",
    "text": "Example 2: Specifying Equal Breaks\nYou can specify equal-width breaks using the breaks parameter. Here’s an example:\n\n# Create a histogram with equal-width breaks\nhist(mtcars$mpg, main = \"Equal Width Breaks\", xlab = \"Miles per Gallon\", breaks = 10)\n\n\n\n\nIn this example, we divided the data into 10 equal-width bins. This approach can help reveal underlying patterns in the data distribution."
  },
  {
    "objectID": "posts/2023-09-17/index.html#example-3-custom-breaks",
    "href": "posts/2023-09-17/index.html#example-3-custom-breaks",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "Example 3: Custom Breaks",
    "text": "Example 3: Custom Breaks\nSometimes, you may have domain knowledge that suggests specific break points. Let’s explore a case where we set custom breaks:\n\n# Create a histogram with custom breaks\ncustom_breaks &lt;- c(10, 15, 20, 25, 30, 35)\nhist(mtcars$mpg, main = \"Custom Breaks\", xlab = \"Miles per Gallon\", breaks = custom_breaks)\n\n\n\n\nHere, we’ve defined custom break points, which can help emphasize critical thresholds in the data."
  },
  {
    "objectID": "posts/2023-09-17/index.html#example-4-logarithmic-breaks",
    "href": "posts/2023-09-17/index.html#example-4-logarithmic-breaks",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "Example 4: Logarithmic Breaks",
    "text": "Example 4: Logarithmic Breaks\nIn some cases, data may follow a logarithmic distribution. You can use logarithmic breaks to visualize such data effectively:\n\n# Create a histogram with logarithmic breaks\nhist(log(mtcars$mpg), main = \"Logarithmic Breaks\", xlab = \"Log(Miles per Gallon)\")\n\n\n\n\nBy taking the logarithm of the data and setting appropriate breaks, you can bring out patterns that might be obscured in a standard histogram."
  },
  {
    "objectID": "posts/2023-09-19/index.html",
    "href": "posts/2023-09-19/index.html",
    "title": "Exploring Data with Scatter Plots by Group in R",
    "section": "",
    "text": "Data visualization is a powerful tool for gaining insights from your data. Scatter plots, in particular, are excellent for visualizing relationships between two continuous variables. But what if you want to compare multiple groups within your data? In this blog post, we’ll explore how to create engaging scatter plots by group in R. We’ll walk through the process step by step, providing several examples and explaining the code blocks in simple terms. So, whether you’re a data scientist, analyst, or just curious about R, let’s dive in and discover how to make your data come to life!"
  },
  {
    "objectID": "posts/2023-09-19/index.html#using-ggplot2",
    "href": "posts/2023-09-19/index.html#using-ggplot2",
    "title": "Exploring Data with Scatter Plots by Group in R",
    "section": "Using ggplot2",
    "text": "Using ggplot2\n\nCreating Scatter Plots by Group:\nTo create scatter plots by group, we’ll use the popular R package, ggplot2. If you haven’t installed it yet, you can do so using the following command:\n\nif(!require(ggplot2)){install.packages(\"ggplot2\")}\n\nNow, let’s load the ggplot2 library:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n\nExample 1: Basic Scatter Plot\nLet’s start with a basic scatter plot that shows the relationship between Sepal.Length and Sepal.Width for all iris species. We’ll color the points by species to distinguish them:\n\n# Create a basic scatter plot\nggplot(\n  data = iris, \n  aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  labs(title = \"Sepal Length vs. Sepal Width by Species\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\") +\n  theme_minimal()\n\n\n\n\nIn this code: - We specify the dataset (iris) and the variables we want to plot. - geom_point() adds the points to the plot. - labs() is used to add a title and label the axes.\n\n\nExample 2: Faceted Scatter Plot\nNow, let’s take it a step further and create separate scatter plots for each iris species using faceting:\n\n# Create faceted scatter plots\nggplot(\n  data = iris, \n  aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_wrap(~Species) +\n  labs(title = \"Sepal Length vs. Sepal Width by Species\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\") +\n  theme_minimal()\n\n\n\n\nIn this example, facet_wrap(~Species) creates three individual scatter plots, one for each iris species. This makes it easier to compare the species’ characteristics.\n\n\nExample 3: Customized Scatter Plot\nLet’s customize our scatter plot further by adding regression lines and adjusting point aesthetics:\n\n# Create a customized scatter plot\nggplot(\n  data = iris, \n  aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3, alpha = 0.7, shape = 19) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Customized Sepal Length vs. Sepal Width by Species\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIn this example: - geom_point() now includes size, alpha (transparency), and shape aesthetics. - geom_smooth() adds linear regression lines to each group."
  },
  {
    "objectID": "posts/2023-09-19/index.html#using-base-r",
    "href": "posts/2023-09-19/index.html#using-base-r",
    "title": "Exploring Data with Scatter Plots by Group in R",
    "section": "Using Base R",
    "text": "Using Base R\n\nExample 1: Basic Scatter Plot in Base R\nTo create a basic scatter plot in base R, we can use the plot() function. Here’s how to create a scatter plot of Sepal.Length vs. Sepal.Width by grouping on the “Species” variable:\n\n# Create a basic scatter plot\nplot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species, \n     pch = 19, main = \"Sepal Length vs. Sepal Width by Species\",\n     xlab = \"Sepal Length\", ylab = \"Sepal Width\")\nlegend(\"topright\", legend = levels(iris$Species), col = 1:3, pch = 19)\n\n\n\n\nIn this code: - plot() is used to create the scatter plot. - We specify the x and y variables, and we use the col argument to color the points by species. - pch specifies the point character (shape). - main, xlab, and ylab are used to add a title and label the axes. - legend() adds a legend to distinguish the species colors.\n\n\nExample 2: Faceted Scatter Plot in Base R\nTo create faceted scatter plots in base R, we can use the split() function to split the data by the “Species” variable and then create individual scatter plots for each group:\n\n# Split the data by species\nsplit_data &lt;- split(iris, iris$Species)\n\n# Create faceted scatter plots\npar(mfrow = c(1, 3))  # Arrange plots in one row and three columns\nfor (i in 1:3) {\n  plot(split_data[[i]]$Sepal.Length, split_data[[i]]$Sepal.Width, \n       pch = 19, main = levels(iris$Species)[i], \n       xlab = \"Sepal Length\", ylab = \"Sepal Width\")\n}\n\n\n\npar(mfrow = c(1, 1))\n\nIn this code: - We first use split() to split the data into three groups based on the “Species” variable. - Then, we use a for loop to create individual scatter plots for each group. - par(mfrow = c(1, 3)) arranges the plots in one row and three columns.\n\n\nExample 3: Customized Scatter Plot in Base R\nTo create a customized scatter plot in base R, we can adjust various graphical parameters. Here’s an example with customized aesthetics and regression lines:\n\n# Create a customized scatter plot with regression lines\nplot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species, \n     pch = 19, main = \"Customized Sepal Length vs. Sepal Width by Species\",\n     xlab = \"Sepal Length\", ylab = \"Sepal Width\")\nlegend(\"topright\", legend = levels(iris$Species), col = 1:3, pch = 19)\n\n# Add regression lines\nfor (i in 1:3) {\n  group_data &lt;- split_data[[i]]\n  lm_fit &lt;- lm(Sepal.Width ~ Sepal.Length, data = group_data)\n  abline(lm_fit, col = i)\n}\n\n\n\n\nIn this code: - We add regression lines to each group using a for loop and the abline() function. - The lm() function is used to fit linear regression models to each group separately.\nNow you have recreated the scatter plots by group using base R. Feel free to explore more customization options and adapt these examples to your specific needs. Happy coding!"
  },
  {
    "objectID": "posts/2023-09-20/index.html",
    "href": "posts/2023-09-20/index.html",
    "title": "Mastering Data Visualization in R: Plotting Predicted Values with the mtcars Dataset",
    "section": "",
    "text": "Introduction\nData visualization is a powerful tool in a data scientist’s toolkit. It not only helps us understand our data but also presents it in a way that is easy to comprehend. In this blog post, we will explore how to plot predicted values in R using the mtcars dataset. We will train a simple regression model to predict the miles per gallon (mpg) of cars based on their attributes and then visualize the predictions. By the end of this tutorial, you’ll have a clear understanding of how to plot predicted values and can apply this knowledge to your own data analysis projects.\nStep 1: Load the Required Libraries\nBefore we dive into the code, let’s make sure we have the necessary libraries installed. We’ll be using ggplot2 for plotting and caret for model training and evaluation. You can install them if you haven’t already using:\ninstall.packages(\"ggplot2\")\ninstall.packages(\"caret\")\nNow, let’s load the libraries:\n\nlibrary(ggplot2)\nlibrary(caret)\n\nStep 2: Load and Explore the Data\nWe’ll use the classic mtcars dataset, which contains various attributes of different car models. Our goal is to predict the fuel efficiency (mpg) of these cars. Let’s load and explore the dataset:\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThis will display the first few rows of the dataset, giving you an idea of what it looks like.\nStep 3: Split the Data into Training and Testing Sets\nBefore we proceed with modeling and prediction, we need to split our data into training and testing sets. We’ll use 80% of the data for training and the remaining 20% for testing:\n\nset.seed(123)  # for reproducibility\nsplitIndex &lt;- createDataPartition(mtcars$mpg, p = 0.8, list = FALSE)\ntraining_data &lt;- mtcars[splitIndex, ]\ntesting_data &lt;- mtcars[-splitIndex, ]\n\nStep 4: Build a Simple Linear Regression Model\nNow, let’s build a simple linear regression model to predict mpg based on other attributes. We’ll use the lm() function:\n\nmodel &lt;- lm(mpg ~ ., data = training_data)\n\nThis line of code fits the linear regression model using the training data.\nStep 5: Make Predictions\nWith our model trained, we can now make predictions on the testing data:\n\npredictions &lt;- predict(model, newdata = testing_data)\n\nStep 6: Create a Scatter Plot of Predicted vs. Actual Values\nThe most exciting part is visualizing the predicted values. We can do this using a scatter plot. Let’s create one:\n\n# Combine actual and predicted values\nplot_data &lt;- data.frame(Actual = testing_data$mpg, Predicted = predictions)\n\n# Create a scatter plot\nggplot(plot_data, aes(x = Actual, y = Predicted)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"red\") +\n  labs(\n    x = \"Actual MPG\", \n    y = \"Predicted MPG\", \n    title = \"Actual vs. Predicted MPG\"\n    ) +\n  theme_minimal()\n\n\n\n\nThis code generates a scatter plot with the actual MPG values on the x-axis and predicted MPG values on the y-axis. The red line represents a linear regression line that helps us see how well our predictions align with the actual data.\nHere is how we also plot the data in base R.\n\n# Combine actual and predicted values\nplot_data &lt;- data.frame(Actual = testing_data$mpg, Predicted = predictions)\n\n# Create a scatter plot\nplot(plot_data$Actual, plot_data$Predicted,\n     xlab = \"Actual MPG\", ylab = \"Predicted MPG\",\n     main = \"Actual vs. Predicted MPG\",\n     pch = 19, col = \"blue\")\n\n# Add a regression line\nabline(lm(Predicted ~ Actual, data = plot_data), col = \"red\")\n\n\n\n\n\n\nConclusion\nCongratulations! You’ve successfully learned how to plot predicted values in R using the mtcars dataset. Visualization is a vital part of data analysis, and it can provide valuable insights into the performance of your predictive models.\nI encourage you to try this on your own datasets and explore more advanced visualization techniques. Experiment with different models and datasets to gain a deeper understanding of data visualization in R. Happy coding!"
  },
  {
    "objectID": "posts/2023-09-22/index.html",
    "href": "posts/2023-09-22/index.html",
    "title": "Creating Confidence Intervals for a Linear Model in R Using Base R and the Iris Dataset",
    "section": "",
    "text": "Introduction\nLinear regression is a fundamental statistical technique used to model the relationship between a dependent variable and one or more independent variables. While fitting a linear model is relatively straightforward in R, it’s also essential to understand the uncertainty associated with our model’s predictions. One way to visualize this uncertainty is by creating confidence intervals around the regression line. In this blog post, we’ll walk through how to perform linear regression and plot confidence intervals using base R with the popular Iris dataset.\n\n\nAbout the Iris Dataset\nThe Iris dataset is a well-known dataset in the field of statistics and machine learning. It contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers: setosa, versicolor, and virginica. For our purposes, we’ll focus on predicting petal length based on petal width for one of the iris species.\n\n\nLoading the Data\nFirst, let’s load the Iris dataset and take a quick look at its structure:\n# Load the Iris dataset\ndata(iris)\nNow view it\n\n# View the first few rows of the dataset\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\nFitting a Linear Model\nWe want to predict petal length (dependent variable) based on petal width (independent variable). To do this, we’ll fit a linear regression model using the lm() function in R:\n\n# Fit a linear regression model\nmodel &lt;- lm(Petal.Length ~ Petal.Width, data = iris)\n\nNow that we have our model, let’s move on to creating confidence intervals for the regression line.\n\n\nCalculating Confidence Intervals\nTo calculate confidence intervals for the regression line, we’ll use the predict() function with the interval argument set to “confidence”:\n\n# Calculate confidence intervals\nconfidence_intervals &lt;- predict(\n  model, \n  interval = \"confidence\", \n  level = 0.95\n)\n\n# View the first few rows of the confidence intervals\nhead(confidence_intervals)\n\n       fit      lwr      upr\n1 1.529546 1.402050 1.657042\n2 1.529546 1.402050 1.657042\n3 1.529546 1.402050 1.657042\n4 1.529546 1.402050 1.657042\n5 1.529546 1.402050 1.657042\n6 1.975534 1.863533 2.087536\n\n\nThe confidence_intervals object now contains the lower and upper bounds of the confidence intervals for our predictions.\n\n\nCreating the Plot\nWith the confidence intervals calculated, we can create a visually appealing plot to display our linear regression model and the associated confidence intervals:\n\n# Create a scatterplot of the data\nplot(\n  iris$Petal.Width, \n  iris$Petal.Length, \n  main = \"Linear Regression with Confidence Intervals\", \n  xlab = \"Petal Width\", ylab = \"Petal Length\"\n)\n\n# Add the regression line\nabline(model, col = \"blue\")\n\n# Add confidence intervals as shaded areas\npolygon(\n  c(iris$Petal.Width, rev(iris$Petal.Width)),\n  c(\n    confidence_intervals[, \"lwr\"], \n    rev(confidence_intervals[, \"upr\"])\n    ), \n  col = rgb(0, 0, 1, 0.2), border = NA)\n\n# Add a legend\nlegend(\n  \"topright\", \n  legend = c(\"Regression Line\", \"95% Confidence Interval\"), \n  col = c(\"blue\", rgb(0, 0, 1, 0.2)), \n  fill = c(NA, rgb(0, 0, 1, 0.2))\n)\n\n\n\n\nIn this plot, we start by creating a scatterplot of the data points, then overlay the regression line in blue. The shaded area represents the 95% confidence interval around the regression line, giving us an idea of the uncertainty in our predictions.\nHere is a slightly different method, the confidence intervals:\n\n# Calculate confidence intervals\nconf_intervals &lt;- predict(model, interval = \"confidence\")\n\nNow the plot:\n\n# Create a scatterplot\nplot(\n  iris$Petal.Width, \n  iris$Petal.Length, \n  main = \"Linear Model with Confidence Intervals\",\n  xlab = \"Petal Width\", \n  ylab = \"Petal Length\", \n  pch = 19, \n  col = \"blue\"\n)\n\n# Add the regression line\nabline(model, col = \"red\")\n\n# Add confidence intervals\nlines(\n  iris$Petal.Width, \n  conf_intervals[, \"lwr\"], \n  col = \"green\", \n  lty = 2\n)\nlines(\n  iris$Petal.Width, \n  conf_intervals[, \"upr\"], \n  col = \"green\", \n  lty = 2\n)\n\n\n\n\n\n\nConclusion\nIn this blog post, we’ve demonstrated how to perform linear regression and plot confidence intervals using base R with the Iris dataset. Understanding and visualizing the uncertainty associated with our regression model is crucial for making informed decisions based on the model’s predictions. You can apply these techniques to other datasets and regression problems to gain deeper insights into your data.\nLinear regression is just one of the many statistical techniques that R offers. As you continue your data analysis journey, you’ll find R to be a powerful tool for exploring, modeling, and visualizing data."
  },
  {
    "objectID": "posts/2023-09-25/index.html",
    "href": "posts/2023-09-25/index.html",
    "title": "Mastering Data Visualization with Pairs Plots in Base R",
    "section": "",
    "text": "Data visualization is a crucial tool in data analysis, allowing us to gain insights from our data quickly. One of the fundamental techniques for exploring relationships between variables is the pairs plot. In this blog post, we’ll dive into the world of pairs plots in base R. We’ll explore what they are, why they are useful, and how to create and interpret them."
  },
  {
    "objectID": "posts/2023-09-25/index.html#customizing-your-pairs-plot",
    "href": "posts/2023-09-25/index.html#customizing-your-pairs-plot",
    "title": "Mastering Data Visualization with Pairs Plots in Base R",
    "section": "Customizing Your Pairs Plot",
    "text": "Customizing Your Pairs Plot\nYou can customize your pairs plot in various ways to make it more informative and visually appealing. Here are some customization options:\n\nColoring by Groups: If your dataset has categorical variables that define groups, you can use colors to distinguish between them. For example, you can color data points by species in the “iris” dataset.\n\n# Color points by species\npairs(iris[, 1:4], main = \"Pairs Plot of Iris Data\", col = iris$Species)\n\n\n\n\nAdding Regression Lines: To visualize linear relationships more clearly, you can add regression lines to the scatterplots.\n\n\n# Add regression lines\npairs(iris[, 1:4], panel=function(x,y){\n  points(x,y)\n  abline(lm(y~x), col='red')})\n\n\n\n\nNow let’s add color back into the plot:\n\npairs(iris[, 1:4], panel=function(x,y){\n  # Get a vector of colors for each point in the plot\n  colors &lt;- ifelse(iris$Species == \"setosa\", \"red\",\n                   ifelse(iris$Species == \"versicolor\", \"green\", \"blue\"))\n\n  # Plot the points with the corresponding colors\n  points(x, y, col = colors)\n\n  # Add a regression line\n  abline(lm(y~x), col='red')\n})"
  },
  {
    "objectID": "posts/2023-09-26/index.html",
    "href": "posts/2023-09-26/index.html",
    "title": "Mastering Data Visualization with ggplot2: A Guide to Using facet_grid()",
    "section": "",
    "text": "Introduction\nData visualization is a crucial tool in the data scientist’s toolkit. It allows us to explore and communicate complex patterns and insights effectively. In the world of R programming, one of the most powerful and versatile packages for data visualization is ggplot2. Among its many features, ggplot2 offers the facet_grid() function, which enables you to create multiple plots arranged in a grid, making it easier to visualize different groups of data simultaneously.\nIn this blog post, we’ll dive into the fascinating world of facet_grid() using a practical example. We’ll generate some synthetic data, split it into multiple groups, and then use facet_grid() to create a visually appealing grid of plots.\n\n\nGenerating Synthetic Data\nLet’s start by generating some synthetic data using the TidyDensity package in R. We’ll create three groups of data, each with 100 observations, and a mean of -2, 0, and 2, respectively, all with a standard deviation of 1. We’ll also perform this simulation five times to create a diverse dataset.\n\nlibrary(TidyDensity)\n\ndata &lt;- tidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\", \n  .param_list = list(\n    .n = 100, \n    .mean = c(-2, 0, 2), \n    .sd = 1, \n    .num_sims = 5\n    )\n  )\n\nNow that we have our data, it’s time to visualize it using facet_grid().\n\n\nUsing facet_grid() to Visualize Multiple Groups\nThe facet_grid() function in ggplot2 is a versatile tool for creating a grid of plots based on one or more categorical variables. It allows you to create small multiples, which are a series of similar plots, each showing a subset of your data.\nIn our synthetic data, we have three groups (mean of -2, 0, and 2), and we want to visualize each group’s distribution. Here’s how you can do it:\n\n# Create a ggplot object\n# Load ggplot2\nlibrary(ggplot2)\n\n# Create a ggplot object\np &lt;- ggplot(data, aes(x = y, color = sim_number, group = sim_number)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(. ~ dist_name)\n\n# Customize the plot\np + labs(title = \"Density Plots of Three Different Means\",\n         x = \"Value\",\n         y = \"Density\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nIn this code:\n\nWe load the ggplot2 package, which is essential for creating our plots.\nWe create a ggplot object p where we specify the aesthetics (x-axis, fill color) and the geometry (density plot). We use facet_grid(. ~ simulation) to split the data into separate facets based on the simulation variable. This means that each facet will represent one of the five simulations.\nWe add labels and customize the plot’s appearance using the labs() and theme_minimal() functions.\nFinally, we display the plot by evaluating p.\n\nThe resulting plot will show a grid of density plots, with each facet representing one simulation. Within each facet, you’ll see the density distribution of the data, colored by the group mean.\n\n\nConclusion\nIn this blog post, we explored the power of ggplot2’s facet_grid() function for visualizing multiple groups of data. By generating synthetic data and using ggplot2, we created an informative grid of density plots, allowing us to compare and contrast the distributions of different groups.\nThe ability to create small multiples with facet_grid() is invaluable for gaining insights from complex datasets. Whether you’re working with synthetic data or real-world data, mastering ggplot2’s facet_grid() function will enhance your data visualization skills and help you communicate your findings more effectively.\nSo, go ahead and experiment with your data. Create your own grid of plots using facet_grid() and unlock new ways to visualize and understand your data. Happy plotting!"
  },
  {
    "objectID": "posts/2023-09-27/index.html",
    "href": "posts/2023-09-27/index.html",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "",
    "text": "Data visualization is a crucial aspect of data analysis and exploration. It allows us to gain insights, spot trends, and communicate our findings effectively. In R, there are numerous packages and libraries available for creating sophisticated plots, but understanding the basics of base R plotting is essential for any data analyst or scientist.\nIn this blog post, we’ll explore how to overlay points or lines on a plot using Base R. We’ll use the plot() function to create the initial plot and then show how to overlay points with points() and lines with lines(). We’ll provide several examples, explaining each code block in simple terms, and encourage you to try them out on your own datasets."
  },
  {
    "objectID": "posts/2023-09-27/index.html#example-1-overlaying-points-on-a-scatter-plot",
    "href": "posts/2023-09-27/index.html#example-1-overlaying-points-on-a-scatter-plot",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "Example 1: Overlaying Points on a Scatter Plot",
    "text": "Example 1: Overlaying Points on a Scatter Plot\nLet’s begin with a simple scatter plot. Suppose we have two vectors, x and y, representing data points.\n\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 1, 5, 7)\n\n# Create the initial scatter plot\nplot(x, y, main = \"Scatter Plot with Overlay Points\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n# Overlay additional points (red circles) on the plot\npoints(x, y, col = \"red\", pch = 16)\n\n\n\n\nIn this example, we use the plot() function to create a scatter plot of x and y. Then, we overlay red circles on the existing plot using points(). The col argument specifies the color, and pch determines the point shape (16 represents circles)."
  },
  {
    "objectID": "posts/2023-09-27/index.html#example-2-overlaying-lines-on-a-line-plot",
    "href": "posts/2023-09-27/index.html#example-2-overlaying-lines-on-a-line-plot",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "Example 2: Overlaying Lines on a Line Plot",
    "text": "Example 2: Overlaying Lines on a Line Plot\nNow, let’s work with line plots. Suppose we have two vectors, x and y, representing data points for a line graph.\n\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 1, 5, 7)\n\n# Create the initial line plot\nplot(x, y, type = \"l\", main = \"Line Plot with Overlay Lines\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n# Overlay a new line (dashed, blue) on the plot\nlines(x, y + 1, col = \"blue\", lty = 2)\n\n\n\n\nIn this example, we use the plot() function with type = \"l\" to create a line plot. Then, we overlay a dashed blue line on the plot using lines(). The col argument sets the line color, and lty specifies the line type (2 stands for dashed)."
  },
  {
    "objectID": "posts/2023-09-27/index.html#example-3-combining-points-and-lines",
    "href": "posts/2023-09-27/index.html#example-3-combining-points-and-lines",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "Example 3: Combining Points and Lines",
    "text": "Example 3: Combining Points and Lines\nIn some cases, you might want to overlay both points and lines on the same plot to illustrate relationships more clearly. Let’s see how to do that:\n\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 1, 5, 7)\n\n# Create the initial scatter plot\nplot(x, y, main = \"Scatter Plot with Overlay Points and Lines\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n# Overlay points (green triangles)\npoints(x, y, col = \"green\", pch = 17)\n\n# Overlay a line (purple) connecting the points\nlines(x, y, col = \"purple\")\n\n\n\n\nIn this example, we start with a scatter plot and overlay green triangles using points() and a purple line using lines(). The combination of points and lines can help emphasize patterns and relationships in your data."
  },
  {
    "objectID": "posts/2023-09-27/index.html#conclusion",
    "href": "posts/2023-09-27/index.html#conclusion",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we’ve explored how to overlay points and lines on a plot in Base R. We’ve covered scatter plots, line plots, and combinations of both. Overlaying points and lines can be a powerful way to enhance your data visualizations and convey insights effectively.\nNow it’s your turn! Experiment with different datasets and customize your plots by adjusting colors, shapes, and line styles. Base R provides a solid foundation for data visualization, and mastering it will enable you to create informative plots for your data analysis projects."
  },
  {
    "objectID": "posts/2023-09-28/index.html",
    "href": "posts/2023-09-28/index.html",
    "title": "How to Reorder Boxplots in R: A Comprehensive Guide",
    "section": "",
    "text": "Boxplots are a great way to visualize the distribution of a dataset. However, sometimes the default ordering of boxplots may not be ideal for the data being presented. In this blog post, we will explore how to reorder boxplots in R using base R. We will provide at least three examples and explain them in simple terms. We encourage readers to try things on their own."
  },
  {
    "objectID": "posts/2023-09-28/index.html#example-1-reorder-based-on-specific-order",
    "href": "posts/2023-09-28/index.html#example-1-reorder-based-on-specific-order",
    "title": "How to Reorder Boxplots in R: A Comprehensive Guide",
    "section": "Example 1: Reorder Based on Specific Order",
    "text": "Example 1: Reorder Based on Specific Order\nThe first example shows how to order the boxplots based on a specific order for the variable being plotted. We will use the built-in airquality dataset in R. The following code shows how to order the boxplots based on the following order for the Month variable: 5, 8, 6, 9, 7.\n\n# Load the airquality dataset\ndata(airquality)\n\n# Reorder Month values\nairquality$Month &lt;- factor(airquality$Month, levels=c(5, 8, 6, 9, 7))\n\n# Create boxplot of temperatures by month using the order we specified\nboxplot(Temp ~ Month, data=airquality, col=\"lightblue\", border=\"black\")\n\n\n\n\nNotice that the boxplots now appear in the order that we specified using the levels argument."
  },
  {
    "objectID": "posts/2023-09-28/index.html#example-2-reorder-based-on-median-value",
    "href": "posts/2023-09-28/index.html#example-2-reorder-based-on-median-value",
    "title": "How to Reorder Boxplots in R: A Comprehensive Guide",
    "section": "Example 2: Reorder Based on Median Value",
    "text": "Example 2: Reorder Based on Median Value\nThe second example shows how to order the boxplots in ascending order based on the median value for each group. We will use the built-in PlantGrowth dataset in R.\n\n# Load the PlantGrowth dataset\ndata(PlantGrowth)\n\n# Create boxplot of weight by group\nboxplot(weight ~ group, data=PlantGrowth, col=\"lightblue\", border=\"black\")\n\n\n\n# Reorder the groups based on median weight\ngroup_order &lt;- names(sort(tapply(PlantGrowth$weight, PlantGrowth$group, median)))\nPlantGrowth$group &lt;- factor(PlantGrowth$group, levels=group_order)\n\n# Create boxplot of weight by group using the new order\nboxplot(weight ~ group, data=PlantGrowth, col=\"lightblue\", border=\"black\")\n\n\n\n\nNotice that the boxplots now appear in ascending order based on the median weight for each group."
  },
  {
    "objectID": "posts/2023-09-28/index.html#example-3-reorder-based-on-custom-function",
    "href": "posts/2023-09-28/index.html#example-3-reorder-based-on-custom-function",
    "title": "How to Reorder Boxplots in R: A Comprehensive Guide",
    "section": "Example 3: Reorder Based on Custom Function",
    "text": "Example 3: Reorder Based on Custom Function\nThe third example shows how to order the boxplots based on a custom function. We will use the built-in ToothGrowth dataset in R.\n\n# Load the ToothGrowth dataset\ndata(ToothGrowth)\n\n# Create boxplot of length by dose\nboxplot(len ~ dose, data=ToothGrowth, col=\"lightblue\", border=\"black\")\n\n# Reorder the groups based on the mean length multiplied by the dose\ngroup_order &lt;- names(sort(tapply(ToothGrowth$len * ToothGrowth$dose, ToothGrowth$dose, mean)))\nToothGrowth$dose &lt;- factor(ToothGrowth$dose, levels=group_order)\n\n# Create boxplot of length by dose using the new order\nboxplot(len ~ dose, data=ToothGrowth, col=\"lightblue\", border=\"black\")\n\n\n\n\nNotice that the boxplots now appear in order based on the mean length multiplied by the dose for each group."
  },
  {
    "objectID": "posts/2023-09-29/index.html",
    "href": "posts/2023-09-29/index.html",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "",
    "text": "Decision trees are a powerful machine learning algorithm that can be used for both classification and regression tasks. They are easy to understand and interpret, and they can be used to build complex models without the need for feature engineering.\nOnce you have trained a decision tree model, you can use it to make predictions on new data. However, it can also be helpful to plot the decision tree to better understand how it works and to identify any potential problems.\nIn this blog post, we will show you how to plot decision trees in R using the rpart and rpart.plot packages. We will also provide an extensive example using the iris data set and explain the code blocks in simple to use terms."
  },
  {
    "objectID": "posts/2023-09-29/index.html#load-the-libraries",
    "href": "posts/2023-09-29/index.html#load-the-libraries",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nlibrary(rpart)\nlibrary(rpart.plot)"
  },
  {
    "objectID": "posts/2023-09-29/index.html#split-the-data-into-training-and-test-sets",
    "href": "posts/2023-09-29/index.html#split-the-data-into-training-and-test-sets",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "Split the data into training and test sets",
    "text": "Split the data into training and test sets\n\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(iris), size = 0.7 * nrow(iris))\ntrain &lt;- iris[train_index, ]\ntest &lt;- iris[-train_index, ]"
  },
  {
    "objectID": "posts/2023-09-29/index.html#train-a-decision-tree-model",
    "href": "posts/2023-09-29/index.html#train-a-decision-tree-model",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "Train a decision tree model",
    "text": "Train a decision tree model\n\ntree &lt;- rpart(Species ~ ., data = train, method = \"class\")"
  },
  {
    "objectID": "posts/2023-09-29/index.html#plot-the-decision-tree",
    "href": "posts/2023-09-29/index.html#plot-the-decision-tree",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "Plot the decision tree",
    "text": "Plot the decision tree\n\nrpart.plot(tree, main = \"Decision Tree for the Iris Dataset\")"
  },
  {
    "objectID": "posts/2023-10-02/index.html",
    "href": "posts/2023-10-02/index.html",
    "title": "Horizontal Boxplots in R using the Palmer Penguins Data Set",
    "section": "",
    "text": "Introduction\nBoxplots are a great way to visualize the distribution of a numerical variable. They show the median, quartiles, and outliers of the data, and can be used to compare the distributions of multiple groups.\nHorizontal boxplots are a variant of the traditional boxplot, where the x-axis is horizontal and the y-axis is vertical. This can be useful for visualizing data where the x-axis variable is categorical, such as species or treatment group.\n\n\nCreating horizontal boxplots in base R\nTo create a horizontal boxplot in base R, we can use the boxplot() function with the horizontal argument set to TRUE.\n\nlibrary(palmerpenguins)\n\n\n# Create a horizontal boxplot of bill length by species\nboxplot(\n  bill_length_mm ~ species,\n  data = penguins,\n  horizontal = TRUE,\n  main = \"Bill length by species in Palmer penguins\",\n  xlab = \"Bill length (mm)\",\n  ylab = \"Species\"\n)\n\n\n\n\nThis code will produce a horizontal boxplot with one box for each species of penguin. The boxes show the median, quartiles, and outliers of the bill length data for each species.\n\n\nCreating horizontal boxplots in ggplot2\nTo create a horizontal boxplot in ggplot2, we can use the geom_boxplot() function with the coord_flip() function.\n\nlibrary(ggplot2)\n\n# Create a horizontal boxplot of bill length by species using ggplot2\nggplot(penguins, aes(x = bill_length_mm, y = species)) +\n  geom_boxplot() +\n  labs(\n    title = \"Bill length by species in Palmer penguins\",\n    x = \"Bill length (mm)\",\n    y = \"Species\"\n  )\n\n\n\n\nThis code will produce a horizontal boxplot that is similar to the one produced by the base R code above. However, the ggplot2 code is more flexible and allows us to customize the appearance of the plot more easily.\n\n\nEncouragement\nI encourage you to try creating horizontal boxplots for your own data. You can use the Palmer penguins data set as a starting point, or you can use your own data. Experiment with different options to customize the appearance of your plots.\nHere are some ideas for things to try:\n\nCompare the distribution of different numerical variables across different groups. For example, you could compare the distribution of bill length across the three species of penguins, or you could compare the distribution of body mass across male and female penguins.\nUse different colors and fill patterns to distinguish between groups.\nAdd jitter to the data points to avoid overplotting.\nAdd a legend to identify the different groups.\nSave your plots to files or export them to other applications.\n\nI hope this blog post has been helpful. If you have any questions, please leave a comment below.\n\n\nConclusion\nHorizontal boxplots can be a useful way to visualize the distribution of data when the x-axis variable is categorical. They are easy to create in both base R and ggplot2."
  },
  {
    "objectID": "posts/2023-10-03/index.html",
    "href": "posts/2023-10-03/index.html",
    "title": "Creating Interactive Radar Charts in R with the ‘fmsb’ Library",
    "section": "",
    "text": "Radar charts, also known as spider, web, polar, or star plots, are a useful way to visualize multivariate data. In R, we can create radar charts using the fmsb library. Here are several examples of how to create radar charts in R using the fmsb library:"
  },
  {
    "objectID": "posts/2023-10-03/index.html#example-1-basic-radar-chart",
    "href": "posts/2023-10-03/index.html#example-1-basic-radar-chart",
    "title": "Creating Interactive Radar Charts in R with the ‘fmsb’ Library",
    "section": "Example 1: Basic radar chart",
    "text": "Example 1: Basic radar chart\nThe following code creates a basic radar chart using the radarchart() function from the fmsb package. The input data format is specific, where each row represents an entity and each column is a variable. The first row should be the maximum values of the data, and the second row should be the minimum values. The default radar chart can be customized using various options, such as line color, fill color, line width, and axis label color.\n\n# Load the fmsb package\nlibrary(fmsb)\n\n# Create sample data\ndata &lt;- as.data.frame(matrix(sample(2:20, 10, replace = T), \n                             ncol = 10))\ncolnames(data) &lt;- c(\"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \n                    \"Var6\", \"Var7\", \"Var8\", \"Var9\", \"Var10\")\ndata &lt;- rbind(rep(20, 10), rep(0, 10), data)\n\n# Create a basic radar chart\nradarchart(data)"
  },
  {
    "objectID": "posts/2023-10-03/index.html#example-2-customized-radar-chart",
    "href": "posts/2023-10-03/index.html#example-2-customized-radar-chart",
    "title": "Creating Interactive Radar Charts in R with the ‘fmsb’ Library",
    "section": "Example 2: Customized radar chart",
    "text": "Example 2: Customized radar chart\nThe following code creates a customized radar chart using the radarchart() function. The axistype argument is set to 1 to customize the polygon, and the pcol, pfcol, and plwd arguments are used to customize the grid and line properties.\n\n# Create sample data\ndata &lt;- as.data.frame(matrix(sample(2:20, 10, replace = T), \n                             ncol = 10))\ncolnames(data) &lt;- c(\"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \n                    \"Var6\", \"Var7\", \"Var8\", \"Var9\", \"Var10\")\ndata &lt;- rbind(rep(20, 10), rep(0, 10), data)\n\n# Customize the radar chart\nradarchart(data, axistype = 1, pcol = rgb(0.2, 0.5, 0.5, 0.9), \n           pfcol = rgb(0.2, 0.5, 0.5, 0.5), plwd = 4, cglcol =\n          \"grey\", cglty = 1)"
  },
  {
    "objectID": "posts/2023-10-03/index.html#example-3-radar-chart-with-multiple-groups",
    "href": "posts/2023-10-03/index.html#example-3-radar-chart-with-multiple-groups",
    "title": "Creating Interactive Radar Charts in R with the ‘fmsb’ Library",
    "section": "Example 3: Radar chart with multiple groups",
    "text": "Example 3: Radar chart with multiple groups\nThe following code creates a radar chart with multiple groups using the radarchart() function. The input data frame should have more than three variables as axes, and the rows indicate cases as series. The first row should show the maximum values, the second row should show the minimum values, and the actual data should be given as row 3 and lower rows.\n\n# Create sample data\nset.seed(1)\ndata &lt;- data.frame(rbind(rep(10, 8), rep(0, 8), \n                         matrix(sample(0:10, 24, replace = TRUE),\n                                nrow = 3)))\ncolnames(data) &lt;- paste(\"Var\", 1:8)\n\n# Create a radar chart with multiple groups\nradarchart(data, axistype = 1, pcol = 1, plwd = 2, \n           pdensity = 10, pangle = 40, cglty = 1, \n           cglcol = \"gray\")"
  },
  {
    "objectID": "posts/2023-10-04/index.html",
    "href": "posts/2023-10-04/index.html",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "",
    "text": "Stacked dot plots are a type of plot that displays frequencies using dots, piled one over the other. In R, there are several ways to create stacked dot plots, including using base R and ggplot2. In this blog post, we will explore how to create stacked dot plots in both Base R and ggplot2, and provide several examples of each."
  },
  {
    "objectID": "posts/2023-10-04/index.html#method-1-the-stripchart-function",
    "href": "posts/2023-10-04/index.html#method-1-the-stripchart-function",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "Method 1: The stripchart() function",
    "text": "Method 1: The stripchart() function\nThe stripchart() function in base R can be used to create a basic stacked dot plot. Here is an example of how to use it:\n\n# create some fake data\nset.seed(0)\ndata &lt;- sample(0:20, 100, replace = TRUE)\n\n# create stacked dot plot\nstripchart(data, method = \"stack\")\n\n\n\n\nThis will create a basic stacked dot plot. However, we can customize it to make it more aesthetically pleasing. Here is an example of how to do that:\n\n# create some fake data\nset.seed(0)\ndata &lt;- sample(0:20, 100, replace = TRUE)\n\n# create stacked dot plot\nstripchart(data, method = \"stack\", offset = .5, at = 0,\n           pch = 19, col = \"steelblue\", \n           main = \"Stacked Dot Plot\", xlab = \"Data Values\")\n\n\n\n\nThis will create a stacked dot plot with a blue color scheme and a more aesthetically pleasing layout."
  },
  {
    "objectID": "posts/2023-10-04/index.html#method-1-the-geom_dotplot-function",
    "href": "posts/2023-10-04/index.html#method-1-the-geom_dotplot-function",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "Method 1: The geom_dotplot() function",
    "text": "Method 1: The geom_dotplot() function\nThe geom_dotplot() function in ggplot2 can be used to create a basic stacked dot plot. Here is an example of how to use it:\n\n# load ggplot2\nlibrary(ggplot2)\n\n# create some fake data\nset.seed(0)\ndata &lt;- data.frame(x = sample(0:20, 100, replace = TRUE))\n\n# create stacked dot plot\nggplot(data, aes(x = x)) + geom_dotplot() + theme_minimal()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nThis will create a basic stacked dot plot. However, we can customize it to make it more aesthetically pleasing. Here is an example of how to do that:\n\n# create some fake data\nset.seed(0)\ndata &lt;- data.frame(x = sample(0:20, 100, replace = TRUE))\n\n# create customized stacked dot plot\nggplot(data, aes(x = x)) + \n  geom_dotplot(dotsize = .75, stackratio = 1.2, \n               fill = \"steelblue\") + \n  scale_y_continuous(NULL, breaks = NULL) + \n  labs(title = \"Stacked Dot Plot\", x = \"Data Values\") +\n  theme_minimal()\n\n\n\n\nThis will create a stacked dot plot with a blue color scheme and a more aesthetically pleasing layout."
  },
  {
    "objectID": "posts/2023-10-04/index.html#method-2-the-geom_jitter-function",
    "href": "posts/2023-10-04/index.html#method-2-the-geom_jitter-function",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "Method 2: The geom_jitter() function",
    "text": "Method 2: The geom_jitter() function\nAnother way to create a stacked dot plot in ggplot2 is to use the geom_jitter() function. Here is an example of how to use it:\n\n# create some fake data\nset.seed(0)\ndata &lt;- data.frame(x = sample(0:20, 100, replace = TRUE))\n\n# create stacked dot plot\nggplot(data, aes(x = x, y = 0)) + \n  geom_jitter(height = .1, width = 0, alpha = .5, \n              color = \"steelblue\") + \n  labs(title = \"Stacked Dot Plot\", x = \"Data Values\") +\n  theme_minimal()\n\n\n\n\nThis will create a stacked dot plot with a blue color scheme and a more aesthetically pleasing layout.\nIn conclusion, creating stacked dot plots in R is a simple and effective way to visualize frequency data. By using either base R or ggplot2, you can create aesthetically pleasing plots that are easy to interpret. We encourage readers to try creating their own stacked dot plots using the examples provided in this blog post."
  },
  {
    "objectID": "posts/2023-10-04/index.html#method-2-the-dotchart-function",
    "href": "posts/2023-10-04/index.html#method-2-the-dotchart-function",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "Method 2: The dotchart() function",
    "text": "Method 2: The dotchart() function\nAnother way to create a stacked dot plot in base R is to use the dotchart() function. Here is an example of how to use it:\n\n# create some fake data\nset.seed(0)\ndata &lt;- sample(0:20, 100, replace = TRUE)\n\n# create stacked dot plot\ndotchart(data, cex = .7, col = \"steelblue\", \n         main = \"Stacked Dot Plot\", xlab = \"Data Values\")\n\n\n\n\nThis will create a stacked dot plot with a blue color scheme and a more aesthetically pleasing layout."
  },
  {
    "objectID": "posts/2023-10-05/index.html",
    "href": "posts/2023-10-05/index.html",
    "title": "Introduction",
    "section": "",
    "text": "As an R programmer, you may want to create added variable plots to visualize the relationship between a predictor variable and the response variable while controlling for the effects of other predictor variables. In this blog post, we will use the car library and the avPlots() function to create added variable plots in R."
  },
  {
    "objectID": "posts/2023-10-05/index.html#interpretation-of-added-variable-plots",
    "href": "posts/2023-10-05/index.html#interpretation-of-added-variable-plots",
    "title": "Introduction",
    "section": "Interpretation of Added Variable Plots",
    "text": "Interpretation of Added Variable Plots\nThe added variable plots show the relationship between each predictor variable and the response variable while controlling for the effects of the other predictor variables. The x-axis represents the partial residuals of the predictor variable, and the y-axis represents the partial residuals of the response variable. The line in the plot represents the fitted values from a linear regression model of the partial residuals of the response variable on the partial residuals of the predictor variable.\nIf the relationship between the predictor variable and the response variable is linear, the line in the plot should be approximately horizontal. If the relationship is non-linear, the line may be curved. If there is an outlier, it may be visible as a point that is far away from the other points in the plot."
  },
  {
    "objectID": "posts/2023-10-05/index.html#conclusion",
    "href": "posts/2023-10-05/index.html#conclusion",
    "title": "Introduction",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have learned how to create added variable plots in R using the car library and the avPlots() function. We have also discussed the interpretation of added variable plots and their usefulness in identifying non-linear relationships and outliers. I encourage you to try creating added variable plots on your own and explore the relationships between predictor variables and response variables in your own datasets."
  },
  {
    "objectID": "posts/2023-10-06/index.html",
    "href": "posts/2023-10-06/index.html",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "",
    "text": "Legends are an essential part of data visualization. They help us understand the meaning behind the colors and shapes in our plots. But what if your legend is too big or clutters your plot? Fear not, fellow R enthusiast! In this blog post, we’ll explore how to draw a legend outside of a plot using base R, with a step-by-step example that’s easy to follow."
  },
  {
    "objectID": "posts/2023-10-06/index.html#step-1-create-your-data",
    "href": "posts/2023-10-06/index.html#step-1-create-your-data",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "Step 1: Create Your Data",
    "text": "Step 1: Create Your Data\nLet’s start by creating some sample data. We’ll use a simple scatterplot to demonstrate how to draw a legend outside of the plot. Imagine we have data on two different species of flowers, and we want to distinguish them with different colors.\n\n# Sample data\nset.seed(123)\ndata &lt;- data.frame(\n  x = rnorm(20),\n  y = rnorm(20),\n  species = rep(c(\"A\", \"B\"), each = 10)\n)"
  },
  {
    "objectID": "posts/2023-10-06/index.html#step-2-create-the-plot",
    "href": "posts/2023-10-06/index.html#step-2-create-the-plot",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "Step 2: Create the Plot",
    "text": "Step 2: Create the Plot\nNext, let’s create a scatterplot of our data using the plot() function.\n\n# Create the scatterplot\nplot(\n  data$x, data$y,\n  pch = ifelse(data$species == \"A\", 16, 17),\n  col = ifelse(data$species == \"A\", \"red\", \"blue\"),\n  main = \"Scatterplot with Legend Outside\"\n)\n\n# create margin around plot\npar(mar = c(3, 3, 3, 8), xpd = TRUE)\n\n# Draw the legend outside the plot with inset\nlegend(\n  \"topright\",                           # Position of the legend\n  legend = c(\"Species A\", \"Species B\"), # Legend labels\n  pch = c(16, 17),                      # Point shapes\n  col = c(\"red\", \"blue\"),               # Colors\n  bty = \"n\",                            # No box around the legend\n  inset = c(-0.1, 0)                   # Adjust the inset (move it to the left\n)\n\n\n\n\nIn this code, we’re using the pch argument to specify different point shapes based on the “species” variable and the col argument to set different colors. This creates a scatterplot with points that represent two species, A and B."
  },
  {
    "objectID": "posts/2023-10-06/index.html#step-3-draw-the-legend-outside",
    "href": "posts/2023-10-06/index.html#step-3-draw-the-legend-outside",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "Step 3: Draw the Legend Outside",
    "text": "Step 3: Draw the Legend Outside\nWe already drew the legend but let’s now understand what we did. We’ll use the legend() function for this. Here’s how you can do it:\n# Draw the legend outside the plot with inset\nlegend(\n  \"topright\",                           # Position of the legend\n  legend = c(\"Species A\", \"Species B\"), # Legend labels\n  pch = c(16, 17),                      # Point shapes\n  col = c(\"red\", \"blue\"),               # Colors\n  bty = \"n\",                            # No box around the legend\n  inset = c(-0.16, 0)                   # Adjust the inset (move it to the left)\n)\nIn this code, we specify the position of the legend using the \"topright\" argument. We also provide labels, point shapes, and colors for our legend. The bty = \"n\" argument removes the box around the legend for a cleaner look."
  },
  {
    "objectID": "posts/2023-10-06/index.html#step-4-enjoy-your-plot",
    "href": "posts/2023-10-06/index.html#step-4-enjoy-your-plot",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "Step 4: Enjoy Your Plot",
    "text": "Step 4: Enjoy Your Plot\nThat’s it! You’ve successfully drawn a legend outside of your plot. Your scatterplot now looks clean, and the legend is clearly separated."
  },
  {
    "objectID": "posts/2023-10-10/index.html",
    "href": "posts/2023-10-10/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Changing the size of the legend on a plot in R can be a handy skill, especially when you want to enhance the readability and aesthetics of your visualizations. In this blog post, we’ll explore different methods to resize legends on R plots with practical examples. Whether you’re a beginner or an experienced R user, this guide should help you master this essential aspect of data visualization."
  },
  {
    "objectID": "posts/2023-10-10/index.html#try-it-yourself",
    "href": "posts/2023-10-10/index.html#try-it-yourself",
    "title": "Introduction",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nNow that you’ve seen how to change the size of legends in R, I encourage you to experiment with your own plots. Adjust the cex parameter, use guides() or customize the theme in ggplot2 to suit your specific needs. Practicing these techniques will enhance your data visualization skills and help you create compelling graphics.\nRemember, effective legends are crucial for conveying the meaning of your plots, so don’t hesitate to tweak their size until your visualizations look just right. Happy coding, and happy plotting!"
  },
  {
    "objectID": "posts/2023-10-11/index.html",
    "href": "posts/2023-10-11/index.html",
    "title": "Horizontal Legends in Base R",
    "section": "",
    "text": "Introduction\nCreating a horizontal legend in base R can be a useful skill when you want to label multiple categories in a plot without taking up too much vertical space. In this blog post, we’ll explore various methods to create horizontal legends in R and provide examples with clear explanations.\n\n\nWhy Do We Need Horizontal Legends?\nVertical legends are great for smaller plots, but in larger visualizations, they can become a space-consuming eyesore. Horizontal legends, on the other hand, allow you to neatly label categories without cluttering the plot area. They are especially useful when you have many categories to label.\n\n\nUsing the legend Function\nThe most straightforward way to create a horizontal legend in base R is by using the legend function. Here’s a simple example:\n\n# Create a sample plot\nplot(1:5, col = 1:5, pch = 19)\n\n# Add a horizontal legend\nlegend(\"top\", legend = c(\"A\", \"B\", \"C\", \"D\", \"E\"), \n       fill = 1:5, horiz = TRUE, x.intersp = 0.2)\n\n\n\n\nIn this code, we first create a basic scatter plot and then use the legend function to add a legend at the top of the plot (\"top\"). The horiz = TRUE argument specifies a horizontal legend.\n\n\nCustomizing the Horizontal Legend\nYou can further customize the horizontal legend to match your preferences. Here are some common parameters:\n\nx.intersp controls the horizontal spacing between legend elements.\ninset adjusts the distance of the legend from the plot.\ntitle adds a title to the legend.\n\n# Customize the horizontal legend\nlegend(\"top\", legend = c(\"A\", \"B\", \"C\", \"D\", \"E\"), \n  fill = 1:5, horiz = TRUE, x.intersp = 0.2, inset = 0.02, \n  title = \"Categories\")\n\n\nAdding Multiple Horizontal Legends\nIn some cases, you might need multiple horizontal legends in a single plot. You can achieve this by specifying different locations for each legend.\n\n# Create a sample plot\nplot(1:5, col = 1:5, pch = 19)\n\n# Add two horizontal legends\nlegend(\"top\", legend = c(\"A\", \"B\", \"C\"), \n       fill = 1:3, horiz = TRUE, x.intersp = 0.2, inset = 0.02,\n       title = \"Top Legend\")\nlegend(\"bottom\", legend = c(\"D\", \"E\"), \n       fill = 4:5, horiz = TRUE, x.intersp = 0.2, inset = 0.02,\n       title = \"Bottom Legend\")\n\n\n\n\nIn this example, we add two horizontal legends at the top and bottom of the plot, each with its set of labels and colors.\n\n\nExperiment\nCreating horizontal legends in base R is a versatile skill that you can use in various data visualization projects. I encourage you to experiment with different plot types, colors, and parameters to create the perfect horizontal legend for your specific needs. Don’t be afraid to get creative and tailor your legends to make your plots more informative and visually appealing.\nBy following these simple steps and experimenting with your own plots, you’ll be able to master the art of horizontal legends in R. So go ahead and give it a try! Your future visualizations will thank you for the extra clarity and elegance that horizontal legends provide. Happy coding!"
  },
  {
    "objectID": "posts/2023-10-12/index.html",
    "href": "posts/2023-10-12/index.html",
    "title": "How to Use cex to Change the Size of Plot Elements in base R",
    "section": "",
    "text": "Introduction\nLet’s dive into the world of R and explore how to use cex to change the size of plot elements in base R. Whether you’re a seasoned R user or just starting out, understanding how to control the size of text and symbols in your plots can greatly enhance the clarity and aesthetics of your data visualizations. In this blog post, we’ll break it down into simple terms and provide several examples to get you started.\n\n\nWhat is cex?\ncex stands for “character expansion.” It’s a parameter in R that allows you to adjust the size of text, symbols, and other graphical elements within your plots. You can think of it as a scaling factor that determines the size of these elements relative to the default size.\n\n\nThe Basics\nLet’s start with the basics. The cex parameter is typically used within functions that create plots, like plot() or text(). It takes a numeric value, where 1.0 represents the default size, and values greater than 1 make elements larger, while values between 0 and 1 make elements smaller.\n\n\nExamples\nHere’s a simple example of changing the size of text in a scatter plot:\n\n# Create a scatter plot\nplot(1:5, 1:5, main=\"Default cex Size\")\n\n\n\n# Change the text size with cex\nplot(1:5, 1:5, main=\"Larger cex\", cex=1.5)\n\n\n\n\nIn the second plot() call, we set cex to 1.5, making the text 1.5 times larger than the default size. Play around with different cex values to see the effect on your plots.\n\n\nText and Labels\ncex is particularly handy when you want to adjust the size of text labels in your plots. For example, when creating a bar plot, you might want to make the bar labels more legible:\n\n# Create a bar plot\nbarplot(1:5, names.arg=c(\"A\", \"B\", \"C\", \"D\", \"E\"), main=\"Default Label Size\")\n\n\n\n# Change the label size with cex\nbarplot(1:5, names.arg=c(\"A\", \"B\", \"C\", \"D\", \"E\"), main=\"Larger Labels\", cex.names=1.5)\n\n\n\n\nIn the second barplot() call, we use cex.names to specifically adjust the size of the labels. This keeps the rest of the plot elements at their default sizes.\n\n\nExperiment!\nThe best way to master the use of cex is to experiment. Try different values, and see how they impact your plots. Whether you’re adjusting text size, label size, or symbol size, cex offers a flexible way to customize your visualizations.\nDon’t hesitate to explore more advanced uses of cex when working on complex plots. With practice, you’ll develop an intuitive sense of how to use this parameter effectively.\nSo, go ahead and give it a try! Experiment with cex in your R plots and discover how it can help you create more engaging and informative data visualizations.\nIn the world of data analysis and visualization, understanding these nuances can be a game-changer, and cex is a valuable tool in your R arsenal. Happy coding!"
  },
  {
    "objectID": "posts/2023-10-13/index.html",
    "href": "posts/2023-10-13/index.html",
    "title": "Mastering the Art of Drawing Circles in Plots with R",
    "section": "",
    "text": "Introduction\nAs an R programmer, you may want to draw circles in plots to highlight certain data points or to create visualizations. Here are some simple steps to draw circles in plots using R:\n\n\nExamples\n\nFirst, create a scatter plot using the plot() function in R. For example, you can create a scatter plot of x and y values using the following code:\n\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 6, 8, 10)\nplot(x, y)\n\n\n\n\n\nTo draw a circle on the plot, you can use the symbols() function in R. The symbols() function allows you to draw various shapes, including circles, squares, triangles, and more. To draw a circle, set the circles argument to TRUE. For example, to draw a circle with a radius of 0.5 at the point (3, 6), use the following code:\n\n\nplot(x, y)\nsymbols(3, 6, circles = 1, add = TRUE)\n\n\n\n\n\nYou can also customize the color and border of the circle using the bg and fg arguments. For example, to draw a red circle with a blue border, use the following code:\n\n\nplot(x, y)\nsymbols(3, 6, circles = 1, add = TRUE, bg = \"red\", fg = \"blue\")\n\n\n\n\n\nTo draw multiple circles on the plot, you can use a loop to iterate over a list of coordinates and radii. For example, to draw three circles with different radii at different points, use the following code:\n\n\nplot(x, y)\n\ncoords &lt;- list(c(2, 4), c(3, 6), c(4, 8))\nradii &lt;- c(0.1, 0.2, 0.3)\n\nfor (i in 1:length(coords)) {\n  symbols(\n    coords[[i]][1], coords[[i]][2], circles = radii[[i]], \n    add = TRUE, bg = \"red\", fg = \"blue\", inches = FALSE\n  )\n}\n\n\n\n\n\nFinally, you can add a title and axis labels to the plot using the title(), xlab(), and ylab() functions. For example, to add a title and axis labels to the plot, use the following code:\n\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 6, 8, 10)\nplot(\n  x, y, main = \"Scatter Plot with Circles\", \n  xlab = \"X Values\", ylab = \"Y Values\"\n)\n\ncoords &lt;- list(c(2, 4), c(3, 6), c(4, 8))\nradii &lt;- c(0.1, 0.2, 0.3)\n\nfor (i in 1:length(coords)) {\n  symbols(\n    coords[[i]][1], coords[[i]][2], circles = radii[[i]], \n    add = TRUE, bg = \"red\", fg = \"blue\", inches = FALSE\n  )\n}\n\n\n\n\nHere is one last exmple:\n\n# Create a scatter plot with multiple circles\nn &lt;- 10\nx &lt;- runif(n, -2, 2)\ny &lt;- runif(n, -2, 2)\nsize &lt;- runif(n, 0.1, 1)\nfill &lt;- sample(colors(), n)\nborder &lt;- sample(colors(), n)\n\nsymbols(x, y, circles = size, inches = FALSE, add = F, bg = fill, fg = border)\n\n\n\n\n\n\nConclusion\nOverall, drawing circles in plots is a simple and effective way to highlight certain data points or to create visualizations. Try experimenting with different coordinates, radii, colors, and borders to create your own custom plots."
  },
  {
    "objectID": "posts/2023-10-16/index.html",
    "href": "posts/2023-10-16/index.html",
    "title": "Analyzing Time Series Growth with ts_growth_rate_vec() in healthyR.ts",
    "section": "",
    "text": "Introduction\nTime series data is essential for understanding trends and making forecasts in various fields, from finance to healthcare. Analyzing the growth rate of time series data is a crucial step in uncovering valuable insights. In the world of R programming, the healthyR.ts library introduces a powerful tool to calculate growth rates and log-differenced growth rates with the ts_growth_rate_vec() function. In this blog post, we’ll explore how this function works and how it can be used for effective time series analysis.\n\n\nUnderstanding ts_growth_rate_vec():\nThe ts_growth_rate_vec() function is part of the healthyR.ts library, designed to work with numeric vectors or time series data. It calculates the growth rate or log-differenced growth rate of the provided data, offering valuable insights into the underlying trends and patterns.\n\n\nSyntax\nHere is the function syntax:\nts_growth_rate_vec(\n  .x, \n  .scale = 100, \n  .power = 1, \n  .log_diff = FALSE, \n  .lags = 1\n)\n\n.x - A numeric vector\n.scale - A numeric value that is used to scale the output\n.power - A numeric value that is used to raise the output to a power\n.log_diff - A logical value that determines whether the output is a log difference\n.lags - An integer that determines the number of lags to use\n\nYou can find the documentation here\n\n\nExamples\nLet’s first take a look at the data we are going to be working with in this post, AirPassengers.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nplot(AirPassengers)\n\n\n\n\nLet’s load in the {healthyR.ts} library and see some examples to illustrate its functionality:\n\nlibrary(healthyR.ts)\n\n\nCalculating Basic Growth Rate:\n\n\nts_growth_rate_vec(AirPassengers) |&gt; head(12)\n\n [1]         NA   5.357143  11.864407  -2.272727  -6.201550  11.570248\n [7]   9.629630   0.000000  -8.108108 -12.500000 -12.605042  13.461538\n\nplot(ts(ts_growth_rate_vec(AirPassengers)))\n\n\n\n\nThe output provides growth rates for the AirPassengers dataset. This basic calculation can help you understand how the data is evolving over time. The growth rates are calculated from one point to the next, giving you an idea of the speed at which the values are changing.\n\nApplying Scaling and Power Transformation:\n\n\nts_growth_rate_vec(AirPassengers, .log_diff = TRUE) |&gt; head(12)\n\n [1]         NA   5.218575  11.211730  -2.298952  -6.402186  10.948423\n [7]   9.193750   0.000000  -8.455739 -13.353139 -13.473259  12.629373\n\nplot(ts(ts_growth_rate_vec(AirPassengers, .log_diff = TRUE)))\n\n\n\n\nThis example introduces the option to apply scaling and a power transformation. The resulting growth rates can help uncover trends that might not be apparent in the original data. Using a log-differenced growth rate is particularly useful for capturing the percentage change, making it easier to interpret the data.\n\nHandling Lagged Data:\n\n\nts_growth_rate_vec(AirPassengers, .lags = -1) |&gt; head(12)\n\n [1]  -5.084746 -10.606061   2.325581   6.611570 -10.370370  -8.783784\n [7]   0.000000   8.823529  14.285714  14.423077 -11.864407   2.608696\n\nplot.ts(ts_growth_rate_vec(AirPassengers, .lags = -1))\n\n\n\n\nIn this case, the function calculates the log differences of the time series with lags. This is helpful when you want to observe the changes between data points at different time intervals. It can reveal patterns that might not be apparent in the basic growth rate calculation.\n\nCombining Scaling, Transformation, and Lags:\n\n\nts_growth_rate_vec(AirPassengers, .log_diff = TRUE, .lags = -1) |&gt; head(12)\n\n [1]  -5.218575 -11.211730   2.298952   6.402186 -10.948423  -9.193750\n [7]   0.000000   8.455739  13.353139  13.473259 -12.629373   2.575250\n\nplot.ts(ts_growth_rate_vec(AirPassengers, .log_diff = TRUE, .lags = -1))\n\n\n\n\nThis example combines all the mentioned features to provide a comprehensive analysis of the data. It’s a powerful way to understand how the growth rate is affected by various factors, such as scaling and time lags.\n\n\nConclusion:\nThe ts_growth_rate_vec() function in the healthyR.ts library is a versatile tool for time series analysis. Whether you need a basic growth rate, want to apply scaling and transformation, or work with lagged data, this function has you covered. It’s a valuable asset for R programmers, helping them uncover hidden insights within time series data.\nIncorporating this function into your data analysis workflow can provide you with a deeper understanding of how values change over time. Whether you’re working with financial data, healthcare data, or any other time series dataset, ts_growth_rate_vec() is a powerful addition to your R programming toolkit. Start exploring your time series data today and discover the trends and patterns that lie within."
  },
  {
    "objectID": "posts/2023-10-17/index.html",
    "href": "posts/2023-10-17/index.html",
    "title": "Testing stationarity with the ts_adf_test() function in R",
    "section": "",
    "text": "Introduction\nHey there, R enthusiasts! Today, we’re going to dive into the fascinating world of time series analysis using the ts_adf_test() function from the healthyR.ts R library. If you’re into data, statistics, and R coding, this is a must-know tool for your arsenal.\n\n\nWhat’s the Deal with Augmented Dickey-Fuller?\nBefore we delve into the ts_adf_test() function, let’s understand the concept behind it. The Augmented Dickey-Fuller (ADF) test is a crucial tool in time series analysis. It’s like the Sherlock Holmes of time series data, helping us detect whether a series is stationary or not. Stationarity is a fundamental assumption in time series modeling because many models work best when applied to stationary data.\nSo, why “Augmented”? Well, it’s an extension of the original Dickey-Fuller test that accounts for more complex relationships within the time series data.\n\n\nThe ts_adf_test() Function\nNow, let’s get to the star of the show, the ts_adf_test() function. This function is part of the healthyR.ts library, and its primary job is to perform the ADF test on a given time series. In R, a time series can be represented as a numeric vector. Here’s the basic syntax:\nts_adf_test(.x, .k = NULL)\n\n.x is your time series data, the numeric vector you want to analyze.\n.k is an optional parameter that allows you to specify the lag order. If you leave it empty (like .k = NULL), don’t worry; the function will calculate it for you based on the number of observations using a clever formula.\n\n\n\nShow Me the Stats!\nSo, what does ts_adf_test() return? It gives you a list object containing two vital pieces of information:\n\nTest Statistic: This is the heart of the ADF test. It tells us how strongly our data deviates from being stationary. A more negative value indicates stronger evidence for stationarity.\nP-Value: This is another critical number. It represents the probability that you’d observe a test statistic as extreme as the one you obtained if the data were not stationary. In simpler terms, a low p-value suggests that your data is likely stationary, while a high p-value implies non-stationarity.\n\n\n\nLet’s Get Practical\nEnough theory! Let’s see some action with a couple of examples. Say we have the AirPassengers and BJsales datasets, and we want to check their stationarity:\n\nlibrary(healthyR.ts)\n\n# ADF test for AirPassengers\nresult_air &lt;- ts_adf_test(AirPassengers)\ncat(\"AirPassengers ADF Test Result:\\n\")\n\nAirPassengers ADF Test Result:\n\nprint(result_air)\n\n$test_stat\n[1] -7.318571\n\n$p_value\n[1] 0.01\n\n# ADF test for BJsales\nresult_bj &lt;- ts_adf_test(BJsales)\ncat(\"\\nBJsales ADF Test Result:\\n\")\n\n\nBJsales ADF Test Result:\n\nprint(result_bj)\n\n$test_stat\n[1] -2.110919\n\n$p_value\n[1] 0.5301832\n\n\nIn the AirPassengers example, we get a test statistic of -7.318571 and a p-value of 0.01. This suggests strong evidence for stationarity in this dataset.\nHowever, for BJsales, we get a test statistic of -2.110919 and a p-value of 0.5301832. The higher p-value here indicates that the data is less likely to be stationary.\nNow let’s see what happens when we change the lags of the series by one period.\n\nts_adf_test(AirPassengers, 1)\n\n$test_stat\n[1] -7.652287\n\n$p_value\n[1] 0.01\n\nts_adf_test(BJsales, 1)\n\n$test_stat\n[1] -1.316414\n\n$p_value\n[1] 0.8611925\n\n\n\n\nConclusion\nThe ts_adf_test() function in the healthyR.ts library is a valuable tool for any data scientist or R coder working with time series data. It helps you determine whether your data is stationary, a crucial step in building reliable time series models.\nSo, the next time you’re faced with a time series dataset, remember to call on your trusty companion, ts_adf_test(), to solve the mystery of stationarity. Happy coding, R enthusiasts!"
  },
  {
    "objectID": "posts/2023-10-18/index.html",
    "href": "posts/2023-10-18/index.html",
    "title": "Making Time Series Stationary Made Easy with auto_stationarize()",
    "section": "",
    "text": "Introduction\nWhen working with time series data, one common challenge is dealing with non-stationary data. Non-stationary time series can be a headache for analysts, but fear not, because we have a handy tool to make your life easier. Say hello to the auto_stationarize() function from the {healthyR.ts} package.\n\n\nWhat’s in the Box?\nBefore we get into the nitty-gritty of how this function works, let’s take a look at its syntax:\nauto_stationarize(.time_series)\nThe .time_series parameter should be a vector or a time series object. This function’s primary goal is to attempt to stationarize your time series data. But what does that mean, and why is it important?\n\n\nStationarity: The Why and the How\nStationarity is a crucial concept in time series analysis. A stationary time series is one whose statistical properties, like mean, variance, and autocorrelation, don’t change over time. Dealing with stationary data is much simpler because many time series models assume stationarity.\nNow, here’s the magic of auto_stationarize(): it automatically handles stationarity for you.\n\n\nThe Swiss Army Knife of Time Series\nThis function is like a Swiss Army knife for your time series data. It checks if your data is already stationary using the Augmented Dickey-Fuller test. If it is, great, you get your original time series back.\nBut what if it’s not? Well, that’s where the real fun begins.\n\n\nTransformations Galore\nIf your time series isn’t stationary, auto_stationarize() goes the extra mile to make it so. It attempts a series of transformations until it succeeds. Here’s the process:\n\nAugmented Dickey-Fuller Test: First, the function runs the Augmented Dickey-Fuller test to determine if your time series is stationary. If it’s already stationary, you’re done.\nLogarithmic Transformation: If the test suggests your data isn’t stationary, the function tries a logarithmic transformation. This transformation can be helpful when dealing with data that grows exponentially over time.\nDifferencing: If logarithmic transformation doesn’t do the trick, the function resorts to differencing. Differencing involves subtracting each value from its previous value, effectively converting your data into the change between time periods.\n\n\n\nWhat You Get\nIf auto_stationarize() succeeds in making your data stationary, it returns a list with two valuable elements:\n\nstationary_ts: This is your shiny, new stationary time series, ready for analysis.\nndiffs: This little number tells you the order of differencing applied to make your data stationary. It’s a useful piece of information if you need to understand how your data was transformed.\n\n\n\nExamples\nLet’s see some examples.\n\nlibrary(healthyR.ts)\n\nauto_stationarize(AirPassengers)\n\nThe time series is already stationary via ts_adf_test().\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nauto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\nDifferencing of order 1 made the time series stationary.\n\n\n$stationary_ts\nTime Series:\nStart = 2 \nEnd = 150 \nFrequency = 1 \n  [1] -0.6 -0.1 -0.5  0.1  1.2 -1.6  1.4  0.3  0.9  0.4 -0.1  0.0  2.0  1.4  2.2\n [16]  3.4  0.0 -0.7 -1.0  0.7  3.7  0.5  1.4  3.6  1.1  0.7  3.3 -1.0  1.0 -2.1\n [31]  0.6 -1.5 -1.4  0.7  0.5 -1.7 -1.1 -0.1 -2.7  0.3  0.6  0.8  0.0  1.0  1.0\n [46]  4.2  2.0 -2.7 -1.5 -0.7 -1.3 -1.7 -1.1 -0.1 -1.7 -1.8  1.6  0.7 -1.0 -1.5\n [61] -0.7  1.7 -0.2  0.4 -1.8  0.8  0.7 -2.0 -0.3 -0.6  1.3 -1.4 -0.3 -0.9  0.0\n [76]  0.0  1.8  1.3  0.9 -0.3  2.3  0.5  2.2  1.3  1.9  1.5  4.5  1.7  4.8  2.5\n [91]  1.4  3.5  3.2  1.5  0.7  0.3  1.4 -0.1  0.2  1.6 -0.4  0.9  0.6  1.0 -2.5\n[106] -1.4  1.2  1.6  0.3  2.3  0.7  1.3  1.2 -0.2  1.4  3.0 -0.4  1.3 -0.9  1.2\n[121] -0.8 -1.0 -0.8 -0.1 -1.5  0.3  0.2 -0.5 -0.1  0.3  1.3 -1.1 -0.1 -0.5  0.3\n[136] -0.7  0.7 -0.5  0.6 -0.3  0.2  2.1  1.5  1.8  0.4 -0.5 -1.0  0.4  0.5\n\n$ndiffs\n[1] 1\n\n\nThe function attempted to stationarize the BJsales data set, let’s take a visuali look at it before and after, we will also use the adf_test() function on it before and after.\n\nplot(BJsales)\n\n\n\nts_adf_test(BJsales)\n\n$test_stat\n[1] -2.110919\n\n$p_value\n[1] 0.5301832\n\nstationary_time_series &lt;- auto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\nDifferencing of order 1 made the time series stationary.\n\nplot(stationary_time_series$stationary_ts)\n\n\n\n\n\n\nTry It Yourself\nThe best way to grasp the power of auto_stationarize() is by trying it yourself. Install the {healthyR.ts} package, load your time series data, and give it a whirl. The ease and simplicity of making your time series stationary with just one function call will leave you impressed.\n\n\nConclusion\nIn the world of time series analysis, making your data stationary is a crucial step. The auto_stationarize() function from the {healthyR.ts} package takes the headache out of this process. Whether you’re dealing with financial data, weather patterns, or any other time series, this function is your trusty companion.\nSo, what are you waiting for? Transform your non-stationary time series into a stationary one with ease, thanks to auto_stationarize(). Your future self will thank you for it.\nHappy coding and data analysis!"
  },
  {
    "objectID": "posts/2023-10-19/index.html",
    "href": "posts/2023-10-19/index.html",
    "title": "Mastering Interaction Plots in R: Unveiling Hidden Relationships",
    "section": "",
    "text": "Introduction\nIn the world of data analysis, uncovering hidden relationships between variables is often the key to making informed decisions. Interaction plots in R can be your secret weapon, revealing how two or more variables interact to affect an outcome. In this blog post, we’ll dive into the world of interaction plots, demystifying the process and showing you how to create these insightful visuals using base R.\n\n\nWhat Are Interaction Plots?\nInteraction plots display how the relationship between two variables changes depending on the value of a third variable. They are particularly useful when dealing with categorical variables, allowing you to see how the effect of one variable on the outcome depends on the levels of another variable. In simpler terms, interaction plots help us understand how the relationship between two variables is influenced by a third variable, making them a valuable tool for data exploration.\n\n\nGetting Started: Preparing Your Data\nBefore we create interaction plots, we need some data. For this example, we’ll use a hypothetical dataset about customer satisfaction, where we want to explore how the relationship between “Product Type” and “Price” is influenced by “Customer Segment.”\n\nset.seed(123)\n# Create a sample dataset\ndata &lt;- data.frame(\n  ProductType = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 10),\n  Price = trunc(runif(40, 15, 35)),\n  CustomerSegment = rep(c(\"Seg. 1\", \"Seg. 2\"), times = 20),\n  Satisfaction = trunc(runif(40, 2, 5))\n)\n\nNow that we have our data, let’s create an interaction plot.\n\n\nCreating the Interaction Plot\nWe’ll use the base R package to create our interaction plot. Here’s how you can do it:\n\n# Create the interaction plot\ninteraction.plot(\n  x.factor = data$ProductType,\n  trace.factor = data$CustomerSegment,\n  response = data$Satisfaction,\n  fun = median,\n  ylab = \"Satisfaction\",\n  xlab = \"Customer Segment\",\n  lty = 1,\n  lwd = 2, \n  col = c(\"steelblue\",\"lightgreen\"),\n  fixed = TRUE,\n  legend = TRUE,\n  trace.label = \"Segment\"\n)\n\n# Adding labels and a title\ntitle(\"Interaction Plot: Product Type vs. Satisfaction by Customer Segment\")\n\n\n\n\nIn the code above: - x.factor represents the variable on the x-axis. - trace.factor represents the variable that distinguishes different lines on the plot. - response is the variable we’re interested in. - type = \"b\" specifies that we want to connect points with lines and plot points. - fixed = TRUE ensures that the x-axis is evenly spaced. - legend = TRUE adds a legend to the plot.\n\n\nInterpreting the Plot\nIn our plot, you’ll see lines for each customer segment (Segment 1 and Segment 2). The lines show how satisfaction levels change with different product types (A, B, C and D). If the lines are parallel, it indicates that there’s no interaction between “Product Type” and “Customer Segment.” However, if the lines cross or diverge, it suggests an interaction, meaning that the effect of the product type on satisfaction differs across customer segments.\n\n\nConclusion: Your Turn to Explore!\nCreating interaction plots in R can be a valuable skill for anyone working with data. They provide deep insights into how variables influence each other. Don’t hesitate to apply this technique to your own datasets and discover the hidden relationships within your data.\nSo, what are you waiting for? Give it a try and start visualizing the interactions in your data. Happy coding!"
  },
  {
    "objectID": "posts/2023-10-20/index.html",
    "href": "posts/2023-10-20/index.html",
    "title": "Creating Pareto Charts in R with the qcc Package",
    "section": "",
    "text": "A Pareto chart is a type of bar chart that shows the frequency of different categories in a dataset, ordered by frequency from highest to lowest. It is often used to identify the most common problems or causes of a problem, so that resources can be focused on addressing them.\nTo create a Pareto chart in R, we can use the qcc package. The qcc package provides a number of functions for quality control, including the pareto.chart() function for creating Pareto charts."
  },
  {
    "objectID": "posts/2023-10-20/index.html#example-1-creating-a-pareto-chart-from-a-data-frame",
    "href": "posts/2023-10-20/index.html#example-1-creating-a-pareto-chart-from-a-data-frame",
    "title": "Creating Pareto Charts in R with the qcc Package",
    "section": "Example 1: Creating a Pareto chart from a data frame",
    "text": "Example 1: Creating a Pareto chart from a data frame\nThe following code shows how to create a Pareto chart from a data frame:\n\nlibrary(qcc)\n\n# Create a data frame with the product and its count\ndf &lt;- data.frame(\n  product = c(\"Office desks\", \"Chairs\", \"Filing cabinets\", \"Bookcases\"),\n  count = c(100, 80, 70, 60)\n)\n\n# Create the Pareto chart\npareto.chart(df$count, main = \"Pareto Chart of Product Sales\")\n\n\n\n\n   \nPareto chart analysis for df$count\n    Frequency Cum.Freq. Percentage Cum.Percent.\n  A 100.00000 100.00000   32.25806     32.25806\n  B  80.00000 180.00000   25.80645     58.06452\n  C  70.00000 250.00000   22.58065     80.64516\n  D  60.00000 310.00000   19.35484    100.00000\n\n\nThis code will create a Pareto chart of the product sales, with the office desks bar at the top and the bookcases bar at the bottom. The cumulative percentage line is also plotted, which shows the percentage of total sales that each product accounts for."
  },
  {
    "objectID": "posts/2023-10-20/index.html#example-2-creating-a-pareto-chart-from-a-vector",
    "href": "posts/2023-10-20/index.html#example-2-creating-a-pareto-chart-from-a-vector",
    "title": "Creating Pareto Charts in R with the qcc Package",
    "section": "Example 2: Creating a Pareto chart from a vector",
    "text": "Example 2: Creating a Pareto chart from a vector\nWe can also create a Pareto chart from a vector. The following code shows how to create a Pareto chart of the number of defects found in a manufacturing process:\n\n# Create a vector with the number of defects found in each category\ndefects &lt;- c(10, 8, 7, 6, 5)\n\n# Create the Pareto chart\npareto.chart(defects, main = \"Pareto Chart of Defects\")\n\n\n\n\n   \nPareto chart analysis for defects\n    Frequency Cum.Freq. Percentage Cum.Percent.\n  A  10.00000  10.00000   27.77778     27.77778\n  B   8.00000  18.00000   22.22222     50.00000\n  C   7.00000  25.00000   19.44444     69.44444\n  D   6.00000  31.00000   16.66667     86.11111\n  E   5.00000  36.00000   13.88889    100.00000\n\n\nThis code will create a Pareto chart of the number of defects found, with the most common defect category at the top and the least common defect category at the bottom. The cumulative percentage line is also plotted, which shows the percentage of total defects that each category accounts for."
  },
  {
    "objectID": "posts/2023-10-20/index.html#customizing-the-pareto-chart",
    "href": "posts/2023-10-20/index.html#customizing-the-pareto-chart",
    "title": "Creating Pareto Charts in R with the qcc Package",
    "section": "Customizing the Pareto chart",
    "text": "Customizing the Pareto chart\nWe can customize the appearance of the Pareto chart using a number of arguments to the pareto.chart() function. For example, we can change the title of the chart, the labels of the x- and y-axes, the colors of the bars, and the line type of the cumulative percentage line.\nThe following code shows how to customize the Pareto chart from the first example:\n\n# Create a data frame with the product and its count\ndf &lt;- data.frame(\n  product = c(\"Office desks\", \"Chairs\", \"Filing cabinets\", \"Bookcases\"),\n  count = c(100, 80, 70, 60)\n)\n\n# Create the Pareto chart\npareto.chart(\n  df$count,\n  main = \"Pareto Chart of Product Sales\",\n  xlab = \"Product\",\n  ylab = \"Count\",\n  col = heat.colors(length(df$count)),\n  lwd = 2\n)\n\n\n\n\n   \nPareto chart analysis for df$count\n    Frequency Cum.Freq. Percentage Cum.Percent.\n  A 100.00000 100.00000   32.25806     32.25806\n  B  80.00000 180.00000   25.80645     58.06452\n  C  70.00000 250.00000   22.58065     80.64516\n  D  60.00000 310.00000   19.35484    100.00000\n\n\nThis code will create a Pareto chart with a title of “Pareto Chart of Product Sales”, x-axis label of “Product”, y-axis label of “Count”, bar colors in a heatmap palette, and a cumulative percentage line width of 2."
  },
  {
    "objectID": "posts/2023-10-23/index.html",
    "href": "posts/2023-10-23/index.html",
    "title": "How to Create a Bubble Chart in R using ggplot2",
    "section": "",
    "text": "Bubble charts are a great way to visualize data with three dimensions. The size of the bubbles represents a third variable, which can be used to show the importance of that variable or to identify relationships between the three variables.\nTo create a bubble chart in R using ggplot2, you will need to use the geom_point() function. This function will plot points on your chart, and you can use the size aesthetic to control the size of the points."
  },
  {
    "objectID": "posts/2023-10-23/index.html#example-1-basic-bubble-chart",
    "href": "posts/2023-10-23/index.html#example-1-basic-bubble-chart",
    "title": "How to Create a Bubble Chart in R using ggplot2",
    "section": "Example 1: Basic Bubble Chart",
    "text": "Example 1: Basic Bubble Chart\nLet’s start with a simple example using randomly generated data. We’ll create a bubble chart that shows the relationship between two variables and represents a third variable using bubble sizes.\n\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Generate random data\nset.seed(123)\ndata &lt;- data.frame(\n  x = rnorm(10),\n  y = rnorm(10),\n  size = runif(10, min = 5, max = 20)\n)\n\n# Create a bubble chart\nggplot(data, aes(x, y, size = size)) +\n  geom_point() +\n  scale_size_continuous(range = c(3, 10)) +\n  labs(\n    title = \"Basic Bubble Chart\", \n    x = \"X-Axis\", \n    y = \"Y-Axis\",\n    size = \"Y\") +\n  theme_minimal()\n\n\n\n\nIn this example, we create a bubble chart with random data points, where x and y are the coordinates, and size represents the bubble size. The geom_point() function is used to add the points, and we adjust the size range using scale_size_continuous()."
  },
  {
    "objectID": "posts/2023-10-23/index.html#example-2-customizing-bubble-chart",
    "href": "posts/2023-10-23/index.html#example-2-customizing-bubble-chart",
    "title": "How to Create a Bubble Chart in R using ggplot2",
    "section": "Example 2: Customizing Bubble Chart",
    "text": "Example 2: Customizing Bubble Chart\nNow, let’s customize our bubble chart further. We’ll use a sample dataset to visualize car data, with car names on the bubbles.\n\n# Sample data\ncars &lt;- mtcars\ncars$name &lt;- rownames(cars)\n\n# Create a bubble chart\nggplot(cars, aes(x = mpg, y = disp, size = hp, label = name)) +\n  geom_point() +\n  geom_text(vjust = 1, hjust = 1, size = 3) +\n  scale_size_continuous(range = c(3, 20)) +\n  labs(\n    title = \"Customized Bubble Chart\", \n    x = \"Miles per Gallon\", \n    y = \"Displacement\",\n    size = \"HP\") +\n  theme_minimal()\n\n\n\n\nIn this example, we’re using the mtcars dataset to create a bubble chart that displays car names using geom_text(). The vjust and hjust parameters control the text placement."
  },
  {
    "objectID": "posts/2023-10-24/index.html",
    "href": "posts/2023-10-24/index.html",
    "title": "Creating a Scree Plot in Base R",
    "section": "",
    "text": "Introduction\nA scree plot is a line plot that shows the eigenvalues or variance explained by each principal component (PC) in a Principal Component Analysis (PCA). It is a useful tool for determining the number of PCs to retain in a PCA model.\nIn this blog post, we will show you how to create a scree plot in base R. We will use the iris dataset as an example.\n\n\nStep 1: Load the dataset and prepare the data\n\n# Drop the non-numerical column\ndf &lt;- iris[, -5]\n\nE Step 2: Perform Principal Component Analysis\n\n# Perform PCA on the iris dataset\npca &lt;- prcomp(df, scale = TRUE)\n\nE Step 3: Create the scree plot\n\n# Extract the eigenvalues from the PCA object\neigenvalues &lt;- pca$sdev^2\n\n# Create a scree plot\nplot(eigenvalues, type = \"b\",\n     xlab = \"Principal Component\",\n     ylab = \"Eigenvalue\")\n\n# Add a line at y = 1 to indicate the elbow\nabline(v = 2, col = \"red\")\n\n\n\n# Percentage of variance explained\nplot(eigenvalues/sum(eigenvalues), type = \"b\",\n     xlab = \"Principal Component\",\n     ylab = \"Percentage of Variance Explained\")\nabline(v = 2, col = \"red\")\n\n\n\n\n\n\nInterpretation\nThe scree plot shows that the first two principal components explain the most variance in the data. The third and fourth principal components explain much less variance.\nBased on the scree plot, we can conclude that the first two principal components are sufficient for capturing the most important information in the data.\nHere are the eigenvalues and the percentage explained\n\neigenvalues\n\n[1] 2.91849782 0.91403047 0.14675688 0.02071484\n\neigenvalues/sum(eigenvalues)\n\n[1] 0.729624454 0.228507618 0.036689219 0.005178709\n\n\n\n\nTry it yourself\nTry creating a scree plot for another dataset of your choice. You can use the same steps outlined above.\nHere are some additional tips for creating scree plots:\n\nIf you are using a dataset with a large number of variables, you may want to consider scaling the data before performing PCA. This will ensure that all of the variables are on the same scale and that no one variable has undue influence on the results.\nYou can also add a line to the scree plot at y = 1 to indicate the elbow. The elbow is the point where the scree plot begins to level off. This is often used as a heuristic for determining the number of PCs to retain.\nFinally, keep in mind that the interpretation of a scree plot is subjective. There is no single rule for determining the number of PCs to retain. The best approach is to consider the scree plot in conjunction with other factors, such as your research goals and the specific dataset you are using."
  },
  {
    "objectID": "posts/2023-10-25/index.html",
    "href": "posts/2023-10-25/index.html",
    "title": "What’s a Bland-Altman Plot? In Base R",
    "section": "",
    "text": "Introduction\nBefore we dive into the code, let’s briefly understand what a Bland-Altman plot is. It’s a graphical method to visualize the agreement between two measurement techniques, often used in fields like medicine or any domain with comparative measurements. The plot displays the differences between two measurements (Y-axis) against their means (X-axis).\n\n\nStep 1: Data Preparation\nStart by loading your data into R. In our example, we’ll create some synthetic data for illustration purposes. You’d replace this with your real data.\n\n# Creating example data\nmethod_A &lt;- c(10, 12, 15, 18, 22, 25)\nmethod_B &lt;- c(9.5, 11, 14, 18, 22, 24.5)\n\n# Calculate the differences and means\ndiff_values &lt;- method_A - method_B\nmean_values &lt;- (method_A + method_B) / 2\n\ndf &lt;- data.frame(method_A, method_B, mean_values, diff_values)\n\n\n\nStep 2: Calculate Average Difference and CI\nNow that we have our data prepared, let’s create the Bland-Altman plot.\n\nmean_diff &lt;- mean(df$diff_values)\nmean_diff\n\n[1] 0.5\n\nlower &lt;- mean_diff - 1.96 * sd(df$diff_values)\nupper &lt;- mean_diff + 1.96 * sd(df$diff_values)\n\nlower\n\n[1] -0.3765386\n\nupper\n\n[1] 1.376539\n\n\n\n\nStep 3: Creating the Bland-Altman Plot\nWe are going to do this in base R.\n\n# Create a scatter plot\nplot(df$mean_values, df$diff_values, \n     xlab = \"Mean of Methods A and B\",\n     ylab = \"Difference (Method A - Method B)\",\n     main = \"Bland-Altman Plot\",\n     ylim = c(lower + (lower * .1), upper * 1.1))\n\n# Add a horizontal line at the mean difference\nabline(h = mean(diff_values), col = \"red\", lty = 2)\n\n# Add Confidence Intervals\nabline(h = upper, col = \"blue\", lty = 2)\nabline(h = lower, col = \"blue\", lty = 2)\n\n\n\n\nThis code will generate a simple Bland-Altman plot, and here’s what each part does:\n\nplot(): Creates the scatter plot with means on the X-axis and differences on the Y-axis.\nabline(h = mean(diff_values), col = \"red\", lty = 2): Adds a red dashed line at the mean difference.\nabline(h = upper, col = \"green\", lty = 2): Adds blue dashed lines representing the 95% limits of agreement.\n\n\n\nStep 4: Interpretation\nNow that you’ve generated your Bland-Altman plot, let’s interpret it:\n\nThe red line represents the mean difference between the two methods.\nThe blue dashed lines show the 95% limits of agreement, which help you assess the spread of the differences.\n\nIf most data points fall within the blue lines, it indicates good agreement between the two methods. If data points are scattered widely outside the lines, there may be a systematic bias or inconsistency between the methods.\n\n\nStep 5: Exploration\nI encourage you to try this out with your own data. Replace the example data with your measurements and see what insights your Bland-Altman plot reveals.\nIn conclusion, creating a Bland-Altman plot in R is a valuable technique to visualize agreement or bias between two measurement methods. It’s an essential tool for quality control and validation in various fields. I hope this step-by-step guide helps you get started. Happy plotting!"
  },
  {
    "objectID": "posts/2023-10-26/index.html",
    "href": "posts/2023-10-26/index.html",
    "title": "Plotting a Logistic Regression In Base R",
    "section": "",
    "text": "Introduction\nLogistic regression is a statistical method used for predicting the probability of a binary outcome. It’s a fundamental tool in machine learning and statistics, often employed in various fields such as healthcare, finance, and marketing. We use logistic regression when we want to understand the relationship between one or more independent variables and a binary outcome, which can be “yes/no,” “1/0,” or any two-class distinction.\n\n\nGetting Started\nBefore we dive into plotting the logistic regression curve, let’s start with the basics. First, you’ll need some data. For this blog post, I’ll assume you have your dataset ready. If you don’t, you can easily find sample datasets online to practice with.\n\n\nLoad the Data\nIn R, we use the read.csv function to load a CSV file into a data frame. For example, if you have a dataset called “mydata.csv,” you can load it like this:\n# Load the data into a data frame\ndata &lt;- read.csv(\"mydata.csv\")\nWe will instead use the following data set:\n\nlibrary(dplyr)\n\nset.seed(123)\ndf &lt;- tibble(\n    x = runif(100, 0, 10),\n    y = rbinom(100, 1, 1 / (1 + exp(-1 * (0.5 * x - 2.5))))\n)\n\nhead(df)\n\n# A tibble: 6 × 2\n      x     y\n  &lt;dbl&gt; &lt;int&gt;\n1 2.88      0\n2 7.88      1\n3 4.09      0\n4 8.83      0\n5 9.40      1\n6 0.456     0\n\n\n\n\nFit a Logistic Regression Model\nNext, we need to fit a logistic regression model to our data. We’ll use the glm (Generalized Linear Model) function to do this. Suppose we want to predict the probability of a “success” (1) based on a single predictor variable “x.”\n\n# Fit a logistic regression model\nmodel &lt;- glm(y ~ x, data = df, family = binomial)\n\nbroom::glance(model)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1          138.      99  -51.5  107.  112.     103.          98   100\n\nbroom::tidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)   -2.63      0.571     -4.60 0.00000422 \n2 x              0.505     0.102      4.96 0.000000699\n\nhead(broom::augment(model), 1) |&gt; \n  dplyr::glimpse()\n\nRows: 1\nColumns: 8\n$ y          &lt;int&gt; 0\n$ x          &lt;dbl&gt; 2.875775\n$ .fitted    &lt;dbl&gt; -1.175925\n$ .resid     &lt;dbl&gt; -0.7333581\n$ .hat       &lt;dbl&gt; 0.01969748\n$ .sigma     &lt;dbl&gt; 1.028093\n$ .cooksd    &lt;dbl&gt; 0.003162007\n$ .std.resid &lt;dbl&gt; -0.7406892\n\n\n\n\nPredict Probabilities\nNow that we have our model, we can use it to predict probabilities. We’ll create a sequence of values for our predictor variable, and for each value, we’ll predict the probability of success, in this case y.\n\n# Create a sequence of predictor values\nx_seq &lt;- seq(0, 10, 0.01)\n\n# Predict probabilities\nprobabilities &lt;- predict(\n  model, \n  newdata = data.frame(x = x_seq), \n  type = \"response\"\n  )\n\nhead(x_seq)\n\n[1] 0.00 0.01 0.02 0.03 0.04 0.05\n\nhead(probabilities)\n\n         1          2          3          4          5          6 \n0.06732923 0.06764710 0.06796636 0.06828702 0.06860908 0.06893255 \n\n\nThe predict function here calculates the probabilities using our logistic regression model.\n\n\nPlot the Logistic Regression Curve\nFinally, let’s plot the logistic regression curve. We’ll use the plot function to create a scatter plot of the data points, and then we’ll overlay the logistic curve using the lines function.\n\n# Plot the data points\nplot(\n  df$x, df$y, \n  pch = 16, \n  col = \"blue\", \n  xlab = \"Predictor Variable\", \n  ylab = \"Probability of Success\"\n  )\n\n# Add the logistic regression curve\nlines(x_seq, probabilities, col = \"red\", lwd = 2)\n\n\n\n\nAnd there you have it! You’ve successfully plotted a logistic regression curve in base R. The blue dots represent your data points, and the red curve is the logistic regression curve, showing how the probability of success changes with the predictor variable.\n\n\nConclusion\nI encourage you to try this out with your own dataset. Logistic regression is a powerful tool for modeling binary outcomes, and visualizing the curve helps you understand the relationship between your predictor variable and the probability of success. Experiment with different datasets and predictor variables to gain a deeper understanding of this essential statistical technique.\nRemember, practice makes perfect, and the more you work with logistic regression in R, the more proficient you’ll become. Happy coding!"
  },
  {
    "objectID": "posts/2023-10-27/index.html",
    "href": "posts/2023-10-27/index.html",
    "title": "Plotting Log Log Plots In Base R",
    "section": "",
    "text": "A log-log plot is a type of graph where both the x-axis and y-axis are in logarithmic scales. This is particularly useful when dealing with data that spans several orders of magnitude. By taking the logarithm of the data, we can compress large values and reveal patterns that might be hidden on a linear scale.\nLet’s start with a simple example using base R."
  },
  {
    "objectID": "posts/2023-10-27/index.html#example-1-scatter-plot-with-log-log-scales",
    "href": "posts/2023-10-27/index.html#example-1-scatter-plot-with-log-log-scales",
    "title": "Plotting Log Log Plots In Base R",
    "section": "Example 1: Scatter Plot with Log-Log Scales",
    "text": "Example 1: Scatter Plot with Log-Log Scales\n\n# Sample data\nx &lt;- c(1, 10, 100, 1000)\ny &lt;- c(0.1, 1, 10, 100)\n\n# Create a log-log plot\nplot(x, y, log = \"xy\", main = \"Log-Log Plot Example\", \n     xlab = \"X (log scale)\", ylab = \"Y (log scale)\")\n\n\n\n\nIn this code, we create a scatter plot with log scales for both the x and y-axes using the plot function. The log = \"xy\" argument specifies that both axes should be in logarithmic scale. This makes it easier to visualize the relationship between x and y."
  },
  {
    "objectID": "posts/2023-10-27/index.html#example-2-line-plot-with-log-log-scales",
    "href": "posts/2023-10-27/index.html#example-2-line-plot-with-log-log-scales",
    "title": "Plotting Log Log Plots In Base R",
    "section": "Example 2: Line Plot with Log-Log Scales",
    "text": "Example 2: Line Plot with Log-Log Scales\nLet’s say you have data for a power law relationship, where y is proportional to x raised to a power. A log-log plot can help you confirm this relationship.\n\n# Generate data for a power law relationship\nx &lt;- 1:10\ny &lt;- 2 * x^2\n\n# Create a log-log plot\nplot(x, y, log = \"xy\", type = \"b\", pch = 19, col = \"blue\", \n     main = \"Log-Log Plot for Power Law\", xlab = \"X (log scale)\", ylab = \"Y (log scale)\")\n\n\n\n\nHere, we generate data for a power law relationship (y = 2 * x^2) and create a log-log plot. The type = \"b\" argument adds both points and lines, making the plot easier to interpret. You can see that on a log-log scale, this power law relationship appears as a straight line."
  },
  {
    "objectID": "posts/2023-10-27/index.html#example-3-customizing-log-log-plots",
    "href": "posts/2023-10-27/index.html#example-3-customizing-log-log-plots",
    "title": "Plotting Log Log Plots In Base R",
    "section": "Example 3: Customizing Log-Log Plots",
    "text": "Example 3: Customizing Log-Log Plots\nYou can further customize your log-log plots with various options.\n\n# Customizing a log-log plot\nx &lt;- c(1, 10, 100, 1000)\ny &lt;- c(0.1, 1, 10, 100)\n\nplot(x, y, log = \"xy\", main = \"Custom Log-Log Plot\",\n     xlab = \"X (log scale)\", ylab = \"Y (log scale)\",\n     xlim = c(0.1, 1000), ylim = c(0.1, 100), col = \"red\", pch = 15)\n\n# Adding grid lines\ngrid()\n\n# Adding a trendline (linear regression)\nabline(lm(log10(y) ~ log10(x)), col = \"blue\")\n\n\n\n\nIn this example, we customize the log-log plot by setting axis limits, changing the point color and type, adding grid lines, and even fitting a trendline using linear regression."
  },
  {
    "objectID": "posts/2023-10-30/index.html",
    "href": "posts/2023-10-30/index.html",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "Randomness is an essential part of many statistical and machine learning tasks. In R, there are a number of functions that can be used to generate random numbers, but the runif() function is the most commonly used.\n\n\nThe runif() function generates random numbers from a uniform distribution. A uniform distribution is a distribution in which all values are equally likely. The runif() function takes three arguments:\n\nn: the number of random numbers to generate\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nThe default values for min and max are 0 and 1, respectively.\nHere is an example of how to use the runif() function to generate 10 random numbers from a uniform distribution between 0 and 1:\n\nset.seed(123)\nr &lt;- runif(10)\n\nOutput:\n\nprint(r)\n\n [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n [8] 0.8924190 0.5514350 0.4566147\n\n\nThe runif() function can also be used to generate random numbers from other distributions, such as the normal distribution, the Poisson distribution, and the binomial distribution.\n\n\n\nThe punif() function calculates the cumulative probability density function (CDF) of the uniform distribution. The CDF is the probability that a random variable will be less than or equal to a certain value.\nThe punif() function takes three arguments:\n\nx: the value at which to calculate the CDF\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the punif() function to calculate the CDF of a uniform distribution between 0 and 1 at the value 0.5:\n\nset.seed(123)\np &lt;- punif(0.5, min = 0, max = 1)\n\nOutput:\n\nprint(p)\n\n[1] 0.5\n\n\nThis means that there is a 50% chance that a random variable from this distribution will be less than or equal to 0.5.\n\n\n\nThe dunif() function calculates the probability density function (PDF) of the uniform distribution. The PDF is the probability that a random variable will be equal to a certain value.\nThe dunif() function takes three arguments:\n\nx: the value at which to calculate the PDF\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the dunif() function to calculate the PDF of a uniform distribution between 0 and 1 at the value 0.5:\n\nset.seed(123)\nd &lt;- dunif(0.5, min = 0, max = 1)\n\nOutput:\nprint(d)\nThis means that the probability of a random variable from this distribution being equal to 0.5 is 1.\n\n\n\nThe quinf() function calculates the quantile function of the uniform distribution. The quantile function is the inverse of the CDF. It takes a probability as an input and returns the value that has that probability.\nThe quinf() function takes two arguments:\n\np: the probability\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the quinf() function to calculate the quantile of a uniform distribution between 0 and 1 at the probability 0.5:\n\nset.seed(123)\nq &lt;- qunif(0.5, min = 0, max = 1)\n\nOutput:\n\nprint(q)\n\n[1] 0.5\n\n\nThis means that there is a 50% chance that a random variable\nIf you want to easily see different versions of the uniform distribution then you can either code them out or use the TidyDensity package. Let’s take a quick look.\n\npacman::p_load(TidyDensity)\n\nn &lt;- 5000\n\ntidy_uniform(.n = n) |&gt;\n  tidy_autoplot()\n\n\n\n\n\n\n\nNow different variations can be visualized with the following workflow:\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_uniform\",\n  .param_list = list(\n    .n = n,\n    .min = 0,\n    .max = c(1,5,10),\n    .num_sims = 5\n  )\n) |&gt;\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/2023-10-30/index.html#the-runif-function",
    "href": "posts/2023-10-30/index.html#the-runif-function",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "The runif() function generates random numbers from a uniform distribution. A uniform distribution is a distribution in which all values are equally likely. The runif() function takes three arguments:\n\nn: the number of random numbers to generate\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nThe default values for min and max are 0 and 1, respectively.\nHere is an example of how to use the runif() function to generate 10 random numbers from a uniform distribution between 0 and 1:\n\nset.seed(123)\nr &lt;- runif(10)\n\nOutput:\n\nprint(r)\n\n [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n [8] 0.8924190 0.5514350 0.4566147\n\n\nThe runif() function can also be used to generate random numbers from other distributions, such as the normal distribution, the Poisson distribution, and the binomial distribution."
  },
  {
    "objectID": "posts/2023-10-30/index.html#the-punif-function",
    "href": "posts/2023-10-30/index.html#the-punif-function",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "The punif() function calculates the cumulative probability density function (CDF) of the uniform distribution. The CDF is the probability that a random variable will be less than or equal to a certain value.\nThe punif() function takes three arguments:\n\nx: the value at which to calculate the CDF\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the punif() function to calculate the CDF of a uniform distribution between 0 and 1 at the value 0.5:\n\nset.seed(123)\np &lt;- punif(0.5, min = 0, max = 1)\n\nOutput:\n\nprint(p)\n\n[1] 0.5\n\n\nThis means that there is a 50% chance that a random variable from this distribution will be less than or equal to 0.5."
  },
  {
    "objectID": "posts/2023-10-30/index.html#the-dunif-function",
    "href": "posts/2023-10-30/index.html#the-dunif-function",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "The dunif() function calculates the probability density function (PDF) of the uniform distribution. The PDF is the probability that a random variable will be equal to a certain value.\nThe dunif() function takes three arguments:\n\nx: the value at which to calculate the PDF\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the dunif() function to calculate the PDF of a uniform distribution between 0 and 1 at the value 0.5:\n\nset.seed(123)\nd &lt;- dunif(0.5, min = 0, max = 1)\n\nOutput:\nprint(d)\nThis means that the probability of a random variable from this distribution being equal to 0.5 is 1."
  },
  {
    "objectID": "posts/2023-10-30/index.html#the-quinf-function",
    "href": "posts/2023-10-30/index.html#the-quinf-function",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "The quinf() function calculates the quantile function of the uniform distribution. The quantile function is the inverse of the CDF. It takes a probability as an input and returns the value that has that probability.\nThe quinf() function takes two arguments:\n\np: the probability\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the quinf() function to calculate the quantile of a uniform distribution between 0 and 1 at the probability 0.5:\n\nset.seed(123)\nq &lt;- qunif(0.5, min = 0, max = 1)\n\nOutput:\n\nprint(q)\n\n[1] 0.5\n\n\nThis means that there is a 50% chance that a random variable\nIf you want to easily see different versions of the uniform distribution then you can either code them out or use the TidyDensity package. Let’s take a quick look.\n\npacman::p_load(TidyDensity)\n\nn &lt;- 5000\n\ntidy_uniform(.n = n) |&gt;\n  tidy_autoplot()"
  },
  {
    "objectID": "posts/2023-10-30/index.html#with-tidydensity",
    "href": "posts/2023-10-30/index.html#with-tidydensity",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "Now different variations can be visualized with the following workflow:\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_uniform\",\n  .param_list = list(\n    .n = n,\n    .min = 0,\n    .max = c(1,5,10),\n    .num_sims = 5\n  )\n) |&gt;\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/2023-10-31/index.html",
    "href": "posts/2023-10-31/index.html",
    "title": "Multinomial Distribution in R",
    "section": "",
    "text": "The multinomial distribution is a probability distribution that describes the probability of obtaining a specific number of counts for k different outcomes, when each outcome has a fixed probability of occurring.\nIn R, we can use the rmultinom() function to simulate random samples from a multinomial distribution, and the dmultinom() function to calculate the probability of a specific outcome."
  },
  {
    "objectID": "posts/2023-10-31/index.html#example-1",
    "href": "posts/2023-10-31/index.html#example-1",
    "title": "Multinomial Distribution in R",
    "section": "Example 1",
    "text": "Example 1\nSuppose we have a fair die, and we want to simulate rolling the die 10 times. We can use the rmultinom() function to do this as follows:\n\n# Simulate rolling a fair die 10 times\ndie_rolls &lt;- rmultinom(\n  n = 10, size = 1, \n  prob = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\n  )\n\n# Print the results\nprint(die_rolls)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    0    0    0    0    1    0    0    0    0     0\n[2,]    0    1    0    0    0    0    0    0    0     0\n[3,]    0    0    0    0    0    0    1    0    0     0\n[4,]    0    0    1    0    0    1    0    0    0     0\n[5,]    0    0    0    1    0    0    0    1    1     0\n[6,]    1    0    0    0    0    0    0    0    0     1"
  },
  {
    "objectID": "posts/2023-10-31/index.html#example-2",
    "href": "posts/2023-10-31/index.html#example-2",
    "title": "Multinomial Distribution in R",
    "section": "Example 2",
    "text": "Example 2\nSuppose we want to calculate the probability of getting exactly two ones, two threes, two fours, two fives, and two sixes when rolling a fair die 10 times. We can use the dmultinom() function to do this as follows:\n\n# Calculate the probability of getting exactly two ones, two threes, two fours, two fives, and two sixes when rolling a fair die 10 times\nprobability &lt;- dmultinom(\n  x = c(2, 0, 2, 2, 2, 2), \n  size = 10, \n  prob = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\n  )\n\n# Print the result\nprint(probability)\n\n[1] 0.001875429"
  },
  {
    "objectID": "posts/2023-10-31/index.html#try-it-on-your-own",
    "href": "posts/2023-10-31/index.html#try-it-on-your-own",
    "title": "Multinomial Distribution in R",
    "section": "Try it on your own!",
    "text": "Try it on your own!\nI encourage readers to try using the rmultinom() and dmultinom() functions on their own data. For example, you could simulate rolling a die 100 times and see how often each outcome occurs. Or, you could calculate the probability of getting a certain number of heads and tails when flipping a coin 10 times.\nHere is an example of how to use the rmultinom() function to simulate flipping a coin 10 times and calculate the probability of getting exactly five heads and five tails:\n\n# Simulate flipping a coin 10 times\ncoin_flips &lt;- rmultinom(n = 10, size = 1, prob = c(0.5, 0.5))\n\n# Calculate the probability of getting exactly five heads and five tails\nprobability &lt;- dmultinom(x = c(5, 5), size = 10, prob = c(0.5, 0.5))\n\n# Print the result\nprint(probability)\n\n[1] 0.2460938\n\n\nI hope this blog post has helped you learn how to use the multinomial distribution in R. Please feel free to leave a comment if you have any questions."
  },
  {
    "objectID": "posts/2023-11-01/index.html",
    "href": "posts/2023-11-01/index.html",
    "title": "Understanding the Triangular Distribution and Its Application in R",
    "section": "",
    "text": "Introduction\nAs an R programmer and enthusiast, I’m excited to delve into the fascinating world of probability distributions. One of the lesser-known but incredibly useful distributions is the Triangular Distribution, and today we’ll explore what it is and how to leverage it in R using the EnvStats library.\n\n\nWhat is the Triangular Distribution?\nThe Triangular Distribution is a continuous probability distribution with a triangular shape, hence the name. It is defined by three parameters: min, max, and mode. These parameters determine the range of values the distribution can take and the most likely value within that range. In mathematical terms, the probability density function (PDF) of the Triangular Distribution is given by:\nf(x) = (2 / (b - a)) * (x - a) / (c - a)      for a ≤ x &lt; c\nf(x) = (2 / (b - a)) * (b - x) / (b - c)      for c ≤ x ≤ b\nWhere: - a is the minimum value (min parameter). - b is the maximum value (max parameter). - c is the mode, which is the peak or most likely value (mode parameter).\n\n\nUsing the EnvStats R Library\nTo work with the Triangular Distribution in R, we can use the functions provided by the EnvStats library. Here are the key functions you need to know:\n\ndtri(x, min = 0, max = 1, mode = 1/2): This function calculates the probability density at a given x. You can specify the min, max, and mode parameters to define the distribution.\nptri(q, min = 0, max = 1, mode = 1/2): Use this function to find the cumulative probability up to a given q. Again, you can customize the min, max, and mode parameters.\nqtri(p, min = 0, max = 1, mode = 1/2): The quantile function, which helps you find the value of x for a given cumulative probability p. As always, you can set min, max, and mode to match your specific distribution.\nrtri(n, min = 0, max = 1, mode = 1/2): This function generates a random set of n numbers following the Triangular Distribution with the specified parameters.\n\n\n\nPractical Example in R\nLet’s see how to use these functions in a practical example. Suppose we want to model the distribution of daily temperatures in a specific region. We have historical data indicating that the minimum temperature is -5°C, the maximum temperature is 30°C, and the most likely temperature (mode) is around 20°C.\nHere’s how you can work with this scenario in R using the EnvStats library:\n\n# Load the EnvStats library\nlibrary(EnvStats)\n\n# Define the parameters\nmin_temp &lt;- -5\nmax_temp &lt;- 30\nmode_temp &lt;- 20\n\n# Calculate the density at x = 15°C\ndensity_at_15 &lt;- dtri(15, min = min_temp, max = max_temp, mode = mode_temp)\ncat(\"Density at 15°C:\", density_at_15, \"\\n\")\n\nDensity at 15°C: 0.04571429 \n\n# Calculate the cumulative probability up to 25°C\ncumulative_prob_up_to_25 &lt;- ptri(25, min = min_temp, max = max_temp, mode = mode_temp)\ncat(\"Cumulative Probability up to 25°C:\", cumulative_prob_up_to_25, \"\\n\")\n\nCumulative Probability up to 25°C: 0.9285714 \n\n# Find the temperature value for a cumulative probability of 0.75\ntemperature_for_prob_0.75 &lt;- qtri(0.75, min = min_temp, max = max_temp, mode = mode_temp)\ncat(\"Temperature for Cumulative Probability 0.75:\", temperature_for_prob_0.75, \"\\n\")\n\nTemperature for Cumulative Probability 0.75: 20.64586 \n\n# Generate a random set of 10 temperatures\nrandom_temperatures &lt;- rtri(10, min = min_temp, max = max_temp, mode = mode_temp)\ncat(\"Random Temperatures:\", random_temperatures, \"\\n\")\n\nRandom Temperatures: 26.21123 6.59049 13.64297 19.18005 9.697022 10.96856 6.626135 18.84034 -3.711269 25.5044 \n\n\nIn this example, we’ve used the Triangular Distribution to model daily temperatures, calculate probabilities, find quantiles, and generate random temperature values.\nThe Triangular Distribution is a versatile tool for modeling scenarios where you have some knowledge about the range and likelihood of an event or outcome. Whether you’re simulating real-world scenarios or conducting risk assessments, the EnvStats library in R makes it easy to work with this distribution.\nSo, the next time you need to model uncertain events with known bounds and modes, remember the Triangular Distribution and its helpful functions in R!"
  },
  {
    "objectID": "posts/2023-11-02/index.html",
    "href": "posts/2023-11-02/index.html",
    "title": "Fitting a Distribution to Data in R",
    "section": "",
    "text": "Introduction\nThe gamma distribution is a continuous probability distribution that is often used to model waiting times or other positively skewed data. It is a two-parameter distribution, where the shape parameter controls the skewness of the distribution and the scale parameter controls the spread of the distribution.\n\n\nFitting a gamma distribution to a dataset in R\nThere are two main ways to fit a gamma distribution to a dataset in R:\n\nMaximum likelihood estimation (MLE): This method estimates the parameters of the gamma distribution that are most likely to have produced the observed data.\nMethod of moments: This method estimates the parameters of the gamma distribution by equating the sample mean and variance to the theoretical mean and variance of the gamma distribution.\n\nMLE is the more common and generally more reliable method of fitting a gamma distribution to a dataset. To fit a gamma distribution to a dataset using MLE, we can use the fitdist() function from the fitdistrplus package.\n\n# Install the fitdistrplus package if necessary\n#install.packages(\"fitdistrplus\")\n\n# Load the fitdistrplus package\nlibrary(fitdistrplus)\nlibrary(TidyDensity)\n\nset.seed(123)\ndata &lt;- tidy_gamma(.n = 500)$y\n\n# Fit a gamma distribution to the data\nfit &lt;- fitdist(data, distr = \"gamma\", method = \"mle\")\n\nThe fit object contains the estimated parameters of the gamma distribution, as well as other information about the fit. We can access the estimated parameters using the coef() function. Now the tidy_gamma() function from the TidyDensity package comes with a default setting of a .scale = 0.3 and shape = 1. The rate is 1/.scale, so by default it is 3.33333\n\n# Get the estimated parameters of the gamma distribution\ncoef(fit)\n\n   shape     rate \n1.031833 3.594773 \n\n\nNow let’s see how that compares to the built in TidyDensity function:\n\nutil_gamma_param_estimate(data)$parameter_tbl[1,c(\"shape\",\"scale\",\"shape_ratio\")]\n\n# A tibble: 1 × 3\n  shape scale shape_ratio\n  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 0.983 0.292        3.36\n\n\nIn the above, the shape_ratio is the rate\n\n\nTry on your own!\nI encourage you to try fitting a gamma distribution to your own data. You can use the fitdistrplus package in R to fit a gamma distribution to any dataset. Once you have fitted a gamma distribution to your data, you can use the estimated parameters to generate random samples from the gamma distribution or to calculate the probability of observing a particular value."
  },
  {
    "objectID": "posts/2023-11-03/index.html",
    "href": "posts/2023-11-03/index.html",
    "title": "Introducing TidyDensity’s New Powerhouse: The convert_to_ts() Function",
    "section": "",
    "text": "Introduction\nIf you’re an R enthusiast like me, you know that data manipulation is at the core of everything we do. The ability to transform your data swiftly and efficiently can make or break your data analysis projects. That’s why I’m thrilled to introduce a game-changing function in TidyDensity, my very own R library. Say hello to convert_to_ts()!\nIn the world of data analysis, time series data is like a treasure chest of insights waiting to be unlocked. Whether you’re tracking stock prices, monitoring patient data, or analyzing the temperature over the years, having your data in a time series format is a crucial step in the process. With convert_to_ts(), that process just got a whole lot easier.\n\n\nThe Basics\nLet’s start with the basics. The syntax of convert_to_ts() is straightforward:\nconvert_to_ts(.data, .return_ts = TRUE, .pivot_longer = FALSE)\n\n.data: This is your data, the data frame or tibble you want to convert into a time series format. It’s the heart of your analysis.\n.return_ts: A logical value that lets you decide whether you want to return the time series data. By default, it’s set to TRUE, which is usually what you’ll want.\n.pivot_longer: Another logical value that determines whether you want to pivot the data into long format. By default, it’s set to FALSE, but you can change that if needed.\n\n\n\nThe Magic of convert_to_ts()\nSo, what exactly does convert_to_ts() do, and why is it a game-changer? Imagine you have a data frame with time-based data in a wide format. You’ve got columns representing different time points, and you want to transform it into a time series format for easier analysis. This is where convert_to_ts() steps in.\nBy simply passing your data frame as the .data argument, convert_to_ts() does the heavy lifting for you. It reshapes your data into a tidy time series format, making it easier to work with and analyze. If you set .return_ts to TRUE, it will return the time series data, ready for your next analysis step.\nBut that’s not all. Sometimes, you might want to pivot your data into long format for specific analyses or visualizations. That’s where the .pivot_longer argument comes into play. If you set it to TRUE, convert_to_ts() will pivot your data into long format, providing you with even more flexibility in your data manipulation.\n\n\nReal-World Applications\nLet’s talk about the real-world applications of convert_to_ts(). Consider you are working with some time series data and it follows some distribution fairly well. You may want to run multiple simulations of that data, which can be done with one of the tidy_ distribution functions, and you can take that output and pipe it right into convert_to_ts() and see different simulations of a time series generated from some distribution. I do this on a regular basis at my day job in healthcare.\nBut it’s not just limited to healthcare. Stock analysts, meteorologists, and anyone dealing with time-based data can benefit from this versatile function. The possibilities are endless, and the power is in your hands.\n\n\nExamples\n\n\nExample 1: Convert data to time series format without returning time series data\n\nlibrary(TidyDensity)\n\nx &lt;- tidy_normal()\nresult &lt;- convert_to_ts(x, FALSE)\nhead(result)\n\n# A tibble: 6 × 1\n        y\n    &lt;dbl&gt;\n1  1.23  \n2  0.715 \n3  0.738 \n4  1.08  \n5 -0.0613\n6  1.19  \n\n\n\n\nExample 2: Convert data to time series format and pivot it into long format\n\nx &lt;- tidy_normal(.num_sims = 4)\nresult &lt;- convert_to_ts(x, FALSE, TRUE)\nhead(result)\n\n# A tibble: 6 × 2\n  sim_number      y\n  &lt;chr&gt;       &lt;dbl&gt;\n1 1           0.881\n2 1          -0.105\n3 1          -0.655\n4 1          -0.564\n5 1           0.600\n6 1          -0.811\n\nunique(result$sim_number)\n\n[1] \"1\" \"2\" \"3\" \"4\"\n\nconvert_to_ts(x, TRUE, TRUE) |&gt; head()\n\n              1           2          3          4\n[1,]  0.8807494  0.04680495 -0.1993147 -1.5549585\n[2,] -0.1045016  0.14023102  0.5433512 -1.9247656\n[3,] -0.6549336 -0.20818900 -0.1689992 -0.2934131\n[4,] -0.5638595  0.87588361  0.4802693 -1.2052377\n[5,]  0.6002684  0.26137176  1.5993445 -0.5379518\n[6,] -0.8111576 -0.60834621 -0.4859808 -0.2178982\n\n\n\n\nExample 3: Convert data to time series format and return the time series data\n\nx &lt;- tidy_normal()\nresult &lt;- convert_to_ts(x)\nhead(result)\n\n               y\n[1,] -0.21509987\n[2,] -0.88989659\n[3,]  0.69464989\n[4,] -0.03296698\n[5,] -0.82499955\n[6,] -0.71676037\n\n\n\n\nConclusion\nIn the ever-evolving world of data analysis, having the right tools at your disposal is crucial. convert_to_ts() in TidyDensity is one such tool that can simplify your data transformation processes and elevate your data analysis game. It’s all about efficiency and flexibility, allowing you to focus on what matters most – deriving valuable insights from your data.\nSo, whether you’re a data enthusiast, a coding wizard, or someone curious about the world of R, convert_to_ts() is here to make your life easier. Give it a try, explore its capabilities, and unlock the potential of your time series data. With TidyDensity, the possibilities are endless, and your data analysis journey just got a whole lot smoother. Happy coding!"
  },
  {
    "objectID": "posts/2023-11-06/index.html",
    "href": "posts/2023-11-06/index.html",
    "title": "Demystifying Data: A Comprehensive Guide to Calculating and Plotting Cumulative Distribution Functions (CDFs) in R",
    "section": "",
    "text": "Introduction\nIn the realm of statistics, a cumulative distribution function (CDF) serves as a crucial tool for understanding the behavior of data. It provides a comprehensive picture of how a variable’s values are distributed across its range. In this blog post, we’ll embark on an exciting journey to unravel the mysteries of CDFs and explore how to effortlessly calculate and visualize them using the powerful R programming language.\n\n\nUnderstanding the Essence of CDFs\nBefore delving into the world of R programming, let’s first grasp the fundamental concept of a CDF. Imagine a group of students eagerly awaiting their exam results. The CDF for their scores would depict the probability of encountering a student with a score less than or equal to a specific value. For instance, if the CDF indicates a value of 0.7 at 80%, it implies that there’s a 70% chance of finding a student with a score of 80 or lower.\n\n\nCalculating CDFs with the ecdf() Function\nR, our trusty programming companion, offers a user-friendly function called ecdf() to calculate CDFs. This function takes a vector of data as input and returns a corresponding CDF object. Let’s put this function into action by generating a sample dataset of exam scores:\n\nexam_scores &lt;- c(75, 82, 94, 68, 88, 90, 72, 85, 91, 79)\n\nNow, we can effortlessly calculate the CDF using the ecdf() function:\n\ncdf_scores &lt;- ecdf(exam_scores)\n\nThe cdf_scores object now holds the calculated CDF values for the exam scores.\n\n\nVisualizing CDFs with the plot() Function\nTo gain a deeper understanding of the CDF, we can visualize it using the plot() function. This function takes the CDF object as input and generates a corresponding plot. Simply type the following command:\n\nplot(cdf_scores)\n\n\n\n\nVoila! You should now see a captivating plot depicting the CDF of the exam scores. The x-axis represents the exam scores, and the y-axis represents the corresponding cumulative probabilities.\n\n\nExplore!\nWe’ve successfully calculated and visualized CDFs in R. Now it’s time for you to explore and experiment with this powerful tool. Gather your own data, calculate the CDF, and interpret its meaning. Remember, data holds valuable insights, and CDFs are the keys to unlocking those insights."
  },
  {
    "objectID": "posts/2023-11-07/index.html",
    "href": "posts/2023-11-07/index.html",
    "title": "How to Simulate & Plot a Bivariate Normal Distribution in R: A Hands-on Guide",
    "section": "",
    "text": "Introduction\nWelcome to the fascinating world of bivariate normal distributions! In this blog post, we’ll embark on a journey to understand, simulate, and visualize these distributions using the powerful R programming language. Whether you’re a seasoned R expert or a curious beginner, this guide will equip you with the necessary tools to explore this intriguing aspect of probability theory.\n\n\nUnderstanding Bivariate Normal Distributions\nImagine two variables, like height and weight, that exhibit a joint distribution. The bivariate normal distribution captures the relationship between these variables, describing how their values tend to cluster around certain means and how they vary together. It’s like a two-dimensional bell curve, where the peak represents the most likely combination of values for both variables.\n\n\nSimulating a Bivariate Normal Distribution\nNow, let’s bring this distribution to life using R. The MASS package provides the mvrnorm() function, which generates random samples from a multivariate normal distribution. We’ll use this function to simulate a bivariate normal distribution with mean vector [10, 20] and covariance matrix [[5, 3], [3, 6]]. These parameters determine the center and shape of the distribution.\n\nlibrary(MASS)\n\n# Simulate 100 observations from a bivariate normal distribution\nset.seed(123) # Set a seed for reproducibility\nbvnData &lt;- mvrnorm(\n  n = 100, \n  mu = c(10, 20), \n  Sigma = matrix(c(5, 3, 3, 6), \n                 ncol = 2)\n  )\n\n\n\nVisualizing the Bivariate Normal Distribution\nTo truly appreciate the beauty of the bivariate normal distribution, let’s visualize it using the plot() and density() functions\n\nlibrary(mnormt)\n\nx &lt;- bvnData[,1] |&gt; sort()\ny &lt;- bvnData[,2] |&gt; sort()\nmu &lt;- c(10, 20)\nsigma &lt;- matrix(c(5, 3, 3, 6), \n                 ncol = 2)\nf &lt;- function(x, y) dmnorm(cbind(x, y), mu, sigma)\nz &lt;- outer(x,y,f)\ncontour(x,y,z)\n\n\n\n# Create a density plot of the simulated data\nplot(density(bvnData))\n\n\n\n\nThis plot should reveal an elliptical shape, with the highest density concentrated around the mean values. The contours represent the regions of equal probability.\n\n\nTry It On Your Own!\nNow, it’s your turn to experiment! Change the mean vector, covariance matrix, and sample size to see how they affect the shape and spread of the distribution. Play with different visualization options to explore different perspectives of the data.\nRemember, R is a vast and ever-evolving language, so there’s always more to learn. Keep exploring, asking questions, and seeking out new challenges to become a master R programmer."
  },
  {
    "objectID": "posts/2023-11-13/index.html",
    "href": "posts/2023-11-13/index.html",
    "title": "Unlocking the Power of Prediction Intervals in R: A Practical Guide",
    "section": "",
    "text": "Introduction\nPrediction intervals are a powerful tool for understanding the uncertainty of your predictions. They allow you to specify a range of values within which you are confident that the true value will fall. This can be useful for many tasks, such as setting realistic goals, making informed decisions, and communicating your findings to others.\nIn this blog post, we will show you how to create a prediction interval in R using the mtcars dataset. The mtcars dataset is a built-in dataset in R that contains information about fuel economy, weight, displacement, and other characteristics of 32 cars.\n\n\nCreating a Prediction Interval\nTo create a prediction interval in R, we can use the predict() function. The predict() function takes a fitted model and a new dataset as input and returns the predicted values for the new dataset.\nWe can also use the predict() function to calculate prediction intervals. To do this, we need to specify the interval argument. The interval argument can take two values: confidence and prediction.\nA confidence interval is the range of values within which we are confident that the true mean of the population will fall. A prediction interval is the range of values within which we are confident that the true value of a new observation will fall.\nTo create a prediction interval for the mpg variable in the mtcars dataset, we can use the following code:\n\n# Fit a linear model\nmodel &lt;- lm(mpg ~ disp, data = mtcars)\n\n# Create a prediction interval\nprediction_intervals &lt;- predict(\n  model, \n  newdata = mtcars, \n  interval = \"prediction\", \n  level = 0.95\n  )\n\n# Print the prediction interval\nhead(prediction_intervals)\n\n                       fit       lwr      upr\nMazda RX4         23.00544 16.227868 29.78300\nMazda RX4 Wag     23.00544 16.227868 29.78300\nDatsun 710        25.14862 18.302683 31.99456\nHornet 4 Drive    18.96635 12.217933 25.71477\nHornet Sportabout 14.76241  7.905308 21.61952\nValiant           20.32645 13.582915 27.06999\n\n\nThe prediction interval shows that we are 95% confident that the true mpg value for a new car with a given displacement will fall within the range specified by the lwr and upr columns.\n\n\nVisualize\nFirst lets bind the data together with cbind()\n\nfull_res &lt;- cbind(mtcars, prediction_intervals)\n\nhead(full_res)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb      fit\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 23.00544\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 23.00544\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 25.14862\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 18.96635\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 14.76241\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 20.32645\n                        lwr      upr\nMazda RX4         16.227868 29.78300\nMazda RX4 Wag     16.227868 29.78300\nDatsun 710        18.302683 31.99456\nHornet 4 Drive    12.217933 25.71477\nHornet Sportabout  7.905308 21.61952\nValiant           13.582915 27.06999\n\n\nNow let’s plot the actual, the fitted and the prediction confidence bands.\n\nlibrary(ggplot2)\n\nfull_res |&gt;\n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point() +\n  geom_point(aes(y = fit), col = \"steelblue\", size = 2.5) +\n  geom_line(aes(y = fit)) +\n  geom_line(aes(y = lwr), linetype = \"dashed\", col = \"red\") +\n  geom_line(aes(y = upr), linetype = \"dashed\", col = \"red\") +\n  theme_minimal() +\n  labs(\n    title = \"mpg ~ disp, data = mtcars\",\n    subtitle = \"With Prediction Intervals\"\n  )\n\n\n\n\nAbove we are capturing the prediction interval which gives us the uncertainty around a single point, whereas the confidence interval gives us the uncertainty around the mean predicted values. This means that the prediction interval will always be wider than the confidence interval for the same value.\n\n\nTrying It Out Yourself\nNow it’s your turn to try out creating a prediction interval in R. Here are some ideas:\n\nTry creating a prediction interval for a different variable in the mtcars dataset, such as wt or hp.\nTry creating a prediction interval for a variable in a different dataset.\nTry creating a prediction interval for a more complex model, such as a multiple linear regression model or a logistic regression model.\n\n\n\nConclusion\nCreating prediction intervals in R is a straightforward process. By using the predict() function, you can easily calculate prediction intervals for any fitted model and any new dataset. This can be a valuable tool for understanding the uncertainty of your predictions and making more informed decisions."
  },
  {
    "objectID": "posts/2023-11-14/index.html",
    "href": "posts/2023-11-14/index.html",
    "title": "How to Predict a Single Value Using a Regression Model in R",
    "section": "",
    "text": "Introduction\nRegression models are a powerful tool for predicting future values based on historical data. They are used in a wide range of industries, including finance, healthcare, and marketing. In this blog post, we will learn how to predict a single value using a regression model in R. We will use the mtcars dataset, which contains information about cars, including their weight, horsepower, and fuel efficiency.\n\n\nBuilding a Linear Regression Model\nThe first step in predicting a single value is to build a regression model. We can do this using the lm() function in R. The lm() function takes two arguments: a formula and a data frame. The formula specifies the relationship between the dependent variable (the variable we want to predict) and the independent variables (the variables we use to predict the dependent variable). The data frame contains the values of the dependent and independent variables.\nTo build a linear regression model to predict the fuel efficiency of a car based on its weight and horsepower, we would use the following code:\n\n# Create a linear regression model\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\nThe model object now contains the fitted regression model. We can inspect the model by using the summary() function.\n\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nThe output of the summary() function shows the estimated coefficients, standard errors, and p-values for the independent variables in the model. The coefficients represent the change in the dependent variable for a one-unit increase in the independent variable, holding all other variables constant.\n\n\nPredicting a Single Value\nOnce we have fitted a regression model, we can use it to predict single values. We can do this using the predict() function. The predict() function takes two arguments: the fitted model and a new data frame containing the values of the independent variables for which we want to make predictions.\nTo predict the fuel efficiency of a car with a weight of 3,000 pounds and a horsepower of 150, we would use the following code:\n\n# Create a new data frame containing the values of the independent \n# variables for which we want to make predictions\nnewdata &lt;- data.frame(wt = 3, hp = 150) # Wt is in 1000 lbs\n\n# Predict the fuel efficiency of the car\nprediction &lt;- predict(model, newdata)\n\n# Print the predicted fuel efficiency\nprint(prediction)\n\n       1 \n20.82784 \n\n\nThe output of the predict() function is a vector containing the predicted values for the dependent variable. In this case, the predicted fuel efficiency is 20.8278358 miles per gallon.\n\n\nConclusion\nIn this blog post, we have learned how to predict a single value using a regression model in R. We used the mtcars dataset to build a linear regression model to predict the fuel efficiency of a car based on its weight and horsepower. We then used the predict() function to predict the fuel efficiency of a car with a specific weight and horsepower.\n\n\nTry It Yourself\nNow that you know how to predict a single value using a regression model in R, try it yourself! Here are some ideas:\n\nBuild a linear regression model to predict the price of a house based on its size and number of bedrooms.\nBuild a linear regression model to predict the salary of a person based on their education level and years of experience.\nBuild a linear regression model to predict the number of customers that will visit a store on a given day based on the day of the week and the weather forecast.\n\nOnce you have built a regression model, you can use it to predict single values for new data. This can be a valuable tool for making decisions about the future."
  },
  {
    "objectID": "posts/2023-11-15/index.html",
    "href": "posts/2023-11-15/index.html",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "",
    "text": "Multiple linear regression is a powerful statistical method that allows us to examine the relationship between a dependent variable and multiple independent variables."
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-1-load-the-dataset",
    "href": "posts/2023-11-15/index.html#step-1-load-the-dataset",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 1: Load the dataset",
    "text": "Step 1: Load the dataset\n# Load the mtcars dataset\ndata(mtcars)"
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-2-build-the-model",
    "href": "posts/2023-11-15/index.html#step-2-build-the-model",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 2: Build the model",
    "text": "Step 2: Build the model\nNow, let’s create the multiple linear regression model using the specified variables: disp, hp, and drat.\n\n# Build the multiple linear regression model\nmodel &lt;- lm(mpg ~ disp + hp + drat, data = mtcars)"
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-3-examine-the-data",
    "href": "posts/2023-11-15/index.html#step-3-examine-the-data",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 3: Examine the data",
    "text": "Step 3: Examine the data\nIt’s always a good idea to take a look at the relationships between variables before diving into the model. The pairs() function helps us with that.\n\n# Examine relationships between variables\npairs(mtcars[,c(\"mpg\",\"disp\",\"hp\",\"drat\")])"
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-4-check-for-multicollinearity",
    "href": "posts/2023-11-15/index.html#step-4-check-for-multicollinearity",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 4: Check for multicollinearity",
    "text": "Step 4: Check for multicollinearity\nMulticollinearity is when independent variables in a regression model are highly correlated. It can affect the stability and reliability of our model. Keep an eye on the scatterplots in the pairs plot to get a sense of this."
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-5-plot-the-residuals",
    "href": "posts/2023-11-15/index.html#step-5-plot-the-residuals",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 5: Plot the residuals",
    "text": "Step 5: Plot the residuals\nNow, let’s check the model’s residuals using a scatterplot. Residuals are the differences between observed and predicted values. They should ideally show no pattern.\n\n# Plot the residuals\nplot(\n  model$residuals, \n  main = \"Residuals vs Fitted Values\", \n  xlab = \"Fitted Values\", \n  ylab = \"Residuals\"\n  )"
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-6-evaluate-the-model",
    "href": "posts/2023-11-15/index.html#step-6-evaluate-the-model",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 6: Evaluate the model",
    "text": "Step 6: Evaluate the model\nBy examining the residuals vs. fitted values plot, we can identify patterns that may suggest non-linearity or heteroscedasticity. Ideally, residuals should be randomly scattered."
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-7-encourage-readers-to-try-it-themselves",
    "href": "posts/2023-11-15/index.html#step-7-encourage-readers-to-try-it-themselves",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 7: Encourage readers to try it themselves",
    "text": "Step 7: Encourage readers to try it themselves\nI’d encourage readers to take the code snippets, run them in their R environment, and explore. Maybe try different variables, tweak the model, or even use another dataset. Hands-on experience is the best teacher!\nRemember, understanding the data and interpreting the results is as important as running the code. It’s a fascinating journey into uncovering patterns and relationships within your data.\nFeel free to reach out if you have any questions or if there’s anything specific you’d like to explore further. Happy coding!"
  },
  {
    "objectID": "posts/2011-11-16/index.html",
    "href": "posts/2011-11-16/index.html",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "",
    "text": "My R package {healthyR.ts} has been updated to version 0.3.0; you can install it from either CRAN, r-universe or GitHub. Let’s go over some of the changes and improvements."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_log_ts---logging-time-series-data",
    "href": "posts/2011-11-16/index.html#util_log_ts---logging-time-series-data",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "1. util_log_ts() - Logging Time Series Data",
    "text": "1. util_log_ts() - Logging Time Series Data\nOne of the standout additions is the introduction of util_log_ts(). This function seems like a game-changer, providing a streamlined way to log time series data. This is incredibly useful, especially when dealing with extensive datasets, making the whole process more efficient and user-friendly. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_singlediff_ts---single-differences-for-time-series",
    "href": "posts/2011-11-16/index.html#util_singlediff_ts---single-differences-for-time-series",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "2. util_singlediff_ts() - Single Differences for Time Series",
    "text": "2. util_singlediff_ts() - Single Differences for Time Series\nThe addition of util_singlediff_ts() expands the toolkit, offering a function dedicated to handling single differences in time series data. This is valuable for various applications, such as identifying trends or preparing data for further analysis. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_doublediff_ts---double-differences-for-time-series",
    "href": "posts/2011-11-16/index.html#util_doublediff_ts---double-differences-for-time-series",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "3. util_doublediff_ts() - Double Differences for Time Series",
    "text": "3. util_doublediff_ts() - Double Differences for Time Series\nBuilding on the concept of differencing, util_doublediff_ts() seems to provide a higher level of sophistication, allowing users to perform double differences on time series data. This could be pivotal in cases where a more refined analysis is required. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_difflog_ts---combining-differences-and-log-transformation",
    "href": "posts/2011-11-16/index.html#util_difflog_ts---combining-differences-and-log-transformation",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "4. util_difflog_ts() - Combining Differences and Log Transformation",
    "text": "4. util_difflog_ts() - Combining Differences and Log Transformation\nThe fusion of differencing and log transformation in util_difflog_ts() is a remarkable addition. This could be particularly beneficial in scenarios where both operations are needed to unlock deeper insights from the time series data. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_doubledifflog_ts---double-differences-with-log-transformation",
    "href": "posts/2011-11-16/index.html#util_doubledifflog_ts---double-differences-with-log-transformation",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "5. util_doubledifflog_ts() - Double Differences with Log Transformation",
    "text": "5. util_doubledifflog_ts() - Double Differences with Log Transformation\nThe introduction of util_doubledifflog_ts() appears to take things a step further by combining double differences and log transformation. This function seems poised to provide a comprehensive solution for users dealing with complex time series data. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#attributes-enhancement-in-ts_growth_rate_vec",
    "href": "posts/2011-11-16/index.html#attributes-enhancement-in-ts_growth_rate_vec",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "1. Attributes Enhancement in ts_growth_rate_vec()",
    "text": "1. Attributes Enhancement in ts_growth_rate_vec()\nThe attention to detail is evident with the addition of attributes to the output of ts_growth_rate_vec(). This enhancement not only improves the clarity of results but also contributes to a more informative and user-friendly experience."
  },
  {
    "objectID": "posts/2011-11-16/index.html#refinement-of-auto_stationarize-in-response-to-user-feedback",
    "href": "posts/2011-11-16/index.html#refinement-of-auto_stationarize-in-response-to-user-feedback",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "2. Refinement of auto_stationarize() in Response to User Feedback",
    "text": "2. Refinement of auto_stationarize() in Response to User Feedback\nUpdates to auto_stationarize() based on user feedback (Fix #481 #483) demonstrate a commitment to refining existing features. This responsiveness to the community’s needs is commendable and ensures that the package evolves in sync with user expectations. It has taken all of the util_ transforms mentioned above in order to improve it’s functionality."
  },
  {
    "objectID": "posts/2011-11-16/index.html#integration-with-auto_arima-engine-in-ts_auto_arima",
    "href": "posts/2011-11-16/index.html#integration-with-auto_arima-engine-in-ts_auto_arima",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "3. Integration with auto_arima Engine in ts_auto_arima()",
    "text": "3. Integration with auto_arima Engine in ts_auto_arima()\nThe integration of ts_auto_arima() with the parsnip engine of auto_arima is a notable improvement. This update, triggered when .tune is set to FALSE, aligns the package with cutting-edge tools, potentially enhancing the efficiency and accuracy of time series modeling.\nIn conclusion, the release of healthyR.ts version 0.3.0 is an exciting leap forward. The new features introduce powerful capabilities, while the minor fixes and improvements showcase a commitment to providing a robust and user-friendly package. Users can look forward to a more versatile and refined experience in time series analysis. Great job on this release, and I’m sure the community is eager to explore these enhancements!"
  },
  {
    "objectID": "posts/2011-11-16/index.html#auto_stationarize",
    "href": "posts/2011-11-16/index.html#auto_stationarize",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "auto_stationarize()",
    "text": "auto_stationarize()\n\nlibrary(healthyR.ts)\n\nauto_stationarize(AirPassengers)\n\nThe time series is already stationary via ts_adf_test().\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nauto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n$stationary_ts\nTime Series:\nStart = 3 \nEnd = 150 \nFrequency = 1 \n  [1]  0.5 -0.4  0.6  1.1 -2.8  3.0 -1.1  0.6 -0.5 -0.5  0.1  2.0 -0.6  0.8  1.2\n [16] -3.4 -0.7 -0.3  1.7  3.0 -3.2  0.9  2.2 -2.5 -0.4  2.6 -4.3  2.0 -3.1  2.7\n [31] -2.1  0.1  2.1 -0.2 -2.2  0.6  1.0 -2.6  3.0  0.3  0.2 -0.8  1.0  0.0  3.2\n [46] -2.2 -4.7  1.2  0.8 -0.6 -0.4  0.6  1.0 -1.6 -0.1  3.4 -0.9 -1.7 -0.5  0.8\n [61]  2.4 -1.9  0.6 -2.2  2.6 -0.1 -2.7  1.7 -0.3  1.9 -2.7  1.1 -0.6  0.9  0.0\n [76]  1.8 -0.5 -0.4 -1.2  2.6 -1.8  1.7 -0.9  0.6 -0.4  3.0 -2.8  3.1 -2.3 -1.1\n [91]  2.1 -0.3 -1.7 -0.8 -0.4  1.1 -1.5  0.3  1.4 -2.0  1.3 -0.3  0.4 -3.5  1.1\n[106]  2.6  0.4 -1.3  2.0 -1.6  0.6 -0.1 -1.4  1.6  1.6 -3.4  1.7 -2.2  2.1 -2.0\n[121] -0.2  0.2  0.7 -1.4  1.8 -0.1 -0.7  0.4  0.4  1.0 -2.4  1.0 -0.4  0.8 -1.0\n[136]  1.4 -1.2  1.1 -0.9  0.5  1.9 -0.6  0.3 -1.4 -0.9 -0.5  1.4  0.1\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -6.562008\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"double_diff\"\n\n$ret\n[1] TRUE\n\nplot.ts(auto_stationarize(BJsales)$stationary_ts)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n\n\nauto_stationarize(BJsales.lead)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n$stationary_ts\nTime Series:\nStart = 2 \nEnd = 150 \nFrequency = 1 \n  [1]  0.06  0.25 -0.57  0.58 -0.20  0.23 -0.04 -0.19  0.03  0.42  0.04  0.24\n [13]  0.34 -0.46 -0.18 -0.08  0.29  0.56 -0.37  0.20  0.54 -0.31  0.03  0.52\n [25] -0.70  0.35 -0.63  0.44 -0.38 -0.01  0.22  0.10 -0.50  0.01  0.30 -0.76\n [37]  0.52  0.15  0.06 -0.10  0.21 -0.01  0.70 -0.22 -0.76  0.06  0.02 -0.17\n [49] -0.08  0.01  0.11 -0.39  0.01  0.50 -0.02 -0.37 -0.13  0.05  0.54 -0.46\n [61]  0.25 -0.52  0.44  0.02 -0.47  0.11  0.06  0.25 -0.35  0.00 -0.06  0.21\n [73] -0.09  0.36  0.09 -0.04 -0.20  0.44 -0.23  0.40 -0.01  0.17  0.08  0.58\n [85] -0.27  0.79 -0.21  0.02  0.30  0.28 -0.27 -0.01  0.03  0.16 -0.28  0.15\n [97]  0.26 -0.36  0.32 -0.11  0.22 -0.65  0.00  0.47  0.16 -0.19  0.48 -0.26\n[109]  0.21  0.00 -0.20  0.35  0.38 -0.48  0.20 -0.32  0.43 -0.50  0.12 -0.17\n[121]  0.15 -0.36  0.35 -0.03 -0.18  0.16  0.07  0.21 -0.50  0.23 -0.13  0.14\n[133] -0.15  0.19 -0.24  0.26 -0.22  0.17  0.37 -0.06  0.29 -0.34 -0.12 -0.16\n[145]  0.25  0.08 -0.07  0.26 -0.37\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -4.838625\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"diff\"\n\n$ret\n[1] TRUE\n\nplot.ts(auto_stationarize(BJsales.lead)$stationary_ts)\n\nThe time series is not stationary. Attempting to make it stationary..."
  },
  {
    "objectID": "posts/2011-11-16/index.html#ts_auto_arima",
    "href": "posts/2011-11-16/index.html#ts_auto_arima",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "ts_auto_arima()",
    "text": "ts_auto_arima()\nThis use to only use the Arima engine if the .tune parameter was set to FALSE, thus it would many times give a simple straight line forecast. This was changed to make the engine auto_arima if .tune is set to FALSE.\n\nlibrary(timetk)\nlibrary(dplyr)\nlibrary(modeltime)\n\ndata &lt;- AirPassengers |&gt;\n  ts_to_tbl() |&gt;\n  select(-index)\n\nsplits &lt;- time_series_split(\n  data\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\nts_aa &lt;- ts_auto_arima(\n  .data = data,\n  .num_cores = 2,\n  .date_col = date_col,\n  .value_col = value,\n  .rsamp_obj = splits,\n  .formula = value ~ .,\n  .grid_size = 5,\n  .cv_slice_limit = 2,\n  .tune = FALSE\n)\n\nts_aa$recipe_info\n\n$recipe_call\nrecipe(.data = data, .date_col = date_col, .value_col = value, \n    .formula = value ~ ., .rsamp_obj = splits, .tune = FALSE, \n    .grid_size = 5, .num_cores = 2, .cv_slice_limit = 2)\n\n$recipe_syntax\n[1] \"ts_arima_recipe &lt;-\"                                                                                                                                                                           \n[2] \"\\n  recipe(.data = data, .date_col = date_col, .value_col = value, .formula = value ~ \\n    ., .rsamp_obj = splits, .tune = FALSE, .grid_size = 5, .num_cores = 2, \\n    .cv_slice_limit = 2)\"\n\n$rec_obj\n\nts_aa$model_info\n\n$model_spec\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nSeries: outcome \nARIMA(1,1,0)(0,1,0)[12] \n\nCoefficients:\n          ar1\n      -0.2431\ns.e.   0.0894\n\nsigma^2 = 109.8:  log likelihood = -447.95\nAIC=899.9   AICc=900.01   BIC=905.46\n\n$was_tuned\n[1] \"not_tuned\"\n\nts_aa$model_calibration\n\n$plot\n\n$calibration_tbl\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc             .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;workflow&gt; ARIMA(1,1,0)(0,1,0)[12] Test  &lt;tibble [12 × 4]&gt;\n\n$model_accuracy\n# A tibble: 1 × 9\n  .model_id .model_desc             .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 ARIMA(1,1,0)(0,1,0)[12] Test   18.5  4.18 0.384  4.03  23.9 0.955\n\nts_aa$model_calibration$plot\n\n\n\n\n\nFinally enhancement to add attributes to ts_growth_rate_vec()\n\nts_growth_rate_vec(AirPassengers)\n\n  [1]          NA   5.3571429  11.8644068  -2.2727273  -6.2015504  11.5702479\n  [7]   9.6296296   0.0000000  -8.1081081 -12.5000000 -12.6050420  13.4615385\n [13]  -2.5423729   9.5652174  11.9047619  -4.2553191  -7.4074074  19.2000000\n [19]  14.0939597   0.0000000  -7.0588235 -15.8227848 -14.2857143  22.8070175\n [25]   3.5714286   3.4482759  18.6666667  -8.4269663   5.5214724   3.4883721\n [31]  11.7977528   0.0000000  -7.5376884 -11.9565217  -9.8765432  13.6986301\n [37]   3.0120482   5.2631579   7.2222222  -6.2176166   1.1049724  19.1256831\n [43]   5.5045872   5.2173913 -13.6363636  -8.6124402  -9.9476440  12.7906977\n [49]   1.0309278   0.0000000  20.4081633  -0.4237288  -2.5531915   6.1135371\n [55]   8.6419753   3.0303030 -12.8676471 -10.9704641 -14.6919431  11.6666667\n [61]   1.4925373  -7.8431373  25.0000000  -3.4042553   3.0837004  12.8205128\n [67]  14.3939394  -2.9801325 -11.6040956 -11.5830116 -11.3537118  12.8078818\n [73]   5.6768559  -3.7190083  14.5922747   0.7490637   0.3717472  16.6666667\n [79]  15.5555556  -4.6703297 -10.0864553 -12.1794872 -13.5036496  17.2995781\n [85]   2.1582734  -2.4647887  14.4404332  -1.2618297   1.5974441  17.6100629\n [91]  10.4278075  -1.9370460 -12.3456790 -13.8028169 -11.4379085  12.9151292\n [97]   2.9411765  -4.4444444  18.2724252  -2.2471910   2.0114943  18.8732394\n[103]  10.1895735   0.4301075 -13.4903640 -14.1089109 -12.1037464  10.1639344\n[109]   1.1904762  -6.4705882  13.8364780  -3.8674033   4.3103448  19.8347107\n[115]  12.8735632   2.8513238 -20.0000000 -11.1386139 -13.6490251   8.7096774\n[121]   6.8249258  -5.0000000  18.7134503  -2.4630542   6.0606061  12.3809524\n[127]  16.1016949   2.0072993 -17.1735242 -12.0950324 -11.0565111  11.8784530\n[133]   2.9629630  -6.2350120   7.1611253  10.0238663   2.3861171  13.3474576\n[139]  16.2616822  -2.5723473 -16.1716172  -9.2519685 -15.4013015  10.7692308\nattr(,\"vector_attributes\")\nattr(,\"vector_attributes\")$tsp\n[1] 1949.000 1960.917   12.000\n\nattr(,\"vector_attributes\")$class\n[1] \"ts\"\n\nattr(,\"name\")\n[1] \"AirPassengers\""
  },
  {
    "objectID": "posts/2023-11-17/index.html",
    "href": "posts/2023-11-17/index.html",
    "title": "Quadratic Regression in R: Unveiling Non-Linear Relationships",
    "section": "",
    "text": "Introduction\nIn the realm of data analysis, quadratic regression emerges as a powerful tool for uncovering the hidden patterns within datasets that exhibit non-linear relationships. Unlike its linear counterpart, quadratic regression ventures beyond straight lines, gracefully capturing curved relationships between variables. This makes it an essential technique for understanding a wide range of phenomena, from predicting stock prices to modeling population growth.\nEmbark on a journey into the world of quadratic regression using the versatile R programming language. We’ll explore the steps involved in fitting a quadratic model, interpreting its parameters, and visualizing the results. Along the way, you’ll gain hands-on experience with this valuable technique, enabling you to tackle your own data analysis challenges with confidence.\n\n\nSetting the Stage: Data Preparation\nBefore embarking on our quadratic regression adventure, let’s assemble our data. Suppose we’re investigating the relationship between study hours and exam scores. We’ve gathered data from a group of students, recording their study hours and corresponding exam scores.\n\n# Create a data frame to store the data\nstudy_hours &lt;- c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60)\nexam_scores &lt;- c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27)\ndata &lt;- data.frame(study_hours, exam_scores)\ndata\n\n   study_hours exam_scores\n1            6          14\n2            9          28\n3           12          50\n4           14          70\n5           30          89\n6           35          94\n7           40          90\n8           47          75\n9           51          59\n10          55          44\n11          60          27\n\n\n\n\nVisualizing the Relationship: A Scatterplot’s Revelation\nTo gain an initial impression of the relationship between study hours and exam scores, let’s create a scatterplot. This simple yet powerful visualization will reveal the underlying pattern in our data.\n\n# Create a scatterplot of exam scores versus study hours\nplot(\n  data$study_hours, \n  data$exam_scores, \n  main = \"Exam Scores vs. Study Hours\", \n  xlab = \"Study Hours\", \n  ylab = \"Exam Scores\"\n  )\n\n\n\n\nUpon examining the scatterplot, a hint of a non-linear relationship emerges. The data points don’t fall along a straight line, suggesting a more complex association between study hours and exam scores. This is where quadratic regression steps in.\n\n\nFitting the Quadratic Model: Capturing the Curve\nTo capture the curvature evident in our data, we’ll employ the lm() function in R to fit a quadratic regression model. This model incorporates a second-degree term, allowing it to represent curved relationships between variables.\n\n# Fit a quadratic regression model to the data\nquadratic_model &lt;- lm(exam_scores ~ study_hours + I(study_hours^2), data = data)\n\nThe I() function in the model formula ensures that the square of study hours is treated as a separate variable, enabling the model to capture the non-linearity.\n\n\nInterpreting the Model: Unraveling the Parameters\nNow that we’ve fitted the quadratic model, let’s delve into its parameters and understand their significance.\n\n# Summarize the quadratic regression model\nsummary(quadratic_model)\n\n\nCall:\nlm(formula = exam_scores ~ study_hours + I(study_hours^2), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2484 -3.7429 -0.1812  1.1464 13.6678 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -18.25364    6.18507  -2.951   0.0184 *  \nstudy_hours        6.74436    0.48551  13.891 6.98e-07 ***\nI(study_hours^2)  -0.10120    0.00746 -13.565 8.38e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.218 on 8 degrees of freedom\nMultiple R-squared:  0.9602,    Adjusted R-squared:  0.9502 \nF-statistic: 96.49 on 2 and 8 DF,  p-value: 2.51e-06\n\n\nThe output of the summary function provides valuable insights into the model’s performance and the significance of its parameters. It indicates the intercept, representing the predicted exam score when study hours are zero, and the coefficients for the linear and quadratic terms.\n\n\nVisualizing the Model: Bringing the Curve to Life\nTo fully appreciate the quadratic model’s ability to capture the non-linear relationship between study hours and exam scores, let’s visualize the model alongside the data points.\n\n# Calculate the predicted exam scores for a range of study hours\npredicted_scores &lt;- predict(\n  quadratic_model, \n  newdata = data.frame(\n    study_hours = seq(min(study_hours), \n                      max(study_hours), \n                      length.out = 100\n                      )\n    )\n  )\n\n# Plot the data points and the predicted scores\nplot(\n  data$study_hours, \n  data$exam_scores, \n  main = \"Exam Scores vs. Study Hours\", \n  xlab = \"Study Hours\", \n  ylab = \"Exam Scores\"\n  )\nlines(seq(min(study_hours), \n          max(study_hours), \n          length.out = 100), \n      predicted_scores, col = \"red\"\n      )\n\n\n\n\nThe resulting plot reveals the graceful curve of the quadratic model, fitting the data points closely. This visualization reinforces the model’s ability to capture the non-linear relationship between study hours and exam scores.\n\n\nYour Turn: Embarking on Your Own Quadratic Regression Adventure\nArmed with the knowledge and skills gained from this tutorial, you’re now ready to embark on your own quadratic regression adventures. Gather your data, fit the model, interpret the parameters, and visualize the results."
  },
  {
    "objectID": "posts/2023-11-20/index.html",
    "href": "posts/2023-11-20/index.html",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "",
    "text": "Hey folks, welcome back to another exciting R programming journey! Today, we’re diving into the fascinating world of exponential regression using base R. Exponential regression is a powerful tool, especially in the realm of data science, and we’ll walk through the process step by step. So, grab your coding hats, and let’s get started!"
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-1-your-data",
    "href": "posts/2023-11-20/index.html#step-1-your-data",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 1: Your Data",
    "text": "Step 1: Your Data\n\nYear &lt;- c(2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, \n          2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)\nPopulation &lt;- c(500, 550, 610, 680, 760, 850, 950, 1060, 1180, 1320, 1470, \n                1640, 1830, 2040, 2280, 2540, 2830, 3140, 3480, 3850)\n\ndf &lt;- data.frame(Year, Population)\n\nMake sure to replace “your_data.csv” with the actual file name and path of your dataset. This is the foundation of our analysis, so choose a dataset that suits your exponential regression exploration."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-2-explore-your-data",
    "href": "posts/2023-11-20/index.html#step-2-explore-your-data",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 2: Explore Your Data",
    "text": "Step 2: Explore Your Data\n\n# Take a sneak peek at your data\nhead(df)\n\n  Year Population\n1 2001        500\n2 2002        550\n3 2003        610\n4 2004        680\n5 2005        760\n6 2006        850\n\nsummary(df)\n\n      Year        Population    \n Min.   :2001   Min.   : 500.0  \n 1st Qu.:2006   1st Qu.: 827.5  \n Median :2010   Median :1395.0  \n Mean   :2010   Mean   :1678.0  \n 3rd Qu.:2015   3rd Qu.:2345.0  \n Max.   :2020   Max.   :3850.0  \n\n\nUnderstanding your data is crucial. The ‘head()’ function displays the first few rows, and ‘summary()’ gives you a statistical summary. Look for patterns that might indicate exponential growth or decay."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-3-plot-your-data",
    "href": "posts/2023-11-20/index.html#step-3-plot-your-data",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 3: Plot Your Data",
    "text": "Step 3: Plot Your Data\n\n# Create a scatter plot\nplot(\n  Year, \n  Population, \n  main = \"Exponential Regression\", \n  xlab = \"Independent Variable\", \n  ylab = \"Dependent Variable\"\n)\n\n\n\n\nVisualizing your data helps in identifying trends. A scatter plot is an excellent choice to see if there’s a potential exponential relationship."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-4-fit-exponential-model",
    "href": "posts/2023-11-20/index.html#step-4-fit-exponential-model",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 4: Fit Exponential Model",
    "text": "Step 4: Fit Exponential Model\n\n# Fit exponential regression model\nmodel &lt;- lm(log(Population) ~ Year, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = log(Population) ~ Year, data = df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0134745 -0.0032271  0.0008587  0.0037029  0.0108613 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.113e+02  4.637e-01  -455.7   &lt;2e-16 ***\nYear         1.087e-01  2.307e-04   471.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.005948 on 18 degrees of freedom\nMultiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 \nF-statistic: 2.221e+05 on 1 and 18 DF,  p-value: &lt; 2.2e-16\n\n\nHere, we take the logarithm of the dependent variable ‘y’ to linearize the relationship. This facilitates using linear regression to model the data."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-5-make-predictions",
    "href": "posts/2023-11-20/index.html#step-5-make-predictions",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 5: Make Predictions",
    "text": "Step 5: Make Predictions\n\n# Make predictions\nprediction_interval &lt;- exp(predict(\n  model, \n  newdata = df,\n  interval=\"prediction\",\n  level = 0.95\n  ))\n\nReplace ‘new_x’ with the values for which you want to predict ‘y’. The ‘exp()’ function is used to reverse the logarithmic transformation."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-6-visualize-results",
    "href": "posts/2023-11-20/index.html#step-6-visualize-results",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 6: Visualize Results",
    "text": "Step 6: Visualize Results\n\n# Plot the original data and the regression line\nplot(df$Year, df$Population, main=\"Exponential Regression\", xlab=\"Year\", \n     ylab=\"Population\", pch=19)\nlines(df$Year, prediction_interval[,1], col=\"red\", lty=2)\nlines(df$Year, prediction_interval[,2], col=\"blue\", lty=2)\nlines(df$Year, prediction_interval[,3], col=\"blue\", lty=2)\nlegend(\"topright\", legend=\"Exponential Regression\", col=\"red\", lwd=2)\n\n\n\n\nThis code adds the exponential regression line to your scatter plot. It’s a visual confirmation of how well your model fits the data."
  },
  {
    "objectID": "posts/2023-11-21/index.html",
    "href": "posts/2023-11-21/index.html",
    "title": "Logarithmic Regression in R: A Step-by-Step Guide with Prediction Intervals",
    "section": "",
    "text": "Introduction\nLogarithmic regression is a statistical technique used to model the relationship between a dependent variable and an independent variable when the relationship is logarithmic. In other words, it is used to model situations where the dependent variable changes at a decreasing rate as the independent variable increases.\nIn this blog post, we will guide you through the process of performing logarithmic regression in R, from data preparation to visualizing the results. We will also discuss how to calculate prediction intervals and plot them along with the regression line.\n\n\nStep 1: Data Preparation\nBefore diving into the analysis, it is essential to ensure that your data is properly formatted and ready for analysis. This may involve data cleaning, checking for missing values, and handling outliers.\n\n\nStep 2: Visualizing the Data\nA quick scatterplot of the dependent variable versus the independent variable can provide valuable insights into the relationship between the two variables. This will help you determine if a logarithmic regression model is appropriate for your data.\n\n# Load the data\nx &lt;- seq(from = 1, to = 100, by = 1)\ny &lt;- log(seq(from = 1000, to = 1, by = -10))\ny &lt;- y * exp(-0.05 * x)\ndata &lt;- data.frame(dependent = y, independent = x)\n\n# Create a scatterplot\nplot(data$independent, data$dependent)\n\n\n\n\n\n\nStep 3: Fitting the Logarithmic Regression Model\nThe lm() function in R can be used to fit a logarithmic regression model. The syntax for fitting a logarithmic regression model is as follows:\n\nmodel &lt;- lm(dependent ~ log(independent), data = data)\n\n\n\nStep 4: Evaluating the Model\nOnce the model has been fitted, it is important to evaluate its performance. There are several metrics that can be used to evaluate the performance of a logarithmic regression model, such as the coefficient of determination (R-squared) and the mean squared error (MSE).\n\nsummary(model)\n\n\nCall:\nlm(formula = dependent ~ log(independent), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.12938 -0.24849 -0.03559  0.23825  0.55343 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       7.70024    0.12087   63.71   &lt;2e-16 ***\nlog(independent) -1.76239    0.03221  -54.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2974 on 98 degrees of freedom\nMultiple R-squared:  0.9683,    Adjusted R-squared:  0.968 \nF-statistic:  2994 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nStep 5: Calculating Prediction Intervals\nPrediction intervals provide a range of values within which we expect the true value of the dependent variable to fall for a given value of the independent variable. There are several methods for calculating prediction intervals, but one common method is to use the predict() function in R.\n\nnewdata &lt;- data.frame(independent = seq(from = 1, to = 100, length.out = 1000))\n\npredictions &lt;- predict(model, \n                       newdata = newdata, \n                       interval = \"prediction\",\n                       level = 0.95)\n\n\n\nStep 6: Plotting the Predictions and Intervals\nPlotting the predictions and intervals along with the regression line can help visualize the relationship between the variables and the uncertainty in the predictions.\n\nplot(data$independent, data$dependent)\nlines(predictions[, 1] ~ newdata$independent, lwd = 2)\nmatlines(newdata$independent, predictions[, 2:3], lty = 2, lwd = 2)\n\n\n\n\n\n\nConclusion\nLogarithmic regression is a powerful statistical technique that can be used to model a variety of relationships between variables. By following the steps outlined in this blog post, you can implement logarithmic regression in R to gain valuable insights from your data.\n\n\nYou Try!!\nWe encourage you to try out logarithmic regression on your own data. Start by exploring the relationship between your variables using a scatterplot. Then, fit a logarithmic regression model using the lm() function and evaluate its performance using the summary() function. Finally, calculate prediction intervals and plot them along with the regression line to visualize the relationship between the variables and the uncertainty in the predictions."
  },
  {
    "objectID": "posts/2023-11-22/index.html",
    "href": "posts/2023-11-22/index.html",
    "title": "Unveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars",
    "section": "",
    "text": "If you’ve ever found yourself grappling with noisy data and yearning for a smoother representation, LOESS regression might be the enchanting solution you’re seeking. In this blog post, we’ll unravel the mysteries of LOESS regression using the power of R, and walk through a practical example using the iconic mtcars dataset."
  },
  {
    "objectID": "posts/2023-11-22/index.html#understanding-loess-the-basics",
    "href": "posts/2023-11-22/index.html#understanding-loess-the-basics",
    "title": "Unveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars",
    "section": "Understanding LOESS: The Basics",
    "text": "Understanding LOESS: The Basics\nNow, let’s delve into the heart of LOESS regression. In R, the magic happens with the loess() function. This function fits a smooth curve through your data, adjusting to the local characteristics.\n\n# Fit a LOESS model\nloess_model &lt;- loess(mpg ~ wt, data = mtcars)\n\nCongratulations, you’ve just cast the LOESS spell on the fuel efficiency and weight relationship of these iconic cars!"
  },
  {
    "objectID": "posts/2023-11-22/index.html#visualizing-the-enchantment",
    "href": "posts/2023-11-22/index.html#visualizing-the-enchantment",
    "title": "Unveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars",
    "section": "Visualizing the Enchantment",
    "text": "Visualizing the Enchantment\nWhat good is magic if you can’t see it? Let’s visualize the results with a compelling plot.\n\n# Generate predictions from the LOESS model\npredictions &lt;- predict(loess_model, newdata = mtcars)\npredictions &lt;- cbind(mtcars, predictions)\npredictions &lt;- predictions[order(predictions$wt), ]\n\n# Create a scatter plot of the original data\nplot(\n  predictions$wt,\n  predictions$mpg, \n  col = \"blue\", \n  main = \"LOESS Regression: Unveiling the Magic with mtcars\", \n  xlab = \"Weight (1000 lbs)\", \n  ylab = \"Miles Per Gallon\"\n)\n\n# Add the LOESS curve to the plot\nlines(predictions$predictions, col = \"red\", lwd = 2)\n\n\n\n\nBehold, as the red curve gracefully dances through the blue points, smoothing out the rough edges and revealing the underlying trends in the relationship between weight and fuel efficiency.\nNow, we did not specify any parameters for the loess() function, so it used the default values. Let’s take a look at the default parameters.\nloess(formula, data, weights, subset, na.action, model = FALSE,\n      span = 0.75, enp.target, degree = 2,\n      parametric = FALSE, drop.square = FALSE, normalize = TRUE,\n      family = c(\"gaussian\", \"symmetric\"),\n      method = c(\"loess\", \"model.frame\"),\n      control = loess.control(...), ...)\nIf you want to see the documentation in R you can use ?loess or help(loess). I have it here for you anyways but it is good to know how to check it on the fly:\nArguments formula - a formula specifying the numeric response and one to four numeric predictors (best specified via an interaction, but can also be specified additively). Will be coerced to a formula if necessary.\ndata - an optional data frame, list or environment (or object coercible by as.data.frame to a data frame) containing the variables in the model. If not found in data, the variables are taken from environment(formula), typically the environment from which loess is called.\nweights - optional weights for each case.\nsubset - an optional specification of a subset of the data to be used.\nna.action - the action to be taken with missing values in the response or predictors. The default is given by getOption(“na.action”).\nmodel - should the model frame be returned?\nspan - the parameter α which controls the degree of smoothing.\nenp.target - an alternative way to specify span, as the approximate equivalent number of parameters to be used.\ndegree - the degree of the polynomials to be used, normally 1 or 2. (Degree 0 is also allowed, but see the ‘Note’.)\nparametric - should any terms be fitted globally rather than locally? Terms can be specified by name, number or as a logical vector of the same length as the number of predictors.\ndrop.square - for fits with more than one predictor and degree = 2, should the quadratic term be dropped for particular predictors? Terms are specified in the same way as for parametric.\nnormalize - should the predictors be normalized to a common scale if there is more than one? The normalization used is to set the 10% trimmed standard deviation to one. Set to false for spatial coordinate predictors and others known to be on a common scale.\nfamily - if “gaussian” fitting is by least-squares, and if “symmetric” a re-descending M estimator is used with Tukey’s biweight function. Can be abbreviated.\nmethod - fit the model or just extract the model frame. Can be abbreviated.\ncontrol - control parameters: see loess.control.\n... - control parameters can also be supplied directly (if control is not specified).\nNow that we see we can set things like span and degree let’s try it out.\n\n# Create the data frame\ndf &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14), \n                 y=c(1, 4, 7, 13, 19, 24, 20, 15, 13, 11, 15, 18, 22, 27))\n\n# Fit LOESS regression models\nloess50 &lt;- loess(y ~ x, data=df, span=0.5)\nsmooth50 &lt;- predict(loess50)\nloess75 &lt;- loess(y ~ x, data=df, span=0.75)\nsmooth75 &lt;- predict(loess75)\nloess90 &lt;- loess(y ~ x, data=df, span=0.9)\nsmooth90 &lt;- predict(loess90)\nloess50_degree1 &lt;- loess(y ~ x, data=df, span=0.5, degree=1)\nsmooth50_degree1 &lt;- predict(loess50_degree1)\nloess50_degree2 &lt;- loess(y ~ x, data=df, span=0.5, degree=2)\nsmooth50_degree2 &lt;- predict(loess50_degree2)\n\n# Create scatterplot with each regression line overlaid\nplot(df$x, df$y, pch=19, main='Loess Regression Models')\nlines(smooth50, x=df$x, col='red')\nlines(smooth75, x=df$x, col='purple')\nlines(smooth90, x=df$x, col='blue')\nlines(smooth50_degree1, x=df$x, col='green')\nlines(smooth50_degree2, x=df$x, col='orange')"
  },
  {
    "objectID": "posts/2023-11-22/index.html#empowering-you-try-it-yourself",
    "href": "posts/2023-11-22/index.html#empowering-you-try-it-yourself",
    "title": "Unveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars",
    "section": "Empowering You: Try It Yourself!",
    "text": "Empowering You: Try It Yourself!\nNow comes the most exciting part – empowering you to wield the magic wand with the mtcars dataset or any other dataset of your choice. Encourage your readers to try the code on their own datasets, and witness the transformative power of LOESS regression.\n# Your readers can replace this with their own dataset\nuser_data &lt;- read.csv(\"user_dataset.csv\")\n\n# Fit a LOESS model on their data\nuser_loess_model &lt;- loess(Y ~ X, data = user_data)\n\n# Visualize the results\nuser_predictions &lt;- predict(user_loess_model, newdata = user_data)\nplot(user_data$X, user_data$Y, col = \"green\", main = \"Your Turn: Unleash LOESS Magic\", xlab = \"X\", ylab = \"Y\")\nlines(user_data$X, user_predictions, col = \"purple\", lwd = 2)"
  },
  {
    "objectID": "posts/2023-11-16/index.html",
    "href": "posts/2023-11-16/index.html",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "",
    "text": "My R package {healthyR.ts} has been updated to version 0.3.0; you can install it from either CRAN, r-universe or GitHub. Let’s go over some of the changes and improvements."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_log_ts---logging-time-series-data",
    "href": "posts/2023-11-16/index.html#util_log_ts---logging-time-series-data",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "1. util_log_ts() - Logging Time Series Data",
    "text": "1. util_log_ts() - Logging Time Series Data\nOne of the standout additions is the introduction of util_log_ts(). This function seems like a game-changer, providing a streamlined way to log time series data. This is incredibly useful, especially when dealing with extensive datasets, making the whole process more efficient and user-friendly. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_singlediff_ts---single-differences-for-time-series",
    "href": "posts/2023-11-16/index.html#util_singlediff_ts---single-differences-for-time-series",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "2. util_singlediff_ts() - Single Differences for Time Series",
    "text": "2. util_singlediff_ts() - Single Differences for Time Series\nThe addition of util_singlediff_ts() expands the toolkit, offering a function dedicated to handling single differences in time series data. This is valuable for various applications, such as identifying trends or preparing data for further analysis. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_doublediff_ts---double-differences-for-time-series",
    "href": "posts/2023-11-16/index.html#util_doublediff_ts---double-differences-for-time-series",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "3. util_doublediff_ts() - Double Differences for Time Series",
    "text": "3. util_doublediff_ts() - Double Differences for Time Series\nBuilding on the concept of differencing, util_doublediff_ts() seems to provide a higher level of sophistication, allowing users to perform double differences on time series data. This could be pivotal in cases where a more refined analysis is required. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_difflog_ts---combining-differences-and-log-transformation",
    "href": "posts/2023-11-16/index.html#util_difflog_ts---combining-differences-and-log-transformation",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "4. util_difflog_ts() - Combining Differences and Log Transformation",
    "text": "4. util_difflog_ts() - Combining Differences and Log Transformation\nThe fusion of differencing and log transformation in util_difflog_ts() is a remarkable addition. This could be particularly beneficial in scenarios where both operations are needed to unlock deeper insights from the time series data. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_doubledifflog_ts---double-differences-with-log-transformation",
    "href": "posts/2023-11-16/index.html#util_doubledifflog_ts---double-differences-with-log-transformation",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "5. util_doubledifflog_ts() - Double Differences with Log Transformation",
    "text": "5. util_doubledifflog_ts() - Double Differences with Log Transformation\nThe introduction of util_doubledifflog_ts() appears to take things a step further by combining double differences and log transformation. This function seems poised to provide a comprehensive solution for users dealing with complex time series data. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#attributes-enhancement-in-ts_growth_rate_vec",
    "href": "posts/2023-11-16/index.html#attributes-enhancement-in-ts_growth_rate_vec",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "1. Attributes Enhancement in ts_growth_rate_vec()",
    "text": "1. Attributes Enhancement in ts_growth_rate_vec()\nThe attention to detail is evident with the addition of attributes to the output of ts_growth_rate_vec(). This enhancement not only improves the clarity of results but also contributes to a more informative and user-friendly experience."
  },
  {
    "objectID": "posts/2023-11-16/index.html#refinement-of-auto_stationarize-in-response-to-user-feedback",
    "href": "posts/2023-11-16/index.html#refinement-of-auto_stationarize-in-response-to-user-feedback",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "2. Refinement of auto_stationarize() in Response to User Feedback",
    "text": "2. Refinement of auto_stationarize() in Response to User Feedback\nUpdates to auto_stationarize() based on user feedback (Fix #481 #483) demonstrate a commitment to refining existing features. This responsiveness to the community’s needs is commendable and ensures that the package evolves in sync with user expectations. It has taken all of the util_ transforms mentioned above in order to improve it’s functionality."
  },
  {
    "objectID": "posts/2023-11-16/index.html#integration-with-auto_arima-engine-in-ts_auto_arima",
    "href": "posts/2023-11-16/index.html#integration-with-auto_arima-engine-in-ts_auto_arima",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "3. Integration with auto_arima Engine in ts_auto_arima()",
    "text": "3. Integration with auto_arima Engine in ts_auto_arima()\nThe integration of ts_auto_arima() with the parsnip engine of auto_arima is a notable improvement. This update, triggered when .tune is set to FALSE, aligns the package with cutting-edge tools, potentially enhancing the efficiency and accuracy of time series modeling.\nIn conclusion, the release of healthyR.ts version 0.3.0 is an exciting leap forward. The new features introduce powerful capabilities, while the minor fixes and improvements showcase a commitment to providing a robust and user-friendly package. Users can look forward to a more versatile and refined experience in time series analysis. Great job on this release, and I’m sure the community is eager to explore these enhancements!"
  },
  {
    "objectID": "posts/2023-11-16/index.html#auto_stationarize",
    "href": "posts/2023-11-16/index.html#auto_stationarize",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "auto_stationarize()",
    "text": "auto_stationarize()\n\nlibrary(healthyR.ts)\n\nauto_stationarize(AirPassengers)\n\nThe time series is already stationary via ts_adf_test().\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nauto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n$stationary_ts\nTime Series:\nStart = 3 \nEnd = 150 \nFrequency = 1 \n  [1]  0.5 -0.4  0.6  1.1 -2.8  3.0 -1.1  0.6 -0.5 -0.5  0.1  2.0 -0.6  0.8  1.2\n [16] -3.4 -0.7 -0.3  1.7  3.0 -3.2  0.9  2.2 -2.5 -0.4  2.6 -4.3  2.0 -3.1  2.7\n [31] -2.1  0.1  2.1 -0.2 -2.2  0.6  1.0 -2.6  3.0  0.3  0.2 -0.8  1.0  0.0  3.2\n [46] -2.2 -4.7  1.2  0.8 -0.6 -0.4  0.6  1.0 -1.6 -0.1  3.4 -0.9 -1.7 -0.5  0.8\n [61]  2.4 -1.9  0.6 -2.2  2.6 -0.1 -2.7  1.7 -0.3  1.9 -2.7  1.1 -0.6  0.9  0.0\n [76]  1.8 -0.5 -0.4 -1.2  2.6 -1.8  1.7 -0.9  0.6 -0.4  3.0 -2.8  3.1 -2.3 -1.1\n [91]  2.1 -0.3 -1.7 -0.8 -0.4  1.1 -1.5  0.3  1.4 -2.0  1.3 -0.3  0.4 -3.5  1.1\n[106]  2.6  0.4 -1.3  2.0 -1.6  0.6 -0.1 -1.4  1.6  1.6 -3.4  1.7 -2.2  2.1 -2.0\n[121] -0.2  0.2  0.7 -1.4  1.8 -0.1 -0.7  0.4  0.4  1.0 -2.4  1.0 -0.4  0.8 -1.0\n[136]  1.4 -1.2  1.1 -0.9  0.5  1.9 -0.6  0.3 -1.4 -0.9 -0.5  1.4  0.1\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -6.562008\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"double_diff\"\n\n$ret\n[1] TRUE\n\nplot.ts(auto_stationarize(BJsales)$stationary_ts)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n\n\nauto_stationarize(BJsales.lead)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n$stationary_ts\nTime Series:\nStart = 2 \nEnd = 150 \nFrequency = 1 \n  [1]  0.06  0.25 -0.57  0.58 -0.20  0.23 -0.04 -0.19  0.03  0.42  0.04  0.24\n [13]  0.34 -0.46 -0.18 -0.08  0.29  0.56 -0.37  0.20  0.54 -0.31  0.03  0.52\n [25] -0.70  0.35 -0.63  0.44 -0.38 -0.01  0.22  0.10 -0.50  0.01  0.30 -0.76\n [37]  0.52  0.15  0.06 -0.10  0.21 -0.01  0.70 -0.22 -0.76  0.06  0.02 -0.17\n [49] -0.08  0.01  0.11 -0.39  0.01  0.50 -0.02 -0.37 -0.13  0.05  0.54 -0.46\n [61]  0.25 -0.52  0.44  0.02 -0.47  0.11  0.06  0.25 -0.35  0.00 -0.06  0.21\n [73] -0.09  0.36  0.09 -0.04 -0.20  0.44 -0.23  0.40 -0.01  0.17  0.08  0.58\n [85] -0.27  0.79 -0.21  0.02  0.30  0.28 -0.27 -0.01  0.03  0.16 -0.28  0.15\n [97]  0.26 -0.36  0.32 -0.11  0.22 -0.65  0.00  0.47  0.16 -0.19  0.48 -0.26\n[109]  0.21  0.00 -0.20  0.35  0.38 -0.48  0.20 -0.32  0.43 -0.50  0.12 -0.17\n[121]  0.15 -0.36  0.35 -0.03 -0.18  0.16  0.07  0.21 -0.50  0.23 -0.13  0.14\n[133] -0.15  0.19 -0.24  0.26 -0.22  0.17  0.37 -0.06  0.29 -0.34 -0.12 -0.16\n[145]  0.25  0.08 -0.07  0.26 -0.37\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -4.838625\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"diff\"\n\n$ret\n[1] TRUE\n\nplot.ts(auto_stationarize(BJsales.lead)$stationary_ts)\n\nThe time series is not stationary. Attempting to make it stationary..."
  },
  {
    "objectID": "posts/2023-11-16/index.html#ts_auto_arima",
    "href": "posts/2023-11-16/index.html#ts_auto_arima",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "ts_auto_arima()",
    "text": "ts_auto_arima()\nThis use to only use the Arima engine if the .tune parameter was set to FALSE, thus it would many times give a simple straight line forecast. This was changed to make the engine auto_arima if .tune is set to FALSE.\n\nlibrary(timetk)\nlibrary(dplyr)\nlibrary(modeltime)\n\ndata &lt;- AirPassengers |&gt;\n  ts_to_tbl() |&gt;\n  select(-index)\n\nsplits &lt;- time_series_split(\n  data\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\nts_aa &lt;- ts_auto_arima(\n  .data = data,\n  .num_cores = 2,\n  .date_col = date_col,\n  .value_col = value,\n  .rsamp_obj = splits,\n  .formula = value ~ .,\n  .grid_size = 5,\n  .cv_slice_limit = 2,\n  .tune = FALSE\n)\n\nts_aa$recipe_info\n\n$recipe_call\nrecipe(.data = data, .date_col = date_col, .value_col = value, \n    .formula = value ~ ., .rsamp_obj = splits, .tune = FALSE, \n    .grid_size = 5, .num_cores = 2, .cv_slice_limit = 2)\n\n$recipe_syntax\n[1] \"ts_arima_recipe &lt;-\"                                                                                                                                                                           \n[2] \"\\n  recipe(.data = data, .date_col = date_col, .value_col = value, .formula = value ~ \\n    ., .rsamp_obj = splits, .tune = FALSE, .grid_size = 5, .num_cores = 2, \\n    .cv_slice_limit = 2)\"\n\n$rec_obj\n\nts_aa$model_info\n\n$model_spec\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nSeries: outcome \nARIMA(1,1,0)(0,1,0)[12] \n\nCoefficients:\n          ar1\n      -0.2431\ns.e.   0.0894\n\nsigma^2 = 109.8:  log likelihood = -447.95\nAIC=899.9   AICc=900.01   BIC=905.46\n\n$was_tuned\n[1] \"not_tuned\"\n\nts_aa$model_calibration\n\n$plot\n\n$calibration_tbl\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc             .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;workflow&gt; ARIMA(1,1,0)(0,1,0)[12] Test  &lt;tibble [12 × 4]&gt;\n\n$model_accuracy\n# A tibble: 1 × 9\n  .model_id .model_desc             .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 ARIMA(1,1,0)(0,1,0)[12] Test   18.5  4.18 0.384  4.03  23.9 0.955\n\nts_aa$model_calibration$plot\n\n\n\n\n\nFinally enhancement to add attributes to ts_growth_rate_vec()\n\nts_growth_rate_vec(AirPassengers)\n\n  [1]          NA   5.3571429  11.8644068  -2.2727273  -6.2015504  11.5702479\n  [7]   9.6296296   0.0000000  -8.1081081 -12.5000000 -12.6050420  13.4615385\n [13]  -2.5423729   9.5652174  11.9047619  -4.2553191  -7.4074074  19.2000000\n [19]  14.0939597   0.0000000  -7.0588235 -15.8227848 -14.2857143  22.8070175\n [25]   3.5714286   3.4482759  18.6666667  -8.4269663   5.5214724   3.4883721\n [31]  11.7977528   0.0000000  -7.5376884 -11.9565217  -9.8765432  13.6986301\n [37]   3.0120482   5.2631579   7.2222222  -6.2176166   1.1049724  19.1256831\n [43]   5.5045872   5.2173913 -13.6363636  -8.6124402  -9.9476440  12.7906977\n [49]   1.0309278   0.0000000  20.4081633  -0.4237288  -2.5531915   6.1135371\n [55]   8.6419753   3.0303030 -12.8676471 -10.9704641 -14.6919431  11.6666667\n [61]   1.4925373  -7.8431373  25.0000000  -3.4042553   3.0837004  12.8205128\n [67]  14.3939394  -2.9801325 -11.6040956 -11.5830116 -11.3537118  12.8078818\n [73]   5.6768559  -3.7190083  14.5922747   0.7490637   0.3717472  16.6666667\n [79]  15.5555556  -4.6703297 -10.0864553 -12.1794872 -13.5036496  17.2995781\n [85]   2.1582734  -2.4647887  14.4404332  -1.2618297   1.5974441  17.6100629\n [91]  10.4278075  -1.9370460 -12.3456790 -13.8028169 -11.4379085  12.9151292\n [97]   2.9411765  -4.4444444  18.2724252  -2.2471910   2.0114943  18.8732394\n[103]  10.1895735   0.4301075 -13.4903640 -14.1089109 -12.1037464  10.1639344\n[109]   1.1904762  -6.4705882  13.8364780  -3.8674033   4.3103448  19.8347107\n[115]  12.8735632   2.8513238 -20.0000000 -11.1386139 -13.6490251   8.7096774\n[121]   6.8249258  -5.0000000  18.7134503  -2.4630542   6.0606061  12.3809524\n[127]  16.1016949   2.0072993 -17.1735242 -12.0950324 -11.0565111  11.8784530\n[133]   2.9629630  -6.2350120   7.1611253  10.0238663   2.3861171  13.3474576\n[139]  16.2616822  -2.5723473 -16.1716172  -9.2519685 -15.4013015  10.7692308\nattr(,\"vector_attributes\")\nattr(,\"vector_attributes\")$tsp\n[1] 1949.000 1960.917   12.000\n\nattr(,\"vector_attributes\")$class\n[1] \"ts\"\n\nattr(,\"name\")\n[1] \"AirPassengers\""
  },
  {
    "objectID": "posts/2023-11-27/index.html",
    "href": "posts/2023-11-27/index.html",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "",
    "text": "In the realm of statistics, power regression stands out as a versatile tool for exploring the relationship between two variables, where one variable is the power of the other. This type of regression is particularly useful when there’s an inherent nonlinear relationship between the variables, often characterized by an exponential or inverse relationship.\nPower regression takes the form of y = ax^b, where:\n\ny: The response variable, the quantity we’re trying to predict\nx: The predictor variable, the quantity we’re using to make predictions\na: The intercept, the value of y when x = 1\nb: The power coefficient, which determines the rate at which y changes as x increases or decreases"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-1-gathering-the-data",
    "href": "posts/2023-11-27/index.html#step-1-gathering-the-data",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 1: Gathering the Data",
    "text": "Step 1: Gathering the Data\nTo embark on our power regression journey, we’ll need some data to work with. Let’s simulate a dataset that exhibits an exponential relationship between two variables:\n\n# Simulate data\nx &lt;- seq(1, 100, 1)\ny &lt;- 2 * x^3 + rnorm(100)"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-2-visualizing-the-data",
    "href": "posts/2023-11-27/index.html#step-2-visualizing-the-data",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 2: Visualizing the Data",
    "text": "Step 2: Visualizing the Data\nBefore diving into the regression analysis, it’s crucial to visualize the data to gain a deeper understanding of the underlying relationship between the variables. A scatterplot can effectively reveal any patterns or trends in the data.\n\n# Create scatterplot\nplot(x, y)"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-3-transforming-the-data",
    "href": "posts/2023-11-27/index.html#step-3-transforming-the-data",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 3: Transforming the Data",
    "text": "Step 3: Transforming the Data\nSince power regression assumes a nonlinear relationship between the variables, we need to transform the data to fit the model’s structure. This involves taking the logarithm of both sides of the power regression equation:\n\n# Transform data\nlog_y &lt;- log(y)\nlog_x &lt;- log(x)"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-4-fitting-the-power-regression-model",
    "href": "posts/2023-11-27/index.html#step-4-fitting-the-power-regression-model",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 4: Fitting the Power Regression Model",
    "text": "Step 4: Fitting the Power Regression Model\nNow that the data is suitably transformed, we can proceed with fitting the power regression model using the lm() function in R:\n\n# Fit power regression model\nmodel &lt;- lm(log_y ~ log_x)"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-5-examining-the-model-results",
    "href": "posts/2023-11-27/index.html#step-5-examining-the-model-results",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 5: Examining the Model Results",
    "text": "Step 5: Examining the Model Results\nThe summary() function provides valuable insights into the model’s performance, including the estimated regression coefficients, their standard errors, and the p-values associated with each coefficient.\n\n# Summarize model results\nsummary(model)\n\n\nCall:\nlm(formula = log_y ~ log_x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10472 -0.01800  0.00221  0.01433  0.61505 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.812631   0.027647   29.39   &lt;2e-16 ***\nlog_x       2.969125   0.007367  403.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06803 on 98 degrees of freedom\nMultiple R-squared:  0.9994,    Adjusted R-squared:  0.9994 \nF-statistic: 1.624e+05 on 1 and 98 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-6-visualizing-the-fitted-model",
    "href": "posts/2023-11-27/index.html#step-6-visualizing-the-fitted-model",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 6: Visualizing the Fitted Model",
    "text": "Step 6: Visualizing the Fitted Model\nVisualizing the fitted model allows us to evaluate how well the model captures the underlying relationship between the variables. We can add the fitted model to the scatterplot using the predict() function (don’t forget to exponentiate!):\n\n# Predict fitted values\nfitted_values &lt;- predict(model, newdata = data.frame(x = x),\n                        interval = \"prediction\",\n                        level = 0.95)\n\n# Add fitted model to scatterplot\nplot(x, y)\nlines(x, exp(fitted_values[, 1]), col = \"red\")"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-7-calculating-prediction-intervals",
    "href": "posts/2023-11-27/index.html#step-7-calculating-prediction-intervals",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 7: Calculating Prediction Intervals",
    "text": "Step 7: Calculating Prediction Intervals\nPrediction intervals provide a range of plausible values for the response variable at a given level of confidence. We calculated the prediction intervals using the predict() function above:\n\n# Add fitted model to scatterplot\nplot(x, y)\nlines(x, exp(fitted_values[, 1]), col = \"red\")\n\n# Add prediction intervals to scatterplot\nlines(x, exp(fitted_values[, 2]), col = \"blue\", lty = 2)\nlines(x, exp(fitted_values[, 3]), col = \"blue\", lty = 2)"
  },
  {
    "objectID": "posts/2023-11-28/index.html",
    "href": "posts/2023-11-28/index.html",
    "title": "Understanding and Implementing Robust Regression in R",
    "section": "",
    "text": "If you’re familiar with linear regression in R, you’ve probably encountered the traditional lm() function. While this is a powerful tool, it might not be the best choice when dealing with outliers or influential observations. In such cases, robust regression comes to the rescue, and in R, the rlm() function from the MASS package is a valuable resource. In this blog post, we’ll delve into the step-by-step process of performing robust regression in R, using a dataset to illustrate the differences between the base R lm model and the robust rlm model."
  },
  {
    "objectID": "posts/2023-11-28/index.html#traditional-linear-regression-lm",
    "href": "posts/2023-11-28/index.html#traditional-linear-regression-lm",
    "title": "Understanding and Implementing Robust Regression in R",
    "section": "Traditional Linear Regression (lm)",
    "text": "Traditional Linear Regression (lm)\n\n# Fit the lm model\nlm_model &lt;- lm(y ~ x1 + x2, data = df)\n\n# Print the summary\nsummary(lm_model)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-72.020 -27.290  -0.138   4.487 124.144 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   41.106     29.940   1.373    0.188\nx1            -0.605      2.066  -0.293    0.773\nx2             1.075      1.857   0.579    0.570\n\nResidual standard error: 49.42 on 17 degrees of freedom\nMultiple R-squared:  0.02203,   Adjusted R-squared:  -0.09303 \nF-statistic: 0.1914 on 2 and 17 DF,  p-value: 0.8275\n\n\nThe lm() function provides a standard linear regression model. However, it assumes that the data follows a normal distribution and is sensitive to outliers. This sensitivity can lead to biased coefficient estimates."
  },
  {
    "objectID": "posts/2023-11-28/index.html#robust-linear-regression-rlm",
    "href": "posts/2023-11-28/index.html#robust-linear-regression-rlm",
    "title": "Understanding and Implementing Robust Regression in R",
    "section": "Robust Linear Regression (rlm)",
    "text": "Robust Linear Regression (rlm)\nNow, let’s contrast this with the robust approach using the rlm() function:\n\n# Load the MASS package\nlibrary(MASS)\n\n# Fit the rlm model\nrobust_model &lt;- rlm(y ~ x1 + x2, data = df)\n\n# Print the summary\nsummary(robust_model)\n\n\nCall: rlm(formula = y ~ x1 + x2, data = df)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.9296  -6.1604  -0.5812   6.4648 170.4612 \n\nCoefficients:\n            Value   Std. Error t value\n(Intercept) 21.4109  5.9703     3.5862\nx1           2.3077  0.4121     5.6004\nx2          -0.2449  0.3703    -0.6615\n\nResidual standard error: 9.369 on 17 degrees of freedom\n\n\nThe rlm() function, part of the MASS package, uses a robust M-estimation approach. It downplays the impact of outliers, making it more suitable for datasets with influential observations."
  },
  {
    "objectID": "posts/2023-11-29/index.html",
    "href": "posts/2023-11-29/index.html",
    "title": "Navigating Quantile Regression with R: A Comprehensive Guide",
    "section": "",
    "text": "Introduction\nQuantile regression is a robust statistical method that goes beyond traditional linear regression by allowing us to model the relationship between variables at different quantiles of the response distribution. In this blog post, we’ll explore how to perform quantile regression in R using the quantreg library.\n\n\nSetting the Stage\nFirst things first, let’s create some data to work with. We’ll generate a data frame df with two variables: ‘hours’ and ‘score’. The relationship between ‘hours’ and ‘score’ will have a bit of noise to make things interesting.\n\n# Create data frame\nhours &lt;- runif(100, 1, 10)\nscore &lt;- 60 + 2 * hours + rnorm(100, mean = 0, sd = 0.45 * hours)\ndf &lt;- data.frame(hours, score)\n\n\n\nVisualizing the Data\nBefore we jump into regression, it’s always a good idea to visualize our data. Let’s start with a scatter plot to get a sense of the relationship between hours and scores.\n\n# Scatter plot\nplot(df$hours, df$score, \n     main = \"Scatter Plot of Hours vs. Score\", \n     xlab = \"Hours\", ylab = \"Score\"\n     )\n\n\n\n\nNow that we’ve got a clear picture of our data, it’s time to perform quantile regression.\n\n\nQuantile Regression with quantreg\nWe’ll use the quantreg library to perform quantile regression. The key function here is rq() (Quantile Regression). We’ll run quantile regression for a few quantiles, say 0.25, 0.5, and 0.75.\n\n# Install and load quantreg if not already installed\n# install.packages(\"quantreg\")\nlibrary(quantreg)\n\n# Quantile regression\nquant_reg_25 &lt;- rq(score ~ hours, data = df, tau = 0.25)\nquant_reg_50 &lt;- rq(score ~ hours, data = df, tau = 0.5)\nquant_reg_75 &lt;- rq(score ~ hours, data = df, tau = 0.75)\n\npurrr::map(list(quant_reg_25, quant_reg_50, quant_reg_75), broom::tidy)\n\n[[1]]\n# A tibble: 2 × 5\n  term        estimate conf.low conf.high   tau\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)    60.3     59.0      61.1   0.25\n2 hours           1.56     1.33      1.82  0.25\n\n[[2]]\n# A tibble: 2 × 5\n  term        estimate conf.low conf.high   tau\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)    60.2     59.6      60.5    0.5\n2 hours           1.96     1.86      2.20   0.5\n\n[[3]]\n# A tibble: 2 × 5\n  term        estimate conf.low conf.high   tau\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)    59.9     59.5      60.7   0.75\n2 hours           2.36     2.16      2.53  0.75\n\npurrr::map(list(quant_reg_25, quant_reg_50, quant_reg_75), broom::glance)\n\n[[1]]\n# A tibble: 1 × 5\n    tau logLik      AIC   BIC df.residual\n  &lt;dbl&gt; &lt;logLik&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;\n1  0.25 -259.6364  523.  528.          98\n\n[[2]]\n# A tibble: 1 × 5\n    tau logLik      AIC   BIC df.residual\n  &lt;dbl&gt; &lt;logLik&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;\n1   0.5 -249.6752  503.  509.          98\n\n[[3]]\n# A tibble: 1 × 5\n    tau logLik      AIC   BIC df.residual\n  &lt;dbl&gt; &lt;logLik&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;\n1  0.75 -252.0106  508.  513.          98\n\n\n\n\nVisualizing Model Performance\nNow, let’s visualize how well our quantile regression models perform. We’ll overlay the regression lines on our scatter plot.\n\n# Scatter plot with regression lines\n# Scatter plot with regression lines\nplot(df$hours, df$score, \n     main = \"Quantile Regression: Hours vs. Score\", \n     xlab = \"Hours\", ylab = \"Score\")\nabline(a = coef(quant_reg_25), \n       b = coef(quant_reg_25)[\"hours\"], \n       col = \"red\", lty = 2)\nabline(a = coef(quant_reg_50), \n       b = coef(quant_reg_50)[\"hours\"], \n       col = \"blue\", lty = 2)\nabline(a = coef(quant_reg_75), \n       b = coef(quant_reg_75)[\"hours\"], \n       col = \"green\", lty = 2)\nlegend(\"topleft\", legend = c(\"Quantile 0.25\", \"Quantile 0.5\", \"Quantile 0.75\"),\n       col = c(\"red\", \"blue\", \"green\"), lty = 2)\n\n\n\n\n\n\nConclusion\nIn this blog post, we delved into the fascinating world of quantile regression using R and the quantreg library. We generated some synthetic data, visualized it, and then performed quantile regression at different quantiles. The final touch was overlaying the regression lines on our scatter plot to visualize how well our models fit the data.\nQuantile regression provides a more nuanced view of the relationship between variables, especially when dealing with skewed or non-normally distributed data. It’s a valuable tool in your statistical toolkit. Happy coding, and may your regressions be ever quantile-wise accurate!"
  },
  {
    "objectID": "posts/2023-12-01/index.html",
    "href": "posts/2023-12-01/index.html",
    "title": "tidyAML: Now supporting gee models",
    "section": "",
    "text": "I am happy to announce that a new version of tidyAML is now available on CRAN. This version includes support for gee models. This is a big step forward for tidyAML as it now supports a wide variety of regression and classification models."
  },
  {
    "objectID": "posts/2023-12-01/index.html#load-library",
    "href": "posts/2023-12-01/index.html#load-library",
    "title": "tidyAML: Now supporting gee models",
    "section": "Load Library",
    "text": "Load Library\n\nlibrary(tidyAML)\n\nNow, let’s build a model that will fail, it’s important I think to see the failure message so you can understand what is happening. It’s likely because the library is not loaded, let’s face it, it has happened to all of us.\n\nlibrary(recipes)\nlibrary(dplyr)\n\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\nfrt_tbl &lt;- fast_regression(\n  mtcars, \n  rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(frt_tbl)\n\nRows: 3\nColumns: 8\n$ .model_id       &lt;int&gt; 1, 2, 3\n$ .parsnip_engine &lt;chr&gt; \"lm\", \"gee\", \"glm\"\n$ .parsnip_mode   &lt;chr&gt; \"regression\", \"regression\", \"regression\"\n$ .parsnip_fns    &lt;chr&gt; \"linear_reg\", \"linear_reg\", \"linear_reg\"\n$ model_spec      &lt;list&gt; [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            &lt;list&gt; [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     &lt;list&gt; [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       &lt;list&gt; [&lt;tbl_df[8 x 1]&gt;], &lt;NULL&gt;, [&lt;tbl_df[8 x 1]&gt;]\n\nfrt_tbl |&gt; pull(pred_wflw) \n\n[[1]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1 22.9 \n2 18.0 \n3 21.1 \n4 32.9 \n5 18.5 \n6 10.4 \n7  9.59\n8 24.7 \n\n[[2]]\nNULL\n\n[[3]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1 22.9 \n2 18.0 \n3 21.1 \n4 32.9 \n5 18.5 \n6 10.4 \n7  9.59\n8 24.7 \n\nfrt_tbl |&gt; pull(fitted_wflw) |&gt; purrr::map(broom::tidy)\n\n[[1]]\n# A tibble: 11 × 5\n   term         estimate std.error statistic p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -40.3       30.5       -1.32   0.209 \n 2 cyl           0.907      1.09       0.830  0.421 \n 3 disp          0.0105     0.0189     0.557  0.587 \n 4 hp           -0.00487    0.0248    -0.196  0.847 \n 5 drat          1.73       2.04       0.846  0.413 \n 6 wt           -5.71       2.35      -2.43   0.0302\n 7 qsec          3.39       1.34       2.54   0.0248\n 8 vs           -3.85       2.99      -1.29   0.220 \n 9 am            2.16       2.29       0.942  0.364 \n10 gear          1.40       1.68       0.838  0.417 \n11 carb          0.200      0.835      0.239  0.815 \n\n[[2]]\n# A tibble: 0 × 0\n\n[[3]]\n# A tibble: 11 × 5\n   term         estimate std.error statistic p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -40.3       30.5       -1.32   0.209 \n 2 cyl           0.907      1.09       0.830  0.421 \n 3 disp          0.0105     0.0189     0.557  0.587 \n 4 hp           -0.00487    0.0248    -0.196  0.847 \n 5 drat          1.73       2.04       0.846  0.413 \n 6 wt           -5.71       2.35      -2.43   0.0302\n 7 qsec          3.39       1.34       2.54   0.0248\n 8 vs           -3.85       2.99      -1.29   0.220 \n 9 am            2.16       2.29       0.942  0.364 \n10 gear          1.40       1.68       0.838  0.417 \n11 carb          0.200      0.835      0.239  0.815 \n\n\nWe see that the gee model failed. This is because we did not load the multilevelmod package. Let’s load it and try again.\n\nlibrary(multilevelmod)\n\nfrt_tbl &lt;- fast_regression(\n  mtcars, \n  rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(frt_tbl)\n\nRows: 3\nColumns: 8\n$ .model_id       &lt;int&gt; 1, 2, 3\n$ .parsnip_engine &lt;chr&gt; \"lm\", \"gee\", \"glm\"\n$ .parsnip_mode   &lt;chr&gt; \"regression\", \"regression\", \"regression\"\n$ .parsnip_fns    &lt;chr&gt; \"linear_reg\", \"linear_reg\", \"linear_reg\"\n$ model_spec      &lt;list&gt; [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            &lt;list&gt; [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     &lt;list&gt; [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       &lt;list&gt; [&lt;tbl_df[8 x 1]&gt;], [&lt;tbl_df[8 x 1]&gt;], [&lt;tbl_df[8 x 1]&gt;…\n\nfrt_tbl |&gt; pull(pred_wflw) \n\n[[1]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1  20.6\n2  15.6\n3  15.8\n4  26.1\n5  27.8\n6  16.6\n7  25.4\n8  21.6\n\n[[2]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1  21.3\n2  15.2\n3  15.3\n4  25.7\n5  27.3\n6  16.5\n7  25.7\n8  20.5\n\n[[3]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1  20.6\n2  15.6\n3  15.8\n4  26.1\n5  27.8\n6  16.6\n7  25.4\n8  21.6\n\nfrt_tbl |&gt; pull(fitted_wflw) |&gt; purrr::map(broom::tidy)\n\n[[1]]\n# A tibble: 11 × 5\n   term        estimate std.error statistic p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -3.70      18.9       -0.195  0.848 \n 2 cyl          0.668      1.03       0.647  0.529 \n 3 disp         0.00831    0.0153     0.544  0.596 \n 4 hp          -0.0124     0.0173    -0.717  0.486 \n 5 drat         2.87       1.44       2.00   0.0674\n 6 wt          -2.87       1.40      -2.05   0.0609\n 7 qsec         0.777      0.618      1.26   0.231 \n 8 vs           0.169      1.64       0.103  0.920 \n 9 am           1.90       1.79       1.07   0.306 \n10 gear         1.31       1.44       0.907  0.381 \n11 carb        -0.601      0.730     -0.823  0.425 \n\n[[2]]\n# A tibble: 10 × 6\n   term         estimate std.error statistic p.value        ``\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  6.54       10.2     0.643     4.01    1.63    \n 2 disp         0.0119      0.0140  0.849     0.0134  0.887   \n 3 hp          -0.0149      0.0165 -0.901     0.0104 -1.43    \n 4 drat         2.36        1.18    2.01      0.859   2.75    \n 5 wt          -3.01        1.35   -2.23      1.35   -2.23    \n 6 qsec         0.577       0.524   1.10      0.193   2.99    \n 7 vs           0.000922    1.58    0.000582  1.07    0.000860\n 8 am           1.35        1.54    0.880     0.886   1.53    \n 9 gear         1.00        1.33    0.752     0.527   1.90    \n10 carb        -0.355       0.611  -0.582     0.378  -0.940   \n\n[[3]]\n# A tibble: 11 × 5\n   term        estimate std.error statistic p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -3.70      18.9       -0.195  0.848 \n 2 cyl          0.668      1.03       0.647  0.529 \n 3 disp         0.00831    0.0153     0.544  0.596 \n 4 hp          -0.0124     0.0173    -0.717  0.486 \n 5 drat         2.87       1.44       2.00   0.0674\n 6 wt          -2.87       1.40      -2.05   0.0609\n 7 qsec         0.777      0.618      1.26   0.231 \n 8 vs           0.169      1.64       0.103  0.920 \n 9 am           1.90       1.79       1.07   0.306 \n10 gear         1.31       1.44       0.907  0.381 \n11 carb        -0.601      0.730     -0.823  0.425"
  },
  {
    "objectID": "posts/2023-12-04/index.html",
    "href": "posts/2023-12-04/index.html",
    "title": "Understanding Spline Regression",
    "section": "",
    "text": "Spline regression is particularly useful when the relationship between the independent and dependent variables is not adequately captured by a linear model. It involves fitting a piecewise continuous curve (spline) to the data. Let’s dive into the process using R."
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-1-load-the-necessary-libraries",
    "href": "posts/2023-12-04/index.html#step-1-load-the-necessary-libraries",
    "title": "Understanding Spline Regression",
    "section": "Step 1: Load the Necessary Libraries",
    "text": "Step 1: Load the Necessary Libraries\n\n# Install and load the required libraries\n# install.packages(\"splines\")\nlibrary(splines)"
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-2-generate-sample-data",
    "href": "posts/2023-12-04/index.html#step-2-generate-sample-data",
    "title": "Understanding Spline Regression",
    "section": "Step 2: Generate Sample Data",
    "text": "Step 2: Generate Sample Data\nFor our example, let’s create a hypothetical dataset:\n\n# Generate sample data\nset.seed(123)\nx &lt;- seq(1, 10, length.out = 100)\ny &lt;- 3 * sin(x) + rnorm(100, mean = 0, sd = 0.5)"
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-3-fit-a-spline-regression-model",
    "href": "posts/2023-12-04/index.html#step-3-fit-a-spline-regression-model",
    "title": "Understanding Spline Regression",
    "section": "Step 3: Fit a Spline Regression Model",
    "text": "Step 3: Fit a Spline Regression Model\nNow, let’s fit a spline regression model to our data:\n\n# Fit a spline regression model\nspline_model &lt;- lm(y ~ ns(x, df = 4))\n\nHere, ns from the splines package is used to create a natural spline basis with 4 degrees of freedom."
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-4-visualize-the-results",
    "href": "posts/2023-12-04/index.html#step-4-visualize-the-results",
    "title": "Understanding Spline Regression",
    "section": "Step 4: Visualize the Results",
    "text": "Step 4: Visualize the Results\nVisualizing the data and the fitted spline is crucial for understanding the model’s performance:\n\n# Visualize the data and fitted spline\nplot(x, y, main = \"Spline Regression Example\", xlab = \"X\", ylab = \"Y\")\nlines(x, predict(spline_model), col = \"red\", lwd = 2)\nlegend(\"topright\", legend = \"Fitted Spline\", col = \"red\", lwd = 2)\n\n\n\n\nThis code generates a plot with the original data points and overlays the fitted spline."
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-5-examine-residuals",
    "href": "posts/2023-12-04/index.html#step-5-examine-residuals",
    "title": "Understanding Spline Regression",
    "section": "Step 5: Examine Residuals",
    "text": "Step 5: Examine Residuals\nChecking residuals helps assess the model’s goodness of fit:\n\n# Examine residuals\nresiduals &lt;- residuals(spline_model)\nplot(x, residuals, main = \"Residuals of Spline Regression\", xlab = \"X\", \n     ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\nThis plot shows the residuals (the differences between observed and predicted values) against the independent variable."
  },
  {
    "objectID": "posts/2023-12-05/index.html",
    "href": "posts/2023-12-05/index.html",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "",
    "text": "Hey folks! 👋 Today, let’s embark on a coding adventure and explore the fascinating world of Polynomial Regression in R. Whether you’re new to R or a seasoned coder, we’re going to break down the complexities and make this journey enjoyable and insightful."
  },
  {
    "objectID": "posts/2023-12-05/index.html#step-1-set-the-stage",
    "href": "posts/2023-12-05/index.html#step-1-set-the-stage",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "Step 1: Set the Stage",
    "text": "Step 1: Set the Stage\nFirst things first, fire up your RStudio and load your favorite dataset. For our journey, I’ll use a hypothetical dataset about, say, the growth of healthyR packages over the years.\n\n# Assume 'years' and 'growth' are our dataset columns\ndata &lt;- data.frame(years = c(1, 2, 3, 4, 5),\n                   growth = c(10, 25, 40, 60, 90))\n\n# Visualize the data\nplot(data$years, data$growth, \n     main = \"HealthyR Package Growth Over the Years\",\n     xlab = \"Years\", ylab = \"Growth\", col = \"blue\", pch = 16)"
  },
  {
    "objectID": "posts/2023-12-05/index.html#step-2-lets-fit-a-polynomial",
    "href": "posts/2023-12-05/index.html#step-2-lets-fit-a-polynomial",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "Step 2: Let’s Fit a Polynomial",
    "text": "Step 2: Let’s Fit a Polynomial\nNow, let’s fit a polynomial regression model to our data. We’ll use the lm() function, and don’t worry, it’s simpler than it sounds!\n\n# Fit a polynomial regression model (let's go quadratic)\nmodel &lt;- lm(growth ~ poly(years, 2), data = data)"
  },
  {
    "objectID": "posts/2023-12-05/index.html#step-3-visualize-the-magic",
    "href": "posts/2023-12-05/index.html#step-3-visualize-the-magic",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "Step 3: Visualize the Magic",
    "text": "Step 3: Visualize the Magic\nTime to visualize the results. We’ll create a smooth curve representing our polynomial fit, compare it against the actual data, and also peek at the residuals.\n\n# Generate points for smooth curve\ncurve_data &lt;- data.frame(years = seq(1, 5, length.out = 100))\n\n# Predict growth based on the model\npredictions &lt;- predict(model, newdata = curve_data)\n\n# The data\nplot(data$years, data$growth, \n     main = \"HealthyR Package Growth Over the Years\",\n     xlab = \"Years\", ylab = \"Growth\", col = \"blue\", pch = 16)\n# Visualize the fitted model\nlines(curve_data$years, predictions, col = \"red\", type = \"l\")"
  },
  {
    "objectID": "posts/2023-12-05/index.html#step-4-assess-residuals",
    "href": "posts/2023-12-05/index.html#step-4-assess-residuals",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "Step 4: Assess Residuals",
    "text": "Step 4: Assess Residuals\nTo ensure our model is doing its job, let’s examine the residuals. These are the differences between our predictions and the actual values.\n\n# Calculate residuals\nresiduals &lt;- residuals(model)\n\n# Visualize the residuals\nplot(data$years, residuals, main = \"Residuals Analysis\",\n     xlab = \"Years\", ylab = \"Residuals\", col = \"green\", pch = 16)\nabline(h = 0, col = \"red\", lty = 2)"
  },
  {
    "objectID": "posts/2023-12-06/index.html",
    "href": "posts/2023-12-06/index.html",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "",
    "text": "Stepwise regression is a powerful technique used to build predictive models by iteratively adding or removing variables based on statistical criteria. In R, this can be achieved using functions like step() or manually with forward and backward selection."
  },
  {
    "objectID": "posts/2023-12-06/index.html#forward-stepwise-regression",
    "href": "posts/2023-12-06/index.html#forward-stepwise-regression",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "Forward Stepwise Regression:",
    "text": "Forward Stepwise Regression:\n\n# Initialize an empty model\nforward_model &lt;- lm(mpg ~ ., data = mtcars)\n\n# Forward stepwise regression\nforward_model &lt;- step(forward_model, direction = \"forward\", scope = formula(~ .))\n\nStart:  AIC=70.9\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n\nIn simple terms, we start with a model containing no predictors (mpg ~ 1) and iteratively add the most statistically significant variables until no improvement is observed."
  },
  {
    "objectID": "posts/2023-12-06/index.html#backward-stepwise-regression",
    "href": "posts/2023-12-06/index.html#backward-stepwise-regression",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "Backward Stepwise Regression:",
    "text": "Backward Stepwise Regression:\n\n# Initialize a model with all predictors\nbackward_model &lt;- lm(mpg ~ ., data = mtcars)\n\n# Backward stepwise regression\nbackward_model &lt;- step(backward_model, direction = \"backward\")\n\nStart:  AIC=70.9\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- cyl   1    0.0799 147.57 68.915\n- vs    1    0.1601 147.66 68.932\n- carb  1    0.4067 147.90 68.986\n- gear  1    1.3531 148.85 69.190\n- drat  1    1.6270 149.12 69.249\n- disp  1    3.9167 151.41 69.736\n- hp    1    6.8399 154.33 70.348\n- qsec  1    8.8641 156.36 70.765\n&lt;none&gt;              147.49 70.898\n- am    1   10.5467 158.04 71.108\n- wt    1   27.0144 174.51 74.280\n\nStep:  AIC=68.92\nmpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- vs    1    0.2685 147.84 66.973\n- carb  1    0.5201 148.09 67.028\n- gear  1    1.8211 149.40 67.308\n- drat  1    1.9826 149.56 67.342\n- disp  1    3.9009 151.47 67.750\n- hp    1    7.3632 154.94 68.473\n&lt;none&gt;              147.57 68.915\n- qsec  1   10.0933 157.67 69.032\n- am    1   11.8359 159.41 69.384\n- wt    1   27.0280 174.60 72.297\n\nStep:  AIC=66.97\nmpg ~ disp + hp + drat + wt + qsec + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- carb  1    0.6855 148.53 65.121\n- gear  1    2.1437 149.99 65.434\n- drat  1    2.2139 150.06 65.449\n- disp  1    3.6467 151.49 65.753\n- hp    1    7.1060 154.95 66.475\n&lt;none&gt;              147.84 66.973\n- am    1   11.5694 159.41 67.384\n- qsec  1   15.6830 163.53 68.200\n- wt    1   27.3799 175.22 70.410\n\nStep:  AIC=65.12\nmpg ~ disp + hp + drat + wt + qsec + am + gear\n\n       Df Sum of Sq    RSS    AIC\n- gear  1     1.565 150.09 63.457\n- drat  1     1.932 150.46 63.535\n&lt;none&gt;              148.53 65.121\n- disp  1    10.110 158.64 65.229\n- am    1    12.323 160.85 65.672\n- hp    1    14.826 163.35 66.166\n- qsec  1    26.408 174.94 68.358\n- wt    1    69.127 217.66 75.350\n\nStep:  AIC=63.46\nmpg ~ disp + hp + drat + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- drat  1     3.345 153.44 62.162\n- disp  1     8.545 158.64 63.229\n&lt;none&gt;              150.09 63.457\n- hp    1    13.285 163.38 64.171\n- am    1    20.036 170.13 65.466\n- qsec  1    25.574 175.67 66.491\n- wt    1    67.572 217.66 73.351\n\nStep:  AIC=62.16\nmpg ~ disp + hp + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- disp  1     6.629 160.07 61.515\n&lt;none&gt;              153.44 62.162\n- hp    1    12.572 166.01 62.682\n- qsec  1    26.470 179.91 65.255\n- am    1    32.198 185.63 66.258\n- wt    1    69.043 222.48 72.051\n\nStep:  AIC=61.52\nmpg ~ hp + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- hp    1     9.219 169.29 61.307\n&lt;none&gt;              160.07 61.515\n- qsec  1    20.225 180.29 63.323\n- am    1    25.993 186.06 64.331\n- wt    1    78.494 238.56 72.284\n\nStep:  AIC=61.31\nmpg ~ wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              169.29 61.307\n- am    1    26.178 195.46 63.908\n- qsec  1   109.034 278.32 75.217\n- wt    1   183.347 352.63 82.790\n\n\nHere, we begin with a model including all predictors and iteratively remove the least statistically significant variables until the model no longer improves."
  },
  {
    "objectID": "posts/2023-12-06/index.html#both-direction-stepwise-regression",
    "href": "posts/2023-12-06/index.html#both-direction-stepwise-regression",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "Both-Direction Stepwise Regression:",
    "text": "Both-Direction Stepwise Regression:\n\n# Initialize a model with all predictors\nboth_model &lt;- lm(mpg ~ ., data = mtcars)\n\n# Both-direction stepwise regression\nboth_model &lt;- step(both_model, direction = \"both\")\n\nStart:  AIC=70.9\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- cyl   1    0.0799 147.57 68.915\n- vs    1    0.1601 147.66 68.932\n- carb  1    0.4067 147.90 68.986\n- gear  1    1.3531 148.85 69.190\n- drat  1    1.6270 149.12 69.249\n- disp  1    3.9167 151.41 69.736\n- hp    1    6.8399 154.33 70.348\n- qsec  1    8.8641 156.36 70.765\n&lt;none&gt;              147.49 70.898\n- am    1   10.5467 158.04 71.108\n- wt    1   27.0144 174.51 74.280\n\nStep:  AIC=68.92\nmpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- vs    1    0.2685 147.84 66.973\n- carb  1    0.5201 148.09 67.028\n- gear  1    1.8211 149.40 67.308\n- drat  1    1.9826 149.56 67.342\n- disp  1    3.9009 151.47 67.750\n- hp    1    7.3632 154.94 68.473\n&lt;none&gt;              147.57 68.915\n- qsec  1   10.0933 157.67 69.032\n- am    1   11.8359 159.41 69.384\n+ cyl   1    0.0799 147.49 70.898\n- wt    1   27.0280 174.60 72.297\n\nStep:  AIC=66.97\nmpg ~ disp + hp + drat + wt + qsec + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- carb  1    0.6855 148.53 65.121\n- gear  1    2.1437 149.99 65.434\n- drat  1    2.2139 150.06 65.449\n- disp  1    3.6467 151.49 65.753\n- hp    1    7.1060 154.95 66.475\n&lt;none&gt;              147.84 66.973\n- am    1   11.5694 159.41 67.384\n- qsec  1   15.6830 163.53 68.200\n+ vs    1    0.2685 147.57 68.915\n+ cyl   1    0.1883 147.66 68.932\n- wt    1   27.3799 175.22 70.410\n\nStep:  AIC=65.12\nmpg ~ disp + hp + drat + wt + qsec + am + gear\n\n       Df Sum of Sq    RSS    AIC\n- gear  1     1.565 150.09 63.457\n- drat  1     1.932 150.46 63.535\n&lt;none&gt;              148.53 65.121\n- disp  1    10.110 158.64 65.229\n- am    1    12.323 160.85 65.672\n- hp    1    14.826 163.35 66.166\n+ carb  1     0.685 147.84 66.973\n+ vs    1     0.434 148.09 67.028\n+ cyl   1     0.414 148.11 67.032\n- qsec  1    26.408 174.94 68.358\n- wt    1    69.127 217.66 75.350\n\nStep:  AIC=63.46\nmpg ~ disp + hp + drat + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- drat  1     3.345 153.44 62.162\n- disp  1     8.545 158.64 63.229\n&lt;none&gt;              150.09 63.457\n- hp    1    13.285 163.38 64.171\n+ gear  1     1.565 148.53 65.121\n+ cyl   1     1.003 149.09 65.242\n+ vs    1     0.645 149.45 65.319\n+ carb  1     0.107 149.99 65.434\n- am    1    20.036 170.13 65.466\n- qsec  1    25.574 175.67 66.491\n- wt    1    67.572 217.66 73.351\n\nStep:  AIC=62.16\nmpg ~ disp + hp + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- disp  1     6.629 160.07 61.515\n&lt;none&gt;              153.44 62.162\n- hp    1    12.572 166.01 62.682\n+ drat  1     3.345 150.09 63.457\n+ gear  1     2.977 150.46 63.535\n+ cyl   1     2.447 150.99 63.648\n+ vs    1     1.121 152.32 63.927\n+ carb  1     0.011 153.43 64.160\n- qsec  1    26.470 179.91 65.255\n- am    1    32.198 185.63 66.258\n- wt    1    69.043 222.48 72.051\n\nStep:  AIC=61.52\nmpg ~ hp + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- hp    1     9.219 169.29 61.307\n&lt;none&gt;              160.07 61.515\n+ disp  1     6.629 153.44 62.162\n+ carb  1     3.227 156.84 62.864\n+ drat  1     1.428 158.64 63.229\n- qsec  1    20.225 180.29 63.323\n+ cyl   1     0.249 159.82 63.465\n+ vs    1     0.249 159.82 63.466\n+ gear  1     0.171 159.90 63.481\n- am    1    25.993 186.06 64.331\n- wt    1    78.494 238.56 72.284\n\nStep:  AIC=61.31\nmpg ~ wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              169.29 61.307\n+ hp    1     9.219 160.07 61.515\n+ carb  1     8.036 161.25 61.751\n+ disp  1     3.276 166.01 62.682\n+ cyl   1     1.501 167.78 63.022\n+ drat  1     1.400 167.89 63.042\n+ gear  1     0.123 169.16 63.284\n+ vs    1     0.000 169.29 63.307\n- am    1    26.178 195.46 63.908\n- qsec  1   109.034 278.32 75.217\n- wt    1   183.347 352.63 82.790\n\n\nIn both-direction regression, the algorithm combines both forward and backward steps, optimizing the model by adding significant variables and removing insignificant ones.\n\nVisualizing Data and Model Fit:\nNow, let’s visualize the data and model fit using base R plots.\n\n# Scatter plot of mpg vs. hp\nplot(mtcars$hp, mtcars$mpg, \n     main = \"Scatter Plot of mpg vs. hp\", \n     xlab = \"hp\", ylab = \"mpg\", pch = 20\n     )\nabline(lm(mpg ~ hp, data = mtcars), col = \"black\", lwd = 2)\npoints(sort(mtcars$hp), forward_model$fitted.values, col = \"red\", pch = 20)\npoints(sort(mtcars$hp), backward_model$fitted.values, col = \"blue\", pch = 20)\npoints(sort(mtcars$hp), both_model$fitted.values, col = \"green\", pch = 20)\n\nlegend(\"topright\", legend = c(\"Forward\", \"Backward\", \"Both-Direction\"), \n       col = c(\"red\", \"blue\", \"green\"), pch = 20)\n\n\n\n\nThis plot displays the scatter plot of mpg against hp with fitted lines for each stepwise regression. The colors correspond to the models created earlier.\n\n\nVisualizing Residuals:\n\n# Residual plots for each model\npar(mfrow = c(2, 2))\n\n# Forward stepwise regression residuals\nplot(forward_model$residuals, main = \"Forward Residuals\", ylab = \"Residuals\")\n\n# Backward stepwise regression residuals\nplot(backward_model$residuals, main = \"Backward Residuals\", ylab = \"Residuals\")\n\n# Both-direction stepwise regression residuals\nplot(both_model$residuals, main = \"Both-Direction Residuals\", ylab = \"Residuals\")\n\n\n\n\nThese plots help assess how well the models fit the data by examining the residuals."
  },
  {
    "objectID": "posts/2023-12-07/index.html",
    "href": "posts/2023-12-07/index.html",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "",
    "text": "Hey there, fellow R enthusiasts! Today, let’s embark on a fascinating journey into the realm of piecewise regression using R. If you’ve ever wondered how to uncover hidden trends and breakpoints in your data, you’re in for a treat. Buckle up, and let’s dive into the world of piecewise regression!"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-1-load-your-data-and-libraries",
    "href": "posts/2023-12-07/index.html#step-1-load-your-data-and-libraries",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 1: Load Your Data and Libraries",
    "text": "Step 1: Load Your Data and Libraries\n\n# Install and load necessary packages\n# install.packages(\"segmented\")\nlibrary(segmented)\n\n# Sample data\nset.seed(123)\nx &lt;- 1:100\ny &lt;- 2 + 1.5 * pmax(x - 35, 0) - 1.5 * pmax(x - 70, 0) + rnorm(100)\n\n# Combine data\ndata &lt;- data.frame(x, y)"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-2-explore-your-data",
    "href": "posts/2023-12-07/index.html#step-2-explore-your-data",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 2: Explore Your Data",
    "text": "Step 2: Explore Your Data\nBefore diving into the regression, let’s take a peek at our data. Visualizing the data often provides insights into potential breakpoints.\n\n# Scatter plot to visualize the data\nplot(\n  data$x, data$y, \n  main = \"Scatter Plot of Your Data\",\n  xlab = \"Independent Variable (x)\", \n  ylab = \"Dependent Variable (y)\")"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-3-perform-piecewise-regression",
    "href": "posts/2023-12-07/index.html#step-3-perform-piecewise-regression",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 3: Perform Piecewise Regression",
    "text": "Step 3: Perform Piecewise Regression\nNow, the exciting part! Let’s fit our piecewise regression model using the segmented package.\n\n# Fit the piecewise regression model\nmodel &lt;- lm(y ~ x, data = data)\nsegmented_model &lt;- segmented(model, seg.Z = ~x)"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-4-visualize-the-results",
    "href": "posts/2023-12-07/index.html#step-4-visualize-the-results",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 4: Visualize the Results:",
    "text": "Step 4: Visualize the Results:\nTo truly understand the magic happening, let’s visualize the fitted model and residuals.\n\nseg_preds &lt;- predict(segmented_model)\nseg_res &lt;- y - seg_preds\n\n# Plot the original data with the fitted model\nplot(\n  data$x, data$y,\n  main = \"Piecewise Regression Fit\",\n  xlab = \"Independent Variable (x)\",\n  ylab = \"Dependent Variable (y)\",\n  col = \"blue\"\n)\nlines(data$x, seg_preds,col = \"red\", lwd = 2)\n\n\n\n# Plot residuals\n# Plot the residuals for each segment\nplot(x, seg_res, main = \"Residuals\")\nabline(h = 0, col = \"red\")"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-5-interpret-the-breakpoints",
    "href": "posts/2023-12-07/index.html#step-5-interpret-the-breakpoints",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 5: Interpret the Breakpoints:",
    "text": "Step 5: Interpret the Breakpoints:\nInspecting the segmented model will reveal the breakpoints and the corresponding regression lines. It’s like deciphering the story your data is trying to tell.\n\n# View breakpoints and coefficients\nsummary(segmented_model)\n\n\n    ***Regression Model with Segmented Relationship(s)***\n\nCall: \nsegmented.lm(obj = model, seg.Z = ~x)\n\nEstimated Break-Point(s):\n          Est. St.Err\npsi1.x 24.757  3.074\n\nCoefficients of the linear terms:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  2.49825    2.55867   0.976    0.331\nx           -0.04055    0.17907  -0.226    0.821\nU1.x         0.93569    0.18186   5.145       NA\n\nResidual standard error: 6.073 on 96 degrees of freedom\nMultiple R-Squared: 0.9333,  Adjusted R-squared: 0.9312 \n\nBoot restarting based on 6 samples. Last fit:\nConvergence attained in 2 iterations (rel. change 2.9855e-12)"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-6-encourage-exploration",
    "href": "posts/2023-12-07/index.html#step-6-encourage-exploration",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 6: Encourage Exploration:",
    "text": "Step 6: Encourage Exploration:\nNow that you’ve conquered piecewise regression, encourage your fellow data explorers to try it themselves. Challenge them to apply this technique to their datasets and share their insights."
  },
  {
    "objectID": "posts/2023-12-08/index.html",
    "href": "posts/2023-12-08/index.html",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "",
    "text": "If you’re a data enthusiast diving into the world of regression analysis in R, you’ve likely encountered the challenges of managing code complexity and juggling different modeling engines. The good news is that there’s a powerful tool to streamline your regression workflow – the tidyAML R package."
  },
  {
    "objectID": "posts/2023-12-08/index.html#try-it-yourself",
    "href": "posts/2023-12-08/index.html#try-it-yourself",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nNow that you’ve seen the power of tidyAML in action, it’s time to try it yourself. Install the package, load your data, and adapt the script to your specific use case. TidyAML provides a clean and efficient way to explore different regression models, making your analysis more manageable and insightful.\ninstall.packages(\"tidyAML\")\nlibrary(tidyAML)\n# Your data loading and analysis code here\nHappy coding, and may your regression analyses be tidy and insightful!"
  },
  {
    "objectID": "posts/2023-12-08/index.html#setting-up-the-recipe",
    "href": "posts/2023-12-08/index.html#setting-up-the-recipe",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "Setting Up the Recipe",
    "text": "Setting Up the Recipe\n\ndf &lt;- mtcars\nrecipe &lt;- recipe(mpg ~ ., data = df)\n\nIn this snippet, we’re creating a recipe for our regression analysis. The response variable (mpg) is modeled against all other variables in the mtcars dataset."
  },
  {
    "objectID": "posts/2023-12-08/index.html#fast-regression-with-tidyaml",
    "href": "posts/2023-12-08/index.html#fast-regression-with-tidyaml",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "Fast Regression with TidyAML",
    "text": "Fast Regression with TidyAML\n\nfr_tbl &lt;- fast_regression(\n  .data = df,\n  .rec_obj = recipe,\n  .parsnip_fns = c(\"linear_reg\", \"mars\", \"bag_mars\", \"rand_forest\",\n                   \"boost_tree\", \"bag_tree\"),\n  .parsnip_eng = c(\"lm\", \"gee\", \"glm\", \"gls\", \"earth\", \"rpart\", \"lightgbm\")\n)\n\nThis is where the magic happens. The fast_regression function performs regression using various modeling functions (linear_reg, mars, etc.) and engines (lm, gee, etc.) specified. It’s a versatile approach to quickly explore different models."
  },
  {
    "objectID": "posts/2023-12-08/index.html#visualizing-residuals",
    "href": "posts/2023-12-08/index.html#visualizing-residuals",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "Visualizing Residuals",
    "text": "Visualizing Residuals\n\nfr_tbl |&gt;\n  mutate(res = map(fitted_wflw, \\(x) x |&gt; \n                     broom::augment(new_data = df))) |&gt;\n  unnest(cols = res) |&gt;\n  mutate(pfe = paste0(.parsnip_engine, \" - \", .parsnip_fns)) |&gt;\n  mutate(.res = mpg - .pred) |&gt;\n  ggplot(aes(x = pfe, y = .res, fill = pfe)) +\n    geom_boxplot() +\n    theme_minimal() +\n    labs(title = \"Residuals by Fitted Model\",\n       subtitle = \"Residuals are mpg - .pred\",\n       x = \"Model\",\n       y = \"Residuals\",\n       fill = \"Engine + Function\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nThis block of code generates a boxplot visualizing residuals by model. Residuals are the differences between observed and predicted values. The plot helps you assess how well your models are performing."
  },
  {
    "objectID": "posts/2023-12-12/index.html",
    "href": "posts/2023-12-12/index.html",
    "title": "Conquering Unequal Variance with Weighted Least Squares in R: A Practical Guide",
    "section": "",
    "text": "Tired of your least-squares regression model giving wonky results because some data points shout louder than others? Meet Weighted Least Squares (WLS), the superhero of regression, ready to tackle unequal variance (heteroscedasticity) and give your model the justice it deserves! Today, we’ll dive into the world of WLS in R, using base functions for maximum transparency. Buckle up, data warriors!"
  },
  {
    "objectID": "posts/2023-12-12/index.html#steps",
    "href": "posts/2023-12-12/index.html#steps",
    "title": "Conquering Unequal Variance with Weighted Least Squares in R: A Practical Guide",
    "section": "Steps",
    "text": "Steps\nStep 1: Gathering the Troops (Data):\nLet’s create some simulated data:\n\n# Generate exam scores and study hours\nset.seed(123)\nscores &lt;- rnorm(100, mean = 70, sd = 10)\nhours &lt;- rnorm(100, mean = 20, sd = 5)\nhours &lt;- rnorm(100, mean = 0, sd = hours * 0.2) # Add heteroscedasticity\n\n# Create a data frame\ndata &lt;- data.frame(scores, hours)\n\nStep 2: Visualizing the Battlefield:\nA scatter plot is our trusty map:\n\nplot(data$hours, data$scores)\n\n\n\n\nDo you see those clusters of high-scoring students with more study hours? They’re the loud ones skewing the OLS line.\nStep 3: Building the WLS Wall:\nIt’s time to define our weights. We want to give less weight to observations with high variance (those loud students) and more weight to those with low variance. Here’s a simple approach:\n\n# Calculate inverse of variance\nweights &lt;- 1 / (data$hours)^2\n\n# Fit WLS model\nwls_model &lt;- lm(scores ~ hours, weights = weights, data = data)\n\nStep 4: Inspecting the Model’s Performance:\nLet’s see if WLS silenced the loud ones:\n\nsummary(wls_model)\n\n\nCall:\nlm(formula = scores ~ hours, data = data, weights = weights)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-75.854  -1.456   0.927   3.509  57.472 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   68.524      0.632 108.421   &lt;2e-16 ***\nhours         -1.085      1.480  -0.733    0.465    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.65 on 98 degrees of freedom\nMultiple R-squared:  0.00545,   Adjusted R-squared:  -0.004698 \nF-statistic: 0.537 on 1 and 98 DF,  p-value: 0.4654\n\n\nCompare this summary to your OLS model’s. Do the coefficients and residuals look more sensible?\nStep 5: Visualizing the Conquered Land:\nTime to see if WLS straightened the line:\n\nplot(data$hours, data$scores)\nlines(data$hours, wls_model$fitted, col = \"red\")\n\n\n\n\nNotice how the red WLS line now passes closer to the majority of data points, unlike the blue OLS line that chased the loud ones.\nStep 6: Residuals: The Echoes of Battle:\nLet’s see if the residuals (errors) are under control:\n\nplot(data$hours, wls_model$residuals)\n\n\n\n\nA random scatterplot of residuals is a good sign! No more funky patterns indicating heteroscedasticity.\nThe Victory Lap:\nWLS has restored justice to your regression model! Remember, this is just a basic example. You can customize your weights based on your specific data and needs.\nNow it’s your turn! Try WLS on your own data and see the magic unfold. Remember, data analysis is an adventure, and WLS is your trusty steed. Ride on, data warrior!\nBonus Tip: Check out the lmtest and sandwich packages for even more advanced WLS analysis.\nHappy coding!"
  },
  {
    "objectID": "posts/2023-12-14/index.html",
    "href": "posts/2023-12-14/index.html",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "",
    "text": "Ever run an R regression and stared at the output, feeling like you’re deciphering an ancient scroll? Fear not, fellow data enthusiasts! Today, we’ll crack the code and turn those statistics into meaningful insights.\nLet’s grab our trusty R arsenal and set up the scene:\n\nDataset: mtcars (a classic car dataset in R)\nRegression: Linear model with mpg as the dependent variable (miles per gallon) and all other variables as independent variables (predictors)"
  },
  {
    "objectID": "posts/2023-12-14/index.html#coefficients",
    "href": "posts/2023-12-14/index.html#coefficients",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "Coefficients",
    "text": "Coefficients\nThese tell you how much, on average, the dependent variable changes for a one-unit increase in the corresponding independent variable (holding other variables constant). For example, a coefficient of 0.05 for cyl means for every one more cylinder, mpg is expected to increase by 0.05 miles per gallon, on average.\n\nmodel$coefficients\n\n(Intercept)         cyl        disp          hp        drat          wt \n12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 \n       qsec          vs          am        gear        carb \n 0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925"
  },
  {
    "objectID": "posts/2023-12-14/index.html#p-values",
    "href": "posts/2023-12-14/index.html#p-values",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "P-values",
    "text": "P-values\nThese whisper secrets about significance. A p-value less than 0.05 would mean the observed relationship between the variable and mpg is unlikely to be due to chance. The following are the individual p-values for each variable:\n\nsummary(model)$coefficients[, 4]\n\n(Intercept)         cyl        disp          hp        drat          wt \n 0.51812440  0.91608738  0.46348865  0.33495531  0.63527790  0.06325215 \n       qsec          vs          am        gear        carb \n 0.27394127  0.88142347  0.23398971  0.66520643  0.81217871 \n\n\nNow the overall p-value for the model:\n\nmodel_p &lt;- function(.model) {\n  \n  # Get p-values\n  fstat &lt;- summary(.model)$fstatistic\n  p &lt;- pf(fstat[1], fstat[2], fstat[3], lower.tail = FALSE)\n  print(p)\n}\n\nmodel_p(.model = model)\n\n       value \n3.793152e-07"
  },
  {
    "objectID": "posts/2023-12-14/index.html#coefficients-1",
    "href": "posts/2023-12-14/index.html#coefficients-1",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "Coefficients",
    "text": "Coefficients\nThink of them as slopes. A positive coefficient means the dependent variable increases with the independent variable. Negative? The opposite! For example, disp has a negative coefficient, so bigger engines (larger displacement) tend to have lower mpg."
  },
  {
    "objectID": "posts/2023-12-14/index.html#p-values-1",
    "href": "posts/2023-12-14/index.html#p-values-1",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "P-values",
    "text": "P-values\nImagine a courtroom. A low p-value is like a strong witness, convincing you the relationship between the variables is real. High p-values (like for am!) are like unreliable witnesses, leaving us unsure."
  },
  {
    "objectID": "posts/2023-12-14/index.html#r-squared",
    "href": "posts/2023-12-14/index.html#r-squared",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "R-squared",
    "text": "R-squared\nThis tells you how well the model explains the variation in mpg. A value close to 1 is fantastic, while closer to 0 means the model needs work. In our case, it’s not bad, but there’s room for improvement.\n\nsummary(model)$r.squared\n\n[1] 0.8690158"
  },
  {
    "objectID": "posts/2023-12-14/index.html#residuals",
    "href": "posts/2023-12-14/index.html#residuals",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "Residuals",
    "text": "Residuals\nThese are the differences between the actual mpg values and the model’s predictions. Analyzing them can reveal hidden patterns and model issues.\n\ndata.frame(model$residuals)\n\n                    model.residuals\nMazda RX4              -1.599505761\nMazda RX4 Wag          -1.111886079\nDatsun 710             -3.450644085\nHornet 4 Drive          0.162595453\nHornet Sportabout       1.006565971\nValiant                -2.283039036\nDuster 360             -0.086256253\nMerc 240D               1.903988115\nMerc 230               -1.619089898\nMerc 280                0.500970058\nMerc 280C              -1.391654392\nMerc 450SE              2.227837890\nMerc 450SL              1.700426404\nMerc 450SLC            -0.542224699\nCadillac Fleetwood     -1.634013415\nLincoln Continental    -0.536437711\nChrysler Imperial       4.206370638\nFiat 128                4.627094192\nHonda Civic             0.503261089\nToyota Corolla          4.387630904\nToyota Corona          -2.143103442\nDodge Challenger       -1.443053221\nAMC Javelin            -2.532181498\nCamaro Z28             -0.006021976\nPontiac Firebird        2.508321011\nFiat X1-9              -0.993468693\nPorsche 914-2          -0.152953961\nLotus Europa            2.763727417\nFord Pantera L         -3.070040803\nFerrari Dino            0.006171846\nMaserati Bora           1.058881618\nVolvo 142E             -2.968267683\n\n\nBonus Tip: Visualize the data! Scatter plots and other graphs can make relationships between variables pop.\nRemember: Interpreting regression output is an art, not a science. Use your domain knowledge, consider the context, and don’t hesitate to explore further!\nSo next time you face regression output, channel your inner R wizard and remember:\n\nCoefficients whisper about slopes and changes.\nP-values tell tales of significance, true or false.\nR-squared unveils the model’s explanatory magic.\nResiduals hold hidden clues, waiting to be discovered.\n\nWith these tools in your belt, you’ll be interpreting regression output like a pro in no time! Now go forth and conquer the data, fellow R adventurers!\nNote: This is just a brief example. For a deeper dive, explore specific diagnostics, model selection techniques, and other advanced topics to truly master the art of regression interpretation."
  },
  {
    "objectID": "posts/2023-12-15/index.html",
    "href": "posts/2023-12-15/index.html",
    "title": "Demystifying Odds Ratios in Logistic Regression: Your R Recipe for Loan Defaults",
    "section": "",
    "text": "Introduction\nEver wondered why some individuals default on loans while others don’t? Logistic regression can shed light on this, and calculating odds ratios in R is the secret sauce. So, strap on your data aprons, folks, and let’s cook up some insights!\n\n\nWhat are Odds Ratios?\nImagine a loan officer flipping a coin to decide whether to approve your loan. Odds ratios tell you how much more likely one factor (like your income) makes the “heads” (approval) side appear compared to another (like your student status).\nIn logistic regression, odds ratios compare the odds of an event (loan default, in our case) for two groups defined by a specific variable. They’re like multipliers: greater than 1 means something increases the chances of default, while less than 1 means it decreases them.\n\n\nThe R Recipe (with ISLR Flavor)\n\nGather your ingredients: Load the ISLR package and the Default dataset. This data tells us whether individuals defaulted on loans, their student status, bank balance, and income.\nWhip up the model: Use the glm() function with family='binomial' to fit a logistic regression model that predicts loan defaults based on student status, balance, and income. Think of it as the base for your delicious insights.\nExtract the spices: Use the summary() function to access the estimated coefficients for each variable. These are the secret ingredients that give your model flavor.\nUnleash the magic of exponentiation: Apply the exp() function to transform the coefficients back to the odds ratio scale. Remember, logistic regression operates on log-odds, so we need to break the code.\nSavor the results: Analyze the odds ratios. Are they greater than 1? Those factors increase default odds. Less than 1? They decrease them. A value near 1 suggests little to no effect.\n\n\n\nExample Time\n\n# Load ISLR package and data\nlibrary(ISLR)\n\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n# Fit the model\nmodel &lt;- glm(default~student+balance+income, family='binomial', data=Default)\n\n#disable scientific notation for model summary\noptions(scipen=999)\n\n# Extract and exponentiate coefficients\nodds_ratios &lt;- exp(coef(model))\n\n# Print the odds ratios\ncat(\"Odds ratios:\")\n\nOdds ratios:\n\nprint(odds_ratios)\n\n  (Intercept)    studentYes       balance        income \n0.00001903854 0.52373166965 1.00575299051 1.00000303345 \n\ncat(\"Odds ratios with confidence intervals:\")\n\nOdds ratios with confidence intervals:\n\nexp(cbind(Odds_Ratio = coef(model), confint(model)))\n\nWaiting for profiling to be done...\n\n\n               Odds_Ratio          2.5 %       97.5 %\n(Intercept) 0.00001903854 0.000007074481 0.0000487808\nstudentYes  0.52373166965 0.329882707270 0.8334223982\nbalance     1.00575299051 1.005308940686 1.0062238757\nincome      1.00000303345 0.999986952969 1.0000191246\n\n\nInterpretation time! Being a student decreases default with log odds by -0.646, while higher income leaves log odds basically flat.\nGo Forth and Experiment!\nThis is just the tip of the iceberg! Play around with different models, variables, and visualizations using RStudio. Remember, the more you experiment, the better you’ll understand the magic of odds ratios and logistic regression. Now, go forth and analyze!\nBonus Tip: Check out the confint() function to calculate confidence intervals for your odds ratios. This adds another layer of spice to your statistical analysis!\nSo, there you have it! Odds ratios in R, made easy with the ISLR package and a dash of culinary magic. Remember, the key ingredients are understanding, practice, and a sprinkle of creativity. Bon appétit, data chefs!"
  },
  {
    "objectID": "posts/2023-12-18/index.html",
    "href": "posts/2023-12-18/index.html",
    "title": "Exploring Variance Inflation Factor (VIF) in R: A Practical Guide",
    "section": "",
    "text": "Introduction\nHey there fellow R enthusiasts! Today, we’re diving into the fascinating world of Variance Inflation Factor (VIF) and how to calculate it using R. VIF is a crucial metric that helps us understand the level of multicollinearity among predictors in a regression model. So, buckle up your seatbelts, and let’s embark on this coding adventure!\n\n\nSetting the Stage\nLet’s start by setting up our stage. We’ll use a linear regression model with the mtcars dataset. Here’s the model we’re going to work with:\n\n# Setting up the model\nmodel &lt;- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n\n\n\nCalculating VIF with car library\nNow, the exciting part! We’ll employ the car library to compute the VIF using the vif function. VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated. It’s a handy tool to identify collinearity issues in your model.\n\n# Installing and loading the 'car' library\n# install.packages(\"car\")\nlibrary(car)\n\n# Calculating VIF\nvif_values &lt;- vif(model)\nvif_values\n\n    disp       hp       wt     drat \n8.209402 2.894373 5.096601 2.279547 \n\n\n\n\nVisualizing the Model and Residuals\nTo gain deeper insights, let’s visualize our model and its residuals. Visualizations often provide a clearer picture of what’s happening under the hood.\n\n# Visualizing the model\nplot(model, which = 1, main = \"Model Fit\")\n\n\n\n\nThese plots will give us a sense of how well our model fits the data and whether there are any patterns in the residuals.\n\n\nVisualizing VIF\nNow, let’s bring our VIF into the spotlight. We’ll use a barplot to showcase the VIF values for each predictor.\n\n# Visualizing VIF\nbarplot(vif_values, col = \"skyblue\", main = \"Variance Inflation Factor (VIF)\")\n\n\n\n\nThis barplot will help us identify predictors that might be causing multicollinearity issues in our model.\n\n\nCorrelation Matrix and Visualization\nTo complete our journey, let’s create a correlation matrix of the predictors and visualize it. Understanding the correlations between variables is crucial in regression analysis.\n\n# Creating a correlation matrix\ncor_matrix &lt;- cor(mtcars[c(\"disp\", \"hp\", \"wt\", \"drat\")])\n\n# Visualizing the correlation matrix\nimage(cor_matrix, main = \"Correlation Matrix\", col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(20))\n\n\n\n\nThis visualization will give us a colorful snapshot of how our predictors are correlated.\n\n\nWrapping Up\nAnd there you have it, folks! We’ve explored the ins and outs of calculating VIF in R, visualized our model, checked residuals, and even took a colorful glance at predictor correlations. These tools are invaluable in ensuring the health and accuracy of our regression models.\nFeel free to tweak and play around with the code, and don’t forget to share your findings with the R community. Happy coding!\nKeep calm and code in R, Steve"
  },
  {
    "objectID": "posts/2023-12-19/index.html",
    "href": "posts/2023-12-19/index.html",
    "title": "A Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!",
    "section": "",
    "text": "Hey data enthusiasts! Today, we’re diving into the fascinating world of count data and its trusty sidekick, Poisson regression. Buckle up, because we’re about to explore how this statistical powerhouse helps us understand the factors influencing, you guessed it, counts.\nScenario: Imagine you’re an education researcher, eager to understand how a student’s GPA might influence their job offer count after graduation. But hold on, job offers aren’t continuous – they’re discrete, ranging from 0 to a handful. That’s where Poisson regression comes in!"
  },
  {
    "objectID": "posts/2023-12-19/index.html#generating-data",
    "href": "posts/2023-12-19/index.html#generating-data",
    "title": "Counting Blessings: A Gentle Dive into Poisson Regression for Job Offers",
    "section": "Generating Data",
    "text": "Generating Data\nWe’ll keep the data generation part the same, just adjusting the variables in our data frame.\n\nlibrary(tidyverse)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Creating data frame\ndata &lt;- data.frame(\n  School = sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE),\n  GPA = c(\n    round(runif(50, 1, 3), 1),\n    round(runif(30, 2, 3.5), 1),\n    round(runif(20, 3, 4), 1)\n  ),\n  JobOffers = c(rep(0, 50), rep(1, 30), rep(2, 10), rep(3, 7), rep(4, 3))\n)\n\nsummary(data)\n\n    School               GPA          JobOffers   \n Length:100         Min.   :1.000   Min.   :0.00  \n Class :character   1st Qu.:1.875   1st Qu.:0.00  \n Mode  :character   Median :2.600   Median :0.50  \n                    Mean   :2.532   Mean   :0.83  \n                    3rd Qu.:3.200   3rd Qu.:1.00  \n                    Max.   :4.000   Max.   :4.00  \n\ndata |&gt;\n  group_by(JobOffers) |&gt;\n  summarise(mean_gpa = mean(GPA))\n\n# A tibble: 5 × 2\n  JobOffers mean_gpa\n      &lt;dbl&gt;    &lt;dbl&gt;\n1         0     1.94\n2         1     2.87\n3         2     3.41\n4         3     3.5 \n5         4     3.8"
  },
  {
    "objectID": "posts/2023-12-19/index.html#visualizing-the-data",
    "href": "posts/2023-12-19/index.html#visualizing-the-data",
    "title": "A Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\nLet’s update the plots to reflect the change in the predictor and outcome.\n\nlibrary(ggplot2)\n\n# Plotting GPA distribution by school\nggplot(data, aes(JobOffers, fill = School)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  theme_minimal()\n\n\n\n\nThe density plot now showcases the distribution of GPA scores for each school.\nNext, let’s visualize the relationship between GPA and job offers.\n\n# Plotting Job Offers vs. GPA\nggplot(data, aes(x = GPA, y = JobOffers, color = School)) +\n  geom_point(aes(y = JobOffers), alpha = .628,\n             position = position_jitter(h = .2)) +\n  labs(title = \"Scatter Plot of Job Offers vs. GPA\",\n       x = \"GPA\", y = \"Job Offers\") +\n  theme_minimal()\n\n\n\n\nThis scatter plot gives us a visual cue that higher GPAs might correlate with more job offers."
  },
  {
    "objectID": "posts/2023-12-19/index.html#poisson-regression",
    "href": "posts/2023-12-19/index.html#poisson-regression",
    "title": "A Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nNow, let’s adjust the Poisson Regression model to reflect the change in predictor and outcome.\n\n# Fitting Poisson Regression model\npoisson_model &lt;- glm(JobOffers ~ GPA + School, data = data, family = \"poisson\")\n\n# Summary of the model\nsummary(poisson_model)\n\n\nCall:\nglm(formula = JobOffers ~ GPA + School, family = \"poisson\", data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2452  -0.5856  -0.3483   0.3221   1.6491  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.25817    0.70621  -7.446 9.65e-14 ***\nGPA          1.73169    0.21241   8.153 3.56e-16 ***\nSchoolB      0.03135    0.27524   0.114    0.909    \nSchoolC     -0.19137    0.27637  -0.692    0.489    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 138.07  on 99  degrees of freedom\nResidual deviance:  43.18  on 96  degrees of freedom\nAIC: 168.06\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe model summary will now provide insights into how GPA influences the number of job offers."
  },
  {
    "objectID": "posts/2023-12-19/index.html#visualizing-model-fits",
    "href": "posts/2023-12-19/index.html#visualizing-model-fits",
    "title": "A Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!",
    "section": "Visualizing Model Fits",
    "text": "Visualizing Model Fits\nLet’s update the plot to reflect the relationship between GPA and predicted job offers.\n\n# Adding predicted values to the data frame\ndata$Predicted &lt;- predict(poisson_model, type = \"response\")\n\n# Plotting observed vs. predicted values\nggplot(data, aes(x = GPA, y = Predicted, color = School)) +\n  geom_point(aes(y = JobOffers), alpha = .628,\n             position = position_jitter(h = .2)) +\n  geom_line() +\n  labs(\n    title = \"Observed vs. Predicted Job Offers\",\n    x = \"GPA\", \n    y = \"Predicted Job Offers\",\n    color = \"School\") +\n  theme_minimal()\n\n\n\n\nThis plot now illustrates how the Poisson Regression model predicts job offers based on GPA."
  },
  {
    "objectID": "posts/2023-12-27/index.html",
    "href": "posts/2023-12-27/index.html",
    "title": "Creating Time Series in R with the ts() Function",
    "section": "",
    "text": "Time series analysis is a powerful tool in the hands of a data scientist or analyst. It allows us to uncover patterns, trends, and insights hidden within temporal data. In this blog post, we’ll explore how to create a time series in R using the base R function ts()."
  },
  {
    "objectID": "posts/2023-12-27/index.html#convert_to_ts-function-details",
    "href": "posts/2023-12-27/index.html#convert_to_ts-function-details",
    "title": "Creating Time Series in R with the ts() Function",
    "section": "convert_to_ts() Function Details",
    "text": "convert_to_ts() Function Details\nThe convert_to_ts() function takes the following arguments:\n\n.data: A data frame or tibble to be converted into a time series format.\n.return_ts: A logical value indicating whether to return the time series data. Default is TRUE.\n.pivot_longer: A logical value indicating whether to pivot the data into long format. Default is FALSE."
  },
  {
    "objectID": "posts/2023-12-27/index.html#how-it-works",
    "href": "posts/2023-12-27/index.html#how-it-works",
    "title": "Creating Time Series in R with the ts() Function",
    "section": "How It Works",
    "text": "How It Works\n\nThe function checks if the input is a data frame or tibble; otherwise, it raises an error.\nIt verifies if the data comes from a tidy_distribution function; otherwise, it raises an error.\nThe data is then converted into a time series format, grouping it by “sim_number” and transforming the “y” column into a time series."
  },
  {
    "objectID": "posts/2023-12-27/index.html#example-usage",
    "href": "posts/2023-12-27/index.html#example-usage",
    "title": "Creating Time Series in R with the ts() Function",
    "section": "Example Usage",
    "text": "Example Usage\n\nlibrary(TidyDensity)\n\n# Assuming you have a tidy data frame 'tidy_data'\ntidy_time_series &lt;- convert_to_ts(.data = tidy_normal(), \n                                  .return_ts = TRUE, \n                                  .pivot_longer = FALSE)\n\n# Display the result\nhead(tidy_time_series)\n\n              y\n[1,] -2.1617445\n[2,]  0.7630891\n[3,]  0.1951564\n[4,]  1.0558584\n[5,] -1.5169866\n[6,] -1.4532770\n\nplot(tidy_time_series)\n\n\n\nmultiple_simulations_series &lt;- convert_to_ts(.data = tidy_normal(.num_sims = 10),\n                                             .return_ts = TRUE, \n                                             .pivot_longer = TRUE)\nhead(multiple_simulations_series)\n\n              1          2           3           4          5          6\n[1,]  0.6591429  1.0850380 -1.41562870 -0.59330831 -0.2680326  0.6516654\n[2,] -1.7456947 -0.5792555  0.95670979  0.35232047  1.3702818 -1.0709930\n[3,]  0.2665711 -2.1701118  2.18141262  0.25480605  1.5762242 -0.8022482\n[4,]  0.3128563  0.4328502  0.55082256  0.06628991  0.7984409  0.3048087\n[5,]  0.6763225 -0.3997367 -0.09709908  1.13736623  1.0121689  0.3383476\n[6,] -0.1086352  1.3522350 -1.00235321  0.14722832  1.3395307 -0.1026343\n              7          8          9         10\n[1,] -0.1113009  1.6959992 -1.1897814 -0.2290430\n[2,] -0.7512943 -0.6969146  1.1334643  0.7554655\n[3,]  1.0782559  0.5296079 -1.0057891  1.1089107\n[4,] -1.8030557  1.5021519  0.7094383 -1.0848102\n[5,] -0.5539205  0.7127801 -1.3130555 -0.6742046\n[6,] -0.7625295 -1.1712384  0.8147821  0.8036737\n\nplot(multiple_simulations_series)\n\n\n\nconvert_to_ts(.data = tidy_normal(.num_sims = 10),\n              .return_ts = FALSE, \n              .pivot_longer = FALSE)\n\n# A tibble: 50 × 10\n       `1`    `2`    `3`      `4`    `5`    `6`     `7`    `8`     `9`   `10`\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  1.34    0.300  0.475 -3.07     1.35   1.18   0.403  -0.104 -0.347  -0.590\n 2  0.153  -0.563 -1.28  -0.0429   0.354  1.24  -0.184   0.850  0.338  -0.700\n 3 -0.448   0.160 -1.22   0.600   -0.532 -0.357 -0.759   0.659  0.691  -0.778\n 4  0.0384 -1.38  -0.918  1.12    -1.09   0.949  0.0276  0.456  0.510   1.10 \n 5 -0.0148 -0.663  0.401  0.00200 -0.790 -1.98   0.714   0.613  0.658   1.02 \n 6  0.528   0.164 -0.104 -0.977   -0.889 -0.589  1.39    0.916  0.496   0.326\n 7  1.08   -1.21   0.116  0.685    1.22   0.132  0.608  -0.322 -1.06   -0.624\n 8 -0.620   1.45   0.666 -1.39    -1.20  -0.175  0.0665  1.21   1.95   -0.237\n 9 -0.143  -1.93  -0.683  0.603   -1.24   0.623 -0.564  -0.417  0.0639  1.34 \n10  0.125   0.869 -1.47   0.953    0.608 -1.80   0.272   1.16   1.17   -1.09 \n# ℹ 40 more rows\n\nconvert_to_ts(.data = tidy_normal(.num_sims = 10),\n              .pivot_longer = TRUE)\n\nTime Series:\nStart = 1 \nEnd = 50 \nFrequency = 1 \n              1            2           3           4           5           6\n 1 -0.655132170  1.124119928 -1.68015013  0.27119005 -1.34222309 -0.85029821\n 2 -0.563753358  0.148496529 -0.08420323  0.52425342 -0.22838128  0.38009238\n 3  0.162642987  0.944409541 -0.31995660  1.62739294  0.04586743 -1.00996184\n 4 -1.946332513  1.693491637 -0.76559306  2.58711915 -0.67796334  0.73113802\n 5  0.009260305  0.563304432 -0.90409541 -0.74206033  1.18933630  1.19979835\n 6  0.422154075 -0.827368569  0.38440082  1.58106342  0.53744939 -0.01699670\n 7  0.672312916  1.917586521  0.18758750  0.52812847  0.04987565  0.42471806\n 8 -0.099748356  0.548332963 -0.44544054 -0.74466540  0.27741779  0.03754982\n 9  0.599374087  2.416047894  0.13083797 -0.71925129  1.46397148  0.21928699\n10  2.137103506  1.012679620  0.10946528 -1.21309250 -0.40298312 -0.06149162\n11 -0.584328794 -0.977150553  1.98763118  0.80100807  1.59761439  0.96962954\n12 -0.977410854  0.153227900 -0.89588458 -0.04738268 -0.99671233 -0.79875286\n13  0.348559098 -0.139254894  0.17014759 -0.41635428  1.16343543  0.45536856\n14  2.050755613  0.379570270 -1.57943509 -0.49670553 -0.49108776 -0.81654431\n15 -0.536403083  0.815378564  0.44909230 -0.17645908  2.14187118  0.85912010\n16  0.154173100 -0.009842145 -0.06548799  1.36411289 -0.26842334  1.69185208\n17  0.526008171 -1.021411558 -0.01515875 -0.39923678 -1.91505446  1.47060381\n18 -0.382628412  1.554043999 -0.99083886  0.03813862 -0.12122244  0.78097490\n19  0.068027520  1.559917873  1.56624132 -0.72556821  0.04364763 -1.08713622\n20 -0.375312760  0.182453162 -0.29118413  0.63647087  1.14593859  0.33562580\n21  0.527199674  0.514358677 -0.85199804 -0.65959289  1.34991539 -0.88580797\n22  0.606739583 -0.349281940 -0.42159565 -0.29988202  0.86897866  0.83607508\n23 -0.750044523 -0.721098218  0.81759595  1.81533167  1.67514533  0.49723656\n24 -0.167121348  1.050352702  1.70669358 -2.78197517  0.70872919 -1.19627981\n25 -0.442427162 -0.009639385  0.66120474  0.07333735  0.39692306  2.72030582\n26 -0.405109397 -0.463937639  2.92697938  0.64263562 -0.59588190  1.27812303\n27  0.554957737  1.508452736 -0.67384362 -1.14296113  0.25859866 -0.09516432\n28 -0.167333762 -0.154519359  1.29634414  2.05276305  1.22046958 -0.20261165\n29 -0.426751358  0.299516899 -0.10270470  0.45218446 -1.01077778  0.41069225\n30 -0.915369502 -1.134302489 -0.45195412 -0.02372924 -0.87979497 -2.22429752\n31  0.398618595 -0.246544276  0.63197257  0.04685569  0.46524825 -0.41017315\n32  0.001083079 -0.530058643  0.82139589  0.39120899  0.63881495  0.63570075\n33 -0.502176823 -0.642359996  0.42880920 -0.44803379 -0.01992917  0.38896456\n34  0.276462923 -1.042478900  1.14313112 -1.55697201  0.52390061  0.07794736\n35  0.923405153 -0.237080174  0.96970857  0.86964379 -0.91567940 -0.63612591\n36 -0.400504232  0.217544505 -0.63904106  0.91258477 -0.01156312 -0.41156245\n37  0.458010625 -1.456299801 -0.95303905 -1.01123779 -0.04321204 -0.84060963\n38 -0.202664022 -0.729410616  1.10544600 -1.54460728  0.48443033 -0.67777269\n39  0.041503245 -0.610875862 -0.45167645 -0.47658183 -2.46133081  0.26514433\n40 -0.121966128 -0.265657879 -0.40754380  0.41665215 -3.23015392 -0.11733447\n41  2.275054279 -0.140453274  1.01738854  1.08335318 -0.72963796 -0.07750213\n42  0.305032329 -0.912897889 -0.54486760 -0.06350812  0.35661866 -0.89575613\n43 -0.097368038  0.879480923  0.42349178  0.90800105  1.22592750 -1.93884437\n44 -0.053250772  0.590070911 -0.04377062  0.38001642 -0.56422962 -1.55652310\n45 -0.631346970  0.510970484  1.03953655 -0.52313314  0.66643930 -0.50328900\n46 -0.573835910  0.662271162 -0.94615866 -1.09348403  1.51469795  0.02769026\n47  2.056373176 -1.438372766  0.64932666 -1.17330573  0.41932438  1.53009528\n48 -2.353078785  0.487698963 -0.81844578 -0.92462341  0.27244456  0.42617475\n49 -3.231722103 -1.271841203  0.24348256 -1.36611307 -0.97603663 -1.95217754\n50  0.753719680 -0.366101153 -0.01044950 -1.59566595  0.08057617  1.11583833\n              7            8            9           10\n 1  0.171342136 -1.143037486  0.798526236 -0.244110488\n 2 -0.940732995 -0.098286237 -0.002464059 -1.329350897\n 3 -0.694053671 -0.861938231 -0.981141759 -0.016408426\n 4  0.715770296  0.891107430 -1.631236257 -0.812448587\n 5 -1.487322100  1.513993297  1.034899433 -1.562309837\n 6 -2.299581079 -1.372057771  1.141053483 -0.523175745\n 7 -1.551226431  0.584053401 -0.124530500  1.386795935\n 8  0.199088420  0.176433940  0.896122531  0.150326444\n 9 -2.610686399  1.619626479 -0.304107194  1.999026100\n10  0.993024200 -1.717659646 -0.936505161  0.249643134\n11  0.242493969 -1.104745018  2.139557395  1.308248416\n12  1.438262730 -0.371852512 -0.367182295 -1.589296525\n13 -0.149204186 -1.054119573 -0.465127766  0.423034528\n14  1.199604760 -0.295676868  1.818224237  1.651671457\n15  0.682116022  1.589055554  0.940553190  0.044546697\n16 -0.023887103 -0.544176304  0.078750649 -1.618718807\n17  0.783402254 -0.024077038  1.530981707  0.610937582\n18  0.840292783 -0.781554633  0.177714516 -0.059345413\n19 -1.313595307 -1.101811653  0.057190918  0.067426355\n20  0.005232829  0.145444788  1.066697084  1.068481723\n21 -0.554820885  0.380379950 -0.162190910  1.185489015\n22 -0.861222004  0.030283953  0.908438632 -0.231394452\n23  1.157935009  0.063995477  2.361496504 -0.396326692\n24 -0.897071507  0.369621973 -0.266053668 -0.131590687\n25  0.035629927 -0.084923255  0.003248558 -0.368614537\n26  0.509364566 -1.832084693  0.890542325  0.888462980\n27  1.207424021 -2.671878721  0.063299112 -0.878590418\n28  0.211237171  1.535283026  0.759650387  0.549046140\n29 -0.595276048 -2.514556134  0.445083701 -0.769968392\n30  1.348793576  0.004755218 -0.301946343 -2.037938159\n31  0.361619164 -1.340745382 -0.706048393 -0.003291719\n32  0.014851985 -0.249794267  0.741063865 -0.398728564\n33 -1.172677388 -0.193834398  1.018583201 -0.351067819\n34 -0.572769045 -2.072442096  0.577545791  1.284331483\n35  0.443800268 -0.108977727  1.866110069 -0.020469667\n36  0.926425998 -0.687618149  1.224365387 -0.096690188\n37 -0.460173605 -0.302608648  0.671541153 -2.696710002\n38  0.277085477  0.335125232 -0.754473314  0.619338071\n39  1.279310040 -0.842097806 -0.275860802 -0.768216600\n40  0.015055026  0.835779589 -0.535925622 -0.990428811\n41  0.690052418  1.488830535  0.318262300 -0.265301715\n42 -2.342151157 -0.587400371  1.794438099  1.190162522\n43 -1.284973383  0.976120498 -0.678730423  0.895248035\n44 -1.857641265 -0.484324204  1.312931115  1.671816010\n45 -0.828061584 -1.461679865  1.175113675  0.392315093\n46 -0.124694812  0.800465295 -0.328118006  0.170025963\n47  0.698593283  0.676449924  1.963221359 -0.477702054\n48  0.118048192  0.257227889 -0.600914093  0.908605679\n49 -0.101844218  0.458018251  0.177924006 -0.469079298\n50  0.249644640 -1.684424651  0.620835209  1.330859032\n\n\nThis example showcases how to leverage TidyDensity’s functionality to convert tidy data into a time series format effortlessly. At this point in time though, the parameters of the ts() function are not utilized, meaning you cannot also pass in a start, end or frequency, but that will be added in the future.\nIn conclusion, mastering the ts() function in base R and exploring additional tools like convert_to_ts() opens up new avenues for time series analysis. So, roll up your sleeves, experiment with your data, and unlock the insights hidden in the temporal dimension. Happy coding!"
  },
  {
    "objectID": "posts/2023-12-28/index.html",
    "href": "posts/2023-12-28/index.html",
    "title": "Unveiling the Time Traveler: Plotting Time Series in R",
    "section": "",
    "text": "Introduction\nReady to journey through time with R? Buckle up, because we’re about to explore the art of visualizing time-dependent data, known as time series analysis. Whether you’re tracking monthly sales patterns or analyzing yearly climate trends, R has your back with powerful tools to visualize these stories through time.\nOur Flight Plan:\n\nLoading Up with Data: Grabbing our trusty dataset, AirPassengers.\nTaking Off with Base R: Creating a basic time series plot using base R functions.\nSoaring with ggplot2: Crafting a visually stunning time series plot using the ggplot2 library.\nNavigating Date Formatting: Customizing axis labels with scale_x_date() for clarity.\nLanding with Your Own Exploration: Encouraging you to take the controls and create your own time series plots!\n\n1. Ready for Takeoff: Loading Data\nWe’ll start by loading the built-in AirPassengers dataset, which chronicles monthly passenger totals from 1949 to 1960:\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\n2. Base R: The Simple and Straightforward Route\nBase R offers a direct path to creating a time series plot:\n\nplot(AirPassengers)\n\n\n\n\nThis generates a basic line plot, revealing an upward trend in air passengers over time.\n3. ggplot2: The High-Flying, Visually Staggering Journey\nFor more customization and visual appeal, we’ll turn to the ggplot2 library and the healthyR.ts library to first convert the AirPassengers Data set into a tibble:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(healthyR.ts)\n\ndf &lt;- ts_to_tbl(AirPassengers)\n\nggplot(df, aes(x = date_col, y = value)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Monthly Air Passengers (1949-1960)\",\n       x = \"Year\",\n       y = \"Passengers\")\n\n\n\n\nThis creates a more refined plot with informative labels and a sleeker aesthetic.\n4. Mastering Time with scale_x_date()\nTo fine-tune the x-axis date labels, ggplot2 offers the versatile scale_x_date() function. Let’s display years and abbreviated months:\n\nggplot(df, aes(x = date_col, y = value)) +\n  geom_line() +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%b %Y\") +\n  labs(title = \"Monthly Air Passengers (1949-1960)\",\n       y = \"Passengers\")\n\n\n\n\n5. Your Turn to Pilot: Experiment and Explore!\nR is your playground for time series visualization! Try these challenges:\n\nExplore other time series datasets in R.\nCustomize plots further with colors, themes, and annotations.\nUse scale_x_date() to display different date formats.\nCombine multiple time series in a single plot.\n\nUnleash your creativity and uncover the captivating stories hidden within time series data! For a start here are some resources:\n\nscale_x_date()\n\nThe scale_x_date() functiontakes the following arguments:\n\n%d: Day as a number between 0 and 31\n%a: Abbreviated weekday (e.g. “Tue”)\n%A: Unabbreviated weekday (e.g. “Tuesday”)\n%m: Month between 0 and 12\n%b: Abbreviated month (e.g. “Jan”)\n%B: Unabbreviated month (e.g. “January”)\n%y: 2-digit year (e.g. “21”)\n%Y: 4-digit year (e.g. “2021”)\n%W: Week of the year between 0 and 52"
  },
  {
    "objectID": "posts/2023-12-29/index.html",
    "href": "posts/2023-12-29/index.html",
    "title": "Unlocking the Power of Time: Transforming Data Frames into Time Series in R",
    "section": "",
    "text": "Hey there, fellow R enthusiasts! Today, we’re diving into the realm of time series, where data dances along the temporal dimension. To join this rhythmic analysis, we’ll first learn how to convert our trusty data frames into time series objects—the heart of time-based exploration in R."
  },
  {
    "objectID": "posts/2023-12-29/index.html#gather-your-data",
    "href": "posts/2023-12-29/index.html#gather-your-data",
    "title": "Unlocking the Power of Time: Transforming Data Frames into Time Series in R",
    "section": "1. Gather Your Data",
    "text": "1. Gather Your Data\nEvery journey begins with preparation. Here’s our sample data frame containing daily sales:\n\ndf &lt;- data.frame(date = as.Date('2022-01-01') + 0:9,\n                 sales = runif(10, 10, 500) + seq(50, 59)^2)"
  },
  {
    "objectID": "posts/2023-12-29/index.html#choose-your-time-series-destination",
    "href": "posts/2023-12-29/index.html#choose-your-time-series-destination",
    "title": "Unlocking the Power of Time: Transforming Data Frames into Time Series in R",
    "section": "2. Choose Your Time Series Destination",
    "text": "2. Choose Your Time Series Destination\nR offers two primary time series classes:\n\n“ts”: Base R’s classic time series object, designed for regularly spaced data.\n“xts”: Part of the ‘xts’ package, offering enhanced flexibility and features."
  },
  {
    "objectID": "posts/2023-12-29/index.html#embark-on-the-conversion-quest",
    "href": "posts/2023-12-29/index.html#embark-on-the-conversion-quest",
    "title": "Unlocking the Power of Time: Transforming Data Frames into Time Series in R",
    "section": "3. Embark on the Conversion Quest",
    "text": "3. Embark on the Conversion Quest\nA. Transforming into “ts”:\n\nlibrary(stats)  # Package for 'ts' class\n\n# Unleash the time series magic!\nts_sales &lt;- ts(df$sales, start = c(2022, 1), frequency = 365)  # Daily data\n\n# Admire your creation:\nprint(ts_sales)\n\nTime Series:\nStart = c(2022, 1) \nEnd = c(2022, 10) \nFrequency = 365 \n [1] 2728.713 3026.967 2769.227 2928.872 3401.730 3129.780 3303.479 3414.551\n [9] 3584.525 3922.348\n\n\nExplanation:\n\nts() function creates the time series object.\ndf$sales specifies the data for conversion.\nstart = c(2022, 1) sets the starting year and month.\nfrequency = 365 indicates daily observations (365 days per year).\n\nB. Shaping into “xts”:\n\nlibrary(xts)  # Package for 'xts' class\n\n# Time to shine!\nxts_sales &lt;- xts(df$sales, order.by = df$date)\n\n# Behold your masterpiece:\nprint(xts_sales)\n\n               [,1]\n2022-01-01 2728.713\n2022-01-02 3026.967\n2022-01-03 2769.227\n2022-01-04 2928.872\n2022-01-05 3401.730\n2022-01-06 3129.780\n2022-01-07 3303.479\n2022-01-08 3414.551\n2022-01-09 3584.525\n2022-01-10 3922.348\n\n\nExplanation:\n\nxts() function constructs the time series object.\ndf$sales provides the data.\norder.by = df$date sets the time-based ordering.\n\n4. Your Time to Experiment!\nNow that you’ve mastered the conversion, unleash your creativity:\n\nVisualize trends with plots.\nForecast future values.\nAnalyze patterns and seasonality.\nDecompose time series into components.\nAnd much more!\n\nThe possibilities are as boundless as time itself.\nRemember:\n\nChoose the time series class that best suits your analysis needs.\nAlways ensure your data frame has a column with valid date or time values.\nExplore the rich functionalities of R’s time series packages.\n\nHappy time series adventures!"
  },
  {
    "objectID": "posts/2024-01-02/index.html",
    "href": "posts/2024-01-02/index.html",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "",
    "text": "Hey fellow R enthusiasts! Today, let’s dive into the fascinating world of Lowess smoothing and learn how to harness its power for creating smooth visualizations of your data. Whether you’re new to R or a seasoned pro, this step-by-step guide will walk you through the process of performing Lowess smoothing, generating data, visualizing the model, and comparing different models with varying smoother spans."
  },
  {
    "objectID": "posts/2024-01-02/index.html#step-1-generate-data",
    "href": "posts/2024-01-02/index.html#step-1-generate-data",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nBefore we can smooth anything, we need some data to work with. Let’s create a synthetic dataset using the rnorm function and introduce a non-linear trend:\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate data\nx &lt;- seq(1, 100, by = 1)\ny &lt;- sin(x/10) + rnorm(100, sd = 0.5)\n\n# Plot the raw data\nplot(x, y, main = \"Raw Data with Non-linear Trend\", col = \"blue\", pch = 16)"
  },
  {
    "objectID": "posts/2024-01-02/index.html#step-2-perform-lowess-smoothing",
    "href": "posts/2024-01-02/index.html#step-2-perform-lowess-smoothing",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "Step 2: Perform Lowess Smoothing",
    "text": "Step 2: Perform Lowess Smoothing\nNow that we have our data, let’s apply Lowess smoothing using the lowess function:\n\n# Apply Lowess smoothing\nsmoothed_data &lt;- lowess(x, y)\n\n# Plot the smoothed data\nplot(x, y, main = \"Lowess Smoothed\", col = \"blue\", pch = 16)\nlines(smoothed_data, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Raw Data\", \"Lowess Smoothed\"), col = c(\"blue\", \"red\"), lwd = 2)"
  },
  {
    "objectID": "posts/2024-01-02/index.html#step-3-visualize-the-model-and-residuals",
    "href": "posts/2024-01-02/index.html#step-3-visualize-the-model-and-residuals",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "Step 3: Visualize the Model and Residuals",
    "text": "Step 3: Visualize the Model and Residuals\nTo better understand our smoothed model, let’s visualize the fitted values along with the residuals:\n\n# Get fitted values and residuals\nfitted_values &lt;- smoothed_data$y\nresiduals &lt;- y - fitted_values\n\n# Plot the model\nplot(x, fitted_values, main = \"Lowess Smoothed Model with Residuals\", col = \"red\", type = \"l\", lwd = 2)\npoints(x, residuals, col = \"green\", pch = 16)\nlegend(\"topleft\", legend = c(\"Smoothed Model\", \"Residuals\"), col = c(\"red\", \"green\"), lwd = 2)"
  },
  {
    "objectID": "posts/2024-01-02/index.html#step-4-compare-different-models",
    "href": "posts/2024-01-02/index.html#step-4-compare-different-models",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "Step 4: Compare Different Models",
    "text": "Step 4: Compare Different Models\nNow, let’s take our Lowess smoothing to the next level by experimenting with different smoother spans. We’ll create three models with varying spans and visualize the differences:\n\n# Generate three smoothed models with different spans\nmodel_1 &lt;- lowess(x, y, f = 0.2)\nmodel_2 &lt;- lowess(x, y, f = 0.5)\nmodel_3 &lt;- lowess(x, y, f = 0.8)\n\n# Plot the original data\nplot(x, y, main = \"Comparison of Lowess Models\", col = \"blue\", pch = 16)\n\n# Plot the smoothed models\nlines(model_1, col = \"red\", lty = 2, lwd = 2)\nlines(model_2, col = \"green\", lty = 3, lwd = 2)\nlines(model_3, col = \"purple\", lty = 4, lwd = 2)\n\n# Add a legend\nlegend(\"bottomleft\", legend = c(\"Raw Data\", \"Model 1\", \"Model 2\", \"Model 3\"), col = c(\"blue\", \"red\", \"green\", \"purple\"), lwd = 2)"
  },
  {
    "objectID": "posts/2024-01-04/index.html",
    "href": "posts/2024-01-04/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review (2023)",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2023, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2024!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2023\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\nlibrary(knitr)\nlibrary(kableExtra)\n\nfp &lt;- \"linkedin_content.xlsx\"\n\nengagement_tbl &lt;- read_excel(fp, sheet = \"ENGAGEMENT\") %&gt;%\n  clean_names() %&gt;%\n  mutate(date = mdy(date)) %&gt;% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %&gt;%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2023-12-31\"\n  )\n\ntop_posts_tbl &lt;- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %&gt;%\n  clean_names()\n\nfollowers_tbl &lt;- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %&gt;%\n  clean_names() %&gt;%\n  mutate(date = mdy(date)) %&gt;%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2023-12-31\"\n  )\n\ndemographics_tbl &lt;- read_excel(fp, sheet = \"DEMOGRAPHICS\") %&gt;%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 365\nColumns: 4\n$ date              &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-01-04, 202…\n$ impressions       &lt;dbl&gt; 4872, 3735, 10360, 12217, 27036, 26084, 8720, 2753, …\n$ engagements       &lt;dbl&gt; 34, 17, 51, 80, 173, 124, 32, 17, 80, 54, 106, 135, …\n$ `Engagement Rate` &lt;dbl&gt; 0.6978654, 0.4551539, 0.4922780, 0.6548252, 0.639887…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 7\n$ post_url_1          &lt;chr&gt; \"https://www.linkedin.com/feed/update/urn:li:activ…\n$ post_publish_date_2 &lt;chr&gt; \"2/16/2023\", \"2/16/2023\", \"3/16/2023\", \"1/24/2023\"…\n$ engagements         &lt;dbl&gt; 281, 227, 220, 194, 181, 172, 160, 145, 138, 124, …\n$ x4                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_5          &lt;chr&gt; \"https://www.linkedin.com/feed/update/urn:li:activ…\n$ post_publish_date_6 &lt;chr&gt; \"2/16/2023\", \"1/5/2023\", \"1/17/2023\", \"1/24/2023\",…\n$ impressions         &lt;dbl&gt; 43951, 38656, 34402, 32505, 25205, 24916, 22656, 2…\n\nglimpse(followers_tbl)\n\nRows: 365\nColumns: 2\n$ date          &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-01-04, 2023-01…\n$ new_followers &lt;dbl&gt; 11, 13, 17, 16, 26, 15, 14, 18, 14, 9, 11, 23, 6, 13, 5,…\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics &lt;chr&gt; \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            &lt;chr&gt; \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       &lt;chr&gt; \"0.05210459977388382\", \"0.03567609563469887\", \"0.0223…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %&gt;%\n  pivot_longer(cols = -date) %&gt;%\n  mutate(name = str_to_title(name)) %&gt;%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %&gt;%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  theme_minimal()\n\n\n\n\nLet’s look at a cumulative view of things.\n\nengagement_tbl %&gt;%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %&gt;%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %&gt;%\n  slice(1:12) %&gt;%\n  pivot_longer(cols = -date) %&gt;%\n  mutate(name = str_to_title(name)) %&gt;%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %&gt;%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %&gt;%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %&gt;%\n  slice(1:12) %&gt;%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\n\n\nKey Stats and Tables\nNow we are going to look at some key stats and tables. First we will look at the top 10 posts by impressions.\n\ntop_posts_tbl %&gt;%\n  select(post_publish_date_2, impressions, post_url_1) %&gt;%\n  arrange(desc(impressions)) %&gt;%\n  head(10) %&gt;%\n  setNames(c(\"Post Date\", \"Impressions\", \"Post URL\")) %&gt;%\n  kable(caption = \"Top 10 Posts by Impressions\", align = \"c\")\n\n\nTop 10 Posts by Impressions\n\n\nPost Date\nImpressions\nPost URL\n\n\n\n\n2/16/2023\n43951\nhttps://www.linkedin.com/feed/update/urn:li:activity:7031977309351890944\n\n\n2/16/2023\n38656\nhttps://www.linkedin.com/feed/update/urn:li:activity:7031977249448820737\n\n\n3/16/2023\n34402\nhttps://www.linkedin.com/feed/update/urn:li:activity:7042173970149728257\n\n\n1/24/2023\n32505\nhttps://www.linkedin.com/feed/update/urn:li:activity:7023663053699207168\n\n\n8/24/2023\n25205\nhttps://www.linkedin.com/feed/update/urn:li:activity:7100535469263699968\n\n\n1/17/2023\n24916\nhttps://www.linkedin.com/feed/update/urn:li:activity:7021114787371614209\n\n\n1/5/2023\n22656\nhttps://www.linkedin.com/feed/update/urn:li:activity:7016769698595749888\n\n\n3/10/2023\n21943\nhttps://www.linkedin.com/feed/update/urn:li:activity:7039960379266883585\n\n\n6/23/2023\n20559\nhttps://www.linkedin.com/feed/update/urn:li:activity:7078000250656808960\n\n\n2/21/2023\n19730\nhttps://www.linkedin.com/feed/update/urn:li:activity:7033888693216018432\n\n\n\n\n\n\n\nNow we will look at the top 10 posts by engagements.\n\ntop_posts_tbl %&gt;%\n  select(post_publish_date_2, engagements, post_url_1) %&gt;%\n  arrange(desc(engagements)) %&gt;%\n  head(10) %&gt;%\n  setNames(c(\"Post Date\", \"Engagements\", \"Post URL\")) %&gt;%\n  kable(caption = \"Top 10 Posts by Engagements\", align = \"c\")\n\n\nTop 10 Posts by Engagements\n\n\nPost Date\nEngagements\nPost URL\n\n\n\n\n2/16/2023\n281\nhttps://www.linkedin.com/feed/update/urn:li:activity:7031977309351890944\n\n\n2/16/2023\n227\nhttps://www.linkedin.com/feed/update/urn:li:activity:7031977249448820737\n\n\n3/16/2023\n220\nhttps://www.linkedin.com/feed/update/urn:li:activity:7042173970149728257\n\n\n1/24/2023\n194\nhttps://www.linkedin.com/feed/update/urn:li:activity:7023663053699207168\n\n\n8/24/2023\n181\nhttps://www.linkedin.com/feed/update/urn:li:activity:7100535469263699968\n\n\n1/17/2023\n172\nhttps://www.linkedin.com/feed/update/urn:li:activity:7021114787371614209\n\n\n1/5/2023\n160\nhttps://www.linkedin.com/feed/update/urn:li:activity:7016769698595749888\n\n\n3/10/2023\n145\nhttps://www.linkedin.com/feed/update/urn:li:activity:7039960379266883585\n\n\n6/23/2023\n138\nhttps://www.linkedin.com/feed/update/urn:li:activity:7078000250656808960\n\n\n2/21/2023\n124\nhttps://www.linkedin.com/feed/update/urn:li:activity:7033888693216018432\n\n\n\n\n\n\n\nNow we will look at the top 10 posts by engagement rate.\n\ntop_posts_tbl %&gt;%\n  select(post_publish_date_2, engagements, impressions, post_url_1) %&gt;%\n  mutate(engagement_rate = engagements / impressions) %&gt;%\n  arrange(desc(engagement_rate)) %&gt;%\n  select(post_publish_date_2, engagement_rate, post_url_1) %&gt;%\n  head(10) %&gt;%\n  setNames(c(\"Post Date\", \"Engagement Rate\", \"Post URL\")) %&gt;%\n  kable(caption = \"Top 10 Posts by Engagement Rate\", align = \"c\")\n\n\nTop 10 Posts by Engagement Rate\n\n\nPost Date\nEngagement Rate\nPost URL\n\n\n\n\n8/24/2023\n0.0071811\nhttps://www.linkedin.com/feed/update/urn:li:activity:7100535469263699968\n\n\n1/5/2023\n0.0070621\nhttps://www.linkedin.com/feed/update/urn:li:activity:7016769698595749888\n\n\n1/17/2023\n0.0069032\nhttps://www.linkedin.com/feed/update/urn:li:activity:7021114787371614209\n\n\n5/10/2023\n0.0068692\nhttps://www.linkedin.com/feed/update/urn:li:activity:7062043144850137088\n\n\n2/6/2023\n0.0067925\nhttps://www.linkedin.com/feed/update/urn:li:activity:7028379188441010176\n\n\n7/17/2023\n0.0067441\nhttps://www.linkedin.com/feed/update/urn:li:activity:7086688278245961728\n\n\n11/29/2023\n0.0067365\nhttps://www.linkedin.com/feed/update/urn:li:activity:7135620032557940736\n\n\n6/23/2023\n0.0067124\nhttps://www.linkedin.com/feed/update/urn:li:activity:7078000250656808960\n\n\n1/25/2023\n0.0066225\nhttps://www.linkedin.com/feed/update/urn:li:activity:7024095415348133889\n\n\n3/10/2023\n0.0066080\nhttps://www.linkedin.com/feed/update/urn:li:activity:7039960379266883585\n\n\n\n\n\n\n\nTotal Impressions: 2,720,605\nTotal Engagements: 20,000\nMean Engagement Rate: 0.0073513\nNew Followers: 6,388\nAnd finally the demographics of people who typically interact with my posts:\n\ndemographics_tbl %&gt;%\n  mutate(percentage = substr(percentage, 1, 4)) %&gt;%\n  kable(\n    caption = \"Demographics of People Who Interact With My Posts\", \n    align = \"c\"\n    )\n\n\nDemographics of People Who Interact With My Posts\n\n\ntop_demographics\nvalue\npercentage\n\n\n\n\nJob titles\nData Scientist\n0.05\n\n\nJob titles\nData Analyst\n0.03\n\n\nJob titles\nSoftware Engineer\n0.02\n\n\nJob titles\nData Engineer\n0.01\n\n\nJob titles\nProfessor\n0.01\n\n\nLocations\nNew York City Metropolitan Area\n0.06\n\n\nLocations\nGreater Bengaluru Area\n0.03\n\n\nLocations\nGreater Delhi Area\n0.02\n\n\nLocations\nPune/Pimpri-Chinchwad Area\n0.02\n\n\nLocations\nMumbai Metropolitan Region\n0.01\n\n\nIndustries\nIT Services and IT Consulting\n0.22\n\n\nIndustries\nSoftware Development\n0.11\n\n\nIndustries\nHigher Education\n0.05\n\n\nIndustries\nFinancial Services\n0.05\n\n\nIndustries\nHospitals and Health Care\n0.05\n\n\nSeniority\nSenior\n0.33\n\n\nSeniority\nEntry\n0.27\n\n\nSeniority\nDirector\n0.03\n\n\nSeniority\nManager\n0.03\n\n\nSeniority\nTraining\n0.02\n\n\nCompany size\n10,001+ employees\n0.17\n\n\nCompany size\n1001-5000 employees\n0.10\n\n\nCompany size\n51-200 employees\n0.08\n\n\nCompany size\n11-50 employees\n0.08\n\n\nCompany size\n1-10 employees\n0.06\n\n\nCompanies\nTata Consultancy Services\n&lt; 1%\n\n\nCompanies\nLong Island Community Hospital\n&lt; 1%\n\n\nCompanies\nAmazon\n&lt; 1%\n\n\nCompanies\nEY\n&lt; 1%\n\n\nCompanies\nCiti\n&lt; 1%\n\n\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/2024-01-05/index.html",
    "href": "posts/2024-01-05/index.html",
    "title": "Unveiling the Smooth Operator: Rolling Averages in R",
    "section": "",
    "text": "Ever felt those data points were a bit too jittery? Smoothing out trends and revealing underlying patterns is a breeze with rolling averages in R. Ready to roll? Let’s dive in!"
  },
  {
    "objectID": "posts/2024-01-05/index.html#creating-a-simple-time-series",
    "href": "posts/2024-01-05/index.html#creating-a-simple-time-series",
    "title": "Unveiling the Smooth Operator: Rolling Averages in R",
    "section": "Creating a Simple Time Series",
    "text": "Creating a Simple Time Series\n\nset.seed(123)  # Set seed for reproducibility (optional\n# Let's imagine some daily sales data\nsales &lt;- trunc(runif(112, min = 100, max = 500))  # Generate some random sales\ndays &lt;- as.Date(1:112, origin = \"2022-12-31\")  # Add some dates!\ndata_zoo &lt;- zoo(sales, days)  # Convert to a zoo object"
  },
  {
    "objectID": "posts/2024-01-05/index.html#calculating-rolling-averages",
    "href": "posts/2024-01-05/index.html#calculating-rolling-averages",
    "title": "Unveiling the Smooth Operator: Rolling Averages in R",
    "section": "Calculating Rolling Averages",
    "text": "Calculating Rolling Averages\n\n# Say we want a 7-day rolling average:\nrolling_avg7 &lt;- rollmean(data_zoo, k = 7)\nrolling_avg7_left &lt;- rollmean(data_zoo, k = 7, align = \"left\")\nrolling_avg7_right &lt;- rollmean(data_zoo, k = 7, align = \"right\")\n\n# How about a 28-day one?\nrolling_avg28 &lt;- rollmean(data_zoo, k = 28)\nrolling_avg28_left &lt;- rollmean(data_zoo, k = 28, align = \"left\")\nrolling_avg28_right &lt;- rollmean(data_zoo, k = 28, align = \"right\")"
  },
  {
    "objectID": "posts/2024-01-05/index.html#visualizing-the-smoothness",
    "href": "posts/2024-01-05/index.html#visualizing-the-smoothness",
    "title": "Unveiling the Smooth Operator: Rolling Averages in R",
    "section": "Visualizing the Smoothness",
    "text": "Visualizing the Smoothness\n\nplot(data_zoo, type = \"l\", col = \"black\", lwd = 1, ylab = \"Sales\")\nlines(rolling_avg7, col = \"red\", lwd = 2, lty = 2)\nlines(rolling_avg7_left, col = \"green\", lwd = 2, lty = 2)\nlines(rolling_avg7_right, col = \"orange\", lwd = 2, lty = 2)\nlegend(\n  \"bottomleft\", \n  legend = c(\n    \"Original Data\", \"7-day Avg\", \"7-day Avg (left-aligned)\", \n    \"7-day Avg (right-aligned)\"\n    ),\n  col = c(\"black\", \"red\", \"green\", \"orange\"), \n  lwd = 1, lty = 1:2,\n  cex = 0.628\n  )\n\n\n\n\n\nplot(data_zoo, type = \"l\", col = \"black\", lwd = 1, ylab = \"Sales\")\nlines(rolling_avg28, col = \"green\", lwd = 2, lty = 2)\nlines(rolling_avg28_left, col = \"steelblue\", lwd = 2, lty = 2)\nlines(rolling_avg28_right, col = \"brown\", lwd = 2, lty = 2)\nlegend(\n  \"bottomleft\", \n  legend = c(\n    \"Original Data\", \"28-day Avg\", \"28-day Avg (left-aligned)\", \n    \"28-day Avg (right-aligned)\"\n    ),\n  col = c(\"black\", \"green\", \"steelblue\", \"brown\"), \n  lwd = 1, lty = 1:2,\n  cex = 0.628\n  )"
  },
  {
    "objectID": "posts/2024-01-08/index.html",
    "href": "posts/2024-01-08/index.html",
    "title": "Conquering Daily Data: How to Aggregate to Months and Years Like a Pro in R",
    "section": "",
    "text": "Introduction\nTaming the beast of daily data can be daunting. While it captures every detail, sometimes you need a bird’s-eye view. Enter aggregation, your secret weapon for transforming daily data into monthly and yearly insights. In this post, we’ll dive into the world of R, where you’ll wield powerful tools like dplyr and lubridate to master this data wrangling art.\n\n\nPackages: Gear Up with the Right Packages\nThink of R packages like your trusty toolbox. Today, we’ll need two essentials:\n\ndplyr: This swiss army knife lets you manipulate and summarize data like a boss.\nlubridate: Time is our domain, and lubridate helps us navigate it with precision, especially for dates.\n\n\n\nSample Data, Our Training Ground\nImagine you have daily sales data for a year. Each row represents a day, with columns for date, product, and sales amount. Let’s create a mini version:\n\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Generate random dates and sales\nset.seed(123)\ndates &lt;- seq(as.Date('2023-01-01'), as.Date('2023-12-31'), by = 'day')\nsales &lt;- runif(365, min=5000, max=10000)\n\n# Create our data frame\ndaily_data &lt;- data.frame(date = dates, sales = sales)\n\n# Peek at our data\nhead(daily_data)\n\n        date    sales\n1 2023-01-01 6437.888\n2 2023-01-02 8941.526\n3 2023-01-03 7044.885\n4 2023-01-04 9415.087\n5 2023-01-05 9702.336\n6 2023-01-06 5227.782\n\n\nThis code generates 10 random dates and sales figures, and stores them in a data frame called daily_data.\n\n\nMonthly Magic – From Days to Months\nNow, let’s transform this daily data into monthly insights. Here’s the incantation:\n\n# Group data by month\nmonthly_data &lt;- daily_data %&gt;%\n   # Group by month extracted from date\n  group_by(month = month(date)) %&gt;%\n  # Calculate total sales for each month\n  summarize(total_sales = sum(sales))\n\nhead(monthly_data)\n\n# A tibble: 6 × 2\n  month total_sales\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     1     245675.\n2     2     199109.\n3     3     233764.\n4     4     227888.\n5     5     230928.\n6     6     222015.\n\n\nLet’s break it down:\n\ngroup_by(month = month(date)): We tell R to group our data by the month extracted from the date column.\nsummarize(total_sales = sum(sales)): Within each month group, we calculate the total sales by summing the sales values.\n\n\n\nYearly Triumph – Conquering the Calendar\nYearning for yearly insights? Fear not! Modify the spell slightly:\n\n# Group data by year\nyearly_data &lt;- daily_data %&gt;%\n  # Group by year extracted from date\n  group_by(year = year(date)) %&gt;%\n  # Calculate average sales for each year\n  summarize(average_sales = mean(sales))\n\nhead(yearly_data)\n\n# A tibble: 1 × 2\n   year average_sales\n  &lt;dbl&gt;         &lt;dbl&gt;\n1  2023         7494.\n\n\nHere, we group by the year extracted from date and then calculate the average sales for each year.\n\n\nBut what about base R?\nSo far, we’ve used dplyr to group and summarize our data. But what if you don’t have dplyr? No problem! You can use base R functions like aggregate() to achieve the same results:\n\nmonthly_data &lt;- aggregate(\n  daily_data$sales, \n  by = list(month = format(daily_data$date, '%m')), \n  FUN = sum\n  )\nhead(monthly_data)\n\n  month        x\n1    01 245675.1\n2    02 199108.7\n3    03 233764.1\n4    04 227888.3\n5    05 230928.0\n6    06 222015.3\n\nyearly_data &lt;- aggregate(\n  daily_data$sales, \n  by = list(year = format(daily_data$date, '%Y')), \n  FUN = mean\n  )\nhead(yearly_data)\n\n  year      x\n1 2023 7493.8\n\n\n\n\nExperiment!\nThe magic doesn’t stop there! You can customize your aggregations to your heart’s content. Try these variations:\n\nCalculate maximum sales per month.\nFind the product with the highest average sales per year.\nGroup data by month and product to see which products perform best each month.\n\n\n\nRemember\n\nPlay around with different summarize() functions like min(), max(), or median().\nUse filter() before group_by() to focus on specific subsets of data.\nExplore other time units like weeks or quarters with lubridate’s powerful tools.\n\n\n\nThe Takeaway\nMastering daily data aggregation is a valuable skill for any data warrior. With the help of R and your newfound knowledge, you can transform mountains of daily data into insightful monthly and yearly summaries. So, go forth, conquer your data, and share your insights with the world!\nBonus Challenge: Share your own R code and insights in the comments below! Let’s learn from each other and become daily data aggregation masters together!"
  },
  {
    "objectID": "posts/2024-01-09/index.html",
    "href": "posts/2024-01-09/index.html",
    "title": "New Horizons for TidyDensity: Version 1.3.0 Release",
    "section": "",
    "text": "Introduction\nThe latest release of the TidyDensity R package brings some major changes and improvements that open up new possibilities for statistical analysis and data visualization. Version 1.3.0 includes breaking changes, new features, and a host of minor fixes and improvements that enhance performance and usability. Let’s dive into what’s new!\n\n\nBreaking Changes\nTwo key functions have been modified in this release:\n\ntidy_multi_single_dist() now requires passing the .return_tibble parameter to specify whether to return a tibble (TRUE) or a list (FALSE). This allows better control over the output.\nThe minimum R version has been bumped to 4.1.0 to leverage the native pipe operator |&gt; instead of %&gt;%.\n\n\n\nNew Features\nSeveral new functions expand the capabilities of TidyDensity:\n\ntidy_triangular() generates a tidy dataframe of points from a triangular distribution.\nutil_triangular_param_estimate() estimates the parameters of a triangular distribution.\nutil_triangular_stats_tbl() computes summary statistics for a triangular distribution.\ntriangle_plot() creates a triangular density plot.\ntidy_autoplot() now supports triangular distributions.\n\n\n\nPerformance Improvements\nMany functions have been optimized for speed:\n\ncvar() and csd() are now vectorized for over 100x speedup.\nUsing data.table in the tidy_ functions typically improves speed by 30% or more.\nOther vectorized improvements speed up cskewness() by 124x and ckurtosis() by 121x.\n\nThe minor fixes address deprecation warnings, documentation, and ensure consistency across functions.\nVersion 1.3.0 takes TidyDensity to the next level with expanded capabilities and boosted performance. Whether you need to model triangular distributions or crunch large datasets, this release has you covered. The pipe workflow makes analyses simpler and faster. Check out the full details in the GitHub repository. Let us know if you have any issues or feature requests!"
  },
  {
    "objectID": "posts/2024-01-10/index.html",
    "href": "posts/2024-01-10/index.html",
    "title": "Exploring the Peaks: A Dive into the Triangular Distribution in TidyDensity",
    "section": "",
    "text": "Welcome back, fellow data enthusiasts! Today, we embark on an exciting journey into the world of statistical distributions with a special focus on the latest addition to the TidyDensity package – the triangular distribution. Tightly packed and versatile, this distribution brings a unique flavor to your data simulations and analyses. In this blog post, we’ll delve into the functions provided, understand their arguments, and explore the wonders of the triangular distribution."
  },
  {
    "objectID": "posts/2024-01-10/index.html#using-tidy_triangular-for-simulations",
    "href": "posts/2024-01-10/index.html#using-tidy_triangular-for-simulations",
    "title": "Exploring the Peaks: A Dive into the Triangular Distribution in TidyDensity",
    "section": "Using tidy_triangular for Simulations",
    "text": "Using tidy_triangular for Simulations\nSuppose you want to simulate a triangular distribution with 100 x values, a minimum of 0, a maximum of 1, and a mode at 0.5. You’d use the following code:\n\nlibrary(TidyDensity)\n\ntriangular_data &lt;- tidy_triangular(\n  .n = 100, \n  .min = 0, \n  .max = 1, \n  .mode = 0.5, \n  .num_sims = 1, \n  .return_tibble = TRUE\n  )\n\ntriangular_data\n\n# A tibble: 100 × 7\n   sim_number     x     y      dx      dy     p     q\n   &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1              1 0.853 -0.140  0.00158 0.957 0.853\n 2 1              2 0.697 -0.128  0.00282 0.816 0.697\n 3 1              3 0.656 -0.116  0.00484 0.764 0.656\n 4 1              4 0.518 -0.103  0.00805 0.536 0.518\n 5 1              5 0.635 -0.0909 0.0130  0.733 0.635\n 6 1              6 0.838 -0.0786 0.0202  0.948 0.838\n 7 1              7 0.645 -0.0662 0.0304  0.748 0.645\n 8 1              8 0.482 -0.0539 0.0444  0.464 0.482\n 9 1              9 0.467 -0.0416 0.0627  0.437 0.467\n10 1             10 0.599 -0.0293 0.0859  0.678 0.599\n# ℹ 90 more rows\n\n\nThis generates a tidy tibble with simulated data, ready for your analysis."
  },
  {
    "objectID": "posts/2024-01-10/index.html#estimating-parameters-and-creating-stats-tables",
    "href": "posts/2024-01-10/index.html#estimating-parameters-and-creating-stats-tables",
    "title": "Exploring the Peaks: A Dive into the Triangular Distribution in TidyDensity",
    "section": "Estimating Parameters and Creating Stats Tables",
    "text": "Estimating Parameters and Creating Stats Tables\nUtilize the util_triangular_param_estimate function to estimate parameters and create tidy empirical data:\n\nparam_estimate &lt;- util_triangular_param_estimate(.x = triangular_data$y)\n\nt(param_estimate$parameter_tbl)\n\n          [,1]        \ndist_type \"Triangular\"\nsamp_size \"100\"       \nmin       \"0.0572515\" \nmax       \"0.8822025\" \nmode      \"0.8822025\" \nmethod    \"Basic\"     \n\n\nFor statistics table creation:\n\nstats_table &lt;- util_triangular_stats_tbl(.data = triangular_data)\nt(stats_table)\n\n                  [,1]                     \ntidy_function     \"tidy_triangular\"        \nfunction_call     \"Triangular c(0, 1, 0.5)\"\ndistribution      \"Triangular\"             \ndistribution_type \"continuous\"             \npoints            \"100\"                    \nsimulations       \"1\"                      \nmean              \"0.5\"                    \nmedian            \"0.3535534\"              \nmode              \"1\"                      \nrange_low         \"0.0572515\"              \nrange_high        \"0.8822025\"              \nvariance          \"0.04166667\"             \nskewness          \"0\"                      \nkurtosis          \"-0.6\"                   \nentropy           \"-0.6931472\"             \ncomputed_std_skew \"-0.1870017\"             \ncomputed_std_kurt \"2.778385\"               \nci_lo             \"0.08311609\"             \nci_hi             \"0.8476985\"              \n\n\nVisualizing the Triangular Distribution: Now, let’s visualize the triangular distribution using the triangle_plot function:\n\ntriangle_plot(.data = triangular_data, .interactive = TRUE)\n\n\n\n\n\n\ntriangle_plot(.data = triangular_data, .interactive = FALSE)\n\n\n\n\nThis will generate an informative plot, and if you set .interactive to TRUE, you can explore the distribution interactively using plotly."
  },
  {
    "objectID": "posts/2023-01-11/index.html",
    "href": "posts/2023-01-11/index.html",
    "title": "Benchmarking the Speed of Cumulative Functions in TidyDensity",
    "section": "",
    "text": "Introduction\nStatistical analysis often involves calculating various measures on large datasets. Speed and efficiency are crucial, especially when dealing with real-time analytics or massive data volumes. The TidyDensity package in R provides a set of fast cumulative functions for common statistical measures like mean, standard deviation, skewness, and kurtosis. But just how fast are these cumulative functions compared to doing the computations directly? In this post, I benchmark the cumulative functions against the base R implementations using the rbenchmark package.\n\n\nSetting the bench\nTo assess the performance of TidyDensity’s cumulative functions, we’ll employ the rbenchmark package for benchmarking and the ggplot2 package for visualization. I’ll benchmark the following cumulative functions on random samples of increasing size:\n\ncgmean() - Cumulative geometric mean\nchmean() - Cumulative harmonic mean\nckurtosis() - Cumulative kurtosis\ncskewness() - Cumulative skewness\ncmean() - Cumulative mean\ncsd() - Cumulative standard deviation\ncvar() - Cumulative variance\n\n\nlibrary(TidyDensity)\nlibrary(rbenchmark)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nset.seed(123)\n\nx1 &lt;- sample(1e2) + 1e2\nx2 &lt;- sample(1e3) + 1e3 \nx3 &lt;- sample(1e4) + 1e4\nx4 &lt;- sample(1e5) + 1e5\nx5 &lt;- sample(1e6) + 1e6\n\ncg_bench &lt;- benchmark(\n  \"100\" = cgmean(x1),\n  \"1000\" = cgmean(x2),\n  \"10000\" = cgmean(x3),\n  \"100000\" = cgmean(x4),\n  \"1000000\" = cgmean(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\n# Run benchmarks for other functions\nch_bench &lt;- benchmark(\n  \"100\" = chmean(x1),\n  \"1000\" = chmean(x2),\n  \"10000\" = chmean(x3),\n  \"100000\" = chmean(x4),\n  \"1000000\" = chmean(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\nck_bench &lt;- benchmark(\n  \"100\" = ckurtosis(x1),\n  \"1000\" = ckurtosis(x2),\n  \"10000\" = ckurtosis(x3),\n  \"100000\" = ckurtosis(x4),\n  \"1000000\" = ckurtosis(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")  \n)\n\ncs_bench &lt;- benchmark(\n  \"100\" = cskewness(x1),\n  \"1000\" = cskewness(x2), \n  \"10000\" = cskewness(x3),\n  \"100000\" = cskewness(x4),\n  \"1000000\" = cskewness(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\ncm_bench &lt;- benchmark(\n  \"100\" = cmean(x1),\n  \"1000\" = cmean(x2),\n  \"10000\" = cmean(x3),\n  \"100000\" = cmean(x4),\n  \"1000000\" = cmean(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\ncsd_bench &lt;- benchmark(\n  \"100\" = csd(x1),\n  \"1000\" = csd(x2),\n  \"10000\" = csd(x3),\n  \"100000\" = csd(x4),\n  \"1000000\" = csd(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")  \n)\n\ncv_bench &lt;- benchmark(\n  \"100\" = cvar(x1),\n  \"1000\" = cvar(x2),\n  \"10000\" = cvar(x3),\n  \"100000\" = cvar(x4), \n  \"1000000\" = cvar(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\nbenchmarks &lt;- rbind(cg_bench, ch_bench, ck_bench, cs_bench, cm_bench, csd_bench, cv_bench)\n\n# Arrange benchmarks and plot\nbench_tbl &lt;- benchmarks |&gt; \n  mutate(func = c(\n    rep(\"cgmean\", 5), \n    rep(\"chmean\", 5),\n    rep(\"ckurtosis\", 5),\n    rep(\"cskewness\", 5),\n    rep(\"cmean\", 5),\n    rep(\"csd\", 5),\n    rep(\"cvar\", 5)\n    )\n  ) |&gt;\n  arrange(func, test) |&gt;\n  select(func, test, everything())\n\nbench_tbl |&gt;\n  ggplot(aes(x=test, y=elapsed, group = func, color = func)) +\n    geom_line() +\n    facet_wrap(~func, scales=\"free_y\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(title=\"Cumulative Function Speed Comparison\",\n       x=\"Sample Size\",\n       y=\"Elapsed Time (sec)\",\n       color = \"Function\")\n\n\n\n\nThe results show that the TidyDensity cumulative functions scale extremely well as the sample size increases. The elapsed time remains very low even at 1 million observations. The base R implementations like var() and sd() perform significantly worse when used inside of an sapply at large sample sizes. What was not tested however is cmedian() and this is because the performance is very slow once we reach 1e4 compared to the other functions as such that it would take too long to run the benchmark if it ran at all.\nSo if you need fast statistical functions that can scale to big datasets, the TidyDensity cumulative functions are a great option! They provide massive speedups over base R while returning the same final result.\nLet me know in the comments if you have any other benchmark ideas for comparing R packages! I’m always looking for interesting performance comparisons to test out."
  },
  {
    "objectID": "posts/2024-01-12/index.html",
    "href": "posts/2024-01-12/index.html",
    "title": "TidyDensity Powers Up with Data.table: Speedier Distributions for Your Data Exploration",
    "section": "",
    "text": "Calling all R enthusiasts who love tidy data and crave efficiency!\nI’m thrilled to announce a major upgrade to the TidyDensity package that’s sure to accelerate your data analysis workflows. We’ve integrated the lightning-fast data.table package for generating tidy distribution data, resulting in a jaw-dropping 30% speed boost.\nHere is one of the tests ran during development where v1 was the current and v2 was the version using data.table:\nn &lt;- 10000\nbenchmark(\n \"tidy_bernoulli_v2\" = {\n   tidy_bernoulli_v2(n, .5, 1, FALSE)\n },\n \"tidy_bernoulli_v1\" = {\n   TidyDensity::tidy_bernoulli(n, .5, 1)\n },\n replications = 100,\n columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |&gt;\n arrange(relative)\n               test replications elapsed relative user.self sys.self\n1 tidy_bernoulli_v2          100    2.50    1.000      2.22     0.26\n2 tidy_bernoulli_v1          100    4.67    1.868      4.34     0.31\n\n\nHere’s what this means for you\n\nFaster Generation of Distribution Data: Whether you’re working with normal, binomial, Poisson, or other distributions, TidyDensity now produces results more swiftly than ever. This means less waiting and more time for exploring insights.\nFlexible Output Formats: Choose the format that best suits your needs:\n\nTibbles for Seamless Integration with Tidyverse: Set .return_tibble = TRUE to receive the data as a tibble, ready for seamless interaction with your favorite tidyverse tools.\ndata.table for Enhanced Performance: Set .return_tibble = FALSE to harness the raw power of data.table objects for memory-efficient and lightning-fast operations.\n\nEnjoy the Speed Boost, No Matter Your Choice: The speed enhancement shines through regardless of your preferred output format, as the data generation itself leverages data.table under the hood.\n\n\n\nHow to experience this boost\n\nUpdate TidyDensity: Ensure you have the latest version installed: install.packages(\"TidyDensity\")\nChoose Your Output Format: Indicate your preference with the .return_tibble parameter:\n# For a tibble:\ntidy_data &lt;- tidy_normal(.return_tibble = TRUE)\n\n# For a data.table:\ntidy_data &lt;- tidy_normal(.return_tibble = FALSE)\nNo matter which output you choose you will still enjoy the speedup because data.table is used to create the data and the conversion to a tibble is done afterwards if that is the output you want.\n\n\n\nLet’s see the output\n\nlibrary(TidyDensity)\n\n# Generate data\nnormal_tibble &lt;- tidy_normal(.return_tibble = TRUE)\nhead(normal_tibble)\n\n# A tibble: 6 × 7\n  sim_number     x       y    dx       dy      p       q\n  &lt;fct&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 1              1  1.05   -2.97 0.000398 0.854   1.05  \n2 1              2  0.0168 -2.84 0.00104  0.507   0.0168\n3 1              3  1.77   -2.72 0.00244  0.961   1.77  \n4 1              4 -1.81   -2.59 0.00518  0.0353 -1.81  \n5 1              5  0.447  -2.46 0.00997  0.673   0.447 \n6 1              6  1.05   -2.33 0.0174   0.854   1.05  \n\nclass(normal_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnormal_dt &lt;- tidy_normal(.return_tibble = FALSE)\nhead(normal_dt)\n\n   sim_number x           y        dx           dy         p           q\n1:          1 1  2.24103518 -3.424949 0.0002787401 0.9874881  2.24103518\n2:          1 2 -0.12769603 -3.286892 0.0008586864 0.4491948 -0.12769603\n3:          1 3 -0.39666069 -3.148835 0.0022824304 0.3458088 -0.39666069\n4:          1 4  0.89626001 -3.010778 0.0052656793 0.8149430  0.89626001\n5:          1 5  0.04267757 -2.872721 0.0105661984 0.5170207  0.04267757\n6:          1 6  0.53424808 -2.734664 0.0185083421 0.7034150  0.53424808\n\nclass(normal_dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\n\n\nReady to unleash the power of TidyDensity and data.table?\nDive into your next data exploration project and experience the efficiency firsthand! Share your discoveries and feedback with the community—we’re eager to hear how this upgrade empowers your analysis.\nHappy tidy data exploration!"
  },
  {
    "objectID": "posts/2024-01-16/index.html",
    "href": "posts/2024-01-16/index.html",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "",
    "text": "Greetings, fellow data enthusiasts! Today, we’re diving into the exciting world of tidyAML 0.0.4, where innovation meets efficiency in the realm of R programming. As we unpack the latest release, we’ll explore the new features, enhancements, and the overall impact of this powerful tool on your data science endeavors."
  },
  {
    "objectID": "posts/2024-01-16/index.html#introducing-extract_regression_residuals",
    "href": "posts/2024-01-16/index.html#introducing-extract_regression_residuals",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Introducing extract_regression_residuals()",
    "text": "Introducing extract_regression_residuals()\nOne of the standout features in this release is the addition of extract_regression_residuals(). This function empowers users to delve deeper into regression models, providing a valuable tool for analyzing and understanding residuals. Whether you’re fine-tuning your models or gaining insights into data patterns, this enhancement adds a crucial layer to your analytical arsenal."
  },
  {
    "objectID": "posts/2024-01-16/index.html#enhanced-classification-with-.drop_na",
    "href": "posts/2024-01-16/index.html#enhanced-classification-with-.drop_na",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "2. Enhanced Classification with .drop_na",
    "text": "2. Enhanced Classification with .drop_na\nResponding to user feedback and aiming for seamless user experience, tidyAML 0.0.4 brings forth an important addition to fast_classification() and fast_regression(). The introduction of the .drop_na parameter allows users to handle missing data more efficiently, streamlining the classification and regression processes."
  },
  {
    "objectID": "posts/2024-01-16/index.html#core-package-expansion",
    "href": "posts/2024-01-16/index.html#core-package-expansion",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Core Package Expansion",
    "text": "Core Package Expansion\nAcknowledging the diverse needs of data scientists, tidyAML now incorporates additional core packages. The inclusion of discrim, mda, sda, sparsediscrim, liquidSVM, kernlab, and klaR extends the scope of possibilities. These additions enhance the versatility of tidyAML, making it an even more comprehensive solution for your modeling requirements."
  },
  {
    "objectID": "posts/2024-01-16/index.html#refined-internal-predictions",
    "href": "posts/2024-01-16/index.html#refined-internal-predictions",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Refined Internal Predictions",
    "text": "Refined Internal Predictions\nThe update addresses #190 by refining the internal_make_wflw_predictions() function. Now, it includes all essential data elements: the actual data, training predictions, and testing predictions. This refinement ensures a more holistic view of your model’s performance, facilitating a comprehensive evaluation of its predictive capabilities."
  },
  {
    "objectID": "posts/2024-01-16/index.html#streamlined-regression-analysis",
    "href": "posts/2024-01-16/index.html#streamlined-regression-analysis",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Streamlined Regression Analysis",
    "text": "Streamlined Regression Analysis\nWith the introduction of extract_regression_residuals(), tidyAML empowers users to conduct in-depth regression analyses with ease. Uncover hidden patterns, identify outliers, and fine-tune your models for optimal performance."
  },
  {
    "objectID": "posts/2024-01-16/index.html#improved-data-handling-in-classification",
    "href": "posts/2024-01-16/index.html#improved-data-handling-in-classification",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Improved Data Handling in Classification",
    "text": "Improved Data Handling in Classification\nThe new .drop_na parameter in fast_classification() and fast_regression() simplifies the management of missing data. Enhance the robustness of your classification models by seamlessly handling missing values, resulting in more reliable and accurate predictions."
  },
  {
    "objectID": "posts/2024-01-16/index.html#comprehensive-core-packages",
    "href": "posts/2024-01-16/index.html#comprehensive-core-packages",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Comprehensive Core Packages",
    "text": "Comprehensive Core Packages\nThe expansion of core packages broadens the toolkit at your disposal. Whether you’re exploring discriminant analysis, support vector machines, or kernel methods, tidyAML now supports an extended range of algorithms, catering to diverse modeling needs."
  },
  {
    "objectID": "posts/2024-01-16/index.html#holistic-model-evaluation",
    "href": "posts/2024-01-16/index.html#holistic-model-evaluation",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Holistic Model Evaluation",
    "text": "Holistic Model Evaluation\nThe refined internal_make_wflw_predictions() ensures that you have all the necessary components for a comprehensive model evaluation. Analyze the actual data alongside training and testing predictions, gaining a 360-degree view of your model’s performance."
  },
  {
    "objectID": "posts/2024-01-16/index.html#enhanced-classificationregression-build-with-.drop_na",
    "href": "posts/2024-01-16/index.html#enhanced-classificationregression-build-with-.drop_na",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Enhanced Classification/Regression build with .drop_na",
    "text": "Enhanced Classification/Regression build with .drop_na\nResponding to user feedback and aiming for seamless user experience, tidyAML 0.0.4 brings forth an important addition to fast_classification() and fast_regression(). The introduction of the .drop_na parameter allows users to handle missing data more efficiently, streamlining the classification and regression processes."
  },
  {
    "objectID": "posts/2024-01-16/index.html#improved-data-handling-in-classification-and-regression",
    "href": "posts/2024-01-16/index.html#improved-data-handling-in-classification-and-regression",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Improved Data Handling in Classification and Regression",
    "text": "Improved Data Handling in Classification and Regression\nThe new .drop_na parameter in fast_classification() and fast_regression() simplifies the management of missing data. Enhance the robustness of your classification models by seamlessly handling missing values, resulting in more reliable and accurate predictions."
  },
  {
    "objectID": "posts/2024-01-17/UntitledQMD.html",
    "href": "posts/2024-01-17/UntitledQMD.html",
    "title": "Using .drop_na in Fast Classification and Regression",
    "section": "",
    "text": "In the newest release of tidyAML there has been an addition of a new parameter to the functions fast_classification() and fast_regression(). The parameter is .drop_na and it is a logical value that defaults to TRUE. This parameter is used to determine if the function should drop rows with missing values from the output if a model cannot be built for some reason. Let’s take a look at the function and it’s arguments.\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL,\n  .drop_na = TRUE\n)\n\n\n.data - The data being passed to the function for the regression problem .rec_obj - The recipe object being passed. .parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported. .parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported. .split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample .split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type. .drop_na - The default is TRUE, which will drop all NA’s from the data.\nNow let’s see this in action."
  },
  {
    "objectID": "posts/2024-01-17/UntitledQMD.html#arguments",
    "href": "posts/2024-01-17/UntitledQMD.html#arguments",
    "title": "Using .drop_na in Fast Classification and Regression",
    "section": "",
    "text": ".data - The data being passed to the function for the regression problem .rec_obj - The recipe object being passed. .parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported. .parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported. .split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample .split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type. .drop_na - The default is TRUE, which will drop all NA’s from the data.\nNow let’s see this in action."
  },
  {
    "objectID": "posts/2024-01-17/index.html",
    "href": "posts/2024-01-17/index.html",
    "title": "Using .drop_na in Fast Classification and Regression",
    "section": "",
    "text": "In the newest release of tidyAML there has been an addition of a new parameter to the functions fast_classification() and fast_regression(). The parameter is .drop_na and it is a logical value that defaults to TRUE. This parameter is used to determine if the function should drop rows with missing values from the output if a model cannot be built for some reason. Let’s take a look at the function and it’s arguments.\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL,\n  .drop_na = TRUE\n)\n\n\n.data - The data being passed to the function for the regression problem .rec_obj - The recipe object being passed. .parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported. .parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported. .split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample .split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type. .drop_na - The default is TRUE, which will drop all NA’s from the data.\nNow let’s see this in action."
  },
  {
    "objectID": "posts/2024-01-17/index.html#arguments",
    "href": "posts/2024-01-17/index.html#arguments",
    "title": "Using .drop_na in Fast Classification and Regression",
    "section": "",
    "text": ".data - The data being passed to the function for the regression problem .rec_obj - The recipe object being passed. .parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported. .parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported. .split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample .split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type. .drop_na - The default is TRUE, which will drop all NA’s from the data.\nNow let’s see this in action."
  },
  {
    "objectID": "posts/2024-01-18/index.html",
    "href": "posts/2024-01-18/index.html",
    "title": "Exploring the Enhanced Features of tidyAML’s internal_make_wflw_predictions()",
    "section": "",
    "text": "Hey R enthusiasts! Steve here, and today I’m excited to share some fantastic updates about a key function in the tidyAML package – internal_make_wflw_predictions(). The latest version addresses issue #190, ensuring that all crucial data is now included in the predictions. Let’s dive into the details!"
  },
  {
    "objectID": "posts/2024-01-18/index.html#arguments",
    "href": "posts/2024-01-18/index.html#arguments",
    "title": "Exploring the Enhanced Features of tidyAML’s internal_make_wflw_predictions()",
    "section": "Arguments:",
    "text": "Arguments:\n\n.model_tbl: The model table generated from a function like fast_regression_parsnip_spec_tbl(). Ensure that it has a class of “tidyaml_mod_spec_tbl.” This is typically used after running the internal_make_fitted_wflw() function and saving the resulting tibble.\n.splits_obj: The splits object obtained from the auto_ml function. It is internal to the auto_ml function."
  },
  {
    "objectID": "posts/2024-01-18/index.html#why-it-matters",
    "href": "posts/2024-01-18/index.html#why-it-matters",
    "title": "Exploring the Enhanced Features of tidyAML’s internal_make_wflw_predictions()",
    "section": "Why It Matters",
    "text": "Why It Matters\nBy including actual data along with training and testing predictions, the internal_make_wflw_predictions() function empowers you to perform a more thorough evaluation of your models. This is a significant step towards ensuring the reliability and generalization capability of your machine learning models.\nSo, R enthusiasts, update your tidyAML package, explore the enhanced features, and let us know how these improvements elevate your modeling experience. Happy coding!"
  },
  {
    "objectID": "posts/2024-01-19/index.html",
    "href": "posts/2024-01-19/index.html",
    "title": "The new function on the block with tidyAML extract_regression_residuals()",
    "section": "",
    "text": "Introduction\nYesterday I discussed the use of the function internal_make_wflw_predictions() in the tidyAML R package. Today I will discuss the use of the function extract_wflw_pred() and the brand new function extract_regression_residuals() in the tidyAML R package. We breifly saw yesterday the output of the function internal_make_wflw_predictions() which is a list of tibbles that are typically inside of a list column in the final output of fast_regression() and fast_classification(). The function extract_wflw_pred() takes this list of tibbles and extracts them from that output. The function extract_regression_residuals() also extracts those tibbles and has the added feature of also returning the residuals. Let’s see how these functions work.\n\n\nThe new function\nFirst, we will go over the syntax of the new function extract_regression_residuals().\nextract_regression_residuals(.model_tbl, .pivot_long = FALSE)\nThe function takes two arguments. The first argument is .model_tbl which is the output of fast_regression() or fast_classification(). The second argument is .pivot_long which is a logical argument that defaults to FALSE. If TRUE then the output will be in a long format. If FALSE then the output will be in a wide format. Let’s see how this works.\n\n\nExample\n\n# Load packages\nlibrary(tidyAML)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(multilevelmod) # for the gee model\n\ntidymodels_prefer() # good practice when using tidyAML\n\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\nfrt_tbl &lt;- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"stan\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n  )\n\nLet’s break down the R code step by step:\n\nLoading Libraries:\n\nlibrary(tidyAML)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(multilevelmod) # for the gee model\nHere, the code is loading several R packages. These packages provide functions and tools for data analysis, modeling, and visualization. tidyAML and tidymodels are particularly relevant for modeling, while tidyverse is a collection of packages for data manipulation and visualization. multilevelmod is included for the Generalized Estimating Equations (gee) model.\n\nSetting Preferences:\ntidymodels_prefer() # good practice when using tidyAML\n\nThis line of code is setting preferences for the tidy modeling workflow using tidymodels_prefer(). It ensures that when using tidyAML, the tidy modeling conventions are followed. Tidy modeling involves an organized and consistent approach to modeling in R.\n\nCreating a Recipe Object:\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\n\nHere, a recipe object (rec_obj) is created using the recipe function from the tidymodels package. The formula mpg ~ . specifies that we want to predict the mpg variable based on all other variables in the dataset (mtcars).\n\nPerforming Fast Regression:\nfrt_tbl &lt;- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"stan\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nThis part involves using the fast_regression function. It performs a fast regression analysis using various engines specified by .parsnip_eng and specific functions specified by .parsnip_fns. In this case, it includes linear models (lm), generalized linear models (glm), Stan models (stan), and the Generalized Estimating Equations model (gee). The results are stored in the frt_tbl table.\nIn summary, the code is setting up a tidy modeling workflow, creating a recipe for predicting mpg based on other variables in the mtcars dataset, and then performing a fast regression using different engines and functions. The choice of engines and functions allows flexibility in exploring different modeling approaches.\nNow that we have the output of fast_regression() stored in frt_tbl, we can use the function extract_wflw_pred() to extract the predictions and from the output. Let’s see how this works. First, the syntax:\nextract_wflw_pred(.data, .model_id = NULL)\nThe function takes two arguments. The first argument is .data which is the output of fast_regression() or fast_classification(). The second argument is .model_id which is a numeric vector that defaults to NULL. If NULL then the function will extract none of the predictions from the output. If a numeric vector is provided then the function will extract the predictions for the models specified by the numeric vector. Let’s see how this works.\n\nextract_wflw_pred(frt_tbl, 1)\n\n# A tibble: 64 × 4\n   .model_type     .data_category .data_type .value\n   &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 54 more rows\n\nextract_wflw_pred(frt_tbl, 1:2)\n\n# A tibble: 128 × 4\n   .model_type     .data_category .data_type .value\n   &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 118 more rows\n\nextract_wflw_pred(frt_tbl, 1:nrow(frt_tbl))\n\n# A tibble: 256 × 4\n   .model_type     .data_category .data_type .value\n   &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 246 more rows\n\n\nThe first line of code extracts the predictions for the first model in the output. The second line of code extracts the predictions for the first two models in the output. The third line of code extracts the predictions for all models in the output.\nNow, let’s visualize the predictions for the models in the output and the actual values. We will use the ggplot2 package for visualization. First, we will extract the predictions for all models in the output and store them in a table called pred_tbl. Then, we will use ggplot2 to visualize the predictions and actual values.\n\npred_tbl &lt;- extract_wflw_pred(frt_tbl, 1:nrow(frt_tbl))\n\npred_tbl |&gt;\n  group_split(.model_type) |&gt;\n  map(\\(x) x |&gt;\n        group_by(.data_category) |&gt;\n        mutate(x = row_number()) |&gt;\n        ungroup() |&gt;\n        pivot_wider(names_from = .data_type, values_from = .value) |&gt;\n        ggplot(aes(x = x, y = actual, group = .data_category)) +\n        geom_line(color = \"black\") +\n        geom_line(aes(x = x, y = training), linetype = \"dashed\", color = \"red\",\n                  linewidth = 1) +\n        geom_line(aes(x = x, y = testing), linetype = \"dashed\", color = \"blue\",\n                  linewidth = 1) +\n        theme_minimal() +\n        labs(\n          x = \"\",\n          y = \"Observed/Predicted Value\",\n          title = \"Observed vs. Predicted Values by Model Type\",\n          subtitle = x$.model_type[1]\n        )\n      )\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nOr we can facet them by model type:\n\npred_tbl |&gt;\n  group_by(.model_type, .data_category) |&gt;\n  mutate(x = row_number()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = x, y = .value)) +\n  geom_line(data = . %&gt;% filter(.data_type == \"actual\"), color = \"black\") +\n  geom_line(data = . %&gt;% filter(.data_type == \"training\"), \n            linetype = \"dashed\", color = \"red\") +\n  geom_line(data = . %&gt;% filter(.data_type == \"testing\"), \n            linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ .model_type, ncol = 2, scales = \"free\") +\n  labs(\n    x = \"\",\n    y = \"Observed/Predicted Value\",\n    title = \"Observed vs. Predicted Values by Model Type\"\n  ) +\n  theme_minimal()\n\n\n\n\nOk, so what about this new function I talked about above? Well let’s go over it here. We have already discussed it’s syntax so no need to go over it again. Let’s just jump right into an example. This function will return the residuals for all models. We will slice off just the first model for demonstration purposes.\n\nextract_regression_residuals(.model_tbl = frt_tbl, .pivot_long = FALSE)[[1]]\n\n# A tibble: 32 × 4\n   .model_type     .actual .predicted .resid\n   &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 lm - linear_reg    15.2       17.3 -2.09 \n 2 lm - linear_reg    10.4       11.9 -1.46 \n 3 lm - linear_reg    33.9       30.8  3.06 \n 4 lm - linear_reg    32.4       28.0  4.35 \n 5 lm - linear_reg    16.4       15.0  1.40 \n 6 lm - linear_reg    21.5       22.3 -0.779\n 7 lm - linear_reg    15.8       17.2 -1.40 \n 8 lm - linear_reg    15         15.1 -0.100\n 9 lm - linear_reg    14.7       10.9  3.85 \n10 lm - linear_reg    10.4       10.8 -0.445\n# ℹ 22 more rows\n\n\nNow let’s set .pivot_long = TRUE:\n\nextract_regression_residuals(.model_tbl = frt_tbl, .pivot_long = TRUE)[[1]]\n\n# A tibble: 96 × 3\n   .model_type     name       value\n   &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;\n 1 lm - linear_reg .actual    15.2 \n 2 lm - linear_reg .predicted 17.3 \n 3 lm - linear_reg .resid     -2.09\n 4 lm - linear_reg .actual    10.4 \n 5 lm - linear_reg .predicted 11.9 \n 6 lm - linear_reg .resid     -1.46\n 7 lm - linear_reg .actual    33.9 \n 8 lm - linear_reg .predicted 30.8 \n 9 lm - linear_reg .resid      3.06\n10 lm - linear_reg .actual    32.4 \n# ℹ 86 more rows\n\n\nNow let’s visualize the data:\n\nresid_tbl &lt;- extract_regression_residuals(frt_tbl, TRUE)\n\nresid_tbl |&gt;\n  map(\\(x) x |&gt;\n        group_by(name) |&gt;\n        mutate(x = row_number()) |&gt;\n        ungroup() |&gt;\n        mutate(plot_group = ifelse(name == \".resid\", \"Residuals\", \"Actual and Predictions\")) |&gt;\n        ggplot(aes(x = x, y = value, group = name, color = name)) +\n        geom_line() +\n        theme_minimal() +\n        facet_wrap(~ plot_group, ncol = 1, scales = \"free\") +\n        labs(\n          x = \"\",\n          y = \"Value\",\n          title = \"Actual, Predicted, and Residual Values by Model Type\",\n          subtitle = x$.model_type[1],\n          color = \"Data Type\"\n        )\n      )\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nAnd that’s it!\nThank you for reading and I would love to hear your feedback. Please feel free to reach out to me."
  },
  {
    "objectID": "posts/2024-01-22/index.html",
    "href": "posts/2024-01-22/index.html",
    "title": "Demystifying bootstrap_stat_plot(): Your Ticket to Insightful Data Exploration",
    "section": "",
    "text": "Ever feel like your data is hiding secrets? Like it’s whispering truths but you just can’t quite grasp them? Well, fear not, fellow data sleuths! Today, we’ll crack the code of an R function that’s like a magnifying glass for your statistical investigations: bootstrap_stat_plot() from the TidyDensity package.\nImagine this: You have a dataset, say, car mileage (MPG) from the classic mtcars dataset. You want to understand the average MPG, but what if that average is just a mirage? What if it’s skewed by a few outliers or doesn’t capture the full story?\nEnter bootstrapping, a statistical technique that’s like taking your data on a wild ride. It creates multiple copies of your data, each with a slight twist, and then calculates the statistic you’re interested in (e.g., average MPG) for each copy. This gives you a distribution of possible averages, revealing the variability and potential biases lurking beneath the surface.\nbootstrap_stat_plot() takes this magic a step further. It not only calculates the distribution but also visualizes it, giving you a clear picture of how the statistic fluctuates across different versions of your data. It’s like a magnifying glass for your statistical investigations!"
  },
  {
    "objectID": "posts/2024-01-22/index.html#syntax",
    "href": "posts/2024-01-22/index.html#syntax",
    "title": "Demystifying bootstrap_stat_plot(): Your Ticket to Insightful Data Exploration",
    "section": "Syntax",
    "text": "Syntax\nLet’s take a look at the function:\nbootstrap_stat_plot(\n  .data,\n  .value,\n  .stat = \"cmean\",\n  .show_groups = FALSE,\n  .show_ci_labels = TRUE,\n  .interactive = FALSE\n)"
  },
  {
    "objectID": "posts/2024-01-22/index.html#arguments",
    "href": "posts/2024-01-22/index.html#arguments",
    "title": "Demystifying bootstrap_stat_plot(): Your Ticket to Insightful Data Exploration",
    "section": "Arguments",
    "text": "Arguments\n1. The Data:\n\n.data: The data frame containing your data.\n\n2. The Value:\n\n.value: The variable you want to calculate the statistic for.\n\n3. The Statistic:\n\n.stat: The statistic you want to calculate. Options include:\n\ncmean: The mean\ncmedian: The median\ncmin: The minimum\ncmax: The maximum\ncsd: The standard deviation\ncvar: The variance\nand many others!\n\n\n4. Show Groups:\n\n.show_groups: Whether to show the groups in the plot. Default is FALSE.\n\n5. Show Confidence Interval Labels:\n\n.show_ci_labels: Whether to show the confidence interval labels in the plot. Default is TRUE.\n\n6. Interactive:\n\n.interactive: Whether to make the plot interactive. Default is FALSE."
  },
  {
    "objectID": "posts/2024-01-23/index.html",
    "href": "posts/2024-01-23/index.html",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "Hey folks! Today, we’re diving into the world of R programming, and our star of the show is the lengths() function. This little gem might not be as famous as some other R functions, but it’s incredibly handy when it comes to exploring the lengths of elements in your data structures.\n\n\nIn a nutshell, lengths() is a function in R that returns a vector of the lengths of the elements in a list, vector, or other data structure. It’s like a measuring tape for your data, allowing you to quickly assess the size of different components.\n\n\n\n\n\n\n# Create a numeric vector\nnumeric_vector &lt;- c(10, 20, 30, 40, 50)\n\n# Use lengths() to get the lengths of elements\nelement_lengths &lt;- lengths(list(numeric_vector))\n\n# Print the result\nprint(element_lengths)\n\n[1] 5\n\n\nIn this example, we create a numeric vector and use lengths() to find out how many elements it contains. The output will be a vector with a single value, representing the length of our numeric vector.\n\n\n\n\n# Create a list with elements of different lengths\nmixed_list &lt;- list(c(1, 2, 3), \"Hello\", matrix(1:6, ncol = 2))\n\n# Use lengths() to get the lengths of elements\nelement_lengths &lt;- lengths(mixed_list)\n\n# Print the result\nprint(element_lengths)\n\n[1] 3 1 6\n\n\nHere, we’ve crafted a list with diverse elements – a numeric vector, a character string, and a matrix. lengths() now gives us a vector containing the lengths of each element in the list.\n\n\n\n\n# Create a data frame\ndata_frame_example &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n                                 Age = c(25, 30, 22),\n                                 Score = c(90, 85, 95))\n\n# Use lengths() to get the lengths of columns in the data frame\ncolumn_lengths &lt;- lengths(data_frame_example)\n\n# Print the result\nprint(column_lengths)\n\n Name   Age Score \n    3     3     3 \n\n\nIn this example, we’re working with a data frame. lengths() allows us to check the number of elements in each column, providing insights into the structure of our data.\n\n\n\n\nUnderstanding the lengths of elements in your data is crucial for efficient data manipulation. Whether you’re dealing with lists, vectors, or data frames, knowing the sizes of different components can guide your analysis and help you avoid unexpected surprises.\n\n\n\nNow that you’ve seen some examples, I encourage you to grab your own datasets, create different structures, and experiment with lengths(). It’s a fantastic tool for quickly grasping the dimensions of your data.\nRemember, the best way to learn is by doing. So fire up your R console, start experimenting, and feel the satisfaction of mastering yet another powerful tool in your R toolkit!\nHappy coding! 🚀✨"
  },
  {
    "objectID": "posts/2024-01-23/index.html#what-is-lengths-and-why-should-you-care",
    "href": "posts/2024-01-23/index.html#what-is-lengths-and-why-should-you-care",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "In a nutshell, lengths() is a function in R that returns a vector of the lengths of the elements in a list, vector, or other data structure. It’s like a measuring tape for your data, allowing you to quickly assess the size of different components."
  },
  {
    "objectID": "posts/2024-01-23/index.html#lets-get-started-with-examples",
    "href": "posts/2024-01-23/index.html#lets-get-started-with-examples",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "# Create a numeric vector\nnumeric_vector &lt;- c(10, 20, 30, 40, 50)\n\n# Use lengths() to get the lengths of elements\nelement_lengths &lt;- lengths(list(numeric_vector))\n\n# Print the result\nprint(element_lengths)\n\n[1] 5\n\n\nIn this example, we create a numeric vector and use lengths() to find out how many elements it contains. The output will be a vector with a single value, representing the length of our numeric vector.\n\n\n\n\n# Create a list with elements of different lengths\nmixed_list &lt;- list(c(1, 2, 3), \"Hello\", matrix(1:6, ncol = 2))\n\n# Use lengths() to get the lengths of elements\nelement_lengths &lt;- lengths(mixed_list)\n\n# Print the result\nprint(element_lengths)\n\n[1] 3 1 6\n\n\nHere, we’ve crafted a list with diverse elements – a numeric vector, a character string, and a matrix. lengths() now gives us a vector containing the lengths of each element in the list.\n\n\n\n\n# Create a data frame\ndata_frame_example &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n                                 Age = c(25, 30, 22),\n                                 Score = c(90, 85, 95))\n\n# Use lengths() to get the lengths of columns in the data frame\ncolumn_lengths &lt;- lengths(data_frame_example)\n\n# Print the result\nprint(column_lengths)\n\n Name   Age Score \n    3     3     3 \n\n\nIn this example, we’re working with a data frame. lengths() allows us to check the number of elements in each column, providing insights into the structure of our data."
  },
  {
    "objectID": "posts/2024-01-23/index.html#why-should-you-experiment",
    "href": "posts/2024-01-23/index.html#why-should-you-experiment",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "Understanding the lengths of elements in your data is crucial for efficient data manipulation. Whether you’re dealing with lists, vectors, or data frames, knowing the sizes of different components can guide your analysis and help you avoid unexpected surprises."
  },
  {
    "objectID": "posts/2024-01-23/index.html#your-turn-to-play",
    "href": "posts/2024-01-23/index.html#your-turn-to-play",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "Now that you’ve seen some examples, I encourage you to grab your own datasets, create different structures, and experiment with lengths(). It’s a fantastic tool for quickly grasping the dimensions of your data.\nRemember, the best way to learn is by doing. So fire up your R console, start experimenting, and feel the satisfaction of mastering yet another powerful tool in your R toolkit!\nHappy coding! 🚀✨"
  },
  {
    "objectID": "posts/2024-01-24/index.html",
    "href": "posts/2024-01-24/index.html",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Greetings fellow R enthusiasts! Today, let’s dive into the fascinating world of date calculations. Whether you’re a data scientist, analyst, or just someone who loves coding in R, understanding how to calculate the number of months between dates is a valuable skill. In this blog post, we’ll explore two approaches using both base R and the lubridate package, ensuring you have the tools to tackle any date-related challenge that comes your way."
  },
  {
    "objectID": "posts/2024-01-24/index.html#base-r-method",
    "href": "posts/2024-01-24/index.html#base-r-method",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Base R Method",
    "text": "Base R Method\nLet’s start with the basics – base R. The difftime function will be our trusty companion in this method. The idea is to find the time difference between two dates and then convert it into months.\n\n# Sample dates\nstart_date &lt;- as.Date(\"2022-01-15\")\nend_date &lt;- as.Date(\"2023-07-20\")\n\n# Calculate time difference in days\ntime_diff_days &lt;- end_date - start_date\n\n# Convert days to months\nmonths_diff_base &lt;- as.numeric(time_diff_days) / 30.44  # average days in a month\n\ncat(\"Number of months using base R:\", round(months_diff_base, 2), \"\\n\")\n\nNumber of months using base R: 18.1"
  },
  {
    "objectID": "posts/2024-01-24/index.html#explanation",
    "href": "posts/2024-01-24/index.html#explanation",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Explanation",
    "text": "Explanation\n\nWe define our start and end dates using the as.Date function.\nCalculate the time difference in days using the subtraction operator.\nConvert the time difference to months by dividing by the average days in a month (30.44)."
  },
  {
    "objectID": "posts/2024-01-24/index.html#lubridate-package-method",
    "href": "posts/2024-01-24/index.html#lubridate-package-method",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Lubridate Package Method",
    "text": "Lubridate Package Method\nNow, let’s add a touch of elegance to our date calculations with the lubridate package. This package simplifies working with dates and times in R, making our code more readable and intuitive.\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Sample dates\nstart_date &lt;- ymd(\"2022-01-15\")\nend_date &lt;- ymd(\"2023-07-20\")\n\n# Calculate months difference using lubridate\nmonths_diff_lubridate &lt;- interval(start_date, end_date) %/% months(1)\n\ncat(\"Number of months using lubridate:\", months_diff_lubridate, \"\\n\")\n\nNumber of months using lubridate: 18"
  },
  {
    "objectID": "posts/2024-01-24/index.html#explanation-1",
    "href": "posts/2024-01-24/index.html#explanation-1",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Explanation",
    "text": "Explanation\n\nWe load the lubridate package to leverage its convenient date functions.\nUse the ymd function to convert our dates into lubridate date objects.\nCreate an interval between the start and end dates and use %/% to get the floor division by months."
  },
  {
    "objectID": "posts/2024-01-24/index.html#handling-partial-months",
    "href": "posts/2024-01-24/index.html#handling-partial-months",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Handling Partial Months",
    "text": "Handling Partial Months\nLife isn’t always about whole months, and our date calculations should reflect that reality. Let’s modify our examples to include partial months.\n\n# Sample dates with partial months\nstart_date_partial &lt;- as.Date(\"2022-01-15\")\nend_date_partial &lt;- as.Date(\"2023-07-20\") - 15  # subtract 15 days for a partial month\n\n# Base R with partial months\ntime_diff_days_partial &lt;- end_date_partial - start_date_partial\nmonths_diff_base_partial &lt;- as.numeric(time_diff_days_partial) / 30.44\n\ncat(\"Number of months (with partial) using base R:\", round(months_diff_base_partial, 2), \"\\n\")\n\nNumber of months (with partial) using base R: 17.61 \n\n# Lubridate with partial months\nmonths_diff_lubridate_partial &lt;- interval(start_date_partial, end_date_partial) / months(1)\n\ncat(\"Number of months (with partial) using lubridate:\", months_diff_lubridate_partial, \"\\n\")\n\nNumber of months (with partial) using lubridate: 17.66667"
  },
  {
    "objectID": "posts/2024-01-24/index.html#more-lubridate-with-interval",
    "href": "posts/2024-01-24/index.html#more-lubridate-with-interval",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "More lubridate with interval()",
    "text": "More lubridate with interval()\nThe lubridate package makes working with dates in R much easier. It provides the interval function to calculate the time difference between two dates:\n\ndate1 &lt;- ymd(\"2023-01-15\")\ndate2 &lt;- ymd(\"2024-04-30\")\n\ninterval(date1, date2) / months(1) \n\n[1] 15.5\n\n\nThis returns the number of months including the partial:\n[1] 15.870968\nTo get just the full months:\n\ninterval(date1, date2) %/% months(1)\n\n[1] 15\n\n\nWhich gives:\n[1] 15\nThe interval function combined with lubridate’s months makes this a very clean way to calculate both full and partial months between dates."
  },
  {
    "objectID": "posts/2024-01-25/index.html",
    "href": "posts/2024-01-25/index.html",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Welcome back, fellow R enthusiasts! Today, we’re diving into a common task in data manipulation: subtracting hours from time objects in R. Whether you’re working with timestamps, time durations, or time series data, knowing how to subtract hours can be incredibly useful. In this post, we’ll explore two popular methods: using base R functions and the lubridate package.\n\n\nBefore we jump into the code, let’s quickly discuss why you might need to subtract hours from time objects. This operation is handy in various scenarios, such as:\n\nAdjusting timestamps for different time zones.\nCalculating time differences between events.\nShifting time points in time series analysis.\n\nNow, let’s get our hands dirty with some code!\n\n\n\nIn base R, we can perform basic arithmetic operations on time objects. To subtract hours from a time object, we’ll use the POSIXct class, which represents date and time information. Here’s a simple example:\n\n# Create a POSIXct object representing a specific time\nmy_time &lt;- as.POSIXct(\"2024-01-25 10:00:00\")\n\n# Subtract 2 hours from the original time\nnew_time &lt;- my_time - (2 * 60 * 60)\n\n# Print the original and modified times\nprint(my_time)\n\n[1] \"2024-01-25 10:00:00 EST\"\n\nprint(new_time)\n\n[1] \"2024-01-25 08:00:00 EST\"\n\n\nIn this code snippet, we first create a POSIXct object my_time representing 10:00 AM on January 25, 2024. Then, we subtract 2 hours and assign the result to new_time. Finally, we print both the original and modified times to see the difference.\n\n\n\nThe lubridate package provides convenient functions for handling date-time data in R. It simplifies common tasks like parsing dates, extracting components, and performing arithmetic operations. Let’s see how we can subtract hours using lubridate:\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Create a POSIXct object representing a specific time\nmy_time &lt;- ymd_hms(\"2024-01-25 10:00:00\")\n\n# Subtract 2 hours from the original time\nnew_time &lt;- my_time - hours(2)\n\n# Print the original and modified times\nprint(my_time)\n\n[1] \"2024-01-25 10:00:00 UTC\"\n\nprint(new_time)\n\n[1] \"2024-01-25 08:00:00 UTC\"\n\n\nIn this example, we start by loading the lubridate package. Then, we use the ymd_hms() function to create a POSIXct object my_time. Next, we subtract 2 hours using the hours() function and assign the result to new_time. Finally, we print both times to compare the changes.\n\n\n\nLet’s explore a few more examples to solidify our understanding:\n\n\n\n# Create a vector of POSIXct times\ntimes &lt;- as.POSIXct(c(\"2024-01-25 08:00:00\", \"2024-01-25 12:00:00\"))\n\n# Subtract 1 hour from each time\nadjusted_times &lt;- times - hours(1)\n\n# Print the original and modified times\nprint(times)\n\n[1] \"2024-01-25 08:00:00 EST\" \"2024-01-25 12:00:00 EST\"\n\nprint(adjusted_times)\n\n[1] \"2024-01-25 07:00:00 EST\" \"2024-01-25 11:00:00 EST\"\n\n\nIn this example, we have a vector of two times, and we subtract 1 hour from each using the hours() function.\n\n\n\n\n# Create a time interval from 9:00 AM to 5:00 PM\ntime_interval &lt;- interval(ymd_hms(\"2024-01-25 09:00:00\"), ymd_hms(\"2024-01-25 17:00:00\"))\n\n# Subtract 2 hours from the interval\nadjusted_interval &lt;- int_shift(time_interval, - hours(2))\n\n# Print the original and modified intervals\nprint(time_interval)\n\n[1] 2024-01-25 09:00:00 UTC--2024-01-25 17:00:00 UTC\n\nprint(adjusted_interval)\n\n[1] 2024-01-25 07:00:00 UTC--2024-01-25 15:00:00 UTC\n\n\nIn this example, we create a time interval representing working hours and subtract 2 hours from it.\n\n\n\n\nSubtracting hours from time objects is a fundamental operation in data manipulation and time series analysis. In this post, we explored two methods: using base R functions and the lubridate package. Whether you prefer the simplicity of base R or the convenience of lubridate, mastering this skill will undoubtedly enhance your R programming repertoire.\nNow it’s your turn! Try out these examples with your own time data and experiment with different hour values. Don’t hesitate to reach out if you have any questions or want to share your experiences. Happy coding!"
  },
  {
    "objectID": "posts/2024-01-25/index.html#why-subtract-hours",
    "href": "posts/2024-01-25/index.html#why-subtract-hours",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Before we jump into the code, let’s quickly discuss why you might need to subtract hours from time objects. This operation is handy in various scenarios, such as:\n\nAdjusting timestamps for different time zones.\nCalculating time differences between events.\nShifting time points in time series analysis.\n\nNow, let’s get our hands dirty with some code!"
  },
  {
    "objectID": "posts/2024-01-25/index.html#using-base-r-functions",
    "href": "posts/2024-01-25/index.html#using-base-r-functions",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "In base R, we can perform basic arithmetic operations on time objects. To subtract hours from a time object, we’ll use the POSIXct class, which represents date and time information. Here’s a simple example:\n\n# Create a POSIXct object representing a specific time\nmy_time &lt;- as.POSIXct(\"2024-01-25 10:00:00\")\n\n# Subtract 2 hours from the original time\nnew_time &lt;- my_time - (2 * 60 * 60)\n\n# Print the original and modified times\nprint(my_time)\n\n[1] \"2024-01-25 10:00:00 EST\"\n\nprint(new_time)\n\n[1] \"2024-01-25 08:00:00 EST\"\n\n\nIn this code snippet, we first create a POSIXct object my_time representing 10:00 AM on January 25, 2024. Then, we subtract 2 hours and assign the result to new_time. Finally, we print both the original and modified times to see the difference."
  },
  {
    "objectID": "posts/2024-01-25/index.html#using-lubridate-package",
    "href": "posts/2024-01-25/index.html#using-lubridate-package",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "The lubridate package provides convenient functions for handling date-time data in R. It simplifies common tasks like parsing dates, extracting components, and performing arithmetic operations. Let’s see how we can subtract hours using lubridate:\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Create a POSIXct object representing a specific time\nmy_time &lt;- ymd_hms(\"2024-01-25 10:00:00\")\n\n# Subtract 2 hours from the original time\nnew_time &lt;- my_time - hours(2)\n\n# Print the original and modified times\nprint(my_time)\n\n[1] \"2024-01-25 10:00:00 UTC\"\n\nprint(new_time)\n\n[1] \"2024-01-25 08:00:00 UTC\"\n\n\nIn this example, we start by loading the lubridate package. Then, we use the ymd_hms() function to create a POSIXct object my_time. Next, we subtract 2 hours using the hours() function and assign the result to new_time. Finally, we print both times to compare the changes."
  },
  {
    "objectID": "posts/2024-01-25/index.html#additional-examples",
    "href": "posts/2024-01-25/index.html#additional-examples",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Let’s explore a few more examples to solidify our understanding:\n\n\n\n# Create a vector of POSIXct times\ntimes &lt;- as.POSIXct(c(\"2024-01-25 08:00:00\", \"2024-01-25 12:00:00\"))\n\n# Subtract 1 hour from each time\nadjusted_times &lt;- times - hours(1)\n\n# Print the original and modified times\nprint(times)\n\n[1] \"2024-01-25 08:00:00 EST\" \"2024-01-25 12:00:00 EST\"\n\nprint(adjusted_times)\n\n[1] \"2024-01-25 07:00:00 EST\" \"2024-01-25 11:00:00 EST\"\n\n\nIn this example, we have a vector of two times, and we subtract 1 hour from each using the hours() function.\n\n\n\n\n# Create a time interval from 9:00 AM to 5:00 PM\ntime_interval &lt;- interval(ymd_hms(\"2024-01-25 09:00:00\"), ymd_hms(\"2024-01-25 17:00:00\"))\n\n# Subtract 2 hours from the interval\nadjusted_interval &lt;- int_shift(time_interval, - hours(2))\n\n# Print the original and modified intervals\nprint(time_interval)\n\n[1] 2024-01-25 09:00:00 UTC--2024-01-25 17:00:00 UTC\n\nprint(adjusted_interval)\n\n[1] 2024-01-25 07:00:00 UTC--2024-01-25 15:00:00 UTC\n\n\nIn this example, we create a time interval representing working hours and subtract 2 hours from it."
  },
  {
    "objectID": "posts/2024-01-25/index.html#conclusion",
    "href": "posts/2024-01-25/index.html#conclusion",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Subtracting hours from time objects is a fundamental operation in data manipulation and time series analysis. In this post, we explored two methods: using base R functions and the lubridate package. Whether you prefer the simplicity of base R or the convenience of lubridate, mastering this skill will undoubtedly enhance your R programming repertoire.\nNow it’s your turn! Try out these examples with your own time data and experiment with different hour values. Don’t hesitate to reach out if you have any questions or want to share your experiences. Happy coding!"
  },
  {
    "objectID": "posts/2024-01-26/index.html",
    "href": "posts/2024-01-26/index.html",
    "title": "Unveiling the Earliest Date: A Journey Through R",
    "section": "",
    "text": "Greetings, fellow data enthusiasts! Today, we embark on a quest to uncover the earliest date lurking within a column of dates using the power of R. Whether you’re a seasoned R programmer or a curious newcomer, fear not, for we shall navigate through this journey step by step, unraveling the mysteries of date manipulation along the way.\nImagine you have a dataset filled with dates, and you’re tasked with finding the earliest one among them. How would you tackle this challenge? Fear not, for R comes to our rescue with its arsenal of functions and packages."
  },
  {
    "objectID": "posts/2024-01-26/index.html#example-1-finding-the-earliest-date-in-a-simple-dataset",
    "href": "posts/2024-01-26/index.html#example-1-finding-the-earliest-date-in-a-simple-dataset",
    "title": "Unveiling the Earliest Date: A Journey Through R",
    "section": "Example 1: Finding the earliest date in a simple dataset:",
    "text": "Example 1: Finding the earliest date in a simple dataset:\n\n# Sample dataset\ndates &lt;- as.Date(c(\"2023-01-15\", \"2023-02-20\", \"2022-12-10\"))\n\n# Finding the earliest date\nearliest_date &lt;- min(dates)\nprint(earliest_date)\n\n[1] \"2022-12-10\""
  },
  {
    "objectID": "posts/2024-01-26/index.html#example-2-handling-missing-values-gracefully",
    "href": "posts/2024-01-26/index.html#example-2-handling-missing-values-gracefully",
    "title": "Unveiling the Earliest Date: A Journey Through R",
    "section": "Example 2: Handling missing values gracefully:",
    "text": "Example 2: Handling missing values gracefully:\n\n# Sample dataset with missing values\ndates_with_na &lt;- as.Date(c(\"2023-01-15\", NA, \"2022-12-10\"))\n\n# Finding the earliest date, ignoring missing values\nearliest_date &lt;- min(dates_with_na, na.rm = TRUE)\nprint(earliest_date)\n\n[1] \"2022-12-10\""
  },
  {
    "objectID": "posts/2024-01-29/index.html",
    "href": "posts/2024-01-29/index.html",
    "title": "How to Extract Month from Date in R (With Examples)",
    "section": "",
    "text": "Greetings fellow R enthusiasts! Today, we’re diving into a fundamental task: extracting the month from a date in R. Whether you’re new to R or a seasoned pro, understanding how to manipulate dates is essential. We’ll explore two popular methods: using base R and the powerful lubridate package. So, let’s roll up our sleeves and get started!\n\n\nFirst up, let’s tackle the task with base R. We’ll use the format() function to extract the month from a date.\n\n\n\n\n# Create a vector of dates\ndates_vector &lt;- as.Date(c(\"2023-01-15\", \"2023-05-20\", \"2023-09-10\"))\n\n# Extract the month\nmonths &lt;- format(dates_vector, \"%m\")\n\n# Print the result\nprint(months)\n\n[1] \"01\" \"05\" \"09\"\n\n\nIn this example, we have a vector of dates. We use the format() function to specify that we want to extract the month (%m), and voila! We get the months corresponding to each date.\n\n\n\n# Create a sample data frame\ndf &lt;- data.frame(date = as.Date(c(\"2023-01-15\", \"2023-05-20\", \"2023-09-10\")))\n\n# Extract the month from the 'date' column\ndf$month &lt;- format(df$date, \"%m\")\n\n# Print the data frame with the new 'month' column\nprint(df)\n\n        date month\n1 2023-01-15    01\n2 2023-05-20    05\n3 2023-09-10    09\n\n\nHere, we’re working with a data frame. We use the $ operator to access the ‘date’ column and apply the format() function to extract the month. The result is a data frame with an additional ‘month’ column containing the extracted months.\n\n\n\n\n# Single date\nsingle_date &lt;- as.Date(\"2023-07-04\")\n\n# Extract the month\nmonth &lt;- format(single_date, \"%m\")\n\n# Print the result\nprint(month)\n\n[1] \"07\"\n\n\nEven if you have just one date, you can still use the format() function to extract the month. Simple and effective!\n\n\n\n\nNow, let’s switch gears and explore how to achieve the same task using the lubridate package, known for its user-friendly date-time functions.\n\n\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Create a sample date\ndate &lt;- ymd(\"2023-11-30\")\n\n# Extract the month using lubridate's month() function\nmonth &lt;- month(date)\n\n# Print the result\nprint(month)\n\n[1] 11\n\n\nWith lubridate, we simplify the process using the month() function directly on the date object. It’s clean, concise, and effortlessly extracts the month."
  },
  {
    "objectID": "posts/2024-01-29/index.html#using-base-r",
    "href": "posts/2024-01-29/index.html#using-base-r",
    "title": "How to Extract Month from Date in R (With Examples)",
    "section": "",
    "text": "First up, let’s tackle the task with base R. We’ll use the format() function to extract the month from a date."
  },
  {
    "objectID": "posts/2024-01-29/index.html#example-1-extracting-month-from-a-vector-of-dates",
    "href": "posts/2024-01-29/index.html#example-1-extracting-month-from-a-vector-of-dates",
    "title": "How to Extract Month from Date in R (With Examples)",
    "section": "",
    "text": "# Create a vector of dates\ndates_vector &lt;- as.Date(c(\"2023-01-15\", \"2023-05-20\", \"2023-09-10\"))\n\n# Extract the month\nmonths &lt;- format(dates_vector, \"%m\")\n\n# Print the result\nprint(months)\n\n[1] \"01\" \"05\" \"09\"\n\n\nIn this example, we have a vector of dates. We use the format() function to specify that we want to extract the month (%m), and voila! We get the months corresponding to each date.\n\n\n\n# Create a sample data frame\ndf &lt;- data.frame(date = as.Date(c(\"2023-01-15\", \"2023-05-20\", \"2023-09-10\")))\n\n# Extract the month from the 'date' column\ndf$month &lt;- format(df$date, \"%m\")\n\n# Print the data frame with the new 'month' column\nprint(df)\n\n        date month\n1 2023-01-15    01\n2 2023-05-20    05\n3 2023-09-10    09\n\n\nHere, we’re working with a data frame. We use the $ operator to access the ‘date’ column and apply the format() function to extract the month. The result is a data frame with an additional ‘month’ column containing the extracted months.\n\n\n\n\n# Single date\nsingle_date &lt;- as.Date(\"2023-07-04\")\n\n# Extract the month\nmonth &lt;- format(single_date, \"%m\")\n\n# Print the result\nprint(month)\n\n[1] \"07\"\n\n\nEven if you have just one date, you can still use the format() function to extract the month. Simple and effective!"
  },
  {
    "objectID": "posts/2024-01-29/index.html#using-lubridate-package",
    "href": "posts/2024-01-29/index.html#using-lubridate-package",
    "title": "How to Extract Month from Date in R (With Examples)",
    "section": "",
    "text": "Now, let’s switch gears and explore how to achieve the same task using the lubridate package, known for its user-friendly date-time functions.\n\n\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Create a sample date\ndate &lt;- ymd(\"2023-11-30\")\n\n# Extract the month using lubridate's month() function\nmonth &lt;- month(date)\n\n# Print the result\nprint(month)\n\n[1] 11\n\n\nWith lubridate, we simplify the process using the month() function directly on the date object. It’s clean, concise, and effortlessly extracts the month."
  },
  {
    "objectID": "posts/2024-01-30/index.html",
    "href": "posts/2024-01-30/index.html",
    "title": "Mastering Time Manipulation in R: Subtracting Hours with Ease",
    "section": "",
    "text": "Ever wished you could rewind time in R, not just for debugging, but for actual data analysis? Well, you don’t need plutonium and flux capacitors! Let’s dive into the fascinating world of time manipulation in R, specifically subtracting hours from timestamps. We’ll explore two approaches: one using base R’s time-bending tricks, and another powered by the lubridate package, our time-traveling companion."
  },
  {
    "objectID": "posts/2024-01-30/index.html#base-r-back-to-the-basics",
    "href": "posts/2024-01-30/index.html#base-r-back-to-the-basics",
    "title": "Mastering Time Manipulation in R: Subtracting Hours with Ease",
    "section": "Base R: Back to the Basics",
    "text": "Base R: Back to the Basics\nImagine a timestamp like a ticking clock. Each second is another notch on the gears, and we want to turn those gears backward a few hours. Base R lets us do this by treating time as numbers. Remember, there are 3600 seconds in an hour, so to subtract 2 hours, we simply:\n\nmy_time &lt;- as.POSIXct(\"2024-01-30 10:00:00\") # Create a time object\nnew_time &lt;- my_time - (2 * 3600) # Subtract 2 hours (2 * 3600 seconds)\nprint(my_time) # See the original time\n\n[1] \"2024-01-30 10:00:00 EST\"\n\nprint(new_time) # Voila! 2 hours back!\n\n[1] \"2024-01-30 08:00:00 EST\"\n\n\nThis code tells R to:\n\nCreate a time object my_time representing “January 30, 2024, 10:00 AM”.\nDefine new_time by subtracting 2 hours from my_time. We multiply 2 by 3600 because, well, you get the point.\nPrint both times to see the magic unfold."
  },
  {
    "objectID": "posts/2024-01-30/index.html#lubridate-time-travel-made-easy",
    "href": "posts/2024-01-30/index.html#lubridate-time-travel-made-easy",
    "title": "Mastering Time Manipulation in R: Subtracting Hours with Ease",
    "section": "Lubridate: Time Travel Made Easy",
    "text": "Lubridate: Time Travel Made Easy\nBut what if you want a fancier ride? This is where lubridate comes in! This package adds superpowers to our time-traveling toolkit. Let’s rewrite the above using its hours() function:\n\nlibrary(lubridate) # Load the lubridate package\n\nmy_time &lt;- as.POSIXct(\"2024-01-30 10:00:00\")\nnew_time &lt;- my_time - hours(2) # Subtract 2 hours with the `hours()` function\nprint(my_time)\n\n[1] \"2024-01-30 10:00:00 EST\"\n\nprint(new_time)\n\n[1] \"2024-01-30 08:00:00 EST\"\n\n\nThis code does the same thing, but with less math and more clarity. We simply tell R to subtract 2 hours using the hours(2) function, making the code cleaner and more readable."
  },
  {
    "objectID": "posts/2024-01-31/index.html",
    "href": "posts/2024-01-31/index.html",
    "title": "Time Flies? Time Travels! Adding Days to Dates in R (Like a Pro)",
    "section": "",
    "text": "Ever wished you could skip ahead a few days for that weekend getaway, or rewind to relive a magical moment? While real-life time travel remains a sci-fi dream, in R, adding days to dates is a breeze! Today, we’ll explore both base R and the powerful lubridate and timetk packages to master this handy skill."
  },
  {
    "objectID": "posts/2024-01-31/index.html#example-1-base-r-basics",
    "href": "posts/2024-01-31/index.html#example-1-base-r-basics",
    "title": "Time Flies? Time Travels! Adding Days to Dates in R (Like a Pro)",
    "section": "Example 1: Base R Basics",
    "text": "Example 1: Base R Basics\nLet’s start with the classic. Imagine you have a date stored as my_date &lt;- \"2024-01-31\" (yes, today!). To add, say, 5 days, you can simply use my_date + 5. Voila! You’ve time-jumped to February 5th, 2024. But wait, this doesn’t handle months or leap years like a pro.\n\n# Create a date object\ndate &lt;- as.Date(\"2024-01-31\")\n\n# Add 5 days to the date\nnew_date &lt;- date + 5\n\nprint(date)\n\n[1] \"2024-01-31\"\n\nprint(new_date)\n\n[1] \"2024-02-05\"\n\nclass(date)\n\n[1] \"Date\"\n\nclass(new_date)\n\n[1] \"Date\""
  },
  {
    "objectID": "posts/2024-01-31/index.html#example-2-enter-lubridate",
    "href": "posts/2024-01-31/index.html#example-2-enter-lubridate",
    "title": "Time Flies? Time Travels! Adding Days to Dates in R (Like a Pro)",
    "section": "Example 2: Enter lubridate",
    "text": "Example 2: Enter lubridate\nThis superhero package offers functions like as.Date() and days() that understand the nuances of dates. Let’s revisit our example:\n\nlibrary(lubridate)\n\nmy_date &lt;- as.Date(\"2024-01-31\") # Convert string to Date object\nfuture_date &lt;- my_date + days(5) # Add 5 days using days()\n\nfuture_date # \"2024-02-05\"\n\n[1] \"2024-02-05\"\n\n\nSee the magic? days(5) tells R to add 5 days specifically. You can even subtract days (imagine reliving that delicious pizza!):\n\npizza_day &lt;- as.Date(\"2024-01-27\") # Date of pizza bliss\nrelive_pizza &lt;- pizza_day - days(2) # Travel back 2 days\n\nrelive_pizza # \"2024-01-25\"\n\n[1] \"2024-01-25\""
  },
  {
    "objectID": "posts/2024-01-31/index.html#example-3-beyond-days-timetk-takes-the-wheel",
    "href": "posts/2024-01-31/index.html#example-3-beyond-days-timetk-takes-the-wheel",
    "title": "Time Flies? Time Travels! Adding Days to Dates in R (Like a Pro)",
    "section": "Example 3: Beyond Days: timetk Takes the Wheel",
    "text": "Example 3: Beyond Days: timetk Takes the Wheel\nWant to add weeks, months, or even years? timetk takes things to the next level with functions like years(), wednesdays(), and more. Check this out:\n\nlibrary(timetk)\n\ngraduation &lt;- as.Date(\"2025-06-15\") # Your graduation date (hopefully!)\n\ngraduation %+time% \"1 hour 34 seconds\"\n\n[1] \"2025-06-15 01:00:34 UTC\"\n\ngraduation %+time% \"3 months\"\n\n[1] \"2025-09-15\"\n\ngraduation %+time% \"1 year 3 months 6 days\"\n\n[1] \"2026-09-21\"\n\n# Backward (Minus Time)\ngraduation %-time% \"1 hour 34 seconds\"\n\n[1] \"2025-06-14 22:59:26 UTC\"\n\ngraduation %-time% \"3 months\"\n\n[1] \"2025-03-15\"\n\ngraduation %-time% \"1 year 3 months 6 days\"\n\n[1] \"2024-03-09\"\n\n\nBonus Tip: Don’t forget about formatting! Use format() with options like \"%Y-%m-%d\" to display your dates in your preferred format."
  },
  {
    "objectID": "posts/2024-02-01/index.html",
    "href": "posts/2024-02-01/index.html",
    "title": "R for the Real World: Counting those Business Days like a Pro!",
    "section": "",
    "text": "Hi fellow coders, data wranglers, and all-around R enthusiasts! Have you ever been stuck calculating the number of business days between two dates? You know, like figuring out how long that project actually took, excluding weekends (because let’s be honest, who works on those?). Well, fret no more! Today, we’re diving into the wonderful world of business day calculations in R with some easy-to-follow examples. Buckle up, it’s gonna be a productive ride!"
  },
  {
    "objectID": "posts/2024-02-01/index.html#step-1-grabbing-the-toolkit",
    "href": "posts/2024-02-01/index.html#step-1-grabbing-the-toolkit",
    "title": "R for the Real World: Counting those Business Days like a Pro!",
    "section": "Step 1: Grabbing the Toolkit",
    "text": "Step 1: Grabbing the Toolkit\nFirst things first, we need the right tools. We’ll be using the mighty bizdays package. Think of it as your personal business day calculator, always ready to lend a hand (or rather, some code). Install it with this magic spell:\n\n# install.packages(\"bizdays\")\nlibrary(bizdays)"
  },
  {
    "objectID": "posts/2024-02-01/index.html#step-2-the-basic-count",
    "href": "posts/2024-02-01/index.html#step-2-the-basic-count",
    "title": "R for the Real World: Counting those Business Days like a Pro!",
    "section": "Step 2: The Basic Count",
    "text": "Step 2: The Basic Count\nAlright, let’s say you want to know how many business days there were between January 1st and December 31st, 2023. Simple, right? Here’s the code:\n\nstart_date &lt;- as.Date(\"2023-01-01\")\nend_date &lt;- as.Date(\"2023-12-31\")\n\nbusiness_days &lt;- bizdays(start_date, end_date, \"weekends\")\n\nprint(paste0(\"There were \", business_days, \" business days in 2023!\"))\n\n[1] \"There were 259 business days in 2023!\"\n\n\nWhat’s happening here? We define the start and end dates, feed them to the bizdays function, and voila! It counts the business days for us, excluding weekends by default. The print function just displays the result with a fun message."
  },
  {
    "objectID": "posts/2024-02-01/index.html#step-3-get-creative-and-explore",
    "href": "posts/2024-02-01/index.html#step-3-get-creative-and-explore",
    "title": "R for the Real World: Counting those Business Days like a Pro!",
    "section": "Step 3: Get Creative and Explore!",
    "text": "Step 3: Get Creative and Explore!\nRemember, this is just the tip of the bizdays iceberg. You can explore its other features like:\n\nAdding or subtracting business days from a date\nHandling custom holiday lists\nWorking with different time zones\n\nBut wait, there’s more! The most important step is to experiment and try things out yourself. Play with different dates, holidays, and weekend definitions. See what results you get and how they fit your specific needs. R is all about exploration and making it work for you!\nSo, fellow coders, go forth and conquer those business day calculations with confidence! And if you get stuck, remember, the R community is always here to help. Happy coding!"
  },
  {
    "objectID": "posts/2024-02-02/index.html",
    "href": "posts/2024-02-02/index.html",
    "title": "Accounts Recievables Pathways in SQL",
    "section": "",
    "text": "Yesterday I was working on a project that required me to create a SQL query to generate a table of accounts receivables pathways. I thought it would be interesting to share the SQL code I wrote for this task. The code is as follows:\n-- Create the table in the specified schema\n-- Create a new table called 'c_tableau_collector_pathway_tbl' in schema 'dbo'\n-- Drop the table if it already exists\nIF OBJECT_ID('dbo.c_tableau_collector_pathway_tbl', 'U') IS NOT NULL\nDROP TABLE dbo.c_tableau_collector_pathway_tbl\nGO\n-- Create the table in the specified schema\nCREATE TABLE dbo.c_tableau_collector_pathway_tbl\n(\n    c_tableau_collector_pathway_tblId INT NOT NULL IDENTITY(1, 1) PRIMARY KEY, -- primary key column\n    pt_no VARCHAR(50) NOT NULL,\n    collector_dept_path VARCHAR(MAX)\n);\n\nWITH tmp AS (\n    SELECT DISTINCT pt_no\n    FROM sms.dbo.c_tableau_times_with_worklist_tbl\n    )\nINSERT INTO sms.dbo.c_tableau_collector_pathway_tbl (\n    pt_no,\n    collector_dept_path\n    )\nSELECT rtrim(ltrim(tmp.pt_no)) AS [pt_no],\n    stuff((\n            SELECT ', ' + z.collector_dept\n            FROM sms.dbo.c_tableau_times_with_worklist_tbl AS z\n            WHERE z.pt_no = tmp.pt_no\n            GROUP BY z.collector_dept\n            ORDER BY max(event_number)\n            FOR XML path('')\n            ), 1, 2, '') AS [collector_dept_path]\nFROM tmp AS tmp;\n\nselect pt_no,\n    [collector_dept_path],  \n    [number_of_distinct_collector_dept] = (LEN(REPLACE(collector_dept_path, ',', '**')) - LEN(collector_dept_path)) + 1\nfrom dbo.c_tableau_collector_pathway_tbl\nSo what does it do? Let’s break it down step by step:\n\nIF OBJECT_ID('dbo.c_tableau_collector_pathway_tbl', 'U') IS NOT NULL\n\nThis part checks if a table named c_tableau_collector_pathway_tbl exists in the dbo schema. If it does, it proceeds to the next step.\n\nDROP TABLE dbo.c_tableau_collector_pathway_tbl\n\nIf the table exists, it drops (deletes) the table c_tableau_collector_pathway_tbl.\n\nCREATE TABLE dbo.c_tableau_collector_pathway_tbl (...)\n\nThis part creates a new table named c_tableau_collector_pathway_tbl in the dbo schema with three columns:\n\nc_tableau_collector_pathway_tblId of type INT, which is the primary key and automatically increments by 1 for each new row.\npt_no of type VARCHAR(50), which stores values up to 50 characters long and cannot be NULL.\ncollector_dept_path of type VARCHAR(MAX), which can store large amounts of text.\n\n\nWITH tmp AS (...)\n\nThis part defines a temporary table (tmp) that contains distinct values of pt_no from another table named sms.dbo.c_tableau_times_with_worklist_tbl.\n\nINSERT INTO sms.dbo.c_tableau_collector_pathway_tbl (...) SELECT ...\n\nThis part inserts data into the newly created c_tableau_collector_pathway_tbl table. It selects distinct pt_no values from the temporary table tmp and concatenates corresponding collector_dept values into a single string, separated by commas. The FOR XML path('') part formats the result as XML, and stuff(..., 1, 2, '') removes the leading comma and space.\n\nSELECT pt_no, [collector_dept_path], [number_of_distinct_collector_dept] = (...)\n\nFinally, this part selects data from the c_tableau_collector_pathway_tbl table. It selects pt_no, collector_dept_path, and calculates the number of distinct collector departments by counting the commas in the collector_dept_path string.\n\n\nIn summary, this SQL code drops an existing table (if it exists), creates a new table with specific columns, inserts data into the new table by concatenating values from another table, and then selects data from the new table along with a calculated value for the number of distinct collector departments."
  },
  {
    "objectID": "posts/2024-02-05/index.html",
    "href": "posts/2024-02-05/index.html",
    "title": "Taming Excel Dates in R: From Numbers to Meaningful Dates!",
    "section": "",
    "text": "Introduction\nHave you ever battled with Excel’s quirky date formats in your R projects? If so, you’re not alone! Those cryptic numbers can be a real headache, but fear not, fellow R warriors! Today, we’ll conquer this challenge and transform those numbers into beautiful, usable dates.\nOur Mission: We’ll convert two date columns in a tibble named “df”:\n\ndate: Stored as numbers, representing days since some mysterious date.\ndatetime: Also in numberland, but with an additional decimal for time.\n\nOur Weapons:\n\nas.Date(): This built-in R function is our date-conversion hero, but we need to give it a secret weapon: origin = \"1899-12-30\". This tells as.Date() where the Excel date system starts counting days from.\nopenxlsx library: This package helps us deal with Excel files. We’ll use its convertToDateTime() function to handle the datetime column, which includes both date and time information.\n\n\n\nLet’s Code!\n\n# Install and load the openxlsx library (if needed)\nif (!require(openxlsx)) install.packages(\"openxlsx\")\nlibrary(openxlsx)\n\n# Our example data\ndf &lt;- data.frame(\n  date = c(44563, 44566, 44635, 44670, 44706, 44716, 44761, 44782, 44864, 44919),\n  datetime = c(44563.17, 44566.51, 44635.64, 44670.40,\n               44706.43, 44716.42, 44761.05, 44782.09,\n               44864.19, 44919.89),\n  sales = c(14, 19, 22, 29, 24, 25, 25, 30, 35, 28)\n)\n\ndf\n\n    date datetime sales\n1  44563 44563.17    14\n2  44566 44566.51    19\n3  44635 44635.64    22\n4  44670 44670.40    29\n5  44706 44706.43    24\n6  44716 44716.42    25\n7  44761 44761.05    25\n8  44782 44782.09    30\n9  44864 44864.19    35\n10 44919 44919.89    28\n\n# Convert \"date\" column using as.Date() and the magic origin\ndf$date &lt;- as.Date(df$date, origin = \"1899-12-30\")\n\n# Convert \"datetime\" column using openxlsx and convertToDateTime()\ndf$datetime &lt;- convertToDateTime(df$datetime)\n\n\n\nBreaking it Down\n\nThe first line checks if openxlsx is installed and loads it if needed.\nWe create our sample data frame df with the date and datetime columns.\nThe magic happens! We use as.Date() on df$date, specifying the origin as “1899-12-30”. This tells R to interpret the numbers as days since that date.\nFor df$datetime, we use convertToDateTime() from the openxlsx package. This function handles both date and time information stored as decimals.\n\nVoila! Our df now has proper date and datetime columns, ready for further analysis and visualization. Let’s see the results:\n\nhead(df, 1)\n\n        date            datetime sales\n1 2022-01-02 2022-01-02 04:04:48    14\n\n\n\n\nYou’re Turn!\nNow it’s your turn! Grab your own Excel data with mysterious date formats and try this code. Play with different origin values if needed (depending on your Excel version). Remember, R is a playground, so have fun exploring and taming those dates!\nBonus Tip: Want to format your dates for readability? Use the format() function, like this:\n\ndf$date &lt;- format(df$date, \"%d/%m/%Y\")\ndf\n\n         date            datetime sales\n1  02/01/2022 2022-01-02 04:04:48    14\n2  05/01/2022 2022-01-05 12:14:24    19\n3  15/03/2022 2022-03-15 15:21:36    22\n4  19/04/2022 2022-04-19 09:36:00    29\n5  25/05/2022 2022-05-25 10:19:12    24\n6  04/06/2022 2022-06-04 10:04:48    25\n7  19/07/2022 2022-07-19 01:12:00    25\n8  09/08/2022 2022-08-09 02:09:36    30\n9  30/10/2022 2022-10-30 04:33:36    35\n10 24/12/2022 2022-12-24 21:21:36    28\n\n\nThis will display your dates in the familiar “day/month/year” format.\nSo there you have it, fellow R enthusiasts! With these tools, you can confidently handle Excel’s date quirks and unleash the power of your data. Happy coding!"
  },
  {
    "objectID": "posts/2024-02-06/index.html",
    "href": "posts/2024-02-06/index.html",
    "title": "Simplifying Date Manipulation: How to Get Week Numbers in R",
    "section": "",
    "text": "When working with dates in R, you may need to extract the week number for any given date. This can be useful for doing time series analysis or visualizations by week.\nIn this post, I’ll demonstrate how to get the week number from dates in R using both base R and the lubridate package. I’ll provide simple examples so you can try it yourself.\n\n\nIn base R, the strftime() function is used to format dates and extract different date components like day, month, year etc.\nThe syntax for strftime() is:\nstrftime(x, format, tz = \"\")\nWhere:\n\nx: is the date object\n\nformat: is the format string specifying which date components to extract\ntz: is an optional time zone string\n\nTo get the week number, we need to use \"%V\" in the format string. This tells strftime() to return the ISO 8601 standard week number.\nLet’s see an example:\n\ndate &lt;- as.Date(\"2023-01-15\")\n\nstrftime(date, format = \"%V\") \n\n[1] \"02\"\n\n\nThis returns the week number as a string. In this case, it’s the second week of the year.\nWe passed the date object to strftime() along with the format string containing \"%V\".\nLet’s try another example on a vector of dates:\n\ndates &lt;- as.Date(c(\"2023-01-15\", \"2023-02-28\", \"2023-12-31\"))\n\nstrftime(dates, format = \"%V\")\n\n[1] \"02\" \"09\" \"52\"\n\n\nThis returns the week number for each date. So with base R, we can use strftime() and %V to easily extract week numbers from dates.\n\n\n\nThe lubridate package provides a wrapper function called week() to get the week number from a date.\nThe syntax for week() is simple:\nweek(x)\nWhere x is the date object.\nLet’s see an example:\n\nlibrary(lubridate)\n\ndate &lt;- ymd(\"2023-01-15\")\n\nweek(date)\n\n[1] 3\n\n\nThis returns a numeric value representing the week number. In this case, it’s the third week of the year.\nFor a vector of dates:\n\ndates &lt;- ymd(c(\"2023-01-15\", \"2023-02-28\", \"2023-12-31\"))\n\nweek(dates) \n\n[1]  3  9 53\n\n\nSo week() makes it easy to extract the week number from dates in lubridate. You will also notice that strftime() returns “52” for the last date of the year, while week() returns “53”. This is because week() follows the ISO 8601 standard for week numbers.\n\n\n\nTo quickly recap the key points:\n\nBase R: strftime(date, format = \"%V\")\n\nlubridate: week(date)\n\nI encourage you to try these functions out on some sample dates in R. Being able to wrangle dates is an important skill for handling temporal data.\nLet me know if you have any other questions!"
  },
  {
    "objectID": "posts/2024-02-06/index.html#using-base-r",
    "href": "posts/2024-02-06/index.html#using-base-r",
    "title": "Simplifying Date Manipulation: How to Get Week Numbers in R",
    "section": "",
    "text": "In base R, the strftime() function is used to format dates and extract different date components like day, month, year etc.\nThe syntax for strftime() is:\nstrftime(x, format, tz = \"\")\nWhere:\n\nx: is the date object\n\nformat: is the format string specifying which date components to extract\ntz: is an optional time zone string\n\nTo get the week number, we need to use \"%V\" in the format string. This tells strftime() to return the ISO 8601 standard week number.\nLet’s see an example:\n\ndate &lt;- as.Date(\"2023-01-15\")\n\nstrftime(date, format = \"%V\") \n\n[1] \"02\"\n\n\nThis returns the week number as a string. In this case, it’s the second week of the year.\nWe passed the date object to strftime() along with the format string containing \"%V\".\nLet’s try another example on a vector of dates:\n\ndates &lt;- as.Date(c(\"2023-01-15\", \"2023-02-28\", \"2023-12-31\"))\n\nstrftime(dates, format = \"%V\")\n\n[1] \"02\" \"09\" \"52\"\n\n\nThis returns the week number for each date. So with base R, we can use strftime() and %V to easily extract week numbers from dates."
  },
  {
    "objectID": "posts/2024-02-06/index.html#using-lubridate",
    "href": "posts/2024-02-06/index.html#using-lubridate",
    "title": "Simplifying Date Manipulation: How to Get Week Numbers in R",
    "section": "",
    "text": "The lubridate package provides a wrapper function called week() to get the week number from a date.\nThe syntax for week() is simple:\nweek(x)\nWhere x is the date object.\nLet’s see an example:\n\nlibrary(lubridate)\n\ndate &lt;- ymd(\"2023-01-15\")\n\nweek(date)\n\n[1] 3\n\n\nThis returns a numeric value representing the week number. In this case, it’s the third week of the year.\nFor a vector of dates:\n\ndates &lt;- ymd(c(\"2023-01-15\", \"2023-02-28\", \"2023-12-31\"))\n\nweek(dates) \n\n[1]  3  9 53\n\n\nSo week() makes it easy to extract the week number from dates in lubridate. You will also notice that strftime() returns “52” for the last date of the year, while week() returns “53”. This is because week() follows the ISO 8601 standard for week numbers."
  },
  {
    "objectID": "posts/2024-02-06/index.html#wrap-up",
    "href": "posts/2024-02-06/index.html#wrap-up",
    "title": "Simplifying Date Manipulation: How to Get Week Numbers in R",
    "section": "",
    "text": "To quickly recap the key points:\n\nBase R: strftime(date, format = \"%V\")\n\nlubridate: week(date)\n\nI encourage you to try these functions out on some sample dates in R. Being able to wrangle dates is an important skill for handling temporal data.\nLet me know if you have any other questions!"
  },
  {
    "objectID": "posts/2024-02-07/index.html",
    "href": "posts/2024-02-07/index.html",
    "title": "How to Check if Date is Between Two Dates in R",
    "section": "",
    "text": "Hello fellow R enthusiasts! Today, we’re diving into a common task in data analysis and manipulation: checking if a date falls between two given dates. Whether you’re working with time-series data, financial data, or any other type of data that includes dates, being able to filter or flag data based on date ranges is an essential skill.\nIn this blog post, we’ll explore two approaches to accomplish this task using base R syntax. We’ll use simple examples and explain the code in easy-to-understand terms. So, let’s get started!"
  },
  {
    "objectID": "posts/2024-02-07/index.html#method-1-using-ifelse-to-create-a-new-column",
    "href": "posts/2024-02-07/index.html#method-1-using-ifelse-to-create-a-new-column",
    "title": "How to Check if Date is Between Two Dates in R",
    "section": "Method 1: Using ifelse() to Create a New Column",
    "text": "Method 1: Using ifelse() to Create a New Column\nOne straightforward way to check if a date is between two dates is by using the ifelse() function to create a new column with an indicator variable.\nHere’s how you can do it:\n\n# Sample data frame with dates\ndf &lt;- data.frame(date = as.Date(c(\"2022-01-01\", \"2022-03-15\", \n                                  \"2022-07-10\", \"2022-11-30\")),\n                 value = c(10, 20, 30, 40))\n\n# Define start and end dates\nstart_date &lt;- as.Date(\"2022-02-01\")\nend_date &lt;- as.Date(\"2022-10-01\")\n\n# Create a new column indicating if date falls between start_date and end_date\ndf$between &lt;- ifelse(df$date &gt;= start_date & df$date &lt;= end_date, 1, 0)\n\n# View the updated data frame\nprint(df)\n\n        date value between\n1 2022-01-01    10       0\n2 2022-03-15    20       1\n3 2022-07-10    30       1\n4 2022-11-30    40       0\n\n\nIn this code snippet, we first define a sample data frame df containing a column of dates. Then, we specify the start_date and end_date between which we want to check if each date falls. We use the ifelse() function to create a new column between, where a value of 1 indicates that the date falls between the specified range, and 0 otherwise."
  },
  {
    "objectID": "posts/2024-02-07/index.html#method-2-using-subsetting-to-filter-data",
    "href": "posts/2024-02-07/index.html#method-2-using-subsetting-to-filter-data",
    "title": "How to Check if Date is Between Two Dates in R",
    "section": "Method 2: Using Subsetting to Filter Data",
    "text": "Method 2: Using Subsetting to Filter Data\nAnother approach is to directly subset the data frame based on the date range. This method can be useful when you want to retrieve or manipulate the subset of data that falls within the specified range.\nHere’s how you can do it:\n\n# Sample data frame with dates\ndf &lt;- data.frame(date = as.Date(c(\"2022-01-01\", \"2022-03-15\", \n                                  \"2022-07-10\", \"2022-11-30\")),\n                 value = c(10, 20, 30, 40))\n\n# Define start and end dates\nstart_date &lt;- as.Date(\"2022-02-01\")\nend_date &lt;- as.Date(\"2022-10-01\")\n\n# Subset data where date falls between start_date and end_date\nsubset_df &lt;- df[df$date &gt;= start_date & df$date &lt;= end_date, ]\n\n# View the subsetted data frame\nprint(subset_df)\n\n        date value\n2 2022-03-15    20\n3 2022-07-10    30\n\n\nIn this code snippet, we use subsetting to filter the df data frame, retaining only the rows where the date falls between start_date and end_date."
  },
  {
    "objectID": "posts/2024-02-08/index.html",
    "href": "posts/2024-02-08/index.html",
    "title": "How to Check if a Column is a Date in R: A Comprehensive Guide with Examples",
    "section": "",
    "text": "As an R programmer, you may often encounter datasets where you need to determine whether a column contains date values. This task is crucial for data cleaning, manipulation, and analysis. In this blog post, we’ll explore various methods to check if a column is a date in R, with a focus on using the lubridate package and the ts_is_date_class() function from the healthyR.ts package."
  },
  {
    "objectID": "posts/2024-02-08/index.html#using-lubridate",
    "href": "posts/2024-02-08/index.html#using-lubridate",
    "title": "How to Check if a Column is a Date in R: A Comprehensive Guide with Examples",
    "section": "Using lubridate",
    "text": "Using lubridate\nlubridate is a powerful package in R for handling date and time data. It provides intuitive functions to parse, manipulate, and work with date-time objects. Let’s see how we can use lubridate to check if a column is a date.\n\n# Load the lubridate package\nlibrary(lubridate)\nlibrary(dplyr)\n\n# Sample data frame\ndf &lt;- data.frame(\n  Date_Column = c(\"2022-01-01\", \"2022-02-15\", \"not a date\", \"2022-03-30\")\n)\n\n# Check if Date_Column is a date\nis_date &lt;- is.Date(df$Date_Column)\n\n# Print the result\nprint(is_date)\n\n[1] FALSE\n\n\nIn this example, we created a sample data frame df with a column named Date_Column. We used the is.Date() function from lubridate to check if the values in Date_Column are dates. The result is a logical with either a value of (TRUE) or (FALSE). In this instance the result is FALSE because the entire vector is not a date. This can change to TRUE if the entire vector is a date. See below:\n\ndf |&gt; \n  mutate(Date_Column = as.Date(Date_Column)) |&gt; \n  pull(Date_Column) |&gt; \n  is.Date()\n\n[1] TRUE\n\n# OR\ndf |&gt;\n  mutate(Date_Column = as.Date(Date_Column) |&gt; is.Date())\n\n  Date_Column\n1        TRUE\n2        TRUE\n3        TRUE\n4        TRUE"
  },
  {
    "objectID": "posts/2024-02-08/index.html#using-ts_is_date_class-from-healthyr.ts",
    "href": "posts/2024-02-08/index.html#using-ts_is_date_class-from-healthyr.ts",
    "title": "How to Check if a Column is a Date in R: A Comprehensive Guide with Examples",
    "section": "Using ts_is_date_class() from healthyR.ts",
    "text": "Using ts_is_date_class() from healthyR.ts\nNow, let’s explore how to achieve the same task using the ts_is_date_class() function from the healthyR.ts package. This function is specifically designed to check if a column is a date class, providing an alternative method for date validation.\n\n# Install and load the healthyR.ts package\n# install.packages(\"healthyR.ts\")\nlibrary(healthyR.ts)\n\n# Check if Date_Column is a date using ts_is_date_class()\nis_date_class &lt;- ts_is_date_class(as.Date(df$Date_Column))\n\n# Print the result\nprint(is_date_class)\n\n[1] TRUE\n\n# OR\n\ndf |&gt;\n  mutate(is_date = ts_is_date_class(as.Date(Date_Column)))\n\n  Date_Column is_date\n1  2022-01-01    TRUE\n2  2022-02-15    TRUE\n3  not a date    TRUE\n4  2022-03-30    TRUE\n\n\nIn this example, we installed and loaded the healthyR.ts package, which contains the ts_is_date_class() function. We then applied this function to df$Date_Column to check if the values are of date class.\nYou will notice both methods incorrectly identify the row “not a date” as a date because the as.Date() function coerces the string “not a date” to an NA inside of the mutate function. If you use rowwise() before the mutate it will fail out completely, this can be a pitfall and is something to watch out for."
  },
  {
    "objectID": "posts/2024-02-09/index.html",
    "href": "posts/2024-02-09/index.html",
    "title": "Demystifying Dates: Finding the Day of the Week in R with lubridate",
    "section": "",
    "text": "Have you ever stared at a date in R and wondered, “What day of the week was this?!” Fear not, fellow data wranglers! Today, we embark on a journey to conquer this seemingly simple, yet surprisingly tricky, task. Buckle up, because we’re about to become date whisperers with the help of the lubridate package."
  },
  {
    "objectID": "posts/2024-02-09/index.html#example-1-using-wday",
    "href": "posts/2024-02-09/index.html#example-1-using-wday",
    "title": "Demystifying Dates: Finding the Day of the Week in R with lubridate",
    "section": "Example 1: Using wday()",
    "text": "Example 1: Using wday()\nThis function is your go-to for both numeric and character representations of the day. Let’s break it down:\n\nlibrary(lubridate)\n\n# Sample date\ndate &lt;- ymd(\"2024-02-09\")\n\n# Numeric day (Monday = 1, Sunday = 7)\nnumeric_day &lt;- wday(date)\nprint(numeric_day)  # Output: 6 (Friday)\n\n[1] 6\n\nclass(numeric_day)\n\n[1] \"numeric\"\n\n# Character day (full name)\nfull_day &lt;- wday(date, label = TRUE)\nprint(full_day)  # Output: Friday\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\nclass(full_day)\n\n[1] \"ordered\" \"factor\" \n\n# Character day (abbreviated)\nabbrev_day &lt;- wday(date, label = TRUE, abbr = TRUE)\nprint(abbrev_day)  # Output: Fri\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\nclass(abbrev_day)\n\n[1] \"ordered\" \"factor\""
  },
  {
    "objectID": "posts/2024-02-09/index.html#example-2.-using-strftime",
    "href": "posts/2024-02-09/index.html#example-2.-using-strftime",
    "title": "Demystifying Dates: Finding the Day of the Week in R with lubridate",
    "section": "Example 2. Using strftime()",
    "text": "Example 2. Using strftime()\nThis function offers more flexibility in formatting dates, including extracting the day of the week.\n\n# Same date as before\ndate &lt;- ymd(\"2024-02-09\")\nclass(date)\n\n[1] \"Date\"\n\n# Day of the week (full name)\nfull_day &lt;- strftime(date, format = \"%A\")\nprint(full_day)  # Output: Friday\n\n[1] \"Friday\"\n\nclass(full_day)\n\n[1] \"character\"\n\n# Day of the week (abbreviated)\nabbrev_day &lt;- strftime(date, format = \"%a\")\nprint(abbrev_day)  # Output: Fri\n\n[1] \"Fri\"\n\nclass(abbrev_day)\n\n[1] \"character\"\n\n\n\nBeyond the Basics: Customizing Your Output\nBoth wday() and strftime() offer options to personalize your results. For example, you can change the starting day of the week (default is Monday) or use different formatting codes for the day name.\nBonus Tip: Check out the lubridate documentation for more advanced options and functionalities!"
  },
  {
    "objectID": "posts/2024-02-12/index.html",
    "href": "posts/2024-02-12/index.html",
    "title": "From Chaos to Clarity: Mastering Weekly Data Wrangling in R with strftime()",
    "section": "",
    "text": "Introduction\nGrouping data by week is a common task in data analysis. It allows you to summarize and analyze your data on a weekly basis. In R, there are a few different ways to group data by week, but one easy method is using the strftime() function.\nThe strftime() function converts a date-time object to a string in a specified format. By using the format %V, we can extract the week number from a date. Let’s walk through an example:\nFirst, let’s create a data frame with some date values:\n\ndates &lt;- as.Date(c(\"2023-01-01\", \"2023-01-15\", \"2023-02-05\", \"2023-02-17\", \"2023-03-01\"))\nvalues &lt;- c(1.5, 3.2, 2.7, 4.1, 2.3) \n\ndf &lt;- data.frame(dates, values)\ndf\n\n       dates values\n1 2023-01-01    1.5\n2 2023-01-15    3.2\n3 2023-02-05    2.7\n4 2023-02-17    4.1\n5 2023-03-01    2.3\n\n\nNow we can use strftime() to extract the week number as follows:\n\ndf$week &lt;- strftime(df$dates, format = \"%V\")\ndf\n\n       dates values week\n1 2023-01-01    1.5   52\n2 2023-01-15    3.2   02\n3 2023-02-05    2.7   05\n4 2023-02-17    4.1   07\n5 2023-03-01    2.3   09\n\n\nThis adds a new column week to the data frame containing the week number for each date.\nWe can now easily group the data by week and summarize the values column:\n\naggregate(values ~ week, df, mean)\n\n  week values\n1   02    3.2\n2   05    2.7\n3   07    4.1\n4   09    2.3\n5   52    1.5\n\n\nAnd there we have it! The data neatly summarized by week. The %V format in strftime() makes it easy to group by week in R.\nI encourage you to try this on your own data. Converting dates to week numbers enables all sorts of weekly time series analyses. Let me know if you have any other questions!"
  }
]