[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steve On Data",
    "section": "",
    "text": "Automate Your Blog Workflow with a Custom R Function: Creating QMD Files\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nautomation\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to List All Open Workbooks Using VBA and Call It from R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering String Conversion to Lowercase in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nStratified Sampling in R: A Practical Guide with Base R and dplyr\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndplyr\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Summary Tables in R with tidyquant and dplyr\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nJul 26, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Wildcard Searches in R with grep()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nGetting the Workbook Name in VBA and Calling It from R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJul 24, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nChecking if a String Contains Multiple Substrings in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\n\n\n\n\n\n\n\nJul 23, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Concatenate Strings in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplify Regression Modeling with tidyAML’s fast_regression()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nJul 18, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nVBA: Saving and Closing a Workbook\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Extract Substring Starting from the End of a String in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\nregex\n\n\nstringr\n\n\nstringi\n\n\n\n\n\n\n\n\n\nJul 16, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\ntidyAML: Automated Machine Learning with tidymodels\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Random Walks with TidyDensity in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the FileDateTime Function in VBA from R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Strings Before a Space in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the Power of TidyDensity: Simplifying Distribution Analysis in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Linear Models with R and Exporting to Excel\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJul 7, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAutomate Your R Scripts with taskscheduleR\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nautomation\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Zoom Functionality in Excel with VBA\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Extract String After a Specific Character in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\n\n\n\n\n\n\n\nJul 2, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnlocking the Power of Administrative Data with healthyR.data\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Execute VBA Code in Excel via R using RDCOMClient\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Random Walks and Brownian Motions with healthyR.ts\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nJun 27, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Run a Macro When a Cell Value Changes in VBA\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nvba\n\n\n\n\n\n\n\n\n\nJun 26, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Extract Strings Between Specific Characters in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to healthyR.ai\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Excel Spreadsheets to Disk with R and Python\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npython\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Examples with healthyR.ts\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Data from Another Workbook Using VBA and Executing It from R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Add Leading Zeros to Numbers in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nJun 18, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing healthyR.ts: A Comprehensive R Package for Time Series Analysis\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Excel Files in R and Python\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nexcel\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to healthyR\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nVBA Code to Check if a Sheet Exists\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Numbers from Strings in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to My Content Series\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Check if a Character is in a String in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\n\n\n\n\n\n\n\nJun 7, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction of My Content Series\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Split a Character String and Get the First Element in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nstrings\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling New Tools in the TidyDensity Arsenal: Distribution Parameter Wrangling\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of the New Parameter Estimate Functions in the TidyDensity Package\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of the New AIC Functions in the TidyDensity Package\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExciting New Updates to TidyDensity: Enhancing Distribution Analysis!\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 30, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing get_provider_meta_data() in healthyR.data\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Power of get_cms_meta_data() in healthyR.data\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate to healthyR.data 1.1.0\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Drop or Select Rows with a Specific String in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Split a Number into Digits in R Using gsub() and strsplit()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Split a Vector into Chunks in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 21, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Remove Specific Elements from a Vector in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering gregexpr() in R: A Comprehensive Guide\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCounting Words in a String in R: A Comprehensive Guide\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Select Columns Containing a Specific String in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nChecking if Multiple Columns are Equal in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Check if a Column Exists in a Data Frame in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Check if a Column Contains a String in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Collapse Text by Group in a Data Frame Using R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 9, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Select Columns by Index in R (Using Base R)\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCounting NA Values in Each Column: Comparing Methods in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Selection with TidyDensity: Understanding AIC for Statistical Distributions\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Data with TidyDensity’s tidy_mcmc_sampling()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nMay 3, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Chisquare Parameters with TidyDensity\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing check_duplicate_rows() from TidyDensity\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nQuantile Normalization in R with the {TidyDensity} Package\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing TidyDensity Version 1.4.0: Enhancing Data Analysis in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring strsplit() with Multiple Delimiters in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying Data Manipulation: How to Drop Columns from Data Frames in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Practical Guide to Selecting Top N Values by Group in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nChecking Row Existence Across Data Frames in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting the Last N’th Row in R Data Frames\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Guide to Selecting Rows with NA Values in R Using Base R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSelecting Rows with Specific Values: Exploring Options in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Chi-Square Distribution Parameters Using R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\ndistribution\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTaking the data out of the glue with regex in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\nglue\n\n\nunglue\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Rows: Selecting by Index in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Guide to Removing Multiple Rows in R Using Base R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 10, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Remove Rows with Some or All NAs in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 9, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nData Frame Merging in R (With Examples)\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Practical Guide to Merging Data Frames Based on Multiple Columns in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 5, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling Car Specs with Multidimensional Scaling in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nScaling Your Data in R: Understanding the Range\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Practical Guide to Data Normalization in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Quantile Normalization in R: A Step-by-Step Guide\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Text Manipulation in R: A Guide to Using gsub() for Multiple Pattern Replacement\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 27, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering the map() Function in R: A Comprehensive Guide\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWrangling Data with R: A Guide to the tapply() Function\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Manipulation in R with the Sweep Function\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Replacement: Using the replace() Function in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Segmentation: A Guide to Using the cut() Function in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Replicate Rows in a Data Frame in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing plot_regression_residuals() from tidyAML: Unveiling the Power of Visualizing Regression Residuals\n\n\n\n\n\n\ntidyaml\n\n\nrtip\n\n\ndata-analysis\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Training and Testing Predictions with tidyAML\n\n\n\n\n\n\ntidyaml\n\n\nrtip\n\n\ndata-analysis\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnleash the Power of Your Data: Extend Excel with Python and R!\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\npython\n\n\ndata-analysis\n\n\nviz\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n🚀 Exciting News! 🚀\n\n\n\n\n\n\ntidyaml\n\n\nrtip\n\n\ndata-analysis\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Random Sampling in R with the sample() Function\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWrangling Names in R: Your Guide to the make.names() Function\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTaming the Nameless: Using the names() Function in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\ndplyr\n\n\ndatatable\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Subset Data Frame in R by Multiple Conditions\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\ndplyr\n\n\ndatatable\n\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Add New Level to Factor in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Rename Factor Levels in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Beginner’s Guide to Renaming Data Frame Columns in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering Rows in R Where Column Value is Between Two Values\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnlocking Efficiency: How to Set a Data Frame Column as Index in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying the melt() Function in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Magic of dcast Function in R’s data.table Package\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTaming the Data Jungle: Filtering data.tables and data.frames in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Data Types in R: A Beginner’s Guide with Code Examples\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Your Plots in R: Adding Superscripts & Subscripts\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nLevel Up Your Data Wrangling: Adding Index Columns in R like a Pro!\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nConquering R’s Apply Family: Your Guide to apply(), lapply(), sapply(), and tapply()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\noperations\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Date Sequences in R: A Comprehensive Guide\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Get First or Last Day of Month in R with lubridate and base R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Chaos to Clarity: Mastering Weekly Data Wrangling in R with strftime()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Dates: Finding the Day of the Week in R with lubridate\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Check if a Column is a Date in R: A Comprehensive Guide with Examples\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Check if Date is Between Two Dates in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying Date Manipulation: How to Get Week Numbers in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTaming Excel Dates in R: From Numbers to Meaningful Dates!\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAccounts Recievables Pathways in SQL\n\n\n\n\n\n\ncode\n\n\nsql\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nR for the Real World: Counting those Business Days like a Pro!\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTime Flies? Time Travels! Adding Days to Dates in R (Like a Pro)\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Time Manipulation in R: Subtracting Hours with Ease\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Extract Month from Date in R (With Examples)\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Earliest Date: A Journey Through R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Data Lengths with R’s lengths() Function\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying bootstrap_stat_plot(): Your Ticket to Insightful Data Exploration\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe new function on the block with tidyAML extract_regression_residuals()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Enhanced Features of tidyAML’s internal_make_wflw_predictions()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUsing .drop_na in Fast Classification and Regression\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTidyDensity Powers Up with Data.table: Speedier Distributions for Your Data Exploration\n\n\n\n\n\n\ncode\n\n\nbenchmark\n\n\ndatatable\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking the Speed of Cumulative Functions in TidyDensity\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Peaks: A Dive into the Triangular Distribution in TidyDensity\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nNew Horizons for TidyDensity: Version 1.3.0 Release\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nConquering Daily Data: How to Aggregate to Months and Years Like a Pro in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Smooth Operator: Rolling Averages in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on the Past Year: A LinkedIn Year in Review (2023)\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlinkedin\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Lowess Smoothing in R: A Step-by-Step Guide\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnlocking the Power of Time: Transforming Data Frames into Time Series in R\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Time Traveler: Plotting Time Series in R\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nDec 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Time Series in R with the ts() Function\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Variance Inflation Factor (VIF) in R: A Practical Guide\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Odds Ratios in Logistic Regression: Your R Recipe for Loan Defaults\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDecoding the Mystery: How to Interpret Regression Output in R Like a Champ\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nConquering Unequal Variance with Weighted Least Squares in R: A Practical Guide\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring TidyAML: Simplifying Regression Analysis in R\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Complete Guide to Stepwise Regression in R\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Spline Regression\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\ntidyAML: Now supporting gee models\n\n\n\n\n\n\nrtip\n\n\ntidyaml\n\n\nregression\n\n\nclassification\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Quantile Regression with R: A Comprehensive Guide\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding and Implementing Robust Regression in R\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling Power Regression: A Step-by-Step Guide in R\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nLogarithmic Regression in R: A Step-by-Step Guide with Prediction Intervals\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Exponential Regression in R: A Step-by-Step Guide\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic Regression in R: Unveiling Non-Linear Relationships\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n{healthyR.ts} New Features: Unlocking More Power\n\n\n\n\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Perform Multiple Linear Regression in R\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Predict a Single Value Using a Regression Model in R\n\n\n\n\n\n\nrtip\n\n\nregression\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnlocking the Power of Prediction Intervals in R: A Practical Guide\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Simulate & Plot a Bivariate Normal Distribution in R: A Hands-on Guide\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Data: A Comprehensive Guide to Calculating and Plotting Cumulative Distribution Functions (CDFs) in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing TidyDensity’s New Powerhouse: The convert_to_ts() Function\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a Distribution to Data in R\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Triangular Distribution and Its Application in R\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Distribution in R\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nRandomness in R: runif(), punif(), dunif(), and quinf()\n\n\n\n\n\n\nrtip\n\n\ndistribution\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Log Log Plots In Base R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting a Logistic Regression In Base R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s a Bland-Altman Plot? In Base R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Scree Plot in Base R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Bubble Chart in R using ggplot2\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\nggplot2\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Pareto Charts in R with the qcc Package\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Interaction Plots in R: Unveiling Hidden Relationships\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Time Series Stationary Made Easy with auto_stationarize()\n\n\n\n\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTesting stationarity with the ts_adf_test() function in R\n\n\n\n\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Time Series Growth with ts_growth_rate_vec() in healthyR.ts\n\n\n\n\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering the Art of Drawing Circles in Plots with R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Use cex to Change the Size of Plot Elements in base R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHorizontal Legends in Base R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nResizing Legends in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Legends in R: Drawing Them Outside the Plot\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Stacked Dot Plots in R: A Guide with Base R and ggplot2\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Interactive Radar Charts in R with the ‘fmsb’ Library\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHorizontal Boxplots in R using the Palmer Penguins Data Set\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Decision Trees in R with rpart and rpart.plot\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Reorder Boxplots in R: A Comprehensive Guide\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Your Data Visualizations with Base R: Overlaying Points and Lines\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Visualization with ggplot2: A Guide to Using facet_grid()\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\nggplot2\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Visualization with Pairs Plots in Base R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Confidence Intervals for a Linear Model in R Using Base R and the Iris Dataset\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Visualization in R: Plotting Predicted Values with the mtcars Dataset\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Data with Scatter Plots by Group in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Histogram Breaks in R: Unveiling the Power of Data Visualization\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHistograms with Two or More Variables in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Histogram with Different Colors in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Plot Multiple Plots on the Same Graph in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Third Dimension with R: A Guide to the persp() Function\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting SVM Decision Boundaries with e1071 in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Population Pyramid Plots in R with ggplot2\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Visualization in R: How to Plot a Subset of Data\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Multivariate Data with Principal Component Analysis (PCA) Biplot in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWhen to use Jitter\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nKernel Density Plots in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships with Correlation Heatmaps in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Your Histograms in R: Adding Vertical Lines for Better Insights\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Plot Multiple Histograms with Base R and ggplot2\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Multiple Lines on a Graph in R: A Step-by-Step Guide\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Data Distribution in R: A Comprehensive Guide\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling Data Distribution Patterns with stripchart() in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Box Plots with Mean Values using Base R and ggplot2\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Data Distribution with Box Plots in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Approximation with R’s approx() Function\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Power of the curve() Function in R\n\n\n\n\n\n\nrtip\n\n\nviz\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSolving Systems of Equations in R using the solve() Function\n\n\n\n\n\n\nrtip\n\n\nlinearequations\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe substring() function in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\npmax() and pmin(): Finding the Parallel Maximum and Minimum in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Grouped Counting in R: A Comprehensive Guide\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Transformation with the scale() Function in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nEnhance Your Plots with the text() Function in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring R’s Versatile str() Function: Unraveling Your Data with Ease!\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe unlist() Function in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nR Functions for Getting Objects\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe replicate() function in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe intersect() function in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the Power of Cumulative Mean in R: A Step-by-Step Guide\n\n\n\n\n\n\nrtip\n\n\ncumulative\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSummarizing Data in R: tapply() vs. group_by() and summarize()\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnraveling Data Insights with R’s fivenum(): A Programmer’s Guide\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Calculate Percentage by Group in R using Base R, dplyr, and data.table\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHarness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplify Your Code with R’s Powerful Functions: with() and within()\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to subset list objects in R\n\n\n\n\n\n\nrtip\n\n\nlist\n\n\nsubset\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiently Finding Duplicate Rows in R: A Comparative Analysis\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\ndplyr\n\n\ndatatable\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Duplicate Values in a Data Frame in R: A Guide Using Base R and dplyr\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance in R with the cov() Function\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying File Existence Checking in R with file.exists()\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Data with colMeans() in R: A Programmer’s Guide\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Closer Look at the R Function identical()\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying File Management in R: Introducing file.rename()\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Use a Windows .bat File to Execute an R Script\n\n\n\n\n\n\nrtip\n\n\nbatchfile\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Rolling Correlation with the rollapply Function: A Powerful Tool for Analyzing Time-Series Data\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe ave() Function in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization in R: Unleashing the Power of the abline() Function\n\n\n\n\n\n\nrtip\n\n\nabline\n\n\nviz\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Function in R: Resampling with the lapply and sample Functions\n\n\n\n\n\n\nrtip\n\n\nbootstrap\n\n\nlapply\n\n\nsample\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Repetition with R’s rep() Function: A Programmer’s Guide\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the Power of Sampling in R: Exploring the Versatile sample() Function\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Aggregation with xtabs() in R\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering the Power of R’s diff() Function: A Programmer’s Guide\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Linear Regression in R: Analyzing the mtcars Dataset with lm()\n\n\n\n\n\n\nrtip\n\n\nlinear\n\n\nregression\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPulling a formula from a recipe object\n\n\n\n\n\n\nrtip\n\n\nrecipes\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying Model Formulas with the R Function ‘reformulate()’\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the file.info() Function in R: Listing Files by Date\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying Data Transformation with pivot_longer() in R’s tidyr Library\n\n\n\n\n\n\nrtip\n\n\ntidyr\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSorting, Ordering, and Ranking: Unraveling R’s Powerful Functions\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe do.call() function in R: Unlocking Efficiency and Flexibility\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Regular Expressions: A Programmer’s Guide for Beginners\n\n\n\n\n\n\nrtip\n\n\nregex\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying Logical Operations with the R Function any()\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Check File Size Output for Different Methods?\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\nopenxlsx\n\n\nxlsx\n\n\nwritexl\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nComparing R Packages for Writing Excel Files: An Analysis of writexl, openxlsx, and xlsx in R\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\nopenxlsx\n\n\nxlsx\n\n\nwritexl\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Data with TidyDensity: A Guide to Using tidy_empirical() and tidy_four_autoplot() in R\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\ndplyr\n\n\npurrr\n\n\n\n\n\n\n\n\n\nMay 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the sink() function? Capturing Output to External Files\n\n\n\n\n\n\nrtip\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate to {TidyDensity}\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMastering File Manipulation with R’s list.files() Function\n\n\n\n\n\n\nrtip\n\n\nfiles\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe which() Function in R\n\n\n\n\n\n\nrtip\n\n\nwhich\n\n\n\n\n\n\n\n\n\nMay 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Dates and Times Pt 4\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Dates and Times Pt 3\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Dates and Times Pt 2: Finding the Next Mothers Day with Simplicity\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Dates and Times Pt 1\n\n\n\n\n\n\nrtip\n\n\ndatetime\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nVBA to R and Back Again: Running R from VBA Pt 2\n\n\n\n\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nVBA to R and Back Again: Running R from VBA\n\n\n\n\n\n\nrtip\n\n\nvba\n\n\nexcel\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUpdates to {healthyR.data}\n\n\n\n\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMaps with {shiny} Pt 2\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nmapping\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMaps with {shiny}\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nmapping\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Download a File from the Internet using download.file()\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\nreadxl\n\n\nexcel\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting a model call from a fitted workflow in {tidymodels}\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 4\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 3\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 2\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding models with {shiny} and {tidyAML} Part 1\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidymodels\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Distributions with {shiny}, {TidyDensity} and {plotly} Part 5\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\nplotly\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 4\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 3\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity} Part 2\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Distributions with {shiny} and {TidyDensity}\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nStyling Tables for Excel with {styledTables}\n\n\n\n\n\n\nrtip\n\n\nexcel\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nReading in Multiple Excel Sheets with lapply and {readxl}\n\n\n\n\n\n\nrtip\n\n\nreadxl\n\n\nlapply\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA New Package for the African Stock Market {BRVM}\n\n\n\n\n\n\nrtip\n\n\nbrvm\n\n\nmarkets\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at Daily Log Returns with tidyquant, TidyDensity, and Shiny\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ntidydensity\n\n\ntidyquant\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA sample Shiny App to view Forecasts on the AirPassengers Data\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ndata\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA sample Shiny App to view CMS Healthcare Data\n\n\n\n\n\n\nrtip\n\n\nshiny\n\n\ndata\n\n\nhealthcare\n\n\ncms\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nA Bootstrapped Time Series Model with auto.arima() from {forecast}\n\n\n\n\n\n\nrtip\n\n\ntimeseries\n\n\nbootstrap\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow fast does a compressed file in Part 2\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\narrow\n\n\nduckdb\n\n\ndatatable\n\n\nreadr\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow fast does a compressed file in?\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHow fast do the files read in?\n\n\n\n\n\n\nrtip\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSome Examples of Cumulative Mean with {TidyDensity}\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nGetting the CCI30 Index Current Makeup\n\n\n\n\n\n\ncrypto\n\n\ncci30\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUse of the apply family of functions\n\n\n\n\n\n\nthanks\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUse of the apply family of functions\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\napply\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Solutions to speedup tidy_bernoulli() with {data.table}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\ndatatable\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nGetting NYS Home Heating Oil Prices with {rvest}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrvest\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\ntidy_bernoulli() with {data.table}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndatatable\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimple examples of imap() from {purrr}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimple examples of pmap() from {purrr}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Timeseries in a list with R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nText Processing Made Easy with {healthyR}’s sql_left(), sql_mid(), and sql_right() Functions in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nsql\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nOpen a File Folder in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nshell\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nQuickly Generate Nested Time Series Models\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nautoarima\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nData Preppers with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\npreprocessor\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCalibrate and Plot a Time Series with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nConverting a {tidyAML} tibble to a {workflowsets}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\nworkflowsets\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nOfficially on CRAN {tidyAML}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMoving Average Plots with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAn example of using {box}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nbox\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nOff to CRAN! {tidyAML}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nGet the Current Hospital Data Set from CMS with {healthyR.data}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating and Predicting Fast Regression Parsnip Models with {tidyAML}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an R Project Directory\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSubsetting Named Lists in R\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlist\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Measurement Functions with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe Argument Matcher: A Function for Selecting the Right Arguments {tidyAML}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlist\n\n\ntidyaml\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDiverging Lollipop Chart: A Visual Tool for Comparing Data with {healthyR}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\nplots\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAttributes in R Functions: An Overview\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nmetadata\n\n\nattributes\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMedian: A Simple Way to Detect Excess Events Over Time with {healthyR}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n{healthyR.ts}: The New and Improved Library for Time Series Analysis\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nService Line Grouping with {healthyR}\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\naugment\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming Your Data: A Guide to Popular Methods and How to Implement Them with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntransforms\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying List Filtering in R with purrr’s keep()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\npurrr\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Non Stationary Data Stationary\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nADF and Phillips-Perron Tests for Stationarity using lists\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\ntimeseries\n\n\nlapply\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAnother Post on Lists\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBoilerplate XGBoost with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nxgboost\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Brownian Motion with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAugmenting a Brownian Motion to a Time Series with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAuto K-Means with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nkmeans\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nJan 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nThe building of {tidyAML}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\npurrr\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAn Update on {tidyAML}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\nautoml\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on the Past Year: A LinkedIn Year in Review\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlinkedin\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Break Points for Histograms with {healthyR}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\nhistograms\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nNew Release of {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBrownian Motion\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMore Randomwalks with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\nrandomwalk\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCalendar Heatmap with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nEvent Analysis with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nGartner Magic Chart and its usefulness in healthcare analytics with {healthyR}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Time Series Model Forecasts with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\ntimeseries\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nDec 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nListing Functions and Parameters\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndplyr\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Statistics with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Walks with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrandomwalk\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nViewing Different Versions of the Same Statistical Distribution with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndistributions\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nModel Scedacity Plots with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Moving Average Plots with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Summaries with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMixture Distributions with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\nmixturemodels\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreate QQ Plots for Time Series Models with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Faceted Historgram Plot with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhistograms\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCreate Multiple {parsnip} Model Specs with {purrr}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nparsnip\n\n\npurrr\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nZ-Score Scaling Step Recipe with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nNaming Items in a List with {purrr}, {dplyr}, or {healthyR}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAuto KNN with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nknn\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nExtract Boilerplate Workflow Metrics with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate Random Walk Data with {healthyR.ts}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Lists\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\nlapply\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDefault Metric Sets with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\ndatatable\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing Scale/Normalize with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Modeling with Base R\n\n\n\n\n\n\ncode\n\n\nbootstrap\n\n\nrtip\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nUpdates to {healthyverse} packages\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyverse\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Modeling with {purrr} and {modler}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nmodelr\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Harmonic Mean with {TidyDensity}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nAuto Prep data for XGBoost with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nxgboost\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nFind Skewed Features with {healthyR.ai}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nskew\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Lag Correlation Plots\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nReading Multiple Files with {purrr}\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nMapping K-Means with healthyR.ai\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nkmeans\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nHyperbolic Transform with healthyR.ai\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Fourier Vec with healthyR.ai\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapping and Plots with TidyDensity\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\nbootstrap\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Skewness\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nPCA with healthyR.ai\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nControl Charts in healthyR.ai\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Variance\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ncumulative\n\n\nsapply\n\n\nlapply\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Clustering with healthyR.ts\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nhealthyR.ai Primer\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nTidyDensity Primer\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nSimple lapply()\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To Steve On Data\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/healthyrai-20221013/index.html",
    "href": "posts/healthyrai-20221013/index.html",
    "title": "healthyR.ai Primer",
    "section": "",
    "text": "This is going to serve as a sort of primer for my r packge {healthyR.ai}. The goal of this package is to help with producing uniform machine learning/ai models either from scratch or by way of one of the boilerplate functions.\nThis particular article is going to focus on k-means clustering with umap projection and visualization.\nFirst things first, lets load in the library:\n\nlibrary(healthyR.ai)\n\n\n== Welcome to healthyR.ai ===========================================================================\nIf you find this package useful, please leave a star: \n   https://github.com/spsanderson/healthyR.ai'\n\nIf you encounter a bug or want to request an enhancement please file an issue at:\n   https://github.com/spsanderson/healthyR.ai/issues\n\nThank you for using healthyR.ai\n\n\n\nInformation\nK-Means is a partition algorithm initially designed for signal processing. The goal is to partition n observations into k clusters where each n is in k. The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters.\nThe aim of this post is to showcase the use of the healthyR.ai wrapper for the kmeans function along with the wrapper and plot for the uwot::umap projection function. We will go through the entire workflow from getting the data to getting the final UMAP plot.\n\n\nGenerate some data\n\nsuppressPackageStartupMessages(library(healthyR.data))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(broom))\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata_tbl <- healthyR_data %>%\n    filter(ip_op_flag == \"I\") %>%\n    filter(payer_grouping != \"Medicare B\") %>%\n    filter(payer_grouping != \"?\") %>%\n    select(service_line, payer_grouping) %>%\n    mutate(record = 1) %>%\n    as_tibble()\n\ndata_tbl %>%\n  glimpse()\n\nRows: 116,823\nColumns: 3\n$ service_line   <chr> \"Medical\", \"Schizophrenia\", \"Syncope\", \"Pneumonia\", \"Ch…\n$ payer_grouping <chr> \"Blue Cross\", \"Medicare A\", \"Medicare A\", \"Medicare A\",…\n$ record         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nNow that we have our data we need to generate what is called a user item table. To do this we use the function hai_kmeans_user_item_tbl which takes in just a few arguments. The purpose of the user item table is to aggregate and normalize the data between the users and the items.\nThe data that we have generated is going to look for clustering amongst the service_lines (the user) and the payer_grouping (item) columns.\nLets now create the user item table.\n\n\nUser Item Tibble\n\nuit_tbl <- hai_kmeans_user_item_tbl(\n  data_tbl, \n  service_line, \n  payer_grouping, \n  record\n)\n\nuit_tbl\n\n# A tibble: 23 × 12\n   service_line   Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷\n   <chr>            <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n 1 Alcohol Abuse   0.0941 0.0321  5.25e-4 0.0116  0.0788 0.158    0.367   0.173 \n 2 Bariatric Sur…  0.317  0.0583  0       0.0518  0.168  0.00324  0.343   0.0485\n 3 Carotid Endar…  0.0845 0.0282  0       0       0.0141 0        0.0282  0.648 \n 4 Cellulitis      0.110  0.0339  1.18e-2 0.00847 0.0805 0.0869   0.192   0.355 \n 5 Chest Pain      0.144  0.0391  2.90e-3 0.00543 0.112  0.0522   0.159   0.324 \n 6 CHF             0.0295 0.00958 5.18e-4 0.00414 0.0205 0.0197   0.0596  0.657 \n 7 COPD            0.0493 0.0228  2.28e-4 0.00548 0.0342 0.0461   0.172   0.520 \n 8 CVA             0.0647 0.0246  1.07e-3 0.0107  0.0524 0.0289   0.0764  0.555 \n 9 GI Hemorrhage   0.0542 0.0175  1.25e-3 0.00834 0.0480 0.0350   0.0855  0.588 \n10 Joint Replace…  0.139  0.0179  3.36e-2 0.00673 0.0516 0        0.0874  0.5   \n# … with 13 more rows, 3 more variables: `Medicare HMO` <dbl>,\n#   `No Fault` <dbl>, `Self Pay` <dbl>, and abbreviated variable names\n#   ¹​`Blue Cross`, ²​Commercial, ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid,\n#   ⁶​`Medicaid HMO`, ⁷​`Medicare A`\n\n\nThe table is aggregated by item for the various users to which the algorithm will be applied.\nNow that we have this data we need to find what will be out optimal k (clusters). To do this we need to generate a table of data that will have a column of k and for that k apply the k-means function to the data with that k and return the total within sum of squares.\nTo do this there is a convienent function called hai_kmeans_mapped_tbl that takes as its sole argument the output from the hai_kmeans_user_item_tbl. There is an argument .centers where the default is set to 15.\n\n\nK-Means Mapped Tibble\n\nkmm_tbl <- hai_kmeans_mapped_tbl(uit_tbl)\nkmm_tbl\n\n# A tibble: 15 × 3\n   centers k_means  glance          \n     <int> <list>   <list>          \n 1       1 <kmeans> <tibble [1 × 4]>\n 2       2 <kmeans> <tibble [1 × 4]>\n 3       3 <kmeans> <tibble [1 × 4]>\n 4       4 <kmeans> <tibble [1 × 4]>\n 5       5 <kmeans> <tibble [1 × 4]>\n 6       6 <kmeans> <tibble [1 × 4]>\n 7       7 <kmeans> <tibble [1 × 4]>\n 8       8 <kmeans> <tibble [1 × 4]>\n 9       9 <kmeans> <tibble [1 × 4]>\n10      10 <kmeans> <tibble [1 × 4]>\n11      11 <kmeans> <tibble [1 × 4]>\n12      12 <kmeans> <tibble [1 × 4]>\n13      13 <kmeans> <tibble [1 × 4]>\n14      14 <kmeans> <tibble [1 × 4]>\n15      15 <kmeans> <tibble [1 × 4]>\n\n\nAs we see there are three columns, centers, k_means and glance. The k_means column is the k_means list object and glance is the tibble returned by the broom::glance function.\n\nkmm_tbl %>%\n  tidyr::unnest(glance)\n\n# A tibble: 15 × 6\n   centers k_means  totss tot.withinss betweenss  iter\n     <int> <list>   <dbl>        <dbl>     <dbl> <int>\n 1       1 <kmeans>  1.41       1.41    1.33e-15     1\n 2       2 <kmeans>  1.41       0.592   8.17e- 1     1\n 3       3 <kmeans>  1.41       0.372   1.04e+ 0     2\n 4       4 <kmeans>  1.41       0.276   1.13e+ 0     2\n 5       5 <kmeans>  1.41       0.202   1.21e+ 0     2\n 6       6 <kmeans>  1.41       0.159   1.25e+ 0     3\n 7       7 <kmeans>  1.41       0.124   1.28e+ 0     3\n 8       8 <kmeans>  1.41       0.0884  1.32e+ 0     2\n 9       9 <kmeans>  1.41       0.0745  1.33e+ 0     3\n10      10 <kmeans>  1.41       0.0576  1.35e+ 0     2\n11      11 <kmeans>  1.41       0.0460  1.36e+ 0     2\n12      12 <kmeans>  1.41       0.0363  1.37e+ 0     3\n13      13 <kmeans>  1.41       0.0293  1.38e+ 0     3\n14      14 <kmeans>  1.41       0.0202  1.39e+ 0     2\n15      15 <kmeans>  1.41       0.0161  1.39e+ 0     2\n\n\nAs stated we use the tot.withinss to decide what will become our k, an easy way to do this is to visualize the Scree Plot, also known as the elbow plot. This is done by ploting the x-axis as the centers and the y-axis as the tot.withinss.\n\n\nScree Plot and Data\n\nhai_kmeans_scree_plt(.data = kmm_tbl)\n\n\n\n\nIf we want to see the scree plot data that creates the plot then we can use another function hai_kmeans_scree_data_tbl.\n\nhai_kmeans_scree_data_tbl(kmm_tbl)\n\n# A tibble: 15 × 2\n   centers tot.withinss\n     <int>        <dbl>\n 1       1       1.41  \n 2       2       0.592 \n 3       3       0.372 \n 4       4       0.276 \n 5       5       0.202 \n 6       6       0.159 \n 7       7       0.124 \n 8       8       0.0884\n 9       9       0.0745\n10      10       0.0576\n11      11       0.0460\n12      12       0.0363\n13      13       0.0293\n14      14       0.0202\n15      15       0.0161\n\n\nWith the above pieces of information we can decide upon a value for k, in this instance we are going to use 3. Now that we have that we can go ahead with creating the umap list object where we can take a look at a great many things associated with the data.\n\n\nUMAP List Object\nNow lets go ahead and create our UMAP list object.\n\nump_lst <- hai_umap_list(.data = uit_tbl, kmm_tbl, 3)\n\nNow that it is created, lets take a look at each item in the list. The umap_list function returns a list of 5 items.\n\numap_obj\numap_results_tbl\nkmeans_obj\nkmeans_cluster_tbl\numap_kmeans_cluster_results_tbl\n\nSince we have the list object we can now inspect the kmeans_obj, first thing we will do is use the hai_kmeans_tidy_tbl function to inspect things.\n\nkm_obj <- ump_lst$kmeans_obj\nhai_kmeans_tidy_tbl(.kmeans_obj = km_obj, .data = uit_tbl, .tidy_type = \"glance\")\n\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  <dbl>        <dbl>     <dbl> <int>\n1  1.41        0.372      1.04     2\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"augment\")\n\n# A tibble: 23 × 2\n   service_line                  cluster\n   <chr>                         <fct>  \n 1 Alcohol Abuse                 1      \n 2 Bariatric Surgery For Obesity 1      \n 3 Carotid Endarterectomy        2      \n 4 Cellulitis                    3      \n 5 Chest Pain                    3      \n 6 CHF                           2      \n 7 COPD                          2      \n 8 CVA                           2      \n 9 GI Hemorrhage                 2      \n10 Joint Replacement             2      \n# … with 13 more rows\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"tidy\")\n\n# A tibble: 3 × 14\n  Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷ Medic…⁸ No Fa…⁹\n    <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1  0.150   0.0368 3.07e-4 0.0207  0.163   0.131   0.314    0.132  0.0319 0.00136\n2  0.0784  0.0218 4.32e-3 0.00620 0.0449  0.0368  0.0800   0.563  0.152  0.00348\n3  0.117   0.0314 1.02e-2 0.0139  0.0982  0.0856  0.147    0.354  0.105  0.00707\n# … with 4 more variables: `Self Pay` <dbl>, size <int>, withinss <dbl>,\n#   cluster <fct>, and abbreviated variable names ¹​`Blue Cross`, ²​Commercial,\n#   ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid, ⁶​`Medicaid HMO`,\n#   ⁷​`Medicare A`, ⁸​`Medicare HMO`, ⁹​`No Fault`\n\n\n\n\nUMAP Plot\nNow that we have all of the above data we can visualize our clusters that are colored by their cluster number.\n\nhai_umap_plot(.data = ump_lst, .point_size = 3, TRUE)"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html",
    "href": "posts/healthyrts-20221021/index.html",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "",
    "text": "There are two components to time-series clustering with {healthyR.ts}. There is the function that will create the clustering data along with a slew of other information and then there is a plotting function that will plot out the data in a time-series fashion colored by cluster.\nThe first function as mentioned is the function ts_feature_cluster(), and the next is ts_feature_cluster_plot()\nFunction Reference:\n\nts_feature_cluster()\nts_feature_cluster_plot()`\n\nWe are going to use the built-in AirPassengers data set for this example so let’s get right to it!"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster()",
    "text": "ts_feature_cluster()\nAs mentioned there are several outputs from the ts_feature_cluster(). Those are as follows:\nData Section\n\nts_feature_tbl\nuser_item_matrix_tbl\nmapped_tbl\nscree_data_tbl\ninput_data_tbl (the original data)\n\nPlots\n\nstatic_plot\nplotly_plot\n\nNow that we have our output, let’s take a look at each individual component of the output.\nts_feature_tbl\n\noutput$data$ts_feature_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nuser_item_matrix_tbl\n\noutput$data$user_item_matrix_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nmapped_tbl\n\noutput$data$mapped_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nscree_data_tbl\n\noutput$data$scree_data_tbl |> glimpse()\n\nRows: 3\nColumns: 2\n$ centers      <int> 1, 2, 3\n$ tot.withinss <dbl> 1.8324477, 0.7364934, 0.4571258\n\n\ninput_data_tbl\n\noutput$data$input_data_tbl |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nNow the plots.\nstatic_plot\n\noutput$plots$static_plot\n\n\n\n\nplotly_plot\n\noutput$plots$plotly_plot\n\n\n\n\n\nNow that we have seen the output of the ts_feature_cluster() function, let’s take a look at the output of the ts_feature_cluster_plot() function."
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster_plot()",
    "text": "ts_feature_cluster_plot()\nThis function itself returns a list object of a multitude of data. First before we get into that lets look at the function call itself:\n\nts_feature_cluster_plot(\n  .data,\n  .date_col,\n  .value_col,\n  ...,\n  .center = 3,\n  .facet_ncol = 3,\n  .smooth = FALSE\n)\n\nThe data that comes back from this function is:\nData Section\n\noriginal_data\nkmm_data_tbl\nuser_item_tbl\ncluster_tbl\n\nPlots\n\nstatic_plot\nplotly_plot\n\nK-Means Object\n\nk-means object\n\nWe will go through the same exercise and show the output of all the sections. First we have to create the output. The static plot will automatically print out.\n\nplot_out <- ts_feature_cluster_plot(\n  .data = output,\n  .date_col = date_col,\n  .value_col = value,\n  .center = 2,\n  group_id\n)\n\nJoining, by = \"group_id\"\n\n\n\n\n\n\nThe Data Section:\noriginal_data\n\nplot_out$data$original_data |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nkmm_data_tbl\n\nplot_out$data$kmm_data_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nuser_item_data\n\nplot_out$data$user_item_data |> glimpse()\n\n NULL\n\n\ncluster_tbl\n\nplot_out$data$cluster_tbl |> glimpse()\n\nRows: 12\nColumns: 9\n$ cluster        <int> 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\n\n\nThe plot data.\nstatic_plot\n\nplot_out$plot$static_plot\n\n\n\n\nplotly_plot\n\nplot_out$plot$plotly_plot\n\n\n\n\n\n\n\nThe K-Means Object\nkmeans_object\n\nplot_out$kmeans_object\n\n[[1]]\nK-means clustering with 2 clusters of sizes 5, 7\n\nCluster means:\n  ts_x_acf1 ts_x_acf10 ts_diff1_acf1 ts_diff1_acf10 ts_diff2_acf1 ts_seas_acf1\n1 0.7456468   1.568532     0.1172685      0.4858013    -0.1799728    0.2876449\n2 0.7387865   1.528308    -0.2909349      0.3638392    -0.5916245    0.2930543\n  ts_entropy\n1  0.4918321\n2  0.6438176\n\nClustering vector:\n [1] 1 1 2 2 2 1 1 1 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 0.3704304 0.3660630\n (between_SS / total_SS =  59.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Simple lapply()",
    "section": "",
    "text": "This is a simple lapply example to start things off.\n\n# Let l be some list of lists, where all elements of lists are numbers\nl <- list(\n  a = 1:10,\n  b = 11:20,\n  c = 21:30\n)\n\nNow let’s take a look at our list l and see it’s structure.\n\nl\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$b\n [1] 11 12 13 14 15 16 17 18 19 20\n\n$c\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nNow that we see the structure, we can use the lapply function to get the sum of each list element, the mean, etc.\n\nlapply(l, sum)\n\n$a\n[1] 55\n\n$b\n[1] 155\n\n$c\n[1] 255\n\nlapply(l, mean)\n\n$a\n[1] 5.5\n\n$b\n[1] 15.5\n\n$c\n[1] 25.5\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2021-01-11/index.html",
    "href": "posts/rtip-2021-01-11/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2022, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2023!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2022\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nfp <- \"linkedin_content.xlsx\"\n\nengagement_tbl <- read_excel(fp, sheet = \"ENGAGEMENT\") %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ntop_posts_tbl <- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %>%\n  clean_names()\n\nfollowers_tbl <- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ndemographics_tbl <- read_excel(fp, sheet = \"DEMOGRAPHICS\") %>%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 362\nColumns: 4\n$ date              <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 202…\n$ impressions       <dbl> 3088, 3911, 3303, 3134, 1118, 799, 3068, 1954, 2663,…\n$ engagements       <dbl> 31, 56, 51, 42, 8, 4, 43, 20, 33, 43, 14, 41, 5, 17,…\n$ `Engagement Rate` <dbl> 1.0038860, 1.4318589, 1.5440509, 1.3401404, 0.715563…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 5\n$ post_url_1  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ engagements <dbl> 241, 136, 123, 117, 117, 115, 107, 106, 104, 104, 95, 81, …\n$ x3          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_4  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ impressions <dbl> 52300, 33903, 30752, 29887, 25953, 24139, 23769, 18522, 18…\n\nglimpse(followers_tbl)\n\nRows: 362\nColumns: 2\n$ date          <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 2022-01…\n$ new_followers <dbl> 10, 10, 12, 5, 12, 13, 9, 8, 11, 4, 9, 6, 7, 9, 10, 11, …\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics <chr> \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            <chr> \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       <chr> \"0.054587073624134064\", \"0.035217467695474625\", \"0.02…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %>%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\n\nYou will notice that I placed a blue line where I started my telegram channel @steveondata and a red line where I started this blog. So far, not bad, it looks like the telegram channel helped a little bit but writing on the blog seems to maybe been helping the most.\nLet’s look at a cumulative view of things.\n\nengagement_tbl %>%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %>%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %>%\n  slice(1:12) %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %>%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %>%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %>%\n  slice(1:12) %>%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-24/index.html",
    "href": "posts/rtip-2022-10-24/index.html",
    "title": "Cumulative Variance",
    "section": "",
    "text": "Introducton\nThis is going to be a simple example on how we can make a function in #base #r that will crate a cumulative variance function. From base R we are going to use seq_along(), stats::var(), and sapply() inside of the function we will call cvar for cumulative variance.\n\n\nGenerate Data\nThe first thing we need to do in order to showcase this function is to generate some data. Lets do that below:\n\nl <- list(\n  a = rnorm(50),\n  b = rnorm(50, 1),\n  c = rnorm(50, 2)\n)\n\nl\n\n$a\n [1] -0.96548479  0.49276394  0.14030455  1.11786377 -1.47239834 -0.06906506\n [7] -1.51133985  1.48910665  0.09444727 -0.01216806  0.35365683 -1.13562871\n[13] -1.27899694  0.10963391 -0.00708945 -1.26718573  0.92143855  0.09716551\n[19] -0.28025814 -0.18046616 -1.75919633  0.01686201 -0.10204673  0.91791398\n[25] -1.70503761  1.50856724 -1.29433294  0.42665133 -0.78176459  0.16141529\n[31]  1.42536506 -0.42168041 -0.30222269  0.05129043 -0.73717680 -1.60823604\n[37] -0.11921815  0.08357566  0.23250949  0.50846618 -0.02674088  0.12101223\n[43]  0.10390867 -1.11476987 -1.42201791 -1.35493159  0.35703193 -1.08176152\n[49] -0.08189606  0.46341303\n\n$b\n [1]  1.67636555  1.22588224  0.44445597  2.06992723  1.82473269 -0.03321279\n [7]  1.29568923 -0.29542080  1.46614555  0.51617492  2.03383464  0.13835453\n[13]  3.18982479 -0.38493278  0.67450796  1.69715532  1.19963387  1.17294403\n[19]  0.83585415  1.49308994  0.53831112  1.76345465  1.80154859  0.47358491\n[25]  1.40422472  2.50254552 -0.07376997  0.38077031  1.13606122 -0.26052567\n[31]  0.88624336  1.89232197  1.37488657  2.53211686  1.77919794  3.42367520\n[37] -0.59175356 -0.04816522  2.08963807  1.40124074 -0.73135934  0.65282741\n[43]  0.87359580  0.14540086  1.52502012  0.52190806  2.29922084  0.62462975\n[49]  2.94462210  1.06173482\n\n$c\n [1]  1.01194711  1.36267530  1.37423091  1.14980487  1.39304340  3.26911528\n [7]  1.71184232  1.88096194  2.90461886  1.39510407  1.86157191  1.14906542\n[13]  1.90072693  1.78998258  1.61307934  0.76604158  2.92366827  2.32424523\n[19]  2.94645235  2.73102591  0.87949048  3.31239943  1.05720691  1.42571354\n[25]  1.79266828  1.84627335  0.81364549  0.25976918  1.48698512  1.10254109\n[31]  1.60219278  1.84545465  1.93508206  2.13570750  2.32733075  2.53404107\n[37]  1.25864169  3.28238628  1.98998276  1.44299079  2.26296491  3.86667748\n[43]  1.84651988  3.24765507  0.18464631 -0.01404234  2.78432762 -0.05193538\n[49]  0.35160392  2.58212054\n\n\n\n\nMake Function\nNow that we have our data, lets make the function:\n\ncvar <- function(.x){\n  sapply(seq_along(.x), function(k, z) stats::var(z[1:k]), z = .x)\n}\n\nOk, now that we have our function, lets take a look at it in use.\n\n\nUse Function\n\nsapply(l, cvar)\n\n              a         b          c\n [1,]        NA        NA         NA\n [2,] 1.0632447 0.1014676 0.06150513\n [3,] 0.5789145 0.3885272 0.04239889\n [4,] 0.7633500 0.4867186 0.03075658\n [5,] 1.1294646 0.4093271 0.02873772\n [6,] 0.9043498 0.6932616 0.69685950\n [7,] 1.0277904 0.5789892 0.58271798\n [8,] 1.2918409 0.7813852 0.50862439\n [9,] 1.1344452 0.7052323 0.62156290\n[10,] 1.0088029 0.6580963 0.56764373\n[11,] 0.9242084 0.6858993 0.51210764\n[12,] 0.9418512 0.7024341 0.49623990\n[13,] 0.9661294 1.0026509 0.45782344\n[14,] 0.8992042 1.1041314 0.42295247\n[15,] 0.8371837 1.0364119 0.39358167\n[16,] 0.8556585 0.9929979 0.42396425\n[17,] 0.8822275 0.9315646 0.49164277\n[18,] 0.8344918 0.8770439 0.48215682\n[19,] 0.7888762 0.8321667 0.52875406\n[20,] 0.7473648 0.7964123 0.54171586\n[21,] 0.8305355 0.7722667 0.56162921\n[22,] 0.7940780 0.7564316 0.63535849\n[23,] 0.7587189 0.7425071 0.63686713\n[24,] 0.7802954 0.7290301 0.61692337\n[25,] 0.8409644 0.7019443 0.59130379\n[26,] 0.9248957 0.7464413 0.56765490\n[27,] 0.9359290 0.7761117 0.58464117\n[28,] 0.9159283 0.7676951 0.64765859\n[29,] 0.8952420 0.7403040 0.62681485\n[30,] 0.8690093 0.7773175 0.61856072\n[31,] 0.9251735 0.7524213 0.59834912\n[32,] 0.8976913 0.7499102 0.57961326\n[33,] 0.8702922 0.7290409 0.56296663\n[34,] 0.8452302 0.7678835 0.55094641\n[35,] 0.8301013 0.7571526 0.54480209\n[36,] 0.8638223 0.8786798 0.54627250\n[37,] 0.8400510 0.9426489 0.53823917\n[38,] 0.8195803 0.9560736 0.58478212\n[39,] 0.8028106 0.9542482 0.57032971\n[40,] 0.7943867 0.9312335 0.55895977\n[41,] 0.7750385 0.9957722 0.55033290\n[42,] 0.7581242 0.9766789 0.63799874\n[43,] 0.7417073 0.9547108 0.62281006\n[44,] 0.7453949 0.9533619 0.65240410\n[45,] 0.7629119 0.9360654 0.70195202\n[46,] 0.7747320 0.9223139 0.76179573\n[47,] 0.7652088 0.9339432 0.76550157\n[48,] 0.7645076 0.9188787 0.82293002\n[49,] 0.7490587 0.9695572 0.84800554\n[50,] 0.7434405 0.9498710 0.84419808\n\nlapply(l, cvar)\n\n$a\n [1]        NA 1.0632447 0.5789145 0.7633500 1.1294646 0.9043498 1.0277904\n [8] 1.2918409 1.1344452 1.0088029 0.9242084 0.9418512 0.9661294 0.8992042\n[15] 0.8371837 0.8556585 0.8822275 0.8344918 0.7888762 0.7473648 0.8305355\n[22] 0.7940780 0.7587189 0.7802954 0.8409644 0.9248957 0.9359290 0.9159283\n[29] 0.8952420 0.8690093 0.9251735 0.8976913 0.8702922 0.8452302 0.8301013\n[36] 0.8638223 0.8400510 0.8195803 0.8028106 0.7943867 0.7750385 0.7581242\n[43] 0.7417073 0.7453949 0.7629119 0.7747320 0.7652088 0.7645076 0.7490587\n[50] 0.7434405\n\n$b\n [1]        NA 0.1014676 0.3885272 0.4867186 0.4093271 0.6932616 0.5789892\n [8] 0.7813852 0.7052323 0.6580963 0.6858993 0.7024341 1.0026509 1.1041314\n[15] 1.0364119 0.9929979 0.9315646 0.8770439 0.8321667 0.7964123 0.7722667\n[22] 0.7564316 0.7425071 0.7290301 0.7019443 0.7464413 0.7761117 0.7676951\n[29] 0.7403040 0.7773175 0.7524213 0.7499102 0.7290409 0.7678835 0.7571526\n[36] 0.8786798 0.9426489 0.9560736 0.9542482 0.9312335 0.9957722 0.9766789\n[43] 0.9547108 0.9533619 0.9360654 0.9223139 0.9339432 0.9188787 0.9695572\n[50] 0.9498710\n\n$c\n [1]         NA 0.06150513 0.04239889 0.03075658 0.02873772 0.69685950\n [7] 0.58271798 0.50862439 0.62156290 0.56764373 0.51210764 0.49623990\n[13] 0.45782344 0.42295247 0.39358167 0.42396425 0.49164277 0.48215682\n[19] 0.52875406 0.54171586 0.56162921 0.63535849 0.63686713 0.61692337\n[25] 0.59130379 0.56765490 0.58464117 0.64765859 0.62681485 0.61856072\n[31] 0.59834912 0.57961326 0.56296663 0.55094641 0.54480209 0.54627250\n[37] 0.53823917 0.58478212 0.57032971 0.55895977 0.55033290 0.63799874\n[43] 0.62281006 0.65240410 0.70195202 0.76179573 0.76550157 0.82293002\n[49] 0.84800554 0.84419808\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-26/index.html",
    "href": "posts/rtip-2022-10-26/index.html",
    "title": "Control Charts in healthyR.ai",
    "section": "",
    "text": "Sometimes you may be working with a time series or some process data and you will want to make a control chart. This is simple to do with the {healthyR.ai} package.\nIf you do not already have it, then you can follow the simple code below to get the latest version.\n\n\nYou can install the released version of healthyR.ai from CRAN with:\n\ninstall.packages(\"healthyR.ai\")\n\nAnd the development version from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/healthyR.ai\")\n\nNow that we have the latest version installed, lets get some data and then use the function."
  },
  {
    "objectID": "posts/rtip-2022-10-31/index.html",
    "href": "posts/rtip-2022-10-31/index.html",
    "title": "Cumulative Skewness",
    "section": "",
    "text": "Function\nIn this post we will make a function cum_skewness() that will generate a vector output of the cumulative skewness of some given vector. The full function call is simply:\n\ncum_skewness(.x)\n\nIt only takes in a numeric vector, we are not going to write type checks in the function as it won’t be necessary for this post.\n\ncum_skewness <- function(.x){\n  skewness <- function(.x){\n    sqrt(length(.x)) * sum((.x - mean(.x))^3 / (sum((.x))^2)^(3/2))\n  }\n  sapply(seq_along(.x), function(k, z) skewness(z[1:k]), z = .x)\n}\n\n\n\nData\nWe are going to use the mtcars data set and use the mpg column for this example. Let’s set x equal to mtcars$mpg\n\nx <- mtcars$mpg\n\n\n\nExample\nNow let’s see the function in use.\n\ncum_skewness(x)\n\n [1]  0.000000e+00  0.000000e+00  8.249747e-06  5.049149e-06 -1.113787e-05\n [6] -8.569220e-06 -1.134377e-04 -8.440629e-05 -8.280585e-05 -5.457236e-05\n[11] -3.209937e-05 -1.758922e-05 -5.567456e-06  1.436318e-07 -6.299325e-05\n[16] -8.605705e-05 -5.869380e-05  1.594511e-04  1.675837e-04  2.221143e-04\n[21]  1.855217e-04  1.936299e-04  1.998527e-04  2.082240e-04  1.897575e-04\n[26]  1.505425e-04  1.180971e-04  9.974055e-05  1.048461e-04  9.801797e-05\n[31]  1.024713e-04  9.107160e-05\n\n\nLet’s plot it out.\n\nplot(cum_skewness(x), type = \"l\")"
  },
  {
    "objectID": "posts/rtip-2022-11-07/index.html",
    "href": "posts/rtip-2022-11-07/index.html",
    "title": "Discrete Fourier Vec with healthyR.ai",
    "section": "",
    "text": "Introduction\nSometimes in modeling you may want to get a discrete 1/0 vector of a fourier transform of some input vector. With {healthyR.ai} we can do this easily.\n\n\nFunction\nHere is the full function call:\n\nhai_fourier_discrete_vec(\n  .x,\n  .period,\n  .order,\n  .scale_type = c(\"sin\", \"cos\", \"sincos\")\n)\n\nHere are the parameters to the function and what they expect:\n\n.x - A numeric vector\n.period - The number of observations that complete a cycle\n.order - The fourier term order\n.scale_type - A character of one of the following: sin,cos,sincos\n\nThe internal caluclation is straightforward:\n\nsin = sin(2 * pi * h * x), where h = .order/.period\ncos = cos(2 * pi * h * x), where h = .order/.period\nsincos = sin(2 * pi * h * x) * cos(2 * pi * h * x) where h = .order/.period\n\n\n\nExample\nLet’s work throught a quick and simple example.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(healthyR.ai)\nlibrary(tidyr)\n\nlen_out <- 24\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n  ),\n  a = rnorm(len_out, sd = 2),\n  fv_sin = hai_fourier_discrete_vec(a, 12, 1, \"sin\"),\n  fv_cos = hai_fourier_discrete_vec(a, 12, 1, \"cos\"),\n  fv_sc  = hai_fourier_discrete_vec(a, 12, 1, \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 24 × 5\n   date_col         a fv_sin fv_cos fv_sc\n   <date>       <dbl>  <dbl>  <dbl> <dbl>\n 1 2021-01-01 -0.486       0      1     0\n 2 2021-02-01 -0.708       0      1     0\n 3 2021-03-01 -0.119       0      1     0\n 4 2021-04-01  0.0405      1      1     1\n 5 2021-05-01  1.19        1      1     1\n 6 2021-06-01  1.88        1      1     1\n 7 2021-07-01 -1.32        0      1     0\n 8 2021-08-01 -0.0214      0      1     0\n 9 2021-09-01  2.80        1      1     1\n10 2021-10-01  1.67        1      1     1\n# … with 14 more rows\n\n\n\n\nVisual\nLet’s visualize.\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-08/index.html",
    "href": "posts/rtip-2022-11-08/index.html",
    "title": "Hyperbolic Transform with healthyR.ai",
    "section": "",
    "text": "Introduction\nIn data modeling there can be instanes where you will want some sort of hyperbolic transformation of your data. In {healthyR.ai} this is easy with the use of the function hai_hyperbolic_vec() along with it’s corresponding augment and step functions.\n\n\nFunction\nThe function takes in a numeric vector as it’s argument and will transform the data with one of the following:\n\nsin\ncos\ntan\nsincos This will do: value = sin(x) * cos(x)\n\nThe full function call is:\n\nhai_hyperbolic_vec(.x, .scale_type = c(\"sin\", \"cos\", \"tan\", \"sincos\"))\n\n\n\nExample\n\nlibrary(dplyr)\nlibrary(healthyR.ai)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nlen_out <- 25\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n    ),\n  b = runif(len_out),\n  fv_sin = hai_hyperbolic_vec(b, .scale_type = \"sin\"),\n  fv_cos = hai_hyperbolic_vec(b, .scale_type = \"cos\"),\n  fv_sc  = hai_hyperbolic_vec(b, .scale_type = \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 25 × 5\n   date_col        b fv_sin fv_cos  fv_sc\n   <date>      <dbl>  <dbl>  <dbl>  <dbl>\n 1 2021-01-01 0.961  0.820   0.573 0.470 \n 2 2021-02-01 0.418  0.406   0.914 0.371 \n 3 2021-03-01 0.0729 0.0728  0.997 0.0726\n 4 2021-04-01 0.426  0.413   0.911 0.376 \n 5 2021-05-01 0.851  0.752   0.659 0.496 \n 6 2021-06-01 0.824  0.734   0.679 0.499 \n 7 2021-07-01 0.659  0.612   0.791 0.484 \n 8 2021-08-01 0.683  0.631   0.776 0.490 \n 9 2021-09-01 0.173  0.172   0.985 0.169 \n10 2021-10-01 0.345  0.338   0.941 0.318 \n# … with 15 more rows\n\n\n\n\nVisual\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")"
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html",
    "href": "posts/rtip-2022-11-09/index.html",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "",
    "text": "K-Means is a clustering algorithm that can be used to find potential clusters in your data.\nThe algorithm does require that you look at different values of K in order to assess which is the optimal value.\nIn the R package {healthyR.ai} there is a utility to do this."
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html#parameters",
    "href": "posts/rtip-2022-11-09/index.html#parameters",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "Parameters",
    "text": "Parameters\nThe parameters take the following arguments:\n\n.data - This is the data that should be an output of the hai_user_item_tbl() or it’s synonym, or should at least be in the user item matrix format.\n.centers - The maximum amount of centers you want to map to the k-means function. The default is 15."
  },
  {
    "objectID": "posts/rtip-2022-11-10/index.html",
    "href": "posts/rtip-2022-11-10/index.html",
    "title": "Reading Multiple Files with {purrr}",
    "section": "",
    "text": "Introduction\nThere may be times when you have multiple structured files in the same folder, maybe they are .csv files. For this short tip, we will say that they are.\nI will show the short script and then discuss it.\n\n# Library Load ----\nlibrary(dplyr)\nlibrary(purrr)\n\n# Set file path ----\nfolder    <- \"FileFolder\"\npath      <- \"C:/Some/Root/Path/\"\nfull_path <- paste0(path,folder,\"/\")\n\n# File List ----\nfile_list <- dir(full_path\n                 , pattern = \"\\\\.csv$\"\n                 , full.names = T)\n\n# Read Files ----\nfiles <- file_list %>%\n  map(read.csv) %>%\n  map(as_tibble)\n\n# Clean File Names ----\nfile_names <- file_list %>%\n  str_remove(full_path) %>%\n  str_replace(\n    pattern = \"_OldStuff.csv\", \n    replacement = \"_NewStuff.csv\"\n  )\n\nnames(files) <- file_names\n\nWe load in {dplyr} for the pipe and the as_tibble function. After this we set out to create the file path. I have chosen to do this in two separate pieces as I have had experience with needing to go through different folders in the same root directory. While this could further be scripted I leave it as is.\nfolder is the folder that has the files of interest, in this case the .csv files. We then get the root path to that folder but not including it, this is defined as path in the above. After we have both folder and path we can create the full_path by using paste0\nNow after this we use the base R function of dir to list out all of the files that fit the specific format of .csv with a regex pattern. I always want the name of the file as it allows me to go back to the file later and lets me name the files in the upcoming list later on.\nSince these are .csv files I use purrr::map and then read.csv to read in all of the .csv files in the list that was created, we then used map again and this time used as_tibble to make sure that each file is a tibble and not something else like data.frame\nSince I provided the argument of T to dir, full.names I can then get a character vector of the names of the files which then is applied to the file list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-14/index.html",
    "href": "posts/rtip-2022-11-14/index.html",
    "title": "Find Skewed Features with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes we may want to quickly find skewed features in a data set. This is easily achiveable using the {healthyR.ai} library. There is a simple function called hai_skewed_features(). We are going to go over this function today.\n\n\nFunction\nLet’s first take a look at the function call.\n\nhai_skewed_features(\n  .data, \n  .threshold = 0.6, \n  .drop_keys = NULL\n  )\n\nNow let’s take a look at the arguments that go to the parameters of the function.\n\n.data - The data.frame/tibble you are passing in.\n.threshold - A level of skewness that indicates where you feel a column should be considered skewed.\n.drop_keys - A c() character vector of columns you do not want passed to the function.\n\n\n\nExample\nHere are a couple of examples.\n\nlibrary(healthyR.ai)\n\nhai_skewed_features(mtcars)\n\n[1] \"mpg\"  \"hp\"   \"carb\"\n\nhai_skewed_features(mtcars, .drop_keys = \"hp\")\n\n[1] \"mpg\"  \"carb\""
  },
  {
    "objectID": "posts/rtip-2022-11-15/index.html",
    "href": "posts/rtip-2022-11-15/index.html",
    "title": "Auto Prep data for XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes we may want to quickly format some data in order to just pass it through some algorithm just to see what happens, how crazy are things, just to get an idea of what may lie ahead…a lot of prep.\nWith my r package {healthyR.ai} there is a set of prepper functions that will automatically do a ‘best effort’ to format you data to be used in the algorithm you choose (should it be supported).\nToday we will talk about [hai_xgboost_data_prepper()\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\nNow let’s go over the arguments that are passed to the function.\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\n\n\nExample\nLet’s go over some examples.\n\nlibrary(ggplot2)\nlibrary(healthyR.ai)\n\n# Regression\nhai_xgboost_data_prepper(.data = diamonds, .recipe_formula = price ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nreg_obj <- hai_xgboost_data_prepper(diamonds, price ~ .)\nget_juiced_data(reg_obj)\n\n# A tibble: 53,940 × 27\n   carat depth table     x     y     z price  cut_1  cut_2  cut_3  cut_4   cut_5\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1  0.23  61.5    55  3.95  3.98  2.43   326  0.359 -0.109 -0.522 -0.567 -0.315 \n 2  0.21  59.8    61  3.89  3.84  2.31   326  0.120 -0.436 -0.298  0.378  0.630 \n 3  0.23  56.9    65  4.05  4.07  2.31   327 -0.359 -0.109  0.522 -0.567  0.315 \n 4  0.29  62.4    58  4.2   4.23  2.63   334  0.120 -0.436 -0.298  0.378  0.630 \n 5  0.31  63.3    58  4.34  4.35  2.75   335 -0.359 -0.109  0.522 -0.567  0.315 \n 6  0.24  62.8    57  3.94  3.96  2.48   336 -0.120 -0.436  0.298  0.378 -0.630 \n 7  0.24  62.3    57  3.95  3.98  2.47   336 -0.120 -0.436  0.298  0.378 -0.630 \n 8  0.26  61.9    55  4.07  4.11  2.53   337 -0.120 -0.436  0.298  0.378 -0.630 \n 9  0.22  65.1    61  3.87  3.78  2.49   337 -0.598  0.546 -0.373  0.189 -0.0630\n10  0.23  59.4    61  4     4.05  2.39   338 -0.120 -0.436  0.298  0.378 -0.630 \n# … with 53,930 more rows, and 15 more variables: color_1 <dbl>, color_2 <dbl>,\n#   color_3 <dbl>, color_4 <dbl>, color_5 <dbl>, color_6 <dbl>, color_7 <dbl>,\n#   clarity_1 <dbl>, clarity_2 <dbl>, clarity_3 <dbl>, clarity_4 <dbl>,\n#   clarity_5 <dbl>, clarity_6 <dbl>, clarity_7 <dbl>, clarity_8 <dbl>\n\n# Classification\nhai_xgboost_data_prepper(Titanic, Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\ncla_obj <- hai_xgboost_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(cla_obj)\n\n# A tibble: 32 × 7\n       n Survived Class_X2nd Class_X3rd Class_Crew Sex_Male Age_Child\n   <dbl> <fct>         <dbl>      <dbl>      <dbl>    <dbl>     <dbl>\n 1     0 No                0          0          0        1         1\n 2     0 No                1          0          0        1         1\n 3    35 No                0          1          0        1         1\n 4     0 No                0          0          1        1         1\n 5     0 No                0          0          0        0         1\n 6     0 No                1          0          0        0         1\n 7    17 No                0          1          0        0         1\n 8     0 No                0          0          1        0         1\n 9   118 No                0          0          0        1         0\n10   154 No                1          0          0        1         0\n# … with 22 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-16/index.html",
    "href": "posts/rtip-2022-11-16/index.html",
    "title": "Cumulative Harmonic Mean with {TidyDensity}",
    "section": "",
    "text": "Introduction\nThere can be times in which you may want to see a cumulative statistic, maybe in this particular case it is the harmonic mean. Well with the {TidyDensity} it is possible with a function called chmean()\nLet’s take a look at the function.\n\n\nFunction\nHere is the function call, it is very simple as it is a vectorized function.\n\nchmean(.x)\n\nThe only argument you provide to this function is a numeric vector. Let’s take a quick look at the construction of the function.\n\nchmean <- function(.x) {\n  1 / (cumsum(1 / .x))\n}\n\n\n\nExamples\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\nx <- mtcars$mpg\n\nchmean(x)\n\n [1] 21.0000000 10.5000000  7.1891892  5.3813575  4.1788087  3.3949947\n [7]  2.7436247  2.4663044  2.2255626  1.9943841  1.7934398  1.6166494\n[13]  1.4784877  1.3474251  1.1928760  1.0701322  0.9975150  0.9677213\n[19]  0.9378663  0.9126181  0.8754572  0.8286539  0.7858140  0.7419753\n[25]  0.7143688  0.6961523  0.6779989  0.6632076  0.6364908  0.6165699\n[31]  0.5922267  0.5762786\n\nmtcars %>%\n  select(mpg) %>%\n  mutate(cum_har_mean = chmean(mpg)) %>%\n  head(10)\n\n                   mpg cum_har_mean\nMazda RX4         21.0    21.000000\nMazda RX4 Wag     21.0    10.500000\nDatsun 710        22.8     7.189189\nHornet 4 Drive    21.4     5.381358\nHornet Sportabout 18.7     4.178809\nValiant           18.1     3.394995\nDuster 360        14.3     2.743625\nMerc 240D         24.4     2.466304\nMerc 230          22.8     2.225563\nMerc 280          19.2     1.994384\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-17/index.html",
    "href": "posts/rtip-2022-11-17/index.html",
    "title": "Bootstrap Modeling with {purrr} and {modler}",
    "section": "",
    "text": "Introduction\nMany times in modeling we want to get the uncertainty in the model, well, bootstrapping to the rescue!\nI am going to go over a very simple example on how to use purrr and modelr for this situation. We will use the mtcars dataset.\n\n\nFunctions\nThe main functions that we are going to showcase are purrr::map() and modelr::bootstrap()\n\n\nExamples\nLet’s get right into it.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndf <- mtcars\n\nfit_boots <- df %>% \n  modelr::bootstrap(n = 200, id = 'boot_num') %>%\n  group_by(boot_num) %>%\n  mutate(fit = map(strap, ~lm(mpg ~ ., data = data.frame(.))))\n\nfit_boots\n\n# A tibble: 200 × 3\n# Groups:   boot_num [200]\n   strap                boot_num fit   \n   <list>               <chr>    <list>\n 1 <resample [32 x 11]> 001      <lm>  \n 2 <resample [32 x 11]> 002      <lm>  \n 3 <resample [32 x 11]> 003      <lm>  \n 4 <resample [32 x 11]> 004      <lm>  \n 5 <resample [32 x 11]> 005      <lm>  \n 6 <resample [32 x 11]> 006      <lm>  \n 7 <resample [32 x 11]> 007      <lm>  \n 8 <resample [32 x 11]> 008      <lm>  \n 9 <resample [32 x 11]> 009      <lm>  \n10 <resample [32 x 11]> 010      <lm>  \n# … with 190 more rows\n\n\nNow lets get our parameter estimates.\n\n# get parameters ####\nparams_boot <- fit_boots %>%\n  mutate(tidy_fit = map(fit, tidy)) %>%\n  unnest(cols = tidy_fit) %>%\n  ungroup()\n\n# get predictions\npreds_boot <- fit_boots %>%\n  mutate(augment_fit = map(fit, augment)) %>%\n  unnest(cols = augment_fit) %>%\n  ungroup()\n\nTime to visualize.\n\nlibrary(patchwork)\n\n# plot distribution of estimated parameters\np1 <- ggplot(params_boot, aes(estimate)) +\n  geom_histogram(col = 'black', fill = 'white') +\n  facet_wrap(~ term, scales = 'free') +\n  theme_minimal()\n\n# plot points with predictions\np2 <- ggplot() +\n  geom_line(aes(mpg, .fitted, group = boot_num), preds_boot, alpha = .03) +\n  geom_point(aes(mpg, .fitted), preds_boot, col = 'steelblue', alpha = 0.05) +\n  theme_minimal()\n  \n# plot both\np1 + p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-21/index.html",
    "href": "posts/rtip-2022-11-21/index.html",
    "title": "Bootstrap Modeling with Base R",
    "section": "",
    "text": "Introduction\nI have previously written about bootstrap modeling with {purrr} and {modelr} here. What if you would like to do some simple bootstrap modeling without importing a library? This itself is easy too!\n\n\nExample\nWe will be using a very simple for loop to accomplish this. You will find an excellent post on this on Stats StackExchange from Francisco Jos Goerlich Gisbert\n\nn    <- 2000\ndf   <- mtcars\npred <- numeric(0)\n\nlibrary(tictoc) # for timing\n\ntic()\nset.seed(123)\nfor (i in 1:n){\n  boot    <- sample(nrow(df), n, replace = TRUE)\n  fit     <- lm(mpg ~ wt, data = df[boot,])\n  pred[i] <- predict(fit, newdata = df[boot,]) +\n    sample(resid(fit), size = 1)\n}\ntoc()\n\n6.8 sec elapsed\n\n\nSo we can see that the process ran pretty quickly and the loop itself is not a very difficult one. Let’s explain a little.\nSo the boot object is a sampling of df which in this case is the mtcars data set. We took a sample with replacement from this data set. We took 2000 samples and did this 2000 times.\nNext we made the fit object by fitting a simple linear model to the data where mpg is a function of wt. Once this is done, we made out predictions.\nThat’s it!"
  },
  {
    "objectID": "posts/rtip-2022-11-22/index.html",
    "href": "posts/rtip-2022-11-22/index.html",
    "title": "Data Preprocessing Scale/Normalize with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nA large portion of data modeling occurrs not only in the data cleaning phase but also in the data preprocessing phase. This can include things like scaling or normalizing data before proceeding to the modeling phase. I will discuss one such function from my r package {healthyR.ai}. In this post I will go over hai_data_scale()\nThis is a {recipes} style step function and is tidymodels compliant.\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_data_scale(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"center\",\n  .range_min = 0,\n  .range_max = 1,\n  .scale_factor = 1\n)\n\nNow let’s go over the arguments that get supplied to the parameters of this function.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“center”\n“normalize”\n“range”\n“scale”\n\nrange_min - A single numeric value for the smallest value in the range. This defaults to 0.\n.range_max - A single numeric value for the largeest value in the range. This defaults to 1.\n.scale_factor - A numeric value of either 1 or 2 that scales the numeric inputs by one or two standard deviations. By dividing by two standard deviations, the coefficients attached to continuous predictors can be interpreted the same way as with binary inputs. Defaults to 1.\n\n\n\nExample\nNow let’s see it in action!\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndate_seq <- seq.Date(\n  from = as.Date(\"2013-01-01\"), \n  length.out = 100, \n  by = \"month\"\n)\n\nval_seq <- rep(rnorm(10, mean = 6, sd = 2), times = 10)\n\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col   value\n   <date>     <dbl>\n 1 2013-01-01  6.66\n 2 2013-02-01  6.66\n 3 2013-03-01  5.09\n 4 2013-04-01  6.94\n 5 2013-05-01  5.96\n 6 2013-06-01  6.18\n 7 2013-07-01  3.62\n 8 2013-08-01  7.31\n 9 2013-09-01  4.58\n10 2013-10-01  7.29\n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nnew_rec_obj <- hai_data_scale(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_scale = \"center\"\n)$scale_rec_obj\n\nnew_rec_obj %>% \n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.633 \n 2 2013-02-01  0.630 \n 3 2013-03-01 -0.935 \n 4 2013-04-01  0.909 \n 5 2013-05-01 -0.0676\n 6 2013-06-01  0.149 \n 7 2013-07-01 -2.41  \n 8 2013-08-01  1.28  \n 9 2013-09-01 -1.45  \n10 2013-10-01  1.26  \n# … with 90 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-28/index.html",
    "href": "posts/rtip-2022-11-28/index.html",
    "title": "Default Metric Sets with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nWhen modeling it is always good to understand your model performance against some metric The {tidymodels} package {yardstick} is a great resource for this.\nIn my R package {healthyR.ai} there are two functions that allow you to either minimize or maximize some cost function against your modeling problem.\nThese functions are: * hai_default_regression_metric_set() * hai_default_classification_metric_set()\n\n\nFunction\nThe functions themselves are {yardstick} metric set functions. Let’s take a look at them.\n\nlibrary(healthyR.ai)\n\nhai_default_classification_metric_set()\n\n# A tibble: 11 × 3\n   metric       class        direction\n   <chr>        <chr>        <chr>    \n 1 sensitivity  class_metric maximize \n 2 specificity  class_metric maximize \n 3 recall       class_metric maximize \n 4 precision    class_metric maximize \n 5 mcc          class_metric maximize \n 6 accuracy     class_metric maximize \n 7 f_meas       class_metric maximize \n 8 kap          class_metric maximize \n 9 ppv          class_metric maximize \n10 npv          class_metric maximize \n11 bal_accuracy class_metric maximize \n\nhai_default_regression_metric_set()\n\n# A tibble: 6 × 3\n  metric class          direction\n  <chr>  <chr>          <chr>    \n1 mae    numeric_metric minimize \n2 mape   numeric_metric minimize \n3 mase   numeric_metric minimize \n4 smape  numeric_metric minimize \n5 rmse   numeric_metric minimize \n6 rsq    numeric_metric maximize \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-29/index.html",
    "href": "posts/rtip-2022-11-29/index.html",
    "title": "Working with Lists",
    "section": "",
    "text": "Introduction\nIn R there are many times where we will work with lists. I won’t go into why lists are great or really the structure of a list but rather simply working with them.\n\n\nExample\nFirst let’s make a list.\n\nl <- list(\n  letters,\n  1:26,\n  rnorm(26)\n)\n\nl\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26\n\n[[3]]\n [1] -1.5647537840 -1.3080486753  1.3331315389 -0.5490502644 -0.4467608750\n [6] -1.5876952894  0.2292049732 -0.2885449316  1.4614499298 -0.0864987690\n[11]  0.5686850031 -0.3897819578  0.1776603862 -1.1326372302 -1.8651290164\n[16]  1.2676006036  0.2405115523 -1.0506728047  1.4069277686 -1.0125778892\n[21] -0.7687818102 -0.1325350681  0.3639485041  0.0005700058 -1.0698214370\n[26]  1.1972767040\n\n\nNow let’s look at somethings we can do with lists. First, let’s see if we can get the class of each item in the list. We are going to use lapply() for this.\n\nlapply(l, class)\n\n[[1]]\n[1] \"character\"\n\n[[2]]\n[1] \"integer\"\n\n[[3]]\n[1] \"numeric\"\n\n\nNow, let’s perform some simple operations on each item of the list.\n\nlapply(l, length)\n\n[[1]]\n[1] 26\n\n[[2]]\n[1] 26\n\n[[3]]\n[1] 26\n\ntry(lapply(l, sum))\n\nError in FUN(X[[i]], ...) : invalid 'type' (character) of argument\n\n\nOk so we see taking the sum of the first element of the list in lapply() did not work because of a class type mismatch. Let’s see how we can get around this an only apply the sum function to a numeric type. To do this we can rely on {purrr} by using a function map_if()\n\nlibrary(purrr)\n\nmap_if(l, is.numeric, sum)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 351\n\n[[3]]\n[1] -5.006323\n\n\n\nmap_if(l, is.numeric, mean)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 13.5\n\n[[3]]\n[1] -0.1925509\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-30/index.html",
    "href": "posts/rtip-2022-11-30/index.html",
    "title": "Generate Random Walk Data with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nGenerating random walk data for timesieries analysis does not have to be difficult, and in fact is not. It can be generated for multiple simulations and have a tidy output. How? ts_random_walk() from the {healthyR.ts} package. Let’s take a look at the function.\n\n\nFunction\nHere is the full function call.\n\nts_random_walk(\n  .mean = 0,\n  .sd = 0.1,\n  .num_walks = 100,\n  .periods = 100,\n  .initial_value = 1000\n)\n\nNow let’s look at the arguments to the parameters.\n\n.mean - The desired mean of the random walks\n.sd - The standard deviation of the random walks\n.num_walks - The number of random walks you want generated\n.periods - The length of the random walk(s) you want generated\n.initial_value - The initial value where the random walks should start\n\nThe underlying data of this function is generated by rnorm()\n\n\nExample\nLet’s take a look at an example and see some visuals.\n\nlibrary(healthyR.ts)\nlibrary(ggplot2)\n\ndf <- ts_random_walk(.num_walks = 100)\n\ndf\n\n# A tibble: 10,000 × 4\n     run     x        y cum_y\n   <dbl> <dbl>    <dbl> <dbl>\n 1     1     1 -0.144    856.\n 2     1     2  0.00648  862.\n 3     1     3  0.0726   924.\n 4     1     4 -0.152    784.\n 5     1     5  0.0228   802.\n 6     1     6 -0.0455   765.\n 7     1     7  0.0972   840.\n 8     1     8 -0.234    643.\n 9     1     9 -0.0501   611.\n10     1    10 -0.0358   589.\n# … with 9,990 more rows\n\n\nThere are attributes attached to the output of this function, let’s see what they are.\n\natb <- attributes(df)\n\nnames_to_print <- names(atb)[which(names(atb) != \"row.names\")]\n\natb[names_to_print]\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$names\n[1] \"run\"   \"x\"     \"y\"     \"cum_y\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 0.1\n\n$.num_walks\n[1] 100\n\n$.periods\n[1] 100\n\n$.initial_value\n[1] 1000\n\n\nNow lets visualize.\n\ndf %>%\n   ggplot(\n       mapping = aes(\n           x = x\n           , y = cum_y\n           , color = factor(run)\n           , group = factor(run)\n        )\n    ) +\n    geom_line(alpha = 0.8) +\n    ts_random_walk_ggplot_layers(df)\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-01/index.html",
    "href": "posts/rtip-2022-12-01/index.html",
    "title": "Extract Boilerplate Workflow Metrics with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nWhen working with the {tidymodels} framework there are ways to pull model metrics from a workflow, since {healthyR.ai} is built on and around the {tidyverse} and {tidymodels} we can do the same. This post will focus on the function hai_auto_wflw_metrics()\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_auto_wflw_metrics(.data)\n\nThe only parameter is .data and this is strictly the output object of one of the hai_auto_ boiler plate functions\n\n\nExample\nSince this function requires the input from an hai_auto function, we will walk through an example with the iris data set. We are going to use the hai_auto_knn() to classify the Species.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_knn_data_prepper(data, Species ~ .)\n\nauto_knn <- hai_auto_knn(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\",\n  .grid_size = 2,\n  .num_cores = 4\n)\n\nhai_auto_wflw_metrics(auto_knn)\n\n# A tibble: 22 × 9\n   neighbors weight_func dist_power .metric  .esti…¹  mean     n std_err .config\n       <int> <chr>            <dbl> <chr>    <chr>   <dbl> <int>   <dbl> <chr>  \n 1         8 rank             0.888 accuracy multic… 0.95     25 0.00652 Prepro…\n 2         8 rank             0.888 bal_acc… macro   0.962    25 0.00471 Prepro…\n 3         8 rank             0.888 f_meas   macro   0.947    25 0.00649 Prepro…\n 4         8 rank             0.888 kap      multic… 0.922    25 0.0102  Prepro…\n 5         8 rank             0.888 mcc      multic… 0.925    25 0.00964 Prepro…\n 6         8 rank             0.888 npv      macro   0.975    25 0.00351 Prepro…\n 7         8 rank             0.888 ppv      macro   0.949    25 0.00663 Prepro…\n 8         8 rank             0.888 precisi… macro   0.949    25 0.00663 Prepro…\n 9         8 rank             0.888 recall   macro   0.949    25 0.00633 Prepro…\n10         8 rank             0.888 sensiti… macro   0.949    25 0.00633 Prepro…\n# … with 12 more rows, and abbreviated variable name ¹​.estimator\n\n\nAs we see this pulls out the full metric table from the workflow.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-05/index.html",
    "href": "posts/rtip-2022-12-05/index.html",
    "title": "Naming Items in a List with {purrr}, {dplyr}, or {healthyR}",
    "section": "",
    "text": "Introduction\nMany times when we are working with a data set we will want to break it up into groups and place them into a list and work with them in that fashion. With this it can be useful to the elements of the list named by the column that the data was split upon. Let’s use the iris set as an example where we split on Species.\nThere are two main functions that we will use in this scenario, namely purrr:map() and dplyr::group_split(), you could also use the split function from base r for this.\nWe will also go over how simple this is using the {healthyR} package. Let’s look at the function from {healthyR}\n\n\nFunction\nFull function call.\n\nnamed_item_list(.data, .group_col)\n\nThere are only two arguments to supply.\n\n.data - The data.frame/tibble.\n.group_col - The column that contains the groupings.\n\nThat’s it.\n\n\nExamples\nLet’s jump into it.\n\nlibrary(purrr)\nlibrary(dplyr)\n\ndata_tbl <- iris\n\ndata_tbl_list <- data_tbl %>%\n  group_split(Species)\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n[[1]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n[[2]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n[[3]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\ndata_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\n[[1]]\n[1] \"setosa\"\n\n[[2]]\n[1] \"versicolor\"\n\n[[3]]\n[1] \"virginica\"\n\n\nNow lets go ahead and apply the names.\n\nnames(data_tbl_list) <- data_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nLet’s now see how we do this in {healthyR}\n\nlibrary(healthyR)\n\nnamed_item_list(iris, Species)\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nIf you use this in conjunction with the healthyR function save_to_excel() then it will write an excel file with a tab for each named item in the list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-06/index.html",
    "href": "posts/rtip-2022-12-06/index.html",
    "title": "Z-Score Scaling Step Recipe with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes one may find it useful or necessary to scale their data during a modeling or analysis phase. One of these such transformations is the z-score scaling.\nThis is done simply by performing the below transform where x is simply some numeric vector:\n\\[ z_x = (x - mu(x))/sd(x) \\]\nLet’s take a look at the recipe function called step_hai_scale_zscore\n\n\nFunction\nHere is the full function call:\n\nstep_hai_scale_zscore(\n  recipe,\n  ...,\n  role = \"predictor\",\n  trained = FALSE,\n  columns = NULL,\n  skip = FALSE,\n  id = rand_id(\"hai_scale_zscore\")\n)\n\nHere are the arguments to the function.\n\nrecipe - A recipe object. The step will be added to the sequence of operations for this recipe.\n... - One or more selector functions to choose which variables that will be used to create the new variables. The selected variables should have class numeric\nrole - For model terms created by this step, what analysis role should they be assigned?. By default, the function assumes that the new variable columns created by the original variables will be used as predictors in a model.\ntrained - A logical to indicate if the quantities for preprocessing have been estimated.\ncolumns - A character string of variables that will be used as inputs. This field is a placeholder and will be populated once recipes::prep() is used.\nskip - A logical. Should the step be skipped when the recipe is baked by bake.recipe()? While all operations are baked when prep.recipe() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations.\nid - A character string that is unique to this step to identify it.\n\n\n\nExample\nHere is a simple example.\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndf <- iris |>\n  as_tibble() |>\n  select(Species, Sepal.Length)\n\nrec_obj <- recipe(Sepal.Length ~ ., data = df) %>%\n  step_hai_scale_zscore(Sepal.Length)\n\nrec_obj\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nZero-One Scale Transformation on Sepal.Length\n\nsummary(rec_obj)\n\n# A tibble: 2 × 4\n  variable     type      role      source  \n  <chr>        <list>    <chr>     <chr>   \n1 Species      <chr [3]> predictor original\n2 Sepal.Length <chr [2]> outcome   original\n\n\nNow let’s take a look at the differences.\n\nlibrary(ggplot2)\nlibrary(plotly)\n\ndf_tbl <- get_juiced_data(rec_obj)\n\ndf_tbl |>\n  purrr::set_names(\"Species\",\"Sepal_Length\",\"Scaled_Sepal_Length\") |>\n  ggplot(aes(x = Sepal_Length)) +\n  geom_histogram(color = \"black\", fill = \"lightgreen\") +\n  geom_histogram(aes(x = Scaled_Sepal_Length), \n                 color = \"black\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    y = \"Count\",\n    x = \"Sepal Length\",\n    title = \"Speal.Length: Original vs. Z-Score Scaled\",\n    subtitle = \"Original (Light Green) Scaled (Steelblue)\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-07/index.html",
    "href": "posts/rtip-2022-12-07/index.html",
    "title": "Create Multiple {parsnip} Model Specs with {purrr}",
    "section": "",
    "text": "Introduction\nIf you want to generate multiple parsnip model specifications at the same time then it’s really not to hard. This sort of thing is being addressed in an upcoming package of mine called {tidyaml}\nThis post is going to be quick and simple, I will showcase how you can generate many different model specifications in one go. I will also discuss the function create_model_spec() that will allow you to do this with a simple function call once the package is actually released.\n\n\nFunction\nHere is the function call for the create_model_spec() for once it is release.\n\ncreate_model_spec(\n  .parsnip_eng = list(\"lm\"),\n  .mode = list(\"regression\"),\n  .parsnip_fns = list(\"linear_reg\"),\n  .return_tibble = TRUE\n)\n\nHere are the arguments to the function. * .parsnip_eng - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c(‘lm’, ‘glm’) * .mode- The input must be a list. The default is ‘regression’ * .parsnip_fns - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(“linear_reg”,“cubist_rules”) * .return_tibble - The default is TRUE. FALSE will return a list object.\n\n\nExample\nHere is the function at work.\n\nlibrary(tidyaml)\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow that we have seen what is to come in the future, let’s take a look at a pseudo solution that is easy to replicate now.\n\n# Load the purrr package\nlibrary(purrr)\nlibrary(parsnip)\n\n# Create a list of parsnip engines\nengines <- list(\n  engine1 = \"lm\",\n  engine2 = \"glm\",\n  engine3 = \"randomForest\"\n)\n\n# Create a list of parsnip call names\nparsnip_calls <- list(\n  call1 = \"linear_reg\",\n  call2 = \"linear_reg\",\n  call3 = \"rand_forest\"\n)\n\n# Use pmap() to create a list of parsnip model specs from the list of engines\n# and parsnip call names\n# Set the mode argument to \"regression\"\nmodel_specs <- pmap(list(engines, parsnip_calls), function(engine, call) {\n  match.fun(call)(engine = engine, mode = \"regression\")\n})\n\n# Print the list of model specs to the console\nmodel_specs\n\n$engine1\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$engine2\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$engine3\nRandom Forest Model Specification (regression)\n\nComputational engine: randomForest \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-08/index.html",
    "href": "posts/rtip-2022-12-08/index.html",
    "title": "Create a Faceted Historgram Plot with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nOne of the most important steps in data analysis is visualizing the distribution of your data. This can help you identify patterns, outliers, and trends in your data, and can also provide valuable insights into the relationships between different variables.\nOne way to visualize data distributions is by using histograms. A histogram is a graphical representation of the distribution of a numeric variable. It shows the number of observations (or the frequency) within each bin or range of values.\nIn this blog post, we will showcase the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create faceted histograms of numeric and factor data in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nhai_histogram_facet_plot(\n  .data,\n  .bins = 10,\n  .scale_data = FALSE,\n  .ncol = 5,\n  .fct_reorder = FALSE,\n  .fct_rev = FALSE,\n  .fill = \"steelblue\",\n  .color = \"white\",\n  .scale = \"free\",\n  .interactive = FALSE\n)\n\nHere are the parameters and the arguments that get passed to them.\n\n.data - The data you want to pass to the function.\n.bins - The number of bins for the histograms.\n.scale_data - This is a boolean set to FALSE. TRUE will use hai_scale_zero_one_vec() to [0, 1] scale the data.\n.ncol - The number of columns for the facet_warp argument.\n.fct_reorder - Should the factor column be reordered? TRUE/FALSE, default of FALSE\n.fct_rev - Should the factor column be reversed? TRUE/FALSE, default of FALSE\n.fill - Default is steelblue\n.color - Default is ‘white’\n.scale - Default is ‘free’\n.interactive - Default is FALSE, TRUE will produce a {plotly} plot.\n\n\n\nExamples\nLet’s take a look at some example.\n\nlibrary(healthyR.ai, quietly = TRUE)\n\nhai_histogram_facet_plot(mtcars)\n\n\n\n\nNow lets scale the data and review.\n\nhai_histogram_facet_plot(mtcars, .scale_data = TRUE)\n\n\n\n\nLet’s take a look the iris data set now.\n\noutput <- hai_histogram_facet_plot(iris, .interactive = TRUE)\noutput$plot\n\n\n\n\n\nIn this blog post, we showcased the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create histogram plots and faceted histograms in R. The hai_histogram_facet_plot() function allows you to quickly and easily visualize the distribution of your data, and can provide valuable insights into the relationships between different variables\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-13/index.html",
    "href": "posts/rtip-2022-12-13/index.html",
    "title": "Mixture Distributions with {TidyDensity}",
    "section": "",
    "text": "Introduction\nA mixture distribution is a type of probability distribution that is created by combining two or more simpler distributions. This allows us to model complex data that may have multiple underlying patterns. For example, a mixture distribution could be used to model a dataset that includes both continuous and discrete variables.\nTo create a mixture distribution, we first need to specify the individual distributions that will be combined, as well as the weights that determine how much each distribution contributes to the overall mixture. Once we have these components, we can use them to calculate the probability of any given value occurring in the mixture distribution.\nMixture distributions can be useful in a variety of applications, such as data analysis and machine learning. In data analysis, they can be used to model data that is not well-described by a single distribution, and in machine learning, they can be used to improve the performance of predictive models. Overall, mixture distributions are a powerful tool for understanding and working with complex data.\n\n\nFunction\nLet’s take a look a function in {TidyDensity} that allows us to do this. At this moment, weights are not a parameter to the function.\n\ntidy_mixture_density(...)\n\nNow let’s take a look at the arguments that get supplied to the ... parameter.\n\n... - The random data you want to pass. Example rnorm(50,0,1) or something like tidy_normal(.mean = 5, .sd = 1)\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\n\noutput <- tidy_mixture_density(\n  rnorm(100, 0, 1), \n  tidy_normal(.mean = 5, .sd = 1)\n)\n\nAs you can see, you can enter a function that outputs a numeric vector or you can use a {TidyDensity} distribution function.\nLet’s take a look at the outputs.\n\noutput$data\n\n$dist_tbl\n# A tibble: 150 × 2\n       x       y\n   <int>   <dbl>\n 1     1  0.442 \n 2     2  1.80  \n 3     3  0.571 \n 4     4 -0.0365\n 5     5  0.854 \n 6     6 -0.634 \n 7     7 -0.189 \n 8     8 -0.415 \n 9     9  1.36  \n10    10  0.107 \n# … with 140 more rows\n\n$dens_tbl\n# A tibble: 150 × 2\n       x         y\n   <dbl>     <dbl>\n 1 -4.17 0.0000995\n 2 -4.08 0.000145 \n 3 -3.98 0.000207 \n 4 -3.89 0.000294 \n 5 -3.79 0.000413 \n 6 -3.70 0.000574 \n 7 -3.60 0.000788 \n 8 -3.51 0.00107  \n 9 -3.41 0.00145  \n10 -3.32 0.00193  \n# … with 140 more rows\n\n$input_data\n$input_data$`rnorm(100, 0, 1)`\n  [1]  0.44169781  1.80418306  0.57133927 -0.03649729  0.85387119 -0.63383074\n  [7] -0.18854658 -0.41451222  1.36023418  0.10726858  0.08526992 -0.64879496\n [13]  0.69255412 -0.75735669  0.19705920 -0.17721516 -0.63079170 -1.39983310\n [19]  1.01755199 -0.83631414  0.72912414 -0.14737137  1.27082258 -1.04753889\n [25] -0.16141490  0.22198899  2.83598596 -0.22484669 -0.58487594 -0.62746477\n [31] -0.81873031  1.74559087  1.36529721  1.45023471 -0.06258668  2.14467649\n [37]  0.10043517 -0.67990809  2.85050168 -1.45216256  0.01049808  0.22827703\n [43] -0.51146361  0.43143915 -0.59915348  1.61324991 -0.58580448 -0.46120961\n [49]  0.98191810 -0.31593955  0.86164296  1.18808250  1.09066101  0.39150090\n [55]  0.50730674  1.88640675  1.55522681 -0.65149477 -0.27561149 -0.31867192\n [61]  0.08555271 -1.00047014  1.12127311 -1.23597493  0.96384070  0.99097697\n [67] -0.25932523  0.25407058 -0.35294377 -0.72055148 -0.40429088 -0.08843004\n [73]  0.95498089 -0.68453125  1.67531797 -0.20665261  0.57318766 -0.12758793\n [79] -0.38044927  1.81833828  1.05959931  0.08519174  0.16865694 -0.15828443\n [85]  0.08736815  0.70222886  1.27180668  0.76483122 -0.43573173  0.02909088\n [91] -1.31286933 -0.09244617  0.22188836 -0.88909052  1.22243358  0.48397190\n [97]  0.82291445  0.46595188  0.68619052 -1.65739185\n\n$input_data$`tidy_normal(.mean = 5, .sd = 1)`\n# A tibble: 50 × 7\n   sim_number     x     y    dx       dy      p     q\n   <fct>      <int> <dbl> <dbl>    <dbl>  <dbl> <dbl>\n 1 1              1  6.20  1.24 0.000230 0.886   6.20\n 2 1              2  3.95  1.40 0.000639 0.146   3.95\n 3 1              3  4.59  1.55 0.00158  0.342   4.59\n 4 1              4  3.16  1.70 0.00349  0.0326  3.16\n 5 1              5  5.03  1.86 0.00689  0.513   5.03\n 6 1              6  4.89  2.01 0.0122   0.455   4.89\n 7 1              7  5.49  2.16 0.0195   0.687   5.49\n 8 1              8  6.78  2.32 0.0283   0.962   6.78\n 9 1              9  5.17  2.47 0.0376   0.566   5.17\n10 1             10  6.36  2.62 0.0464   0.913   6.36\n# … with 40 more rows\n\n\nAnd now the visuals that come with it.\n\noutput$plots\n\n$line_plot\n\n\n\n\n\n\n$dens_plot\n\n\n\n\n\nThe function also lists the input functions as well.\n\noutput$input_fns\n\n[[1]]\nrnorm(100, 0, 1)\n\n[[2]]\ntidy_normal(.mean = 5, .sd = 1)\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-14/index.html",
    "href": "posts/rtip-2022-12-14/index.html",
    "title": "Distribution Summaries with {TidyDensity}",
    "section": "",
    "text": "Introduction\n{TidyDensity} is an R package that provides tools for working with probability distributions in a tidy data format. One of the key functions in the package is tidy_distribution_summary_tbl(), which allows users to quickly and easily get summary information about a probability distribution.\nThe tidy_distribution_summary_tbl() function takes a vector of data as input and returns a table with basic statistics about the distribution of the data. This includes the mean, standard deviation, kurtosis, and skewness of the data, as well as other useful information.\nUsing tidy_distribution_summary_tbl(), users can easily get a high-level overview of their data, which can be useful for exploratory data analysis, data visualization, and other tasks. The function is designed to work seamlessly with the other tools in the {TidyDensity} package, making it easy to combine with other operations and build complex data analysis pipelines.\nOverall, TidyDensity and its tidy_distribution_summary_tbl() function are valuable tools for anyone working with probability distributions in R. Whether you are a seasoned data scientist or a beginner, TidyDensity can help you quickly and easily explore and understand your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_distribution_summary_tbl(.data, ...)\n\nHere are the arguments that go to the parameters.\n\n.data - The data that is going to be passed from a a tidy_ distribution function.\n... - This is the grouping variable that gets passed to dplyr::group_by() and dplyr::select().\n\n\n\nExample\nNow let’s go over a simple example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <- tidy_normal(.num_sims = 5)\ntb <- tidy_beta(.num_sims = 5)\n\ntidy_distribution_summary_tbl(tn) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> -0.044964\n$ median_val <dbl> -0.0266966\n$ std_val    <dbl> 1.020322\n$ min_val    <dbl> -2.834123\n$ max_val    <dbl> 3.336879\n$ skewness   <dbl> 0.03115634\n$ kurtosis   <dbl> 2.772527\n$ range      <dbl> 6.171002\n$ iqr        <dbl> 1.447849\n$ variance   <dbl> 1.041057\n$ ci_low     <dbl> -1.873091\n$ ci_high    <dbl> 1.868382\n\ntidy_distribution_summary_tbl(tn, sim_number) |>\n  glimpse()\n\nRows: 5\nColumns: 13\n$ sim_number <fct> 1, 2, 3, 4, 5\n$ mean_val   <dbl> -0.09684833, -0.13886169, 0.23257556, -0.32487778, 0.103192…\n$ median_val <dbl> -0.1358051, -0.2550682, 0.3069263, -0.1334922, 0.2898412\n$ std_val    <dbl> 1.1231699, 1.0954659, 0.8902380, 0.9270631, 0.9919932\n$ min_val    <dbl> -2.834123, -2.340575, -1.963215, -2.396105, -1.827744\n$ max_val    <dbl> 3.336879, 1.987640, 2.066451, 1.526231, 2.093211\n$ skewness   <dbl> 0.352771389, 0.132723834, -0.282840344, -0.191853538, 0.006…\n$ kurtosis   <dbl> 3.652828, 2.169309, 2.749967, 2.332081, 2.409223\n$ range      <dbl> 6.171002, 4.328215, 4.029666, 3.922336, 3.920956\n$ iqr        <dbl> 1.5256470, 1.6335396, 0.9368546, 1.3968485, 1.3469671\n$ variance   <dbl> 1.2615106, 1.2000455, 0.7925236, 0.8594460, 0.9840505\n$ ci_low     <dbl> -1.834548, -1.844197, -1.428713, -2.193065, -1.626225\n$ ci_high    <dbl> 1.860755, 1.858576, 1.644153, 1.090125, 1.976371\n\ndata_tbl <- tidy_combine_distributions(tn, tb)\n\ntidy_distribution_summary_tbl(data_tbl) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> 0.2413251\n$ median_val <dbl> 0.3687409\n$ std_val    <dbl> 0.8030476\n$ min_val    <dbl> -2.834123\n$ max_val    <dbl> 3.336879\n$ skewness   <dbl> -0.7608556\n$ kurtosis   <dbl> 4.248452\n$ range      <dbl> 6.171002\n$ iqr        <dbl> 0.7835065\n$ variance   <dbl> 0.6448855\n$ ci_low     <dbl> -1.695096\n$ ci_high    <dbl> 1.585147\n\ntidy_distribution_summary_tbl(data_tbl, dist_type) |>\n  glimpse()\n\nRows: 2\nColumns: 13\n$ dist_type  <fct> \"Gaussian c(0, 1)\", \"Beta c(1, 1, 0)\"\n$ mean_val   <dbl> -0.0449640, 0.5276142\n$ median_val <dbl> -0.0266966, 0.5301650\n$ std_val    <dbl> 1.0203220, 0.2944871\n$ min_val    <dbl> -2.834123047, 0.001236575\n$ max_val    <dbl> 3.3368786, 0.9992146\n$ skewness   <dbl> 0.03115634, -0.08744219\n$ kurtosis   <dbl> 2.772527, 1.751248\n$ range      <dbl> 6.171002, 0.997978\n$ iqr        <dbl> 1.447849, 0.511105\n$ variance   <dbl> 1.04105699, 0.08672268\n$ ci_low     <dbl> -1.87309115, 0.04220623\n$ ci_high    <dbl> 1.8683817, 0.9771898"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html",
    "href": "posts/rtip-2022-12-15/index.html",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "The R package {healthyR.ts}, is an R package that allows users to easily plot and analyze their time series data. The package includes a variety of functions, but one of the standout features is the ts_sma_plot() function, which allows users to quickly visualize their time series data and any number of simple moving averages (SMAs) of their choosing.\nSMAs are a common tool used by analysts and investors to smooth out short-term fluctuations in data and identify longer-term trends. By overlaying SMAs of different time periods on top of the original time series data, the ts_sma_plot() function makes it easy to compare and contrast different time periods and identify potential trends and patterns in the data.\nWith {healthyR.ts} and the ts_sma_plot() function, users can quickly and easily gain valuable insights into their time series data and make more informed decisions based on the trends and patterns they uncover.\nOk enough of that, let’s see the function."
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#data",
    "href": "posts/rtip-2022-12-15/index.html#data",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Data",
    "text": "Data\n\nout$data\n\n# A tibble: 288 × 5\n   index     date_col   value sma_order sma_value\n   <yearmon> <date>     <dbl> <fct>         <dbl>\n 1 Jan 1949  1949-01-01   112 3               NA \n 2 Feb 1949  1949-02-01   118 3              121.\n 3 Mar 1949  1949-03-01   132 3              126.\n 4 Apr 1949  1949-04-01   129 3              127.\n 5 May 1949  1949-05-01   121 3              128.\n 6 Jun 1949  1949-06-01   135 3              135.\n 7 Jul 1949  1949-07-01   148 3              144.\n 8 Aug 1949  1949-08-01   148 3              144 \n 9 Sep 1949  1949-09-01   136 3              134.\n10 Oct 1949  1949-10-01   119 3              120.\n# … with 278 more rows"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#plots",
    "href": "posts/rtip-2022-12-15/index.html#plots",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Plots",
    "text": "Plots\n\nout$plots$static_plot\n\nWarning: Removed 7 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nout$plots$interactive_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-19/index.html",
    "href": "posts/rtip-2022-12-19/index.html",
    "title": "Viewing Different Versions of the Same Statistical Distribution with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIn statistics, it is often useful to view different versions of the same statistical distribution. For example, when working with the normal distribution, it may be helpful to see how the distribution changes as the mean and standard deviation are varied.\nOne way to do this is by using the R library {TidyDensity}, which has a function called tidy_multi_single_dist(). This function allows a user to easily generate multiple versions of the same statistical distribution which can be plotted on the same graph, with each version representing a different combination of mean and standard deviation.\nTo use this function, the user simply needs to specify the distribution they want to plot (e.g. “normal”), the range of values for the mean and standard deviation, and the number of versions they want to plot. The function will then generate a plot showing the different versions of the distribution, with each version represented by a different color.\nThere are several reasons why it might be a good idea to view different versions of the same statistical distribution. For one, it can help the user understand how the shape of the distribution changes as the mean and standard deviation are varied. This can be particularly useful for distributions that have a wide range of possible values for the mean and standard deviation, such as the normal distribution.\nIn addition, viewing different versions of the same distribution can also help the user identify patterns and trends in the data. For example, the user may notice that the distribution becomes more spread out as the standard deviation increases, or that the distribution shifts to the left or right as the mean changes.\nOverall, the TidyDensity function tidy_multi_single_dist() is a useful tool for anyone interested in visualizing different versions of the same statistical distribution. Whether you are a student learning about statistics for the first time, or an experienced data scientist looking to better understand your data, this function can help you gain a deeper understanding of the underlying distribution and identify patterns and trends in your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_multi_single_dist(\n  .tidy_dist = NULL, \n  .param_list = list()\n  )\n\nNow let’s look at the arguments that go to the parameters.\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the {TidyDensity} ‘tidy_’ distribution function.\n\n\n\nExample\nLet’s run through an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <-tidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 200,\n    .mean = c(-1, 0, 1),\n    .sd = 1,\n    .num_sims = 3\n  )\n)\n\nNow that we have generated the data, let’s take a look and see if these different distributions have indeed been created.\n\ntidy_distribution_summary_tbl(tn, dist_name) |>\n  select(dist_name, mean_val, std_val)\n\n# A tibble: 3 × 3\n  dist_name         mean_val std_val\n  <fct>                <dbl>   <dbl>\n1 Gaussian c(-1, 1)  -1.03     0.988\n2 Gaussian c(0, 1)    0.0136   1.01 \n3 Gaussian c(1, 1)    0.990    1.02 \n\n\nLook’s good there, now let’s visualize.\n\ntn %>%\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-20/index.html",
    "href": "posts/rtip-2022-12-20/index.html",
    "title": "Random Walks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a type of stochastic process that can be used to model the movement of a particle or system over time. At each time step, the position of the particle is updated based on a random step drawn from a given probability distribution. This process can be represented as a sequence of independent and identically distributed (i.i.d.) random variables, and the resulting path traced by the particle is known as a random walk.\nRandom walks have a wide range of applications, including modeling the movement of stock prices, animal migration, and the spread of infectious diseases. They are also a fundamental concept in probability and statistics, and have been studied extensively in the literature.\nThe {TidyDensity} package provides a convenient way to generate and visualize random walks using the tidy_random_walk() function. This function takes a probability distribution as an argument, and generates a random walk by sampling from this distribution at each time step. For example, to generate a random walk with normally distributed steps, we can use the tidy_normal() function as follows:\n\nlibrary(TidyDensity)\n\n# Generate a random walk with normally distributed steps\nrw <- tidy_random_walk(tidy_normal())\n\nThe resulting object rw is a tibble with the typical tidy_ distribution columns and one augmented column called random_walk_value. The columns that are output are:\n\nsim_number The current simulation number from the tidy_ distribution\nx (You can think of this as time t)\ny The randomly generated value.\ndx & dy The density estimates of y at x\np & q The probability and quantile values of y\nrandom_walk_value The random walk value generated from tidy_random_walk() (You can think of this as the position of the particle at time t or the x)\n\nTo visualize the random walk, we can use the tidy_random_walk_autoplot() function, which creates a ggplot object showing the position of the particle at each time step. For example:\n\n# Visualize the random walk\ntidy_random_walk_autoplot(rw)\n\nThis will produce a plot showing the trajectory of the particle over time. You can customize the appearance of the plot by passing additional arguments to the tidy_random_walk_autoplot() function, such as the geom argument to specify the type of plot to use (e.g. geom = “line” for a line plot, or geom = “point” for a scatter plot).\nIn summary, the {TidyDensity} package provides a convenient and user-friendly interface for generating and visualizing random walks. With the tidy_random_walk() and tidy_random_walk_autoplot() functions, you can easily explore the behavior of random walks and their applications in a wide range of contexts.\nLet’s take a look at these functions.\n\n\nFunction\nFirstly we will look at the tidy_random_walk() function.\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\nHere are the arguments that get provided to the parameters of this function.\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\nNow let’s do the same with the tidy_random_walk_autoplot() function.\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\nHere are the arguments that get provided to the parameters.\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExamples\nLet’s go over some examples.\n\nlibrary(TidyDensity)\n\ndist_data <- tidy_normal(.sd = .1, .num_sims = 5)\n\ntidy_random_walk(.data = dist_data, .value_type = \"cum_sum\") %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nAnd another.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nNow let’s get an interactive one.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot(.interactive = TRUE)\n\n\n\n\n\nOne last example, let’s use a different distribution. Let’s use a cauchy distribution.\n\ntidy_cauchy(.num_sims = 9, .location = .5) %>%\n  tidy_random_walk(.value_type = \"cum_sum\", .sample = TRUE) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-21/index.html",
    "href": "posts/rtip-2022-12-21/index.html",
    "title": "Distribution Statistics with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re working with statistical distributions in R, you may be interested in the {TidyDensity} package. This package provides a set of functions for creating, manipulating, and visualizing probability distributions in a tidy format. One of these functions is tidy_chisquare(), which allows you to create a chi-square distribution with a specified number of degrees of freedom and a non-centrality parameter.\nOnce you’ve created a chi-square distribution using tidy_chisquare(), you may want to get some summary statistics about the distribution. This is where the util_chisquare_stats_tbl() function comes in handy. This function takes a chi-square distribution (created with tidy_chisquare()) as input and returns a tibble with several statistics about the distribution.\nSome of the statistics included in the table are:\n\nMean: The mean of the chi-square distribution, also known as the expected value.\nVariance: The variance of the chi-square distribution, which is a measure of how spread out the data is.\nSkewness: The skewness of the chi-square distribution, which is a measure of the symmetry of the data.\nKurtosis: The kurtosis of the chi-square distribution, which is a measure of the peakedness of the data.\n\nTo use the util_chisquare_stats_tbl() function, you’ll need to install and load the {TidyDensity} package first. Then, you can create a chi-square distribution using tidy_chisquare() and pass it to util_chisquare_stats_tbl() like this:\n\n# install and load TidyDensity\ninstall.packages(\"TidyDensity\")\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# create a chi-square distribution with 5 degrees of freedom\ndistribution <- tidy_chisquare(.df = 5)\n\n# get statistics about the distribution\nutil_chisquare_stats_tbl(distribution) |>\n  glimpse()\n\nThe output will be a table with the mean, variance, skewness, and kurtosis of the chi-square distribution. These statistics can be useful for understanding the characteristics of the distribution and making statistical inferences.\nOverall, the {TidyDensity} package is a useful tool for working with statistical distributions in R. The util_chisquare_stats_tbl() function is just one of many functions available in the package that can help you analyze and understand your data. Give it a try and see how it can help with your statistical analysis!\n\n\nFunction\nLet’s take a look at the full function call.\n\nutil_chisquare_stats_tbl(.data)\n\nLet’s take a look at the arguments that get supplied to the function parameters.\n\n.data - The data being passed from a tidy_ distribution function.\n\n\n\nExample\nNow for a full example with output.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntidy_chisquare() %>%\n  util_chisquare_stats_tbl() %>%\n  glimpse()\n\nRows: 1\nColumns: 17\n$ tidy_function     <chr> \"tidy_chisquare\"\n$ function_call     <chr> \"Chisquare c(1, 1)\"\n$ distribution      <chr> \"Chisquare\"\n$ distribution_type <chr> \"continuous\"\n$ points            <dbl> 50\n$ simulations       <dbl> 1\n$ mean              <dbl> 1\n$ median            <dbl> 0.3333333\n$ mode              <chr> \"undefined\"\n$ std_dv            <dbl> 1.414214\n$ coeff_var         <dbl> 1.414214\n$ skewness          <dbl> 2.828427\n$ kurtosis          <dbl> 15\n$ computed_std_skew <dbl> 1.132669\n$ computed_std_kurt <dbl> 3.894553\n$ ci_lo             <dbl> 0.002189912\n$ ci_hi             <dbl> 6.521727\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-22/index.html",
    "href": "posts/rtip-2022-12-22/index.html",
    "title": "Listing Functions and Parameters",
    "section": "",
    "text": "Introduction\nI got a little bored one day and decided I wanted to list out all of the functions inside of a package along with their parameters in a tibble. Not sure if this serves any particular purpose or not, I was just bored.\nThis does not work for packages that have data as an export like {healthyR} or {healthyR.data} but it will work for packages like {TidyDensity}.\nLet’s run through it\n\n\nExamples\nHere we go.\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:TidyDensity\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nbootstrap_density_augment\n.data\nbootstrap_density_augment(.data)\n\n\nbootstrap_p_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_p_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_p_vec\n.x\nbootstrap_p_vec(.x)\n\n\nbootstrap_q_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_q_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_q_vec\n.x\nbootstrap_q_vec(.x)\n\n\nbootstrap_stat_plot\nc(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\nbootstrap_stat_plot(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\n\n\nbootstrap_unnest_tbl\n.data\nbootstrap_unnest_tbl(.data)\n\n\ncgmean\n.x\ncgmean(.x)\n\n\nchmean\n.x\nchmean(.x)\n\n\nci_hi\nc(“.x”, “.na_rm”)\nci_hi(“.x”, “.na_rm”)\n\n\nci_lo\nc(“.x”, “.na_rm”)\nci_lo(“.x”, “.na_rm”)\n\n\nckurtosis\n.x\nckurtosis(.x)\n\n\ncmean\n.x\ncmean(.x)\n\n\ncmedian\n.x\ncmedian(.x)\n\n\ncolor_blind\nNULL\ncolor_blind(NULL)\n\n\ncsd\n.x\ncsd(.x)\n\n\ncskewness\n.x\ncskewness(.x)\n\n\ncvar\n.x\ncvar(.x)\n\n\ndist_type_extractor\n.x\ndist_type_extractor(.x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\ntd_scale_color_colorblind\nc(“…”, “theme”)\ntd_scale_color_colorblind(“…”, “theme”)\n\n\ntd_scale_fill_colorblind\nc(“…”, “theme”)\ntd_scale_fill_colorblind(“…”, “theme”)\n\n\ntidy_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_bernoulli\nc(“.n”, “.prob”, “.num_sims”)\ntidy_bernoulli(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_beta\nc(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\ntidy_beta(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\n\n\ntidy_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_bootstrap\nc(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\ntidy_bootstrap(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\n\n\ntidy_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_cauchy\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_cauchy(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_chisquare\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_chisquare(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_combine_distributions\n…\ntidy_combine_distributions(…)\n\n\ntidy_combined_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_combined_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_distribution_comparison\nc(“.x”, “.distribution_type”)\ntidy_distribution_comparison(“.x”, “.distribution_type”)\n\n\ntidy_distribution_summary_tbl\nc(“.data”, “…”)\ntidy_distribution_summary_tbl(“.data”, “…”)\n\n\ntidy_empirical\nc(“.x”, “.num_sims”, “.distribution_type”)\ntidy_empirical(“.x”, “.num_sims”, “.distribution_type”)\n\n\ntidy_exponential\nc(“.n”, “.rate”, “.num_sims”)\ntidy_exponential(“.n”, “.rate”, “.num_sims”)\n\n\ntidy_f\nc(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\ntidy_f(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\n\n\ntidy_four_autoplot\nc(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_four_autoplot(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_gamma\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_gamma(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_beta\nc(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_beta(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_pareto\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_pareto(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_hypergeometric\nc(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\ntidy_hypergeometric(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\n\n\ntidy_inverse_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_exponential\nc(“.n”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_exponential(“.n”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_gamma\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_gamma(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_normal\nc(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\ntidy_inverse_normal(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\n\n\ntidy_inverse_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_inverse_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_weibull\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_weibull(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_kurtosis_vec\n.x\ntidy_kurtosis_vec(.x)\n\n\ntidy_logistic\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_logistic(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_lognormal\nc(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\ntidy_lognormal(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\n\n\ntidy_mixture_density\n…\ntidy_mixture_density(…)\n\n\ntidy_multi_dist_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_multi_dist_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_multi_single_dist\nc(“.tidy_dist”, “.param_list”)\ntidy_multi_single_dist(“.tidy_dist”, “.param_list”)\n\n\ntidy_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_normal\nc(“.n”, “.mean”, “.sd”, “.num_sims”)\ntidy_normal(“.n”, “.mean”, “.sd”, “.num_sims”)\n\n\ntidy_paralogistic\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_paralogistic(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_pareto1\nc(“.n”, “.shape”, “.min”, “.num_sims”)\ntidy_pareto1(“.n”, “.shape”, “.min”, “.num_sims”)\n\n\ntidy_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\ntidy_random_walk\nc(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\ntidy_random_walk(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\n\n\ntidy_random_walk_autoplot\nc(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\ntidy_random_walk_autoplot(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\n\n\ntidy_range_statistic\n.x\ntidy_range_statistic(.x)\n\n\ntidy_scale_zero_one_vec\n.x\ntidy_scale_zero_one_vec(.x)\n\n\ntidy_skewness_vec\n.x\ntidy_skewness_vec(.x)\n\n\ntidy_stat_tbl\nc(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\ntidy_stat_tbl(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\n\n\ntidy_t\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_t(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_uniform\nc(“.n”, “.min”, “.max”, “.num_sims”)\ntidy_uniform(“.n”, “.min”, “.max”, “.num_sims”)\n\n\ntidy_weibull\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_weibull(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_zero_truncated_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_zero_truncated_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_zero_truncated_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\nutil_bernoulli_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_bernoulli_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_bernoulli_stats_tbl\n.data\nutil_bernoulli_stats_tbl(.data)\n\n\nutil_beta_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_beta_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_beta_stats_tbl\n.data\nutil_beta_stats_tbl(.data)\n\n\nutil_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_binomial_stats_tbl\n.data\nutil_binomial_stats_tbl(.data)\n\n\nutil_cauchy_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_cauchy_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_cauchy_stats_tbl\n.data\nutil_cauchy_stats_tbl(.data)\n\n\nutil_chisquare_stats_tbl\n.data\nutil_chisquare_stats_tbl(.data)\n\n\nutil_exponential_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_exponential_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_exponential_stats_tbl\n.data\nutil_exponential_stats_tbl(.data)\n\n\nutil_f_stats_tbl\n.data\nutil_f_stats_tbl(.data)\n\n\nutil_gamma_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_gamma_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_gamma_stats_tbl\n.data\nutil_gamma_stats_tbl(.data)\n\n\nutil_geometric_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_geometric_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_geometric_stats_tbl\n.data\nutil_geometric_stats_tbl(.data)\n\n\nutil_hypergeometric_param_estimate\nc(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\nutil_hypergeometric_param_estimate(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\n\n\nutil_hypergeometric_stats_tbl\n.data\nutil_hypergeometric_stats_tbl(.data)\n\n\nutil_logistic_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_logistic_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_logistic_stats_tbl\n.data\nutil_logistic_stats_tbl(.data)\n\n\nutil_lognormal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_lognormal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_lognormal_stats_tbl\n.data\nutil_lognormal_stats_tbl(.data)\n\n\nutil_negative_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_negative_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_negative_binomial_stats_tbl\n.data\nutil_negative_binomial_stats_tbl(.data)\n\n\nutil_normal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_normal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_normal_stats_tbl\n.data\nutil_normal_stats_tbl(.data)\n\n\nutil_pareto_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_pareto_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_pareto_stats_tbl\n.data\nutil_pareto_stats_tbl(.data)\n\n\nutil_poisson_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_poisson_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_poisson_stats_tbl\n.data\nutil_poisson_stats_tbl(.data)\n\n\nutil_t_stats_tbl\n.data\nutil_t_stats_tbl(.data)\n\n\nutil_uniform_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_uniform_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_uniform_stats_tbl\n.data\nutil_uniform_stats_tbl(.data)\n\n\nutil_weibull_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_weibull_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_weibull_stats_tbl\n.data\nutil_weibull_stats_tbl(.data)\n\n\n\n\n\nAnother example.\n\nlibrary(healthyverse)\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:healthyverse\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\nexpr\nexpr\nexpr(expr)\n\n\nhealthyverse_conflicts\nNULL\nhealthyverse_conflicts(NULL)\n\n\nhealthyverse_deps\nc(“recursive”, “repos”)\nhealthyverse_deps(“recursive”, “repos”)\n\n\nhealthyverse_packages\ninclude_self\nhealthyverse_packages(inlude_self)\n\n\nhealthyverse_sitrep\nNULL\nhealthyverse_sitrep(NULL)\n\n\nhealthyverse_update\nc(“recursive”, “repos”)\nhealthyverse_update(“recursive”, “repos”)\n\n\nsym\nx\nsym(x)\n\n\nsyms\nx\nsyms(x)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-29/index.html",
    "href": "posts/rtip-2022-12-29/index.html",
    "title": "Gartner Magic Chart and its usefulness in healthcare analytics with {healthyR}",
    "section": "",
    "text": "Introduction\nThe Gartner Magic Chart is a powerful tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. It was developed by Dr. James Gartner in the early 2000s as a way to visualize the relationship between two key metrics, for example: Excess Length of Stay (ELOS) and Excess Readmit Rate.\nIn healthcare, length of stay (LOS) refers to the amount of time a patient spends in the hospital. Excess LOS is the difference between the actual LOS of a patient and the expected LOS for that patient, based on their diagnosis and other factors. Excess readmit rate is the percentage of patients who are readmitted to the hospital within a certain time period after being discharged, above and beyond what is expected based on their diagnosis and other factors.\nThe Gartner Magic Chart can plot excess LOS on the x-axis and excess readmit rate on the y-axis. The resulting chart is divided into four quadrants, with the top right quadrant representing high excess LOS and high excess readmit rate, the bottom left quadrant representing low excess LOS and low excess readmit rate, and the other two quadrants representing intermediate values of these metrics.\nOne of the key benefits of the Gartner Magic Chart is that it allows healthcare professionals to quickly and easily identify areas of concern and opportunities for improvement. For example, if a hospital has a high excess LOS and a high excess readmit rate, it may be an indication that the hospital is not effectively managing patient care and is instead relying on costly and unnecessary readmissions to address problems that could have been avoided in the first place.\nThe Gartner Magic Chart can also be used to identify trends over time, allowing healthcare professionals to track progress and see the impact of changes they have made to patient care processes.\nIf you are interested in creating a Gartner Magic Chart for your own healthcare data, the R package {healthyR} has a convenient function called gartner_magic_chart_plt() that allows you to easily create this chart from data supplied by the end user. Simply input your excess LOS and excess readmit rate data, and the function will generate the chart for you.\nIn summary, the Gartner Magic Chart is a valuable tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. By using the gartner_magic_chart_plt() function from the {healthyR} package, you can easily create this chart for your own data and start using it to improve patient care and outcomes.\n\n\nFunction\nLet’s take a look at the full function call for gartner_magic_chart_plt().\n\ngartner_magic_chart_plt(\n  .data,\n  .x_col,\n  .y_col,\n  .point_size_col = NULL,\n  .y_lab,\n  .x_lab,\n  .plt_title,\n  .tl_lbl,\n  .tr_lbl,\n  .br_lbl,\n  .bl_lbl\n)\n\nNow let’s take a look at the arguments to the parameters.\n\n.data - The data set you want to plot\n.x_col - The x-axis for the plot\n.y_col - The y-axis for the plot\n.point_size_col - The default is NULL, if you want to size the dots by a column in the data.frame/tibble then enter the column name here.\n.y_lab - The y-axis label\n.x_lab - The x-axis label\n.plt_title - The title of the plot\n.tl_lbl - The top left label\n.tr_lbl - The top right label\n.br_lbl - The bottom right label\n.bl_lbl - The bottom left label\n\n\n\nExample\nLet’s see the function in action.\n\nlibrary(dplyr)\nlibrary(healthyR)\n\ndata_tbl <- tibble(\n    x = rnorm(100, 0, 1),\n    y = rnorm(100, 0, 1),\n    z = abs(x) + abs(y)\n )\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = z,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)\n\n\n\n\nExample two.\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = NULL,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)"
  },
  {
    "objectID": "posts/rtip-2023-01-03/index.html",
    "href": "posts/rtip-2023-01-03/index.html",
    "title": "Calendar Heatmap with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nCalendar heatmaps are a useful visualization tool for understanding patterns and trends in time series data. They are particularly useful for displaying daily data, as they allow for the visualization of multiple weeks or months at a time.\nThe ts_calendar_heatmap_plot() function from the R library {healthyR.ts} is a powerful tool for creating calendar heatmaps. This function takes in a time series object and creates a heatmap plot with daily data plotted on the calendar. The intensity of the color on each day corresponds to the value of the data for that day.\nOne application of calendar heatmaps is in understanding daily patterns in data such as website traffic or sales. For example, a business owner could use a calendar heatmap to identify trends in their daily sales data and make informed decisions about their operations. Similarly, a website owner could use a calendar heatmap to understand the daily traffic patterns on their site and optimize their content strategy accordingly.\nCalendar heatmaps are also useful for identifying anomalies or unusual patterns in time series data. For example, a calendar heatmap could be used to identify unexpected spikes or dips in daily sales data, or to identify unusual patterns in website traffic.\nIn addition to their practical applications, calendar heatmaps are also aesthetically pleasing and can be a fun way to visualize data. The ts_calendar_heatmap_plot() function allows for customization of the color palette and other visual options, making it easy to create visually appealing heatmaps.\nOverall, calendar heatmaps are a useful tool for understanding patterns and trends in daily time series data. The ts_calendar_heatmap_plot() function from the R library healthyR.ts is a powerful tool for creating calendar heatmaps and can be easily customized to suit the needs of the user.\n\n\nFunction\nLet’s take a look at the function.\n\nts_calendar_heatmap_plot(\n  .data,\n  .date_col,\n  .value_col,\n  .low = \"red\",\n  .high = \"green\",\n  .plt_title = \"\",\n  .interactive = TRUE\n)\n\nNow let’s see the arguments to the parameters.\n\n.data - The time-series data with a date column and value column.\n.date_col - The column that has the datetime values\n.value_col - The column that has the values\n.low - The color for the low value, must be quoted like “red”. The default is “red”\n.high - The color for the high value, must be quoted like “green”. The default is “green”\n.plt_title - The title of the plot\n.interactive - Default is TRUE to get an interactive plot using plotly::ggplotly(). It can be set to FALSE to get a ggplot plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\n\ndata_tbl <- data.frame(\n  date_col = seq.Date(\n    from = as.Date(\"2020-01-01\"),\n    to   = as.Date(\"2022-06-01\"),\n    length.out = 365*2 + 180\n    ),\n  value = rnorm(365*2+180, mean = 100)\n)\n\nts_calendar_heatmap_plot(\n  .data          = data_tbl\n  , .date_col    = date_col\n  , .value_col   = value\n  , .interactive = TRUE\n)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-05/index.html",
    "href": "posts/rtip-2023-01-05/index.html",
    "title": "More Randomwalks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a mathematical concept that have found various applications in fields such as economics, biology, and computer science. At a high level, a random walk refers to a process in which a set of objects move in a random direction at each time step. The path that the objects take over time forms a random walk.\nOne of the main uses of random walks is in modeling the behavior of stock prices. In the stock market, prices can be thought of as performing a random walk because they are influenced by a variety of unpredictable factors such as market trends, news events, and investor sentiment. By modeling stock prices as a random walk, it is possible to make predictions about future price movements and to understand the underlying factors that drive these movements.\nAnother application of random walks is in studying the movement patterns of animals. For example, biologists have used random walk models to understand the foraging behavior of ants and the migration patterns of animals such as birds and whales.\nOne interesting aspect of random walks is that they can be generated with different statistical distributions. For example, a random walk could be generated with a standard normal distribution (mean = 0, standard deviation = 1) or with a distribution that has a different mean and standard deviation. By looking at random walks with different distributional parameters, it is possible to understand how the underlying distribution affects the overall shape and pattern of the random walk.\nTo generate random walks with different distributional parameters, you can use the R package {TidyDensity}. This package provides functions for generating random walks and visualizing them using density plots. With {TidyDensity}, you can easily compare random walks with different mean and standard deviation values to see how these parameters affect the shape of the random walk.\nIn summary, random walks are a useful tool for modeling the behavior of various systems over time. They are particularly useful for understanding the movement patterns of stock prices and animals, and can be generated with different statistical distributions using the R package {TidyDensity}.\n\n\nFunctions\nThere are a couple of functions that we are going to use, below you will find them with their full function call and parameter arguments.\ntidy_multi_single_dist()\n\ntidy_multi_single_dist(.tidy_dist = NULL, .param_list = list())\n\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the TidyDensity tidy_ distribution function.\n\ntidy_random_walk()\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\ntidy_random_walk_autoplot()\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExample\n\nlibrary(TidyDensity)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 250,\n    .mean = 0,\n    .sd = c(.025, .05, .1, .15),\n    .num_sims = 25\n  )\n) %>%\n  tidy_random_walk(.initial_value = 1000, .value_type = \"cum_prod\") %>%\n  tidy_random_walk_autoplot() +\n  facet_wrap(~ dist_name, scales = \"free\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-09/index.html",
    "href": "posts/rtip-2023-01-09/index.html",
    "title": "New Release of {healthyR.ts}",
    "section": "",
    "text": "Introduction\nHello R users!\nI am excited to announce a new update to the {healthyR.ts} package: the ts_brownian_motion() function.\nThis function allows you to easily simulate brownian motion, also known as a Wiener process, using just a few parameters. You can specify the length of the simulation using the ‘.time’ parameter, the number of simulations to run using the ‘.num_sims’ parameter, the time step size (standard deviation) using the ‘.delta_time’ parameter, and the initial value (which is set to 0 by default) using the ‘.initial_value’ parameter.\nBut what is brownian motion, and why might you want to simulate it? Brownian motion is a random process that describes the movement of particles suspended in a fluid. It is named after the botanist Robert Brown, who observed the random movement of pollen grains suspended in water under a microscope in the 19th century.\nIn finance, brownian motion is often used to model the movement of stock prices over time. By simulating brownian motion, you can get a sense of how prices might fluctuate in the future, and use this information to inform your investment decisions.\nI hope that the ts_brownian_motion() function will be a useful tool for anyone interested in simulating brownian motion, whether for financial modeling or any other application. Give it a try and see what you can do with it!\nRight now the function is a bit slow at .num_sims > 500 so I am working on optimizing it. I will also later on be introducing the Geometric Brownian Motion to {healthyR.ts}\nAs always, we welcome feedback and suggestions for new features and improvements. Thank you for using the {healthyR.ts} package, and happy simulating!\n\n\nFunction\nHere is the full function call:\n\nts_brownian_motion(\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = 1,\n  .initial_value = 0\n)\n\n\n\nExample\nA simple example of the output.\n\nlibrary(healthyR.ts)\n\nts_brownian_motion()\n\n# A tibble: 1,010 × 3\n   sim_number     t     y\n   <fct>      <dbl> <dbl>\n 1 1              0  0   \n 2 1              1  1.46\n 3 1              2  2.68\n 4 1              3  2.78\n 5 1              4  3.07\n 6 1              5  3.43\n 7 1              6  3.05\n 8 1              7  4.43\n 9 1              8  6.04\n10 1              9  6.89\n# … with 1,000 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-10/index.html",
    "href": "posts/rtip-2023-01-10/index.html",
    "title": "Optimal Break Points for Histograms with {healthyR}",
    "section": "",
    "text": "Introduction\nHistogram binning is a technique used in data visualization to group continuous data into a set of discrete bins, or intervals. The purpose of histogram binning is to represent the distribution of a dataset in a graphical format, allowing for easy identification of patterns and outliers. However, there are several challenges that can arise when working with histogram binning.\nOne major challenge is determining the appropriate number of bins to use. If there are too few bins, the histogram may not accurately represent the underlying distribution of the data. On the other hand, if there are too many bins, the histogram may become cluttered and difficult to interpret. To overcome this challenge, there are several strategies that can be employed, such as the Scott’s normal reference rule, Freedman-Diaconis rule, or the Rice rule, to determine the optimal number of bins.\nAnother challenge with histogram binning is dealing with outliers. If a dataset has outliers, they can greatly skew the distribution of the data and make it difficult to interpret the histogram. One strategy to handle outliers is to use a log-scale on the x-axis, which can help to reduce their impact on the histogram. Alternatively, one could remove the outlier data points before creating the histogram.\nA further challenge is to choose the width of the bin that best represents the data. Too narrow bins might cause overfitting and too wide bins may cause loss of information. Different widths of bin can lead to a different representation of the data and hence a different conclusion. To overcome this, one could use the Freedman-Diaconis rule which take into consideration the range and the size of the sample to provide a robust and adaptive way to choose the width of the bin\nA simple solution to these challenges is the opt_bin() function in the {healthyR} library for R. This function uses an optimal binning algorithm to automatically determine the number of bins and bin widths that best represent the data. This can save a lot of time and effort when working with histograms and can help to ensure that the resulting histograms are accurate and easy to interpret.\nIn conclusion, histogram binning is a useful technique for visualizing the distribution of data, but it can be challenging to determine the appropriate number of bins and bin widths. Strategies such as Scott’s normal reference rule, Freedman-Diaconis rule, or the Rice rule can be used to determine the optimal number of bins. Outliers and bin width selection also can be challenges to take into account, and a function such as opt_bin() in the {healthyR} library can be used to overcome these challenges and create high-quality histograms with ease.\n\n\nFunction\nHere is the full call.\n\nopt_bin(.data, .value_col, .iters = 30)\n\nHere are the arguments that get passed to the parameters.\n\n.data - The data set in question\n.value_col - The column that holds the values\n.iters - How many times the cost function loop should run\n\nNow under I will provide some code and it’s explanation under the hood that exaplains how this works.\nHere is a breakdown of what each part of the code is doing:\n\nn <- 2:iters: this line creates a sequence of numbers starting at 2 and ending at the number specified by the variable “iters”\nc <- base::numeric(base::length(n)) and d <- c: These lines create two empty numeric vectors (arrays) called “c” and “d” with the same length as the sequence “n”\nfor (i in 1:length(n)) {...}: This is a for loop that iterates through each number in the sequence “n”\nd[i] <- diff(range( data ) ) / n[i]: Inside the loop, this line calculates the width of the bin for the current iteration by dividing the range of the data by the current number in the sequence “n”\nhp <- graphics::hist(data, breaks = edges, plot = FALSE) and ki <- hp$counts: This creates a histogram of the data using the current bin width and then gets the count of data points in each bin\nk <- mean(ki) and v <- sum((ki-k)^2/n[i]): this line uses the counts from the previous step to calculate the average count across all bins (k) and the variance of the counts across all bins (v)\nc[i] <- (2*k - v)/d[i]^2: this line calculates the cost function for the current bin width, which is based on k and v\nidx <- which.min(c) and opt_d <- d[idx]: this line finds the index in the “c” vector where the cost function is the lowest and stores that value in the variable “idx”\nedges <- seq(min(data), max(data), length = n[idx]) and edges <- tibble::as_tibble(edges): this line creates a new sequence of numbers representing the edges of the bins with the optimal bin width\nreturn(edges): this line returns the sequence of optimal bin edges that the function has determined\n\nOverall, this function will return an optimal set of bin edges for a histogram of the given data, the function uses an iterative process and consider the balance between the number of bins and the width of the bins to find the optimal width of the bins for the histogram. This could save a lot of time and effort for the data analysts as it can help to ensure that the resulting histograms are accurate and easy to interpret.\n\n\nExample\nLet’s look at some examples.\n\nlibrary(healthyR)\nlibrary(tidyverse)\n\ndf_tbl <- rnorm(n = 1000, mean = 0, sd = 1)\ndf_tbl <- df_tbl %>%\n  as_tibble()\n\nopt_bin(\n  .data = df_tbl,\n  .value_col = value\n  , .iters = 100\n)\n\n# A tibble: 6 × 1\n   value\n   <dbl>\n1 -3.04 \n2 -1.81 \n3 -0.585\n4  0.644\n5  1.87 \n6  3.10 \n\n\nNow lets user a smaller n to see how the output changes\n\nopt_bin(\n  .data = as_tibble(rnorm(n = 50)),\n  value,\n  100\n)\n\n# A tibble: 11 × 1\n     value\n     <dbl>\n 1 -1.69  \n 2 -1.24  \n 3 -0.797 \n 4 -0.351 \n 5  0.0944\n 6  0.540 \n 7  0.986 \n 8  1.43  \n 9  1.88  \n10  2.32  \n11  2.77  \n\n\nLet’s visualize.\n\nrn <- rnorm(50)\nhist(rn)\n\n\n\n\nNow let’s use opt_bin()\n\nhist(rn, breaks = opt_bin(as_tibble(rn), value) %>% pull())\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-12/index.html",
    "href": "posts/rtip-2023-01-12/index.html",
    "title": "An Update on {tidyAML}",
    "section": "",
    "text": "Introduction\nI have been doing a lot of work on a new package called {tidyAML}. {tidyAML} is a new R package that makes it easy to use the {tidymodels} ecosystem to perform automated machine learning (AutoML). This package provides a simple and intuitive interface that allows users to quickly generate machine learning models without worrying about the underlying details. It also includes a safety mechanism that ensures that the package will fail gracefully if any required extension packages are not installed on the user’s machine. With {tidyAML}, users can easily build high-quality machine learning models in just a few lines of code. Whether you are a beginner or an experienced machine learning practitioner, {tidyAML} has something to offer.\nSome ideas are that we should be able to generate regression models on the fly without having to actually go through the process of building the specification, especially if it is a non-tuning model, meaning we are not planing on tuning hyper-parameters like penalty and cost.\nThe idea is not to re-write the excellent work the {tidymodels} team has done (because it’s not possible) but rather to try and make an enhanced easy to use set of functions that do what they say and can generate many models and predictions at once.\nThis is similar to the great {h2o} package, but, {tidyAML} does not require java to be setup properly like {h2o} because {tidyAML} is built on {tidymodels}.\nThis package is not yet release, so you can only install from GitHub with the following:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/tidyAML\")\n\n\n\nExample\n\nlibrary(tidyAML)\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n11        11 stan_glmer      regression    linear_reg   <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 glm             regression    linear_reg   <spec[+]> \n3         3 glm             regression    poisson_reg  <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\",\"gee\"), \n                                 .parsnip_fns = \"linear_reg\")\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 gee             regression    linear_reg   <spec[+]> \n3         3 glm             regression    linear_reg   <spec[+]> \n\n\nAs shown we can easily select the models we want either by choosing the supported parsnip function like linear_reg() or by choose the desired engine, you can also use them both in conjunction with each other!\nNow, what if you want to create a non-tuning model spec without using the fast_regression_parsnip_spec_tbl() function. Well, you can. The function is called create_model_spec().\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow the reason we are here. Let’s take a look at the first function for modeling with tidyAML, fast_regression().\n\nlibrary(recipes)\nlibrary(dplyr)\nlibrary(purrr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(frt_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nNow lets take a look at a few different things in the frt_tbl.\n\nnames(frt_tbl)\n\n[1] \".model_id\"       \".parsnip_engine\" \".parsnip_mode\"   \".parsnip_fns\"   \n[5] \"model_spec\"      \"wflw\"            \"fitted_wflw\"     \"pred_wflw\"      \n\n\nLet’s look at a single model spec.\n\nfrt_tbl %>% slice(1) %>% select(model_spec) %>% pull() %>% pluck(1)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the wflw column.\n\nfrt_tbl %>% slice(1) %>% select(wflw) %>% pull() %>% pluck(1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe fitted wflw object.\n\nfrt_tbl %>% slice(1) %>% select(fitted_wflw) %>% pull() %>% pluck(1)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   11.77621      0.59296      0.01626     -0.03191     -0.55350     -5.30785  \n       qsec           vs           am         gear         carb  \n    0.97840      2.64023      1.68549      0.87059      0.58785  \n\nfrt_tbl %>% slice(1) %>% select(fitted_wflw) %>% pull() %>% pluck(1) %>%\n  broom::glance() %>%\n  glimpse()\n\nRows: 1\nColumns: 12\n$ r.squared     <dbl> 0.9085669\n$ adj.r.squared <dbl> 0.8382337\n$ sigma         <dbl> 2.337527\n$ statistic     <dbl> 12.91804\n$ p.value       <dbl> 3.367361e-05\n$ df            <dbl> 10\n$ logLik        <dbl> -47.07551\n$ AIC           <dbl> 118.151\n$ BIC           <dbl> 132.2877\n$ deviance      <dbl> 71.03241\n$ df.residual   <int> 13\n$ nobs          <int> 24\n\n\nAnd finally the predictions (this one I am probably going to change up).\n\nfrt_tbl %>% slice(1) %>% select(pred_wflw) %>% pull() %>% pluck(1)\n\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  17.4\n 2  28.4\n 3  17.2\n 4  10.7\n 5  13.4\n 6  17.0\n 7  22.8\n 8  14.3\n 9  22.4\n10  15.5\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-16/index.html",
    "href": "posts/rtip-2023-01-16/index.html",
    "title": "Auto K-Means with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nToday’s post is going to center around the automatic k-means functionality of {healthyR.ai}. I am not going to get into what it is or how it works, but rather the function call itself and how it works and what it puts out. The function is called hai_kmeans_automl. This function is a wrapper around the h2o::h2o.kmeans() function, but also does some processing to enhance the output at the end. Let’s get to it!\n\n\nFunction\nHere is the full function call.\n\nhai_kmeans_automl(\n  .data,\n  .split_ratio = 0.8,\n  .seed = 1234,\n  .centers = 10,\n  .standardize = TRUE,\n  .print_model_summary = TRUE,\n  .predictors,\n  .categorical_encoding = \"auto\",\n  .initialization_mode = \"Furthest\",\n  .max_iterations = 100\n)\n\nNow let’s go over the function arguments:\n\n.data - The data that is to be passed for clustering.\n.split_ratio - The ratio for training and testing splits.\n.seed - The default is 1234, but can be set to any integer.\n.centers - The default is 1. Specify the number of clusters (groups of data) in a data set.\n.standardize - The default is set to TRUE. When TRUE all numeric columns will be set to zero mean and unit variance.\n.print_model_summary - This is a boolean and controls if the model summary is printed to the console. The default is TRUE.\n.predictors - This must be in the form of c(“column_1”, “column_2”, … “column_n”)\n.categorical_encoding - Can be one of the following:\n\n“auto”\n“enum”\n“one_hot_explicit”\n“binary”\n“eigen”\n“label_encoder”\n“sort_by_response”\n“enum_limited”\n\n.initialization_mode - This can be one of the following:\n\n“Random”\n“Furthest (default)\n“PlusPlus”\n\n.max_iterations - The default is 100. This specifies the number of training iterations\n\n\n\nExamples\nTime for some examples.\n\nlibrary(healthyR.ai)\nlibrary(h2o)\n\nh2o.init()\n\noutput <- hai_kmeans_automl(\n  .data = iris,\n  .predictors = c(\n    \"Sepal.Width\", \"Sepal.Length\", \"Petal.Width\", \"Petal.Length\"\n    ),\n  .standardize = TRUE,\n  .split_ratio = .8\n)\n\nh2o.shutdown(prompt = FALSE)\n\nNow let’s take a look at the output. There are going to be 4 pieces of main output. Here they are:\n\ndata\nauto_kmeans_obj\nmodel_id\nscree_plt\n\nLet’s take a look at each one. First the data output which itself has 6 different objects in it.\n\noutput$data\n\n$splits\n$splits$training_tbl\n# A tibble: 123 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          5           3.6          1.4         0.2 setosa \n 5          5.4         3.9          1.7         0.4 setosa \n 6          4.6         3.4          1.4         0.3 setosa \n 7          5           3.4          1.5         0.2 setosa \n 8          4.4         2.9          1.4         0.2 setosa \n 9          5.4         3.7          1.5         0.2 setosa \n10          4.8         3.4          1.6         0.2 setosa \n# … with 113 more rows\n\n$splits$validate_tbl\n# A tibble: 27 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.6         3.1          1.5         0.2 setosa \n 2          4.9         3.1          1.5         0.1 setosa \n 3          5.8         4            1.2         0.2 setosa \n 4          5.1         3.5          1.4         0.3 setosa \n 5          5.7         3.8          1.7         0.3 setosa \n 6          5.1         3.8          1.5         0.3 setosa \n 7          5.4         3.4          1.7         0.2 setosa \n 8          5.1         3.7          1.5         0.4 setosa \n 9          5           3.4          1.6         0.4 setosa \n10          4.7         3.2          1.6         0.2 setosa \n# … with 17 more rows\n\n\n$metrics\n$metrics$training_metrics\n# A tibble: 2 × 3\n  centroid  size within_cluster_sum_of_squares\n     <dbl> <dbl>                         <dbl>\n1        1    87                         145. \n2        2    36                          38.3\n\n$metrics$validation_metrics\n# A tibble: 2 × 3\n  centroid  size within_cluster_sum_of_squares\n     <dbl> <dbl>                         <dbl>\n1        1    13                          35.7\n2        2    14                          13.4\n\n$metrics$cv_metric_summary\n# A tibble: 5 × 8\n  metric_name   mean    sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid cv_5_va…¹\n  <chr>        <dbl> <dbl>      <dbl>      <dbl>      <dbl>      <dbl>     <dbl>\n1 betweenss     62.5 12.4        66.7       72.7       71.6       42.5      59.1\n2 mse          NaN    0         NaN        NaN        NaN        NaN       NaN  \n3 rmse         NaN    0         NaN        NaN        NaN        NaN       NaN  \n4 tot_withinss  33.6  7.36       45.6       29.7       34.8       26.5      31.3\n5 totss         96.1 17.1       112.       102.       106.        69.0      90.3\n# … with abbreviated variable name ¹​cv_5_valid\n\n\n$original_data\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n[150 rows x 5 columns] \n\n$scree_data_tbl\n# A tibble: 2 × 2\n  centers   wss\n    <dbl> <dbl>\n1       1  745.\n2       2  226.\n\n$scoring_history_tbl\n# A tibble: 6 × 6\n  timestamp           duration     iterations number_of_clusters numbe…¹ withi…²\n  <chr>               <chr>             <dbl>              <dbl>   <dbl>   <dbl>\n1 2023-01-16 09:41:25 \" 0.241 sec\"          0                  0     NaN    NaN \n2 2023-01-16 09:41:25 \" 0.246 sec\"          1                  1     123   1003.\n3 2023-01-16 09:41:25 \" 0.247 sec\"          2                  1       0    488 \n4 2023-01-16 09:41:25 \" 0.250 sec\"          3                  2      26    311.\n5 2023-01-16 09:41:25 \" 0.251 sec\"          4                  2       2    184.\n6 2023-01-16 09:41:25 \" 0.251 sec\"          5                  2       0    183.\n# … with abbreviated variable names ¹​number_of_reassigned_observations,\n#   ²​within_cluster_sum_of_squares\n\n$model_summary_tbl\n# A tibble: 7 × 2\n  name                           value\n  <chr>                          <dbl>\n1 number_of_rows                  123 \n2 number_of_clusters                2 \n3 number_of_categorical_columns     0 \n4 number_of_iterations              5 \n5 within_cluster_sum_of_squares   183.\n6 total_sum_of_squares            488 \n7 between_cluster_sum_of_squares  305.\n\n\nNow lets take a look at the auto_kmeans_obj\n\noutput$auto_kmeans_obj\n\nModel Details:\n==============\n\nH2OClusteringModel: kmeans\nModel ID:  KMeans_model_R_1673880074548_1 \nModel Summary: \n  number_of_rows number_of_clusters number_of_categorical_columns\n1            123                  2                             0\n  number_of_iterations within_cluster_sum_of_squares total_sum_of_squares\n1                    5                     183.42511            488.00000\n  between_cluster_sum_of_squares\n1                      304.57489\n\n\nH2OClusteringMetrics: kmeans\n** Reported on training data. **\n\n\nTotal Within SS:  183.4251\nBetween SS:  304.5749\nTotal SS:  488 \nCentroid Statistics: \n  centroid     size within_cluster_sum_of_squares\n1        1 87.00000                     145.14706\n2        2 36.00000                      38.27805\n\nH2OClusteringMetrics: kmeans\n** Reported on validation data. **\n\n\nTotal Within SS:  49.05625\nBetween SS:  53.9303\nTotal SS:  102.9865 \nCentroid Statistics: \n  centroid     size within_cluster_sum_of_squares\n1        1 13.00000                      35.67618\n2        2 14.00000                      13.38007\n\nH2OClusteringMetrics: kmeans\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\n\nTotal Within SS:  167.8887\nBetween SS:  320.1113\nTotal SS:  488 \nCentroid statistics are not available.\nCross-Validation Metrics Summary: \n                  mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\nbetweenss    62.496490 12.417224  66.671760  72.679665  71.595474  42.470100\nmse                 NA  0.000000         NA         NA         NA         NA\nrmse                NA  0.000000         NA         NA         NA         NA\ntot_withinss 33.577747  7.357984  45.607327  29.705257  34.811970  26.512373\ntotss        96.074234 17.148642 112.279080 102.384926 106.407450  68.982475\n             cv_5_valid\nbetweenss     59.065456\nmse                  NA\nrmse                 NA\ntot_withinss  31.251806\ntotss         90.317260\n\n\nThe model id:\n\noutput$model_id\n\n[1] \"KMeans_model_R_1673880074548_1\"\n\n\nAnd finally the scree_plt.\n\noutput$scree_plt\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-17/index.html",
    "href": "posts/rtip-2023-01-17/index.html",
    "title": "Augmenting a Brownian Motion to a Time Series with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series analysis is a crucial tool for forecasting and understanding trends in various industries, including finance, economics, and engineering. However, traditional time series analysis methods can be limiting, and they may not always capture the complex dynamics of real-world data. That’s where the R package {healthyR.ts} comes in.\nThe {healthyR.ts} package is a powerful tool for time series analysis that offers a wide range of functions for cleaning, transforming, and analyzing time series data. One of its standout features is the ts_brownian_motion_augment() function, which allows you to add a brownian motion to a given time series dataset. This powerful tool can be used to simulate more realistic and complex scenarios, making it an invaluable tool for forecasters and data analysts.\nBrownian motion is a random walk process that can be used to model the movement of particles in a fluid. It has been widely used in mathematical finance, physics, and engineering to model the random movements of stock prices, pollutant concentrations, and other phenomena. By adding a brownian motion to a time series dataset, the ts_brownian_motion_augment() function allows users to capture the unpredictable and random nature of real-world data, making time series analysis more accurate and reliable.\nThe ts_brownian_motion_augment() function is easy to use and requires no prior knowledge of brownian motion or advanced mathematics. With just a few lines of code, users can quickly add a brownian motion to their time series dataset and begin analyzing the data with greater precision and confidence.\nThis set of functionality will be included in the next release which will be coming soon as it also speeds up the current ts_brownian_motion() function by 49x!\n\n\nFunction\nHere is the full function call.\n\nts_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = NULL\n)\n\nLet’s take a look at the arguments for the parameters.\n\n.data - The data.frame/tibble being augmented.\n.date_col - The column that holds the date.\n.value_col - The value that is going to get augmented. The last value of this column becomes the initial value internally.\n.time - How many time steps ahead.\n.num_sims - How many simulations should be run.\n.delta_time - Time step size.\n\n\n\nExample\nNow for an example.\n\nlibrary(tidyquant)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\ndf <- FANG %>%\n  filter(symbol == \"FB\") %>%\n  select(symbol, date, adjusted) %>%\n  filter_by_time(.date_var = date, .start_date = \"2016-01-01\") %>%\n  tq_mutate(select = adjusted, mutate_fun = periodReturn,\n            period = \"daily\", type = \"log\",\n            col_rename = \"daily_returns\")\n\nLet’s take a look at our initial data.\n\ndf\n\n# A tibble: 252 × 4\n   symbol date       adjusted daily_returns\n   <chr>  <date>        <dbl>         <dbl>\n 1 FB     2016-01-04    102.        0      \n 2 FB     2016-01-05    103.        0.00498\n 3 FB     2016-01-06    103.        0.00233\n 4 FB     2016-01-07     97.9      -0.0503 \n 5 FB     2016-01-08     97.3      -0.00604\n 6 FB     2016-01-11     97.5       0.00185\n 7 FB     2016-01-12     99.4       0.0189 \n 8 FB     2016-01-13     95.4      -0.0404 \n 9 FB     2016-01-14     98.4       0.0302 \n10 FB     2016-01-15     95.0      -0.0352 \n# … with 242 more rows\n\n\nNow let’s augment it with the brownian motion and see that data set before we visualize it.\n\ndf %>%\n  ts_brownian_motion_augment(\n    .date_col = date,\n    .num_sims = 50,\n    .value_col = daily_returns,\n    .delta_time = 0.00005\n  )\n\n# A tibble: 5,302 × 3\n   sim_number  date       daily_returns\n   <fct>       <date>             <dbl>\n 1 actual_data 2016-01-04       0      \n 2 actual_data 2016-01-05       0.00498\n 3 actual_data 2016-01-06       0.00233\n 4 actual_data 2016-01-07      -0.0503 \n 5 actual_data 2016-01-08      -0.00604\n 6 actual_data 2016-01-11       0.00185\n 7 actual_data 2016-01-12       0.0189 \n 8 actual_data 2016-01-13      -0.0404 \n 9 actual_data 2016-01-14       0.0302 \n10 actual_data 2016-01-15      -0.0352 \n# … with 5,292 more rows\n\n\nAs you see the function preserves the names of the input columns!\nNow, let’s see it!\n\ndf %>%\n  ts_brownian_motion_augment(\n    .date_col = date,\n    .num_sims = 50,\n    .value_col = daily_returns,\n    .delta_time = 0.00005\n  ) %>%\n  ggplot(aes(x = date, y = daily_returns\n             , group = sim_number, color = sim_number)) +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"FB Log Daily Returns for 2016\",\n    x = \"Date\",\n    y = \"Log Daily Returns\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-18/index.html",
    "href": "posts/rtip-2023-01-18/index.html",
    "title": "Geometric Brownian Motion with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nGeometric Brownian motion (GBM) is a widely used model in financial analysis for modeling the behavior of stock prices. It is a stochastic process that describes the evolution of a stock price over time, assuming that the stock price follows a random walk with a drift term and a volatility term.\nOne of the advantages of GBM is that it can capture the randomness and volatility of stock prices, which is a key feature of financial markets. GBM can also be used to estimate the expected return and volatility of a stock, which are important inputs for financial decision making.\nAnother advantage of GBM is that it can be used to generate simulations of future stock prices. These simulations can be used to estimate the probability of different outcomes, such as the probability of a stock price reaching a certain level in the future. This can be useful for risk management and for evaluating investment strategies.\nGBM is also very easy to implement, making it a popular choice among financial analysts and traders.\nThe equation for GBM is: \\[\ndS(t) = μS(t)dt + σS(t)dW(t)\n\\] Where:\n\\(dS(t)\\) is the change in the stock price at time \\(t\\)\n\\(S(t)\\) is the stock price at time \\(t\\)\n\\(μ\\) is the expected return of the stock\n\\(σ\\) is the volatility of the stock\n\\(dW(t)\\) is a Wiener process (a random variable that describes the rate of change of a random variable over time)\nIt’s important to keep in mind that GBM is a model and not always a perfect fit to real-world stock prices. However, it’s a widely accepted model due to its capability to captures the key characteristics of stock prices and its mathematical tractability.\nAttention R users! Are you looking for a reliable and accurate way to model stock prices? We have some exciting news for you! The next release of the R package {healthyR.ts} will include a new function, ts_geometric_brownian_motion(). This powerful function utilizes the geometric Brownian motion model to simulate stock prices, providing you with valuable insights and predictions for your financial analysis.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\nNow let’s go over the arguments to the parameters.\n\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.initial_value - Integer representing the initial value.\n.delta_time - Time step size.\n.return_tibble The default is TRUE. If set to FALSE then an object of class matrix will be returned.\n\n\n\nExample\nLet’s go over a few examples.\n\nlibrary(healthyR.ts)\n\nts_geometric_brownian_motion()\n\n# A tibble: 2,600 × 3\n   sim_number        t     y\n   <fct>         <int> <dbl>\n 1 sim_number 1      1   100\n 2 sim_number 2      1   100\n 3 sim_number 3      1   100\n 4 sim_number 4      1   100\n 5 sim_number 5      1   100\n 6 sim_number 6      1   100\n 7 sim_number 7      1   100\n 8 sim_number 8      1   100\n 9 sim_number 9      1   100\n10 sim_number 10     1   100\n# … with 2,590 more rows\n\n\nNow without returning a tibble object.\n\nts_geometric_brownian_motion(.num_sims = 5, .return_tibble = FALSE)\n\n      sim_number 1 sim_number 2 sim_number 3 sim_number 4 sim_number 5\n [1,]    100.00000     100.0000    100.00000    100.00000     100.0000\n [2,]    101.04170     100.6583    100.46420     99.68513     100.3776\n [3,]    101.58155     100.8959    100.03621     98.91656     101.5732\n [4,]    100.91680     100.7494     99.47735     98.57117     101.1525\n [5,]     99.96787     101.3298     98.70899     99.03101     101.1557\n [6,]     99.29069     101.4187     98.32176     98.33018     101.5584\n [7,]     99.40451     101.5124     98.26237     97.79356     101.4934\n [8,]     99.35345     101.0328     98.69587     97.46604     101.9630\n [9,]     97.94177     100.9534     98.32630     96.95231     102.1643\n[10,]     97.95812     101.3813     98.36934     96.64048     101.8546\n[11,]     98.47820     101.8262     98.21492     96.12851     102.5529\n[12,]     99.53016     102.5522     97.92270     95.97443     102.8912\n[13,]     98.82850     102.7482     96.66348     96.26008     103.1899\n[14,]     99.87335     102.9351     96.69635     96.15058     103.9259\n[15,]    101.03605     103.3796     96.60162     96.63562     103.3790\n[16,]    101.83475     103.1900     97.63875     96.00162     103.0422\n[17,]    102.10155     103.5851     97.12873     95.99579     103.0913\n[18,]    102.16085     103.2966     96.26772     95.95174     103.7034\n[19,]    102.35736     103.7429     96.37355     96.02805     102.8406\n[20,]    102.49297     104.5301     96.44318     96.28293     103.3507\n[21,]    102.36953     105.1809     96.87639     97.32625     104.0307\n[22,]    103.30672     104.7480     96.90017     97.16507     104.0751\n[23,]    103.55433     104.9848     97.40063     97.49375     102.6901\n[24,]    103.44429     104.3553     97.35982     97.39390     102.8163\n[25,]    103.23952     102.9840     97.30287     97.66737     103.2160\n[26,]    103.48365     103.6117     97.96290     97.91773     103.0579\nattr(,\".time\")\n[1] 25\nattr(,\".num_sims\")\n[1] 5\nattr(,\".mean\")\n[1] 0\nattr(,\".sigma\")\n[1] 0.1\nattr(,\".initial_value\")\n[1] 100\nattr(,\".delta_time\")\n[1] 0.002739726\nattr(,\".return_tibble\")\n[1] FALSE\n\n\nLet’s visualize the GBM at different levels of volatility.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ngbm <- rbind(\n  ts_geometric_brownian_motion(.sigma = 0.05) %>%\n    mutate(volatility = as.factor(\"A) Sigma = 5%\")),\n  ts_geometric_brownian_motion(.sigma = 0.1) %>%\n    mutate(volatility = as.factor(\"B) Sigma = 10%\")),\n  ts_geometric_brownian_motion(.sigma = .15) %>%\n    mutate(volatility = as.factor(\"C) Sigma = 15%\")),\n  ts_geometric_brownian_motion(.sigma = .2) %>%\n    mutate(volatility = as.factor(\"D) Sigma = 20%\"))\n)\n\ngbm %>%\n  ggplot(aes(x = t, y = y, group = sim_number, color = sim_number)) + \n  facet_wrap(~ volatility, scales = \"free\") +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-19/index.html",
    "href": "posts/rtip-2023-01-19/index.html",
    "title": "Boilerplate XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nXGBoost, short for “eXtreme Gradient Boosting,” is a powerful and popular machine learning library that is specifically designed for gradient boosting. It is an open-source library and is available in many programming languages, including R.\nGradient boosting is a technique that combines the predictions of multiple weak models to create a strong, more accurate model. XGBoost is an optimized version of gradient boosting that is designed to run faster and more efficiently than other implementations.\nLet’s take a look at a simple example of how to use XGBoost in R. We will use the iris dataset, a well-known dataset that contains 150 observations of iris flowers, each with four features (sepal length, sepal width, petal length, and petal width) and one target variable (the species of iris). Our goal is to train a model to predict the species of an iris flower based on its features.\nFirst, we need to install the “xgboost” package in R:\n\ninstall.packages(\"xgboost\")\n\nNext, we load the iris dataset and split it into training and test sets:\n\ndata(iris)\nset.seed(123)\nindices <- sample(1:nrow(iris), 0.8*nrow(iris))\ntrain_data <- iris[indices, 1:4]\ntrain_label <- iris[indices, 5]\ntest_data <- iris[-indices, 1:4]\ntest_label <- iris[-indices, 5]\n\nNow we can train our XGBoost model:\n\nlibrary(xgboost)\nxgb_model <- xgboost(\n  data = train_data, \n  label = train_label, \n  nrounds = 100, \n  objective = \"multi:softmax\", \n  num_class = 3\n  )\n\nHere, we specified the training data, labels, number of rounds (iterations) to run, the objective (multiclass classification) and the number of classes.\nFinally, we can use the trained model to make predictions on the test set:\n\npredictions <- predict(xgb_model, test_data)\n\nWe can also evaluate the performance of our model by comparing the predicted labels to the true labels using metrics such as accuracy:\n\naccuracy <- mean(predictions == test_label)\n\nIn this example, we used XGBoost to train a model to predict the species of iris flowers based on their features. We saw that XGBoost is a powerful and efficient library for gradient boosting, and it can be easily integrated into a R script.\nKeep in mind that this is a simple example, and in real-world scenarios, more preprocessing and parameter tuning is necessary to achieve optimal performance. Also, the dataset is small, and the number of rounds used is also small, which is not ideal for real-world scenarios. But this example shows the basic usage of XGBoost in R.\nOk, so, what’s the point? Is there a possibly easier way to do this…yes! You can use the boilerplace function hai_auto_xgboost() and it’s data prep helper hai_xgboost_data_prepper() from the {healthyR.ai} library. Let’s see how that works.\n\n\nFunction\nHere is the data prepper function and it’s arguments.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\nHere is the boilerplate function\n\nhai_auto_xgboost(\n  .data,\n  .rec_obj,\n  .splits_obj = NULL,\n  .rsamp_obj = NULL,\n  .tune = TRUE,\n  .grid_size = 10,\n  .num_cores = 1,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\"\n)\n\nHere are it’s arguments.\n\n.data - The data being passed to the function. The time-series object.\n.rec_obj - This is the recipe object you want to use. You can use hai_xgboost_data_prepper() an automatic recipe_object.\n.splits_obj - NULL is the default, when NULL then one will be created.\n.rsamp_obj - NULL is the default, when NULL then one will be created. It will default to creating an rsample::mc_cv() object.\n.tune - Default is TRUE, this will create a tuning grid and tuned workflow\n.grid_size - Default is 10\n.num_cores - Default is 1\n.best_metric - Default is “f_meas”. You can choose a metric depending on the model_type used. If regression then see hai_default_regression_metric_set(), if classification then see hai_default_classification_metric_set().\n.model_type - Default is classification, can also be regression.\n\n\n\nExample\nLet’s take a look at an example and it’s output. This is using {parsnip} under the hood.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_xgboost_data_prepper(data, Species ~ .)\n\nauto_xgb <- hai_auto_xgboost(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .num_cores = 1\n)\n\nThere are three main outputs to this function, which are:\n\nrecipe_info\nmodel_info\ntuned_info\n\nLet’s take a look at each. First the recipe_info\n\nauto_xgb$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nNow the model_info\n\nauto_xgb$model_info\n\n$model_spec\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune::tune()\n  min_n = tune::tune()\n  tree_depth = tune::tune()\n  learn_rate = tune::tune()\n  loss_reduction = tune::tune()\n  sample_size = tune::tune()\n\nComputational engine: xgboost \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune::tune()\n  min_n = tune::tune()\n  tree_depth = tune::tune()\n  learn_rate = tune::tune()\n  loss_reduction = tune::tune()\n  sample_size = tune::tune()\n\nComputational engine: xgboost \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 2.5 Mb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.10962507492329, max_depth = 13L, \n    gamma = 0.000498577409120534, colsample_bytree = 1, colsample_bynode = 1, \n    min_child_weight = 3L, subsample = 0.594320066112559), data = x$data, \n    nrounds = 1240L, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"multi:softprob\", num_class = 3L)\nparams (as set within xgb.train):\n  eta = \"0.10962507492329\", max_depth = \"13\", gamma = \"0.000498577409120534\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"3\", subsample = \"0.594320066112559\", nthread = \"1\", objective = \"multi:softprob\", num_class = \"3\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 4 \nniter: 1240\nnfeatures : 4 \nevaluation_log:\n    iter training_mlogloss\n       1        0.96929822\n       2        0.85785438\n---                       \n    1239        0.07815044\n    1240        0.07808817\n\n$was_tuned\n[1] \"tuned\"\n\nNow the tuned_info\n\nauto_xgb$tuned_info\n\n$tuning_grid\n# A tibble: 10 × 6\n   trees min_n tree_depth learn_rate loss_reduction sample_size\n   <int> <int>      <int>      <dbl>          <dbl>       <dbl>\n 1   926     6          2    0.0246        2.21e- 1       0.952\n 2  1510    25         14    0.00189       1.01e+ 1       0.424\n 3  1077    29          9    0.195         1.34e- 5       0.319\n 4   795    32          3    0.00102       1.64e- 3       0.686\n 5   368    22          4    0.00549       2.97e- 7       0.735\n 6  1240     3         13    0.110         4.99e- 4       0.594\n 7  1839    18          5    0.0501        1.67e- 7       0.273\n 8   139    11         10    0.0153        1.17e- 2       0.483\n 9   470    40          8    0.0906        6.79e-10       0.168\n10  1732    16         11    0.00667       9.19e- 9       0.883\n\n$cv_obj\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n$tuned_results\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics            .notes          \n   <list>          <chr>      <list>              <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 10]> <tibble [1 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 10]> <tibble [1 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 10]> <tibble [1 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 10]> <tibble [1 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 10]> <tibble [1 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 10]> <tibble [1 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 10]> <tibble [1 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 10]> <tibble [1 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 10]> <tibble [1 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 10]> <tibble [1 × 3]>\n# … with 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n$grid_size\n[1] 10\n\n$best_metric\n[1] \"f_meas\"\n\n$best_result_set\n# A tibble: 1 × 12\n  trees min_n tree_depth learn_rate loss_r…¹ sampl…² .metric .esti…³  mean     n\n  <int> <int>      <int>      <dbl>    <dbl>   <dbl> <chr>   <chr>   <dbl> <int>\n1  1240     3         13      0.110 0.000499   0.594 f_meas  macro   0.944    25\n# … with 2 more variables: std_err <dbl>, .config <chr>, and abbreviated\n#   variable names ¹​loss_reduction, ²​sample_size, ³​.estimator\n\n$tuning_grid_plot\n\n\n\n\nTuning Grid\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-23/index.html",
    "href": "posts/rtip-2023-01-23/index.html",
    "title": "ADF and Phillips-Perron Tests for Stationarity using lists",
    "section": "",
    "text": "Introduction\nA time series is a set of data points collected at regular intervals of time. Sometimes, the data points in a time series change over time in a predictable way. This is called a stationary time series. Other times, the data points change in an unpredictable way. This is called a non-stationary time series.\nImagine you are playing a game of catch with a friend. If you throw the ball back and forth at the same speed and distance, that’s like a stationary time series. But if you keep throwing the ball harder and farther, that’s like a non-stationary time series.\nThere are two tests that we can use to see if a time series is stationary or non-stationary. The first test is called the ADF test, which stands for Augmented Dickey-Fuller test. The second test is called the Phillips-Perron test.\nThe ADF test looks at the data points and checks to see if the average value of the data points is the same over time. If the average value is the same, then the time series is stationary. If the average value is not the same, then the time series is non-stationary.\nThe Phillips-Perron test is similar to the ADF test, but it is a bit more advanced. It checks to see if the data points are changing in a predictable way. If the data points are changing in a predictable way, then the time series is stationary. If the data points are changing in an unpredictable way, then the time series is non-stationary.\nSo, in short, The ADF test checks if the mean of the time series is constant over time and Phillips-Perron test checks if the variance of the time series is constant over time.\nNow, you can use these two tests to see if the time series you are studying is stationary or non-stationary, just like how you can use the game of catch to see if your throws are the same or different.\n\n\nFunction\nTo perform these test we can use two libraries, one is the {tseries} library for the adf.test() and the other is the {aTSA} for the pp.test()\nLet’s see some examples.\n\n\nExamples\nLet’s first make our time series obejcts and place them in a list.\n\nlibrary(tseries)\nlibrary(aTSA)\n\n# create time series objects\nts1 <- ts(rnorm(100), start = c(1990,1), frequency = 12)\nts2 <- ts(rnorm(100), start = c(1995,1), frequency = 12)\nts3 <- ts(rnorm(100), start = c(2000,1), frequency = 12)\n\n# create list of time series\nts_list <- list(ts1, ts2, ts3)\n\nNow let’s make our functions.\n\n# function to test for stationarity\nadf_is_stationary <- function(x) {\n  adf.test(x)$p.value > 0.05\n}\n\npp_is_stationary <- function(x) {\n  pp_df <- pp.test(x) |> as.data.frame() \n  pp_df$p.value > 0.05\n}\n\nTime to use lapply()!\n\n# apply function to each time series in list\nlapply(ts_list, adf_is_stationary)\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -10.97    0.01\n[2,]   1  -7.70    0.01\n[3,]   2  -5.23    0.01\n[4,]   3  -4.05    0.01\n[5,]   4  -4.03    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -11.48    0.01\n[2,]   1  -8.32    0.01\n[3,]   2  -5.81    0.01\n[4,]   3  -4.59    0.01\n[5,]   4  -4.63    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -11.42    0.01\n[2,]   1  -8.28    0.01\n[3,]   2  -5.77    0.01\n[4,]   3  -4.56    0.01\n[5,]   4  -4.59    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -10.60    0.01\n[2,]   1  -7.88    0.01\n[3,]   2  -5.96    0.01\n[4,]   3  -5.26    0.01\n[5,]   4  -4.90    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -10.64    0.01\n[2,]   1  -7.98    0.01\n[3,]   2  -6.08    0.01\n[4,]   3  -5.41    0.01\n[5,]   4  -5.08    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -10.58    0.01\n[2,]   1  -7.94    0.01\n[3,]   2  -6.06    0.01\n[4,]   3  -5.39    0.01\n[5,]   4  -5.07    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -9.19    0.01\n[2,]   1 -6.65    0.01\n[3,]   2 -5.41    0.01\n[4,]   3 -5.33    0.01\n[5,]   4 -4.77    0.01\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -9.14    0.01\n[2,]   1 -6.62    0.01\n[3,]   2 -5.39    0.01\n[4,]   3 -5.30    0.01\n[5,]   4 -4.75    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -9.11    0.01\n[2,]   1 -6.59    0.01\n[3,]   2 -5.36    0.01\n[4,]   3 -5.29    0.01\n[5,]   4 -4.73    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\n[[1]]\nlogical(0)\n\n[[2]]\nlogical(0)\n\n[[3]]\nlogical(0)\n\nlapply(ts_list, pp_is_stationary)\n\nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3  -111    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3  -110    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3  -110    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3  -101    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3  -101    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3  -101    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3 -92.9    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3 -92.9    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3   -93    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \n\n\n[[1]]\n[1] FALSE FALSE FALSE\n\n[[2]]\n[1] FALSE FALSE FALSE\n\n[[3]]\n[1] FALSE FALSE FALSE\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-24/index.html",
    "href": "posts/rtip-2023-01-24/index.html",
    "title": "Making Non Stationary Data Stationary",
    "section": "",
    "text": "Introduction\nIn the most basic sense for time series, a series is stationary if the properties of the generating process (the process that generates the data) do not change over time, the process remains constant. This does not mean the data does not change, it simply means the process does not change. You can bake a vanilla cake or a chocolate cake but you still cook it in the oven.\nA non-stationary time series is like a toy car that doesn’t run in a straight line. Sometimes it goes fast and sometimes it goes slow, so it’s hard to predict what it will do next. But, just like how you can fix a toy car by adjusting it, we can fix a non-stationary time series by making it “stationary.”\nOne way we can do this is by taking the difference in the time series vector. This is like taking the toy car apart and looking at how each piece moves. By subtracting one piece from another, we can see if they are moving at the same speed or not. If they are not, we can adjust them so they are moving at the same speed. This makes it easier to predict what the toy car will do next because it’s moving at a steady pace.\nAnother way we can make a non-stationary time series stationary is by taking the second difference of the log of the data. This is like looking at the toy car from a different angle. By taking the log of the data, we can see how much each piece has changed over time. Then, by taking the second difference, we can see if the changes are happening at the same rate or not. If they are not, we can adjust them so they are happening at the same rate.\nIn simple terms, these methods help to stabilize the time series by making the data move at a consistent speed, which allows for better predictions.\nIn summary, a non-stationary time series is like a toy car that doesn’t run in a straight line. By taking the difference in the time series vector or taking the second difference of the log of the data, we can fix the toy car and make it run in a straight line. This is helpful for making accurate predictions.\n\n\nFunction\nWe are going to use the adf.test() function from the {aTSA} library. Here is the function:\n\nadf.test(x, nlag = NULL, output = TRUE)\n\nHere are the arugments to the parameters.\n\nx- a numeric vector or time series.\nalternative - the lag order with default to calculate the test statistic. See details for the default.\noutput - a logical value indicating to print the test results in R console. The default is TRUE.\n\n\n\nExamples\nAs an example, we are going to use the R built in data set AirPassengers as our timeseries. This data is both cyclical and trending so it is good for this purpose.\n\nlibrary(aTSA)\n\nplot(AirPassengers)\n\n\n\n\nNow that we know what it looks like, lets see if it is stationary right off the bat.\n\nadf.test(AirPassengers)\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag      ADF p.value\n[1,]   0  0.04712   0.657\n[2,]   1 -0.35240   0.542\n[3,]   2 -0.00582   0.641\n[4,]   3  0.26034   0.718\n[5,]   4  0.82238   0.879\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -1.748   0.427\n[2,]   1 -2.345   0.194\n[3,]   2 -1.811   0.402\n[4,]   3 -1.536   0.509\n[5,]   4 -0.986   0.701\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -4.64    0.01\n[2,]   1 -7.65    0.01\n[3,]   2 -7.09    0.01\n[4,]   3 -6.94    0.01\n[5,]   4 -5.95    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nSo we can see that right off the bat that “Type 1” and “Type 2” fail as there is significant trend in this data as we can plainly see. Let’s see what happens when we take a simpmle diff() of the series.\n\nplot(diff(AirPassengers))\n\n\n\n\nLooking like its still going to fail, but let’s run the test anyways.\n\nadf.test(diff(AirPassengers))\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -8.58    0.01\n[2,]   1 -8.68    0.01\n[3,]   2 -8.13    0.01\n[4,]   3 -8.48    0.01\n[5,]   4 -6.59    0.01\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -8.58    0.01\n[2,]   1 -8.69    0.01\n[3,]   2 -8.17    0.01\n[4,]   3 -8.60    0.01\n[5,]   4 -6.70    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -8.55    0.01\n[2,]   1 -8.66    0.01\n[3,]   2 -8.14    0.01\n[4,]   3 -8.57    0.01\n[5,]   4 -6.69    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nThe adf.test comes back with a p.value <= 0.01 as the data is no longer presenting a trend, but as we can plainly see, the data has non constant variance overtime which we know we need. Here we will use the {TidyDensity} package to use the cvar() (cumulative variance) function to see the ongoing variance.\n\nlibrary(TidyDensity)\n\nplot(cvar(diff(AirPassengers)), type = \"l\")\n\n\n\n\nReject the null that the data is stationary. So lets proceed with a diff diff log of the data and see what we get. First let’s visualize.\n\nplot(diff(diff(log(AirPassengers))))\n\n\n\nplot(cvar(diff(diff(log(AirPassengers)))), type = \"l\")\n\n\n\n\nLooking good!\n\nadf.test(diff(diff(log(AirPassengers))))\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -15.90    0.01\n[2,]   1 -12.78    0.01\n[3,]   2  -9.28    0.01\n[4,]   3 -10.76    0.01\n[5,]   4  -9.72    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -15.85    0.01\n[2,]   1 -12.73    0.01\n[3,]   2  -9.24    0.01\n[4,]   3 -10.73    0.01\n[5,]   4  -9.68    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -15.79    0.01\n[2,]   1 -12.68    0.01\n[3,]   2  -9.21    0.01\n[4,]   3 -10.68    0.01\n[5,]   4  -9.64    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nVoila!\n\n\nReferences\n\nhttps://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322\nhttps://www.statology.org/dickey-fuller-test-in-r/"
  },
  {
    "objectID": "posts/rtip-2023-01-25/index.html",
    "href": "posts/rtip-2023-01-25/index.html",
    "title": "Simplifying List Filtering in R with purrr’s keep()",
    "section": "",
    "text": "Introduction\nThe {purrr} package in R is a powerful tool for working with lists and other data structures. One particularly useful function in the package is keep(), which allows you to filter a list by keeping only the elements that meet certain conditions.\nThe keep() function takes two arguments: the list to filter, and a function that returns a logical value indicating whether each element of the list should be kept. The function can be specified as an anonymous function or a named function, and it should take a single argument (the current element of the list).\nFor example, let’s say we have a list of numbers and we want to keep only the even numbers. We could use the keep() function with an anonymous function that checks the remainder of the current element divided by 2:\n\nlibrary(purrr)\n\nnumbers <- c(1, 2, 3, 4, 5, 6)\neven_numbers <- keep(numbers, function(x) x %% 2 == 0)\neven_numbers\n\n[1] 2 4 6\n\n\nWe see that this keeps [1] 2 4 6.\nThe purrr package also provides a convenient shorthand for this operation, .p, which can be used inside the keep function to return the element.\n\neven_numbers <- keep(numbers, ~ .x %% 2 == 0)\neven_numbers\n\n[1] 2 4 6\n\n\nYou can also use the keep() function to filter a list of other types of objects, such as strings or lists. For example, you could use it to keep only the strings that are longer than a certain length:\n\nwords <- c(\"cat\", \"dog\", \"elephant\", \"bird\")\nlong_words <- keep(words, function(x) nchar(x) > 4)\nlong_words\n\n[1] \"elephant\"\n\n\nWe see that this keeps “elephant” & “bird”.\nIn summary, the {purrr} package’s keep() function is a powerful tool for filtering lists in R, and the .p parameter can be used as a shorthand. It can be used to keep only the items in a list that meet a user-given condition, and it can be used with a variety of data types.\n\n\nFunction\nHere is the keep() function and it’s parameters.\n\nkeep(.x, .p, ...)\n\nHere are the arguments to the parameters.\n\n.x - A list or vector.\n.p - A predicate function (i.e. a function that returns either TRUE or FALSE) specified in one of the following ways:\n\nA named function, e.g. is.character.\nAn anonymous function, e.g. \\(x) all(x < 0) or function(x) all(x < 0).\nA formula, e.g. ~ all(.x < 0). You must use .x to refer to the first argument). Only recommended if you require backward compatibility with older versions of R.\n\n... - Additional arguments passed on to .p.\n\n\n\nExamples\nI recently came across wanting to filter a list that is given as an argument to a parameter. The function I am working for my upcoming {tidyAML} package has a function called create_workflow_set() that has a parameter .recipe_list which is set to list(). The user must only place recipes in this list or else I want it to fail. So I was able to write a quick check using keep() like so:\n\n# Checks ----\n# only keep() recipes\nrec_list <- purrr::keep(rec_list, ~ inherits(.x, \"recipe\"))\n\nVoila!"
  },
  {
    "objectID": "posts/tidydensity-20221007/index.html",
    "href": "posts/tidydensity-20221007/index.html",
    "title": "TidyDensity Primer",
    "section": "",
    "text": "This is going to serve as a sort of primer for the {TidyDensity} package.\nThe goal of {TidyDensity} is to make working with random numbers from different distributions easy. All tidy_ distribution functions provide the following components:\n\n[r_]\n[d_]\n[q_]\n[p_]\n\n\nInstallation\nYou can install the released version of {TidyDensity} from CRAN with:\ninstall.packages(\"TidyDensity\")\nAnd the development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/TidyDensity\")\n\n\nExample Data\nThis is a basic example which shows you how to solve a common problem, which is, how do we generate randomly generated data from a normal distribution of some mean, and some standard deviation with n points and sims number of simulations?\nWith the function tidy_normal() we can generate such data. All functions that are condsidered tidy_ distribution functions, meaning those that generate randomly generated data from some distribution, have the same API call structure.\nFor example, using tidy_normal() the full function call at it’s default is as follows:\ntidy_normal(.n = 50, .mean = 0, .sd = 1, .num_sims = 1).\nWhat this means is that we want to generate 50 points from a standard normal distribution of mean 0 and with a standard deviation of 1, and we want to generate a single simulation of this data.\nLet’s see an example below:\n\nsuppressPackageStartupMessages(library(TidyDensity))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(ggplot2))\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nset.seed(123)\ntidy_normal()\n\n# A tibble: 50 × 7\n   sim_number     x       y    dx       dy     p       q\n   <fct>      <int>   <dbl> <dbl>    <dbl> <dbl>   <dbl>\n 1 1              1 -0.560  -3.11 0.000256 0.288 -0.560 \n 2 1              2 -0.230  -2.98 0.000691 0.409 -0.230 \n 3 1              3  1.56   -2.85 0.00167  0.940  1.56  \n 4 1              4  0.0705 -2.72 0.00362  0.528  0.0705\n 5 1              5  0.129  -2.59 0.00707  0.551  0.129 \n 6 1              6  1.72   -2.45 0.0125   0.957  1.72  \n 7 1              7  0.461  -2.32 0.0201   0.678  0.461 \n 8 1              8 -1.27   -2.19 0.0298   0.103 -1.27  \n 9 1              9 -0.687  -2.06 0.0415   0.246 -0.687 \n10 1             10 -0.446  -1.93 0.0552   0.328 -0.446 \n# … with 40 more rows\n\n\nWhat comes back we see is a tibble. This is true for all functions in the {TidyDensity} library. It was a goal to return items that are consistent with the tidyverse.\nNow let’s talk a bit about what was actually returned. There are a few columns that are returned, these are referred to as the r, d, p, and q\n\n[r_] Shows as y and is the randomly generated data from the underlying distribution.\n[d_] Two components come back, dx and dy where these are generated from the [stats::density()] function with n set to .n from the function input.\n[p_] Shows as p and is the results of the p_ function, in this case pnorm() where the x of the input goes from 0-1 with .n points.\n[q_] Shows as q and is the results of the q_ function, in this case qnorm() where the x of the input goes from 0-1 with .n points.\n\nNow you will also see two more columns, namely, sim_number a factor column and x an integer column. The sim_number column represents the current simulation for which data was drawn, and the x represents the nth point in that simulation.\n\n\nVisualization Example\nWith data typically comes the need to see it! Show me the data! TidyDensity has a variety of autoplot functionality that will present only data from a tidy_ distribution function. We will take a look at output from tidy_normal() and set a see otherwise everytime this site is rendered the data would change.\n\nset.seed(123)\ntn <- tidy_normal(.n = 100, .num_sims = 6)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe can see that the plots are faily informative. There are the regular density plot, the quantile plot, probability and qq plots. The title and subtitle of these plots are generated from attributes that are attached to the output of the tidy_ distribution function. Let’s take a look at the attributes of tn\n\nattributes(tn)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n[235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n[253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n[271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n[289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n[307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n[325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n[343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n[361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n[379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n[397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n[415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n[433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n[451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n[469] 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n[487] 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n[505] 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n[523] 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n[541] 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n[559] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n[577] 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n[595] 595 596 597 598 599 600\n\n$names\n[1] \"sim_number\" \"x\"          \"y\"          \"dx\"         \"dy\"        \n[6] \"p\"          \"q\"         \n\n$distribution_family_type\n[1] \"continuous\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 1\n\n$.n\n[1] 100\n\n$.num_sims\n[1] 6\n\n$tibble_type\n[1] \"tidy_gaussian\"\n\n$ps\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$qs\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$param_grid\n# A tibble: 1 × 2\n  .mean   .sd\n  <dbl> <dbl>\n1     0     1\n\n$param_grid_txt\n[1] \"c(0, 1)\"\n\n$dist_with_params\n[1] \"Gaussian c(0, 1)\"\n\n\nI won’t go over them but as you can see, the attribute list can get long and has a lot of great information in it.\nNow what if we have simulations over 9? The legend would get fairly large making the visualization difficult to read.\nLet’s take a look at 20 simulations.\n\ntn <- tidy_normal(.n = 100, .num_sims = 20)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe see that the legend disappears! That’s great, but what if we still want to see what simulation is what? Well, make the plot interactive!\n\ntidy_autoplot(tn, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-2022-12-23/index.html",
    "href": "posts/weekly-rtip-2022-12-23/index.html",
    "title": "Simulating Time Series Model Forecasts with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series models are a powerful tool for forecasting future values of a time-dependent variable. These models are commonly used in a variety of fields, including finance, economics, and engineering, to predict future outcomes based on past data.\nOne important aspect of time series modeling is the ability to simulate model forecasts. This allows us to evaluate the performance of different forecasting methods and to compare the results of different models. Simulating forecasts also allows us to assess the uncertainty associated with our predictions, which can be especially useful when making important decisions based on the forecast.\nThere are several benefits to simulating time series model forecasts:\n\nImproved accuracy: By simulating forecasts, we can identify the best forecasting method for a given time series and optimize its parameters. This can lead to more accurate forecasts, especially for long-term predictions.\nEnhanced understanding: Simulating forecasts helps us to understand how different factors, such as seasonality and trend, affect the prediction. This understanding can inform our decision-making and allow us to make more informed predictions.\nImproved communication: Simulating forecasts allows us to present the uncertainty associated with our predictions, which can be useful for communicating the potential risks and benefits of different courses of action.\n\nThe R package {healthyR.ts} includes a function called ts_forecast_simulator() that can be used to simulate time series model forecasts. This function allows users to specify the forecasting method, the number of simulations to run, and the length of the forecast horizon. It also provides options for visualizing the results, including plots of the forecast distribution and summary statistics such as the mean and standard deviation of the forecasts.\nIn summary, simulating time series model forecasts is a valuable tool for improving the accuracy and understanding of our predictions, as well as for communicating the uncertainty associated with these forecasts. The ts_forecast_simulator() function in the {healthyR.ts} package is a useful tool for performing these simulations in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_forecast_simulator(\n  .model,\n  .data,\n  .ext_reg = NULL,\n  .frequency = NULL,\n  .bootstrap = TRUE,\n  .horizon = 4,\n  .iterations = 25,\n  .sim_color = \"steelblue\",\n  .alpha = 0.05\n)\n\nNow let’s take a look at the arguments that get provided to the parameters.\n\n.model - A forecasting model of one of the following from the forecast package:\n\nArima\nauto.arima\nets\nnnetar\nArima() with xreg\n\n.data - The data that is used for the .model parameter. This is used with timetk::tk_index()\n.ext_reg - A tibble or matrix of future xregs that should be the same length as the horizon you want to forecast.\n.frequency - This is for the conversion of an internal table and should match the time frequency of the data.\n.bootstrap - A boolean value of TRUE/FALSE. From forecast::simulate.Arima() Do simulation using resampled errors rather than normally distributed errors.\n.horizon - An integer defining the forecast horizon.\n.iterations - An integer, set the number of iterations of the simulation.\n.sim_color - Set the color of the simulation paths lines.\n.alpha - Set the opacity level of the simulation path lines.\n\nGreat, now let’s take a look at examples.\n\n\nExamples\n\nlibrary(healthyR.ts)\nlibrary(forecast)\n\nfit <- auto.arima(AirPassengers)\ndata_tbl <- ts_to_tbl(AirPassengers)\n\n# Simulate 50 possible forecast paths, with .horizon of 12 months\noutput <- ts_forecast_simulator(\n  .model        = fit\n  , .horizon    = 12\n  , .iterations = 50\n  , .data       = data_tbl\n)\n\nOk, so now we have our output object, which is a list object. Let’s see what it contains.\n\n\n\nForecast Simulation Output\n\n\nNow, let’s explore each element.\nFirst the forecast simulation data.\n\noutput$forecast_sim\n\n            sim_1    sim_2    sim_3    sim_4    sim_5    sim_6    sim_7\nJan 1961 445.7399 434.7175 462.6173 447.8849 453.5069 443.0944 464.4749\nFeb 1961 424.1606 423.3814 431.9512 420.4460 426.8245 423.4784 443.1545\nMar 1961 444.0015 467.6276 459.3452 450.2936 453.6116 461.3751 462.4351\nApr 1961 490.2370 504.9129 492.0632 491.2198 494.6883 489.6674 498.7902\nMay 1961 502.0907 517.7794 504.5420 503.7614 504.8544 504.1816 521.9044\nJun 1961 552.6359 588.4650 560.1199 567.5266 563.5353 552.8411 543.5723\nJul 1961 655.1872 666.8450 642.9804 653.4262 646.1559 648.1923 639.6587\nAug 1961 633.4657 635.5080 644.1405 650.1333 631.2688 630.4410 623.3784\nSep 1961 536.3259 551.6068 548.9176 554.8289 533.3672 553.5334 529.2182\nOct 1961 486.2815 513.2851 514.8842 513.2190 481.4636 495.1295 490.7402\nNov 1961 406.9061 428.0550 437.2211 435.3443 426.4892 425.9885 416.1940\nDec 1961 454.1048 478.1804 477.3923 475.0052 504.0761 486.3584 472.2933\n            sim_8    sim_9   sim_10   sim_11   sim_12   sim_13   sim_14\nJan 1961 444.3152 444.3203 453.5722 438.0253 438.0253 425.2956 458.6829\nFeb 1961 418.1653 418.2569 423.9229 413.1814 404.0688 403.3318 445.9250\nMar 1961 450.8166 437.2694 452.5463 433.7963 450.5365 433.6445 448.2279\nApr 1961 489.4801 501.5545 504.4428 488.2243 503.9232 476.7711 495.1634\nMay 1961 499.7523 499.2059 508.6748 496.3915 520.8837 489.1480 502.1013\nJun 1961 562.7791 574.2660 571.6261 560.3126 578.8797 564.5855 581.8097\nJul 1961 653.2719 655.1924 655.0359 647.2704 664.1505 660.7023 654.6549\nAug 1961 649.6533 639.4611 655.1187 610.5014 661.2034 606.4399 654.0751\nSep 1961 542.5110 548.6498 560.1903 505.3303 552.4413 515.2890 547.9581\nOct 1961 505.5032 490.4686 495.1026 464.1546 503.7735 479.0465 501.3405\nNov 1961 423.0761 414.9520 445.9182 399.1634 435.6692 405.7617 447.0409\nDec 1961 454.4311 452.3740 473.5319 435.0425 468.3699 449.6422 474.2666\n           sim_15   sim_16   sim_17   sim_18   sim_19   sim_20   sim_21\nJan 1961 455.4787 451.0585 459.4839 444.3242 435.1001 443.7523 444.9274\nFeb 1961 439.4881 422.0447 442.7488 427.5273 412.6110 430.0844 418.5412\nMar 1961 473.0626 449.9922 479.0381 454.4156 443.6824 454.0246 455.8716\nApr 1961 497.0203 501.5911 517.4542 495.3593 476.1640 489.3426 492.7721\nMay 1961 515.2215 526.5615 506.5715 487.1920 504.6323 500.4116 506.1908\nJun 1961 576.5544 581.4492 560.9466 560.1496 557.4915 551.0780 580.6906\nJul 1961 662.4222 675.8571 643.9878 645.8877 634.9354 649.0339 655.5332\nAug 1961 648.2923 653.8785 639.1892 624.4315 643.3871 630.6072 638.5270\nSep 1961 548.8370 547.7029 548.0178 529.7305 552.4617 533.3734 541.2910\nOct 1961 500.8130 500.6410 509.7552 481.4202 500.3374 484.8628 513.4386\nNov 1961 448.3088 427.7597 434.1320 424.8617 468.9838 421.0139 460.0366\nDec 1961 479.2161 468.6392 473.0734 460.6548 492.5949 445.3285 483.7701\n           sim_22   sim_23   sim_24   sim_25   sim_26   sim_27   sim_28\nJan 1961 431.9061 436.9803 445.6030 470.3241 438.1303 438.0355 442.8814\nFeb 1961 405.5096 406.4255 431.2780 451.1454 438.4957 414.3992 406.9653\nMar 1961 437.8384 437.3900 455.2998 472.2864 457.3227 452.7172 439.0830\nApr 1961 480.2212 479.1317 491.4019 520.8334 492.1962 497.7494 495.6963\nMay 1961 507.5101 491.4699 502.3217 522.6236 504.1139 498.8398 490.4093\nJun 1961 565.3556 555.2957 562.9435 569.6062 575.3107 565.8427 558.4052\nJul 1961 631.8589 643.0705 649.3241 656.5773 699.1111 660.5835 656.7441\nAug 1961 636.9621 620.3416 635.3089 658.5513 666.7588 655.0455 634.5556\nSep 1961 533.4397 546.0061 537.0957 552.6849 578.5525 563.2023 557.6656\nOct 1961 493.0531 483.6816 489.7577 517.3445 535.8427 509.2783 506.7000\nNov 1961 421.6430 429.6623 419.7854 427.1834 456.2713 429.0018 434.0667\nDec 1961 452.2728 457.5337 460.9375 470.2428 514.2062 482.2928 490.3966\n           sim_29   sim_30   sim_31   sim_32   sim_33   sim_34   sim_35\nJan 1961 455.3775 444.3272 461.2236 433.8962 464.4749 438.0355 439.1887\nFeb 1961 426.9441 418.2812 447.4852 424.0317 409.7277 415.1395 433.4188\nMar 1961 457.7136 440.5773 468.7090 447.5754 438.9636 443.0306 420.3961\nApr 1961 505.5824 483.8879 508.5020 490.2527 475.6410 497.7693 480.1376\nMay 1961 502.1128 495.2827 526.9527 488.3082 510.2163 524.8456 504.3970\nJun 1961 564.2560 565.1836 571.4699 554.6782 548.1972 582.0468 557.3177\nJul 1961 675.1975 656.7991 674.3476 643.9736 631.1844 665.9546 650.1157\nAug 1961 642.3301 604.3293 649.4941 612.8896 632.0786 640.5119 626.3405\nSep 1961 546.0228 520.2840 550.9040 533.3240 544.4996 540.7844 555.0268\nOct 1961 517.2212 480.8524 494.4332 467.0373 505.1177 495.0939 511.8599\nNov 1961 436.9353 409.0263 423.5813 407.0598 433.9868 420.4208 435.3652\nDec 1961 477.7085 444.2295 463.2785 447.8678 471.4819 477.0888 486.3902\n           sim_36   sim_37   sim_38   sim_39   sim_40   sim_41   sim_42\nJan 1961 458.2243 453.5069 442.9633 437.7823 456.7181 439.1887 444.2746\nFeb 1961 437.0087 430.5385 443.4436 434.5281 441.0503 415.1167 410.6375\nMar 1961 450.4477 456.5806 462.1142 454.3194 453.8004 433.9958 450.8734\nApr 1961 507.2234 499.3962 524.0761 495.3350 498.6223 465.6067 489.4286\nMay 1961 521.1416 518.5159 541.6983 514.5651 504.7369 474.7817 504.5533\nJun 1961 579.8129 580.3206 598.3710 558.8196 564.7209 544.2263 569.3150\nJul 1961 685.2838 663.2947 685.4005 652.3353 664.4497 622.2188 654.6807\nAug 1961 671.5436 671.9516 655.8918 630.8036 628.7970 615.9232 637.1259\nSep 1961 566.4307 567.5127 576.3038 539.0021 523.1752 530.4702 525.4079\nOct 1961 506.4949 510.5275 513.7364 488.4346 483.6546 479.5688 482.5080\nNov 1961 455.9362 453.4418 460.7156 417.2549 413.5535 408.8823 397.0385\nDec 1961 487.4205 481.4040 488.4877 455.6214 481.9331 453.4355 448.3142\n           sim_43   sim_44   sim_45   sim_46   sim_47   sim_48   sim_49\nJan 1961 456.7181 448.9127 444.1944 439.1887 444.3762 443.9540 449.6957\nFeb 1961 415.5481 417.4993 432.0998 418.8621 411.7694 419.4677 432.5796\nMar 1961 441.1190 446.8376 453.2327 448.5254 448.2663 470.9662 455.3371\nApr 1961 484.0184 474.0491 510.7521 487.2088 508.1952 505.5636 510.9074\nMay 1961 474.5845 493.5429 520.5754 493.9236 521.0746 494.3940 527.4129\nJun 1961 540.3995 553.2761 571.1053 570.3663 579.1209 553.5803 571.8349\nJul 1961 649.1646 657.5268 653.2605 657.5574 676.0876 607.8539 654.0949\nAug 1961 643.3467 635.9533 636.3980 656.9203 673.6242 590.6108 632.1774\nSep 1961 539.6460 532.2568 541.7711 546.2181 564.7882 492.5588 501.7329\nOct 1961 490.9799 483.5566 507.1510 500.1024 505.8764 471.2807 448.5220\nNov 1961 412.2859 415.6316 428.0756 413.5517 434.7768 384.6290 382.9796\nDec 1961 461.1189 422.9218 457.3822 459.4905 484.2661 434.8731 429.3802\n           sim_50\nJan 1961 442.9633\nFeb 1961 382.3990\nMar 1961 411.8363\nApr 1961 459.1659\nMay 1961 458.3228\nJun 1961 524.1910\nJul 1961 615.5164\nAug 1961 614.8716\nSep 1961 506.6345\nOct 1961 444.9304\nNov 1961 383.3882\nDec 1961 426.0341\n\noutput$forecast_sim_tbl\n\n# A tibble: 600 × 4\n       x     y n        id\n   <dbl> <dbl> <chr> <int>\n 1 1961.  446. sim_1     1\n 2 1961.  424. sim_1     2\n 3 1961.  444. sim_1     3\n 4 1961.  490. sim_1     4\n 5 1961.  502. sim_1     5\n 6 1961.  553. sim_1     6\n 7 1962.  655. sim_1     7\n 8 1962.  633. sim_1     8\n 9 1962.  536. sim_1     9\n10 1962.  486. sim_1    10\n# … with 590 more rows\n\n\nThe time series that was used.\n\noutput$time_series\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nThe fitted values in two different formats\n\noutput$fitted_values\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1949 111.9353 117.9664 131.9662 128.9774 120.9892 134.9782 147.9692 147.9731\n1950 115.4270 121.3807 138.5312 137.2522 127.5180 139.7865 158.9159 166.3207\n1951 132.8130 151.5147 164.7662 165.5934 153.1413 186.9835 201.0564 197.1598\n1952 171.2150 174.9573 205.3873 181.9983 189.5111 191.9692 229.3659 230.2887\n1953 197.4932 205.0669 211.9492 214.7030 229.4790 261.9978 259.3789 272.3394\n1954 205.8921 206.1940 236.4993 236.1932 227.3492 247.7618 281.4459 303.3972\n1955 229.7559 221.0431 274.5806 260.1968 270.5411 299.1164 345.0987 346.2332\n1956 283.9172 273.9637 307.7212 313.9008 312.5977 358.8094 415.5070 394.8641\n1957 313.6903 307.2137 343.5753 347.2471 353.0923 409.4687 455.6903 452.6105\n1958 346.2681 328.3116 377.2767 360.7205 361.5534 432.0632 479.8147 490.8438\n1959 346.5194 335.2349 385.8185 384.9676 406.8419 485.3013 531.0698 553.0149\n1960 418.4120 397.2579 454.0138 420.2105 468.2327 523.6974 603.6761 623.9211\n          Sep      Oct      Nov      Dec\n1949 135.9874 119.0049 104.0187 118.0778\n1950 156.2023 139.1631 119.1047 128.9974\n1951 185.1990 158.4085 140.6879 169.2312\n1952 220.7255 190.3056 172.8446 191.9035\n1953 238.8716 218.7628 194.3686 207.1910\n1954 261.6971 232.5912 199.3564 222.5606\n1955 309.8394 277.5670 246.5073 264.0691\n1956 363.2702 318.0959 271.0990 310.9291\n1957 409.6920 354.9782 312.2741 341.2514\n1958 438.0323 360.3301 317.3131 346.5759\n1959 454.6235 412.1130 357.5358 384.6475\n1960 513.8591 450.7760 410.8955 439.9468\n\noutput$fitted_values_tbl\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949   112.\n 2 Feb 1949   118.\n 3 Mar 1949   132.\n 4 Apr 1949   129.\n 5 May 1949   121.\n 6 Jun 1949   135.\n 7 Jul 1949   148.\n 8 Aug 1949   148.\n 9 Sep 1949   136.\n10 Oct 1949   119.\n# … with 134 more rows\n\n\nThe residual values in two different formats\n\noutput$residual_values\n\n               Jan           Feb           Mar           Apr           May\n1949   0.064663218   0.033565844   0.033806149   0.022551853   0.010753877\n1950  -0.426993657   4.619296276   2.468817592  -2.252222539  -2.517970381\n1951  12.187004737  -1.514734464  13.233788623  -2.593406722  18.858703025\n1952  -0.215049450   5.042657391 -12.387298452  -0.998279160  -6.511066526\n1953  -1.493243603  -9.066863638  24.050789583  20.296981218  -0.479038408\n1954  -1.892145409 -18.194023466  -1.499295070  -9.193240870   6.650775967\n1955  12.244087324  11.956865808  -7.580632008   8.803192574  -0.541058565\n1956   0.082775370   3.036286487   9.278782426  -0.900756132   5.402300489\n1957   1.309660582  -6.213658164  12.424731881   0.752860797   1.907693986\n1958  -6.268081829 -10.311568418 -15.276674427 -12.720532082   1.446551672\n1959  13.480639348   6.765147100  20.181485627  11.032373098  13.158068772\n1960  -1.412019863  -6.257914445 -35.013829003  40.789527421   3.767286916\n               Jun           Jul           Aug           Sep           Oct\n1949   0.021825883   0.030792890   0.026927580   0.012574909  -0.004856125\n1950   9.213518228  11.084130208   3.679258769   1.797714396  -6.163078961\n1951  -8.983477860  -2.056432959   1.840247841  -1.199018264   3.591516610\n1952  26.030752750   0.634055619  11.711292178 -11.725472510   0.694363698\n1953 -18.997830355   4.621111100  -0.339432546  -1.871647726  -7.762821892\n1954  16.238163730  20.554095186 -10.397216564  -2.697121208  -3.591173672\n1955  15.883576851  18.901348128   0.766758698   2.160561460  -3.567021842\n1956  15.190550278  -2.507027255  10.135908860  -8.270245332 -12.095862166\n1957  12.531339373   9.309735715  14.389487605  -5.692026179  -7.978198549\n1958   2.936791273  11.185298228  14.156225803 -34.032306461  -1.330101426\n1959 -13.301336524  16.930226311   5.985059029   8.376509562  -5.112953069\n1960  11.302583143  18.323916561 -17.921058274  -5.859106651  10.223989361\n               Nov           Dec\n1949  -0.018746667  -0.077775679\n1950  -5.104668785  11.002554045\n1951   5.312079856  -3.231170377\n1952  -0.844622054   2.096450492\n1953 -14.368618246  -6.190983141\n1954   3.643587684   6.439397882\n1955  -9.507327199  13.930943896\n1956  -0.099013624  -4.929146809\n1957  -7.274091927  -5.251369244\n1958  -7.313133943  -9.575869505\n1959   4.464243872  20.352533059\n1960 -20.895479201  -7.946822359\n\noutput$residual_values_tbl\n\n# A tibble: 144 × 2\n   index        value\n   <yearmon>    <dbl>\n 1 Jan 1949   0.0647 \n 2 Feb 1949   0.0336 \n 3 Mar 1949   0.0338 \n 4 Apr 1949   0.0226 \n 5 May 1949   0.0108 \n 6 Jun 1949   0.0218 \n 7 Jul 1949   0.0308 \n 8 Aug 1949   0.0269 \n 9 Sep 1949   0.0126 \n10 Oct 1949  -0.00486\n# … with 134 more rows\n\n\nThe input data itself\n\noutput$input_data\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949    112\n 2 Feb 1949    118\n 3 Mar 1949    132\n 4 Apr 1949    129\n 5 May 1949    121\n 6 Jun 1949    135\n 7 Jul 1949    148\n 8 Aug 1949    148\n 9 Sep 1949    136\n10 Oct 1949    119\n# … with 134 more rows\n\n\nThe time series simulations\n\noutput$sim_ts_tbl\n\n# A tibble: 600 × 5\n   index         x     y n        id\n   <yearmon> <dbl> <dbl> <chr> <int>\n 1 Jan 1961  1961.  446. sim_1     1\n 2 Feb 1961  1961.  424. sim_1     2\n 3 Mar 1961  1961.  444. sim_1     3\n 4 Apr 1961  1961.  490. sim_1     4\n 5 May 1961  1961.  502. sim_1     5\n 6 Jun 1961  1961.  553. sim_1     6\n 7 Jul 1961  1962.  655. sim_1     7\n 8 Aug 1961  1962.  633. sim_1     8\n 9 Sep 1961  1962.  536. sim_1     9\n10 Oct 1961  1962.  486. sim_1    10\n# … with 590 more rows\n\n\nNow, the visuals, first the static ggplot\n\noutput$ggplot\n\n\n\n\nThe interactive plotly plot.\n\noutput$plotly_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-2023-01-06/index.html",
    "href": "posts/weekly-rtip-2023-01-06/index.html",
    "title": "Brownian Motion",
    "section": "",
    "text": "Introduction\nBrownian motion, also known as the random motion of particles suspended in a fluid, is a phenomenon that was first described by Scottish botanist Robert Brown in 1827. It occurs when a particle is subjected to a series of random collisions with the molecules in the fluid.\nThe motion of the particle can be described mathematically using the following equation:\n\\[ \\frac{dx_t}{dt} = \\mu + \\sigma \\cdot W_t \\]\nWhere \\(x_t\\) represents the position of the particle at time t, \\(\\mu\\) is the drift coefficient, \\(\\sigma\\) is the diffusion coefficient, and \\(W_t\\) is a Wiener process (a type of random process).\nBrownian motion has a number of important applications, including in the field of finance. It is used to model the random movements of financial assets, such as stocks, over time. It can also be used to estimate the volatility of an asset, as well as to calculate the prices of financial derivatives such as options.\nIn physics, Brownian motion is used to study the behavior of small particles suspended in a fluid, as well as to understand the properties of fluids at the molecular level. It has also been used to study the motion of biological molecules, such as proteins, within cells.\nOverall, Brownian motion is a fundamental concept that has wide-ranging applications in a variety of fields, including finance, physics, and biology.\n\n\nFunction\nLet’s take a look at a function to produce such results. This type of functionality will be coming to my R packages {TidyDensity} and to {healthyR.ts}\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(forcats)\n\nbrownian_motion <- function(T, N, delta_t) {\n  # T: total time of the simulation (in seconds)\n  # N: number of simulations to generate\n  # delta_t: time step size (in seconds)\n  \n  # Initialize empty data.frame to store the simulations\n  sim_data <- data.frame()\n  \n  # Generate N simulations\n  for (i in 1:N) {\n    # Initialize the current simulation with a starting value of 0\n    sim <- c(0)\n    \n    # Generate the brownian motion values for each time step\n    for (t in 1:(T / delta_t)) {\n      sim <- c(sim, sim[t] + rnorm(1, mean = 0, sd = sqrt(delta_t)))\n    }\n    \n    # Bind the time steps, simulation values, and simulation number together\n    # in a data.frame and add it to the result\n    sim_data <- rbind(\n      sim_data, \n      data.frame(\n        t = seq(0, T, delta_t), \n        y = sim, \n        sim_number = i\n        )\n      ) %>%\n      as_tibble()\n  }\n  \n  sim_data <- sim_data %>%\n    mutate(sim_number = as_factor(sim_number)\n                  )\n  return(sim_data)\n}\n\nWe see that the internal variable sim is set to 0, this in the future will be set to an initial value that a user can provide.\n\n\nExamples\nLet’s take a look at a couple of examples.\n\nbrownian_motion(40, 25, .2) %>%\n  ggplot(aes(x = t, y = y, group = sim_number, color = sim_number)) +\n  geom_line() +\n  geom_smooth(se = FALSE, \n              aes(group = FALSE), \n              color = \"black\", \n              linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    x = \"Time t\",\n    y = \"Brownian Motion Value y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nNow lets take a look at the change in a few different ones at the same time.\n\nbm_tbl <- rbind(\n  brownian_motion(40, 25, .2) %>%\n    mutate(label = \"20% Volatility\"),\n  brownian_motion(40, 25, .1) %>%\n    mutate(label = \"10% Volatility\"),\n  brownian_motion(40, 25, .05) %>%\n    mutate(label = \"5% Volatility\"),\n  brownian_motion(40, 25, .025) %>%\n    mutate(label = \"2.5% Volatility\")\n)\n\nggplot(bm_tbl, aes(x = t, y = y, group = sim_number, color = sim_number)) +\n  geom_line() +\n  facet_wrap(~ label, scales = \"free\") +\n    geom_smooth(se = FALSE, \n              aes(group = FALSE), \n              color = \"black\", \n              linetype = \"dashed\") +\n  theme_minimal() +\n    labs(\n    x = \"Time t\",\n    y = \"Brownian Motion Value y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "title": "PCA with healthyR.ai",
    "section": "",
    "text": "In this post we are going to talk about how you can perform principal component analysis in R with {healthyR.ai} in a tidyverse compliant fashion.\nThe specific function we are going to discuss on this post is pca_your_recipe()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "title": "PCA with healthyR.ai",
    "section": "Library Load",
    "text": "Library Load\n\npacman::p_load(\n  \"healthyR.ai\",\n  \"healthyR.data\",\n  \"timetk\",\n  \"dplyr\",\n  \"purrr\",\n  \"rsample\",\n  \"recipes\"\n)\n\nNow that we have our libraries loaded lets get the data."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "title": "PCA with healthyR.ai",
    "section": "Data",
    "text": "Data\n\ndata_tbl <- healthyR_data %>%\n  select(visit_end_date_time) %>%\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by       = \"month\",\n    value     = n()\n  ) %>%\n  set_names(\"date_col\", \"value\") %>%\n  filter_by_time(\n    .date_var = date_col,\n    .start_date = \"2013\",\n    .end_date = \"2020\"\n  )\n\nhead(data_tbl, 5)\n\n# A tibble: 5 × 2\n  date_col            value\n  <dttm>              <int>\n1 2013-01-01 00:00:00  2082\n2 2013-02-01 00:00:00  1719\n3 2013-03-01 00:00:00  1796\n4 2013-04-01 00:00:00  1865\n5 2013-05-01 00:00:00  2028\n\n\nNow for the splits object."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "title": "PCA with healthyR.ai",
    "section": "Splits",
    "text": "Splits\n\nsplits <- initial_split(data = data_tbl, prop = 0.8)\n\nsplits\n\n<Training/Testing/Total>\n<76/19/95>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "title": "PCA with healthyR.ai",
    "section": "Recipe and Output",
    "text": "Recipe and Output\nNow it is time for the recipe and the output objects.\n\nrec_obj <- recipe(value ~ ., training(splits)) %>%\n  step_timeseries_signature(date_col) %>%\n  step_rm(matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\"))\n\noutput_list <- pca_your_recipe(rec_obj, .data = data_tbl)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "title": "PCA with healthyR.ai",
    "section": "PCA Transform",
    "text": "PCA Transform\n\noutput_list$pca_transform\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nTimeseries signature features from date_col\nVariables removed matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\")\nCentering for recipes::all_numeric()\nScaling for recipes::all_numeric()\nSparse, unbalanced variable filter on recipes::all_numeric()\nPCA extraction with recipes::all_numeric_predictors()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "title": "PCA with healthyR.ai",
    "section": "Variable Loadings",
    "text": "Variable Loadings\n\noutput_list$variable_loadings\n\n# A tibble: 169 × 4\n   terms                 value component id       \n   <chr>                 <dbl> <chr>     <chr>    \n 1 date_col_index.num -0.00137 PC1       pca_bVc37\n 2 date_col_year       0.0529  PC1       pca_bVc37\n 3 date_col_half      -0.385   PC1       pca_bVc37\n 4 date_col_quarter   -0.434   PC1       pca_bVc37\n 5 date_col_month     -0.437   PC1       pca_bVc37\n 6 date_col_wday      -0.0159  PC1       pca_bVc37\n 7 date_col_qday      -0.0608  PC1       pca_bVc37\n 8 date_col_yday      -0.437   PC1       pca_bVc37\n 9 date_col_mweek      0.0537  PC1       pca_bVc37\n10 date_col_week      -0.438   PC1       pca_bVc37\n# … with 159 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "title": "PCA with healthyR.ai",
    "section": "Variable Variance",
    "text": "Variable Variance\n\noutput_list$variable_variance\n\n# A tibble: 52 × 4\n   terms       value component id       \n   <chr>       <dbl>     <int> <chr>    \n 1 variance 5.14             1 pca_bVc37\n 2 variance 2.08             2 pca_bVc37\n 3 variance 1.47             3 pca_bVc37\n 4 variance 1.40             4 pca_bVc37\n 5 variance 1.07             5 pca_bVc37\n 6 variance 0.684            6 pca_bVc37\n 7 variance 0.583            7 pca_bVc37\n 8 variance 0.519            8 pca_bVc37\n 9 variance 0.0534           9 pca_bVc37\n10 variance 0.000231        10 pca_bVc37\n# … with 42 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Estimates",
    "text": "PCA Estimates\n\noutput_list$pca_estimates\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nTraining data contained 76 data points and no missing data.\n\nOperations:\n\nTimeseries signature features from date_col [trained]\nVariables removed date_col_year.iso, date_col_month.xts, date_col_hour, d... [trained]\nCentering for value, date_col_index.num, date_col_year, date_... [trained]\nScaling for value, date_col_index.num, date_col_year, date_... [trained]\nSparse, unbalanced variable filter removed date_col_day, date_col_mday, date_col_m... [trained]\nPCA extraction with date_col_index.num, date_col_year, date_col_half... [trained]"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Juiced Estimates",
    "text": "PCA Juiced Estimates\n\noutput_list$pca_juiced_estimates\n\n# A tibble: 76 × 8\n   date_col              value date_col…¹ date_…²     PC1     PC2     PC3    PC4\n   <dttm>                <dbl> <ord>      <ord>     <dbl>   <dbl>   <dbl>  <dbl>\n 1 2018-06-01 00:00:00  0.676  June       Friday   0.733   1.23   -1.30    0.536\n 2 2016-01-01 00:00:00 -0.133  January    Friday   3.51   -0.102  -0.500  -0.556\n 3 2013-05-01 00:00:00  1.67   May        Wednes…  1.15   -2.07   -1.68   -0.319\n 4 2018-10-01 00:00:00  0.337  October    Monday  -2.30    0.499   1.91   -1.92 \n 5 2016-09-01 00:00:00 -0.130  September  Thursd… -1.08    0.0972  0.410   2.64 \n 6 2016-07-01 00:00:00 -0.364  July       Friday  -0.0591  0.0211 -0.333   1.11 \n 7 2020-02-01 00:00:00 -0.646  February   Saturd…  3.08    2.48   -0.0249  0.214\n 8 2020-08-01 00:00:00 -1.42   August     Saturd… -0.491   2.60    0.142   1.88 \n 9 2018-03-01 00:00:00  0.243  March      Thursd…  2.74    0.848  -1.53    0.260\n10 2015-05-01 00:00:00 -0.0148 May        Friday   1.18   -0.636  -1.88   -0.147\n# … with 66 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "title": "PCA with healthyR.ai",
    "section": "PCA Baked Data",
    "text": "PCA Baked Data\n\noutput_list$pca_baked_data\n\n# A tibble: 95 × 8\n   date_col            value date_col_month…¹ date_…²    PC1   PC2    PC3    PC4\n   <dttm>              <dbl> <ord>            <ord>    <dbl> <dbl>  <dbl>  <dbl>\n 1 2013-01-01 00:00:00 1.86  January          Tuesday  3.60  -2.64  1.13  -0.871\n 2 2013-02-01 00:00:00 0.596 February         Friday   2.93  -1.76 -0.532  0.424\n 3 2013-03-01 00:00:00 0.864 March            Friday   2.62  -1.95 -2.24   0.643\n 4 2013-04-01 00:00:00 1.10  April            Monday   2.09  -2.68  1.39  -0.867\n 5 2013-05-01 00:00:00 1.67  May              Wednes…  1.15  -2.07 -1.68  -0.319\n 6 2013-06-01 00:00:00 0.923 June             Saturd…  0.612 -1.57 -2.01   0.919\n 7 2013-07-01 00:00:00 1.29  July             Monday  -0.669 -2.41  2.29  -0.420\n 8 2013-08-01 00:00:00 1.18  August           Thursd… -0.628 -1.76 -0.165  1.96 \n 9 2013-09-01 00:00:00 0.714 September        Sunday  -1.11  -2.19  0.910  2.25 \n10 2013-10-01 00:00:00 1.40  October          Tuesday -2.55  -1.91 -0.128 -1.48 \n# … with 85 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Data Frame",
    "text": "PCA Variance Data Frame\n\noutput_list$pca_variance_df\n\n# A tibble: 13 × 6\n   PC    var_explained var_pct_txt cum_var_pct cum_var_pct_txt ou_threshold\n   <chr>         <dbl> <chr>             <dbl> <chr>           <fct>       \n 1 PC1   0.395         39.55%            0.395 39.55%          Under       \n 2 PC2   0.160         16.02%            0.556 55.57%          Under       \n 3 PC3   0.113         11.29%            0.669 66.86%          Under       \n 4 PC4   0.107         10.75%            0.776 77.61%          Over        \n 5 PC5   0.0824        8.24%             0.858 85.85%          Over        \n 6 PC6   0.0526        5.26%             0.911 91.11%          Over        \n 7 PC7   0.0449        4.49%             0.956 95.59%          Over        \n 8 PC8   0.0400        4.00%             0.996 99.59%          Over        \n 9 PC9   0.00411       0.41%             1.00  100.00%         Over        \n10 PC10  0.0000178     0.00%             1.00  100.00%         Over        \n11 PC11  0.000000712   0.00%             1.00  100.00%         Over        \n12 PC12  0.000000273   0.00%             1.00  100.00%         Over        \n13 PC13  0.00000000196 0.00%             1     100.00%         Over"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Rotation Data Frame",
    "text": "PCA Rotation Data Frame\n\noutput_list$pca_rotation_df\n\n# A tibble: 13 × 13\n        PC1      PC2      PC3     PC4     PC5     PC6      PC7     PC8      PC9\n      <dbl>    <dbl>    <dbl>   <dbl>   <dbl>   <dbl>    <dbl>   <dbl>    <dbl>\n 1 -0.00137  0.671    0.116   -0.0521 -0.183   0.0165  0.0471   0.0277 -0.0177 \n 2  0.0529   0.667    0.116   -0.0610 -0.173   0.0146  0.0466   0.0313  0.00904\n 3 -0.385    0.0115   0.222    0.173   0.140   0.217   0.250   -0.0112  0.802  \n 4 -0.434    0.00824  0.0752  -0.0427  0.0615  0.135   0.00843  0.0332 -0.272  \n 5 -0.437    0.00278 -0.00879  0.0737 -0.0734  0.0167  0.00135 -0.0264 -0.213  \n 6 -0.0159   0.265   -0.406    0.274   0.468   0.254  -0.520   -0.366   0.0484 \n 7 -0.0608  -0.0200  -0.325    0.480  -0.542  -0.476  -0.0318  -0.244   0.187  \n 8 -0.437    0.00435 -0.00655  0.0740 -0.0733  0.0149  0.00231 -0.0303 -0.216  \n 9  0.0537  -0.164    0.562   -0.0242 -0.414   0.267  -0.572   -0.285   0.0519 \n10 -0.438    0.00638 -0.00736  0.0676 -0.0704  0.0236  0.00470 -0.0266 -0.219  \n11  0.250   -0.0238   0.208    0.420   0.0436  0.301   0.544   -0.492  -0.293  \n12 -0.0474   0.0762   0.516    0.142   0.460  -0.676  -0.108   -0.144  -0.0636 \n13  0.123    0.00776  0.150    0.666   0.0183  0.152  -0.161    0.676  -0.111  \n# … with 4 more variables: PC10 <dbl>, PC11 <dbl>, PC12 <dbl>, PC13 <dbl>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Scree Plot",
    "text": "PCA Variance Scree Plot\n\noutput_list$pca_variance_scree_plt"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Loadings Plot",
    "text": "PCA Loadings Plot\n\noutput_list$pca_loadings_plt\n\n\n\noutput_list$pca_loadings_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "title": "PCA with healthyR.ai",
    "section": "Top N Loadings Plots",
    "text": "Top N Loadings Plots\n\noutput_list$pca_top_n_loadings_plt\n\n\n\noutput_list$pca_top_n_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "",
    "text": "Minimal coding ML is not something that is unheard of and is rather prolific, think h2o and pycaret just to name two. There is also no shortage available for R with the h2o interface, and tidyfit. There are also similar low-code workflows in my r package {healthyR.ai}. Today I will specifically go through the workflow for Automatic KNN classification for the Iris data set where we will classify the Species."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Recipe Output",
    "text": "Recipe Output\n\nauto_knn$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\nCentering and scaling for recipes::all_numeric()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Model Info",
    "text": "Model Info\n\nauto_knn$model_info$was_tuned\n\n[1] \"tuned\"\n\n\n\nauto_knn$model_info$model_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$wflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$fitted_wflw\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(13L,     data, 5), distance = ~1.69935879141092, kernel = ~\"rank\")\n\nType of response variable: nominal\nMinimal misclassification: 0.03571429\nBest kernel: rank\nBest k: 13"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Tuning Info",
    "text": "Tuning Info\n\nauto_knn$tuned_info$tuning_grid\n\n# A tibble: 10 × 3\n   neighbors weight_func  dist_power\n       <int> <chr>             <dbl>\n 1         4 triangular        0.764\n 2        11 rectangular       0.219\n 3         5 gaussian          1.35 \n 4        14 triweight         0.351\n 5         5 biweight          1.05 \n 6         9 optimal           1.87 \n 7         7 cos               0.665\n 8        11 inv               1.18 \n 9        13 rank              1.70 \n10         1 epanechnikov      1.58 \n\n\n\nauto_knn$tuned_info$cv_obj\n\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$tuned_results\n\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics           .notes          \n   <list>          <chr>      <list>             <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 7]> <tibble [0 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 7]> <tibble [0 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 7]> <tibble [0 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 7]> <tibble [0 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 7]> <tibble [0 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 7]> <tibble [0 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 7]> <tibble [0 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 7]> <tibble [0 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 7]> <tibble [0 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 7]> <tibble [0 × 3]>\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$grid_size\n\n[1] 10\n\n\n\nauto_knn$tuned_info$best_metric\n\n[1] \"f_meas\"\n\n\n\nauto_knn$tuned_info$best_result_set\n\n# A tibble: 1 × 9\n  neighbors weight_func dist_power .metric .estima…¹  mean     n std_err .config\n      <int> <chr>            <dbl> <chr>   <chr>     <dbl> <int>   <dbl> <chr>  \n1        13 rank              1.70 f_meas  macro     0.957    25 0.00655 Prepro…\n# … with abbreviated variable name ¹​.estimator\n\n\n\nauto_knn$tuned_info$tuning_grid_plot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nauto_knn$tuned_info$plotly_grid_plot\n\n\n\n\n\nVoila!\nThank you for reading."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "title": "Time Series Lag Correlation Plots",
    "section": "",
    "text": "In time series analysis there is something called a lag. This simply means we take a look at some past event from some point in time t. This is a non-statistical method for looking at a relationship between a timeseries and its lags.\n{healthyR.ts} has a function called ts_lag_correlation(). This function, as described by it’s name, provides more than just a simple lag plot.\nThis function provides a lot of extra information for the end user. First let’s go over the function call."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Call",
    "text": "Function Call\nHere is the full call:\n\nts_lag_correlation(\n  .data,\n  .date_col,\n  .value_col,\n  .lags = 1,\n  .heatmap_color_low = \"white\",\n  .heatmap_color_hi = \"steelblue\"\n)\n\nHere are the arguments that get supplied to the different parameters.\n\n.data - A tibble of time series data\n.date_col - A date column\n.value_col - The value column being analyzed\n.lags - This is a vector of integer lags, ie 1 or c(1,6,12)\n.heatmap_color_low - What color should the low values of the heatmap of the correlation matrix be, the default is ‘white’\n.heatmap_color_hi - What color should the low values of the heatmap of the correlation matrix be, the default is ‘steelblue’"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Return",
    "text": "Function Return\nThe function itself returns a list object. The list has the following elements in it:\nData Elements\n\nlag_list\nlag_tbl\ncorrelation_lag_matrix\ncorrelation_lag_tbl\n\nPlot Elements\n\nlag_plot\nplotly_lag_plot\ncorrelation_heatmap\nplotly_heatmap"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Data Elements",
    "text": "Data Elements\nHere are the data elements.\n\noutput$data$lag_list\n\n[[1]]\n# A tibble: 143 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 1       118          112\n 2 1       132          118\n 3 1       129          132\n 4 1       121          129\n 5 1       135          121\n 6 1       148          135\n 7 1       148          148\n 8 1       136          148\n 9 1       119          136\n10 1       104          119\n# … with 133 more rows\n\n[[2]]\n# A tibble: 141 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 3       129          112\n 2 3       121          118\n 3 3       135          132\n 4 3       148          129\n 5 3       148          121\n 6 3       136          135\n 7 3       119          148\n 8 3       104          148\n 9 3       118          136\n10 3       115          119\n# … with 131 more rows\n\n[[3]]\n# A tibble: 138 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 6       148          112\n 2 6       148          118\n 3 6       136          132\n 4 6       119          129\n 5 6       104          121\n 6 6       118          135\n 7 6       115          148\n 8 6       126          148\n 9 6       141          136\n10 6       135          119\n# … with 128 more rows\n\n[[4]]\n# A tibble: 132 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 12      115          112\n 2 12      126          118\n 3 12      141          132\n 4 12      135          129\n 5 12      125          121\n 6 12      149          135\n 7 12      170          148\n 8 12      170          148\n 9 12      158          136\n10 12      133          119\n# … with 122 more rows\n\n\nThis is a list of all the tibbles of the different lags that were chosen.\n\noutput$data$lag_tbl\n\n# A tibble: 554 × 4\n   lag   value lagged_value lag_title\n   <fct> <dbl>        <dbl> <fct>    \n 1 1       118          112 Lag: 1   \n 2 1       132          118 Lag: 1   \n 3 1       129          132 Lag: 1   \n 4 1       121          129 Lag: 1   \n 5 1       135          121 Lag: 1   \n 6 1       148          135 Lag: 1   \n 7 1       148          148 Lag: 1   \n 8 1       136          148 Lag: 1   \n 9 1       119          136 Lag: 1   \n10 1       104          119 Lag: 1   \n# … with 544 more rows\n\n\nThis is the long lag tibble with all of the lags in it.\n\noutput$data$correlation_lag_matrix\n\n                value value_lag1 value_lag3 value_lag6 value_lag12\nvalue       1.0000000  0.9542938  0.8186636  0.7657001   0.9905274\nvalue_lag1  0.9542938  1.0000000  0.8828054  0.7726530   0.9492382\nvalue_lag3  0.8186636  0.8828054  1.0000000  0.8349550   0.8218493\nvalue_lag6  0.7657001  0.7726530  0.8349550  1.0000000   0.7780911\nvalue_lag12 0.9905274  0.9492382  0.8218493  0.7780911   1.0000000\n\n\nThis is the correlation matrix.\n\noutput$data$correlation_lag_tbl\n\n# A tibble: 25 × 3\n   name        data_names value\n   <fct>       <fct>      <dbl>\n 1 value       value      1    \n 2 value_lag1  value      0.954\n 3 value_lag3  value      0.819\n 4 value_lag6  value      0.766\n 5 value_lag12 value      0.991\n 6 value       value_lag1 0.954\n 7 value_lag1  value_lag1 1    \n 8 value_lag3  value_lag1 0.883\n 9 value_lag6  value_lag1 0.773\n10 value_lag12 value_lag1 0.949\n# … with 15 more rows\n\n\nThis is the correlation lag tibble"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Plot Elements",
    "text": "Plot Elements\n\noutput$plots$lag_plot\n\n\n\n\nThe Lag Plot itself.\n\noutput$plots$plotly_lag_plot\n\n\n\n\n\nA plotly version of the lag plot.\n\noutput$plots$correlation_heatmap\n\n\n\n\nA heatmap of the correlations.\n\noutput$plots$plotly_heatmap\n\n\n\n\n\nA plotly version of the correlation heatmap.\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "title": "Create QQ Plots for Time Series Models with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nA Q-Q plot, or quantile-quantile plot, is a graphical tool for comparing two sets of data to assess whether they come from the same distribution. In the context of time series modeling, a Q-Q plot can be used to check whether the residuals of a fitted time series model follow the normal distribution. This is important because many time series models, such as the autoregressive moving average (ARMA) model, assume that the residuals are normally distributed.\nTo create a Q-Q plot, the data are first sorted in ascending order and then divided into quantiles. The quantiles of the first dataset are then plotted against the quantiles of the second dataset. If the two datasets come from the same distribution, the points on the Q-Q plot will fall approximately on a straight line. Deviations from this line can indicate departures from the assumed distribution.\nFor example, if we have a time series dataset and we fit an ARMA model to it, we can use a Q-Q plot to check whether the residuals of the fitted model are normally distributed. If the Q-Q plot shows that the residuals do not follow the normal distribution, we may need to consider using a different time series model that does not assume normality of the residuals.\nIn summary, Q-Q plots are a useful tool for assessing the distribution of a dataset and for checking whether a time series model has produced satisfactory residuals.\nIn the R package {healthyR.ts} there is a function to view the QQ plot. This function is called ts_qq_plot() and it is meant to work with a calibration tibble from the excellent {modeltime} which is a {parsnip} extension package.\n\n\nFunction\nLet’s take a look at the full function call and the arguments that get provided to the parameters.\n\nts_qq_plot(\n  .calibration_tbl, \n  .model_id = NULL, \n  .interactive = FALSE\n  )\n\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nLet’s work through an example, and since we already spoke about ARMA let’s try out an ARMA model.\n\nlibrary(healthyR.ts)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(recipes)\nlibrary(parsnip)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_qq_plot(calibration_tbl, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "title": "Model Scedacity Plots with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nScedacity plots are a useful tool for evaluating the performance of time series models and identifying trends or patterns in the data. They are a type of scatter plot that compares the predicted values produced by a model to the observed values in the data, with a diagonal reference line indicating perfect agreement between the two.\nThe {healthyR.ts} package in R provides a convenient function for creating scedacity plots for time series data, called ts_scedacity_scatter_plot(). This function takes as input a calibration tibble which you would get from using the {modeltime} library, and produces a scedacity plot showing the predicted values against the observed values.\nOne of the main benefits of using a scedacity plot is that it allows you to visualize the accuracy of the model’s predictions. If the points on the plot fall close to the reference line, it indicates that the model is able to accurately predict the values in the data. On the other hand, if the points are scattered far from the reference line, it suggests that the model is not performing well and may need to be improved or refined.\nIn addition to evaluating the accuracy of the model, scedacity plots can also be used to identify trends or patterns in the data. For example, if there is a clear upward or downward trend in the points on the plot, it may indicate that the model is over- or under-estimating the values in the data. By identifying these trends, you can adjust the model or try different approaches to improve its performance.\nOverall, scedacity plots are a useful tool for evaluating the performance of time series models and identifying trends or patterns in the data. The ts_scedacity_scatter_plot() function in the {healthyR.ts} package makes it easy to create these plots and gain insights into the performance of your time series models.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_scedacity_scatter_plot(\n  .calibration_tbl,\n  .model_id = NULL,\n  .interactive = FALSE\n)\n\nLet’s take a look at the arguments that get provided to the parameters.\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(recipes)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nmodel_spec_mars <- mars(mode = \"regression\") %>%\n  set_engine(\"earth\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nwflw_fit_mars <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_mars) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima, wflw_fit_mars)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_scedacity_scatter_plot(calibration_tbl)\n\n\n\n\nNow the interactive plot.\n\nts_scedacity_scatter_plot(calibration_tbl, .interactive = TRUE)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "title": "Event Analysis with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime-to-event analysis, also known as survival analysis, is a statistical technique used to analyze the length of time until an event occurs. This type of analysis is often used in fields such as healthcare, engineering, and finance to understand the factors that influence the likelihood of an event occurring and to make predictions about future events.\nIn economics, an event study is a statistical technique used to analyze the effect of a specific event on a particular market or financial instrument. Event studies are commonly used in finance to understand how events, such as the announcement of a new product, the release of financial earnings, or a change in government policy, may impact the price or performance of a company’s stock or other financial instruments.\nTo conduct an event study, analysts typically collect data on the performance of a market or financial instrument before and after the event in question. This data is then used to estimate the effect of the event on the market or instrument.\nThere are several different methods that can be used to conduct an event study, including the market model, the abnormal return method, and the buy-and-hold abnormal return method. These methods allow analysts to quantify the effect of the event on the market or instrument and to identify any changes in market behavior that may have occurred as a result of the event.\nOverall, event studies are a valuable tool for understanding how specific events may impact financial markets and instruments, and are widely used in finance and economics to inform investment decisions and to better understand market behavior.\nIn this post we are going to examine a function from the R package {healthyR.ts} has a function called ts_time_event_analysis_tbl() that will help us understand what happens after a specified event, in this instance it will always be some percentage decrease or increase in a value.\nThere is a great article from Investopedia on this economic topic here\n\n\nFunction\nThe function is ts_time_event_analysis_tbl() and it’s complimentary plotting function ts_event_analysis_plot().\nHere is the tibble data return function.\n\nts_time_event_analysis_tbl(\n  .data,\n  .date_col,\n  .value_col,\n  .percent_change = 0.05,\n  .horizon = 12,\n  .precision = 2,\n  .direction = \"forward\",\n  .filter_non_event_groups = TRUE\n)\n\nLet’s take a look at the arguments to the parameters for this one.\n\n.data - The date.frame/tibble that holds the data.\n.date_col - The column with the date value.\n.value_col - The column with the value you are measuring.\n.percent_change - This defaults to 0.05 which is a 5% increase in the value_col.\n.horizon - How far do you want to look back or ahead.\n.precision - The default is 2 which means it rounds the lagged 1 value percent change to 2 decimal points. You may want more for more finely tuned results, this will result in fewer groupings.\n.direction - The default is forward. You can supply either forward, backwards or both.\nfilter_non_event_groups - The default is TRUE, this drops groupings with no events on the rare occasion it does occur.\n\nNow the plotting function.\n\nts_event_analysis_plot(\n  .data,\n  .plot_type = \"mean\",\n  .plot_ci = TRUE,\n  .interactive = FALSE\n)\n\n\n.data - The data that comes from the ts_time_event_analysis_tbl()\n.plot_type - The default is “mean” which will show the mean event change of the output from the analysis tibble. The possible values for this are: mean, median, and individual.\n.plot_ci - The default is TRUE. This will only work if you choose one of the aggregate plots of either “mean” or “median”\n.interactive - The default is FALSE. TRUE will return a plotly plot.\n\n\n\nExamples\nLet’s go through a couple examples using the AirPassengers data. We will first transform it into a tibble and then we will use a look period of 6. Let’s see the data output and then we will visualize.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndf <- ts_to_tbl(AirPassengers) %>% select(-index)\n\nevent_tbl <- ts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"both\"\n)\n\nglimpse(event_tbl)\n\nRows: 33\nColumns: 18\n$ rowid                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ date_col             <date> 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value                <dbl> 118, 132, 129, 121, 135, 148, 148, 199, 184, 162,…\n$ lag_val              <dbl> 112, 118, 132, 129, 121, 135, 148, 199, 199, 184,…\n$ adj_diff             <dbl> 6, 14, -3, -8, 14, 13, 0, 0, -15, -22, -16, 20, 5…\n$ relative_change_raw  <dbl> 0.05357143, 0.11864407, -0.02272727, -0.06201550,…\n$ relative_change      <dbl> 0.05, 0.12, -0.02, -0.06, 0.12, 0.10, 0.00, 0.00,…\n$ pct_chg_mark         <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ event_base_change    <dbl> 0.00000000, 0.11864407, -0.02272727, -0.06201550,…\n$ group_number         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ numeric_group_number <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ group_event_number   <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ x                    <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ mean_event_change    <dbl> 0.00000000, 0.03849647, -0.06815622, -0.04991040,…\n$ median_event_change  <dbl> 0.00000000, 0.07222222, -0.06217617, -0.06201550,…\n$ event_change_ci_low  <dbl> 0.00000000, -0.06799693, -0.11669576, -0.09692794…\n$ event_change_ci_high <dbl> 0.000000000, 0.116322976, -0.024699717, 0.0073964…\n$ event_type           <fct> Before, Before, Before, Before, Before, Before, A…\n\n\nLet’s visualize!\n\nts_event_analysis_plot(\n  .data = event_tbl\n)\n\n\n\n\nLet’s see the median now.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\"\n)\n\n\n\n\nNow let’s see it as an interactive plot.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\",\n  .interactive = TRUE\n)\n\n\n\n\n\nNow let’s see all the individual groups.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"individual\",\n  .interactive = TRUE\n)\n\n\n\n\n\nSingle direction plotting.\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"backward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nAnd…\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"forward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-lists-2023-01-20/index.html",
    "href": "posts/weekly-rtip-lists-2023-01-20/index.html",
    "title": "Another Post on Lists",
    "section": "",
    "text": "Introduction\nManipulating lists in R is a powerful tool for organizing and analyzing data. Here are a few common ways to manipulate lists:\n\nIndexing: Lists can be indexed using square brackets “[ ]” and numeric indices. For example, to access the first element of a list called “mylist”, you would use the expression “mylist[1]”.\nSubsetting: Lists can be subsetted using the same square bracket notation, but with a logical vector indicating which elements to keep. For example, to select all elements of “mylist” that are greater than 5, you would use the expression “mylist[mylist > 5]”.\nModifying elements: Elements of a list can be modified by assigning new values to them using the assignment operator “<-”. For example, to change the third element of “mylist” to 10, you would use the expression “mylist[3] <- 10”.\nAdding elements: New elements can be added to a list using the concatenation operator “c()” or the “append()” function. For example, to add the number 7 to the end of “mylist”, you would use the expression “mylist <- c(mylist, 7)”.\nRemoving elements: Elements can be removed from a list using the “-” operator. For example, to remove the second element of “mylist”, you would use the expression “mylist <- mylist[-2]”.\n\n\n\nExamples\nHere is an example of how these methods can be used to manipulate a list in R:\n\nmylist <- list(1,2,3,4,5)\n\n# Indexing\nmylist[[1]] # Returns 1\n\n[1] 1\n\n# Subsetting\nmylist[mylist > 3] # Returns 4 & 5\n\n[[1]]\n[1] 4\n\n[[2]]\n[1] 5\n\n# Modifying elements\nmylist[[3]] <- 10\nmylist # Returns 1 2 10 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n# Adding elements\nmylist <- c(mylist, 7)\nmylist # Returns 1 2 10 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n[[6]]\n[1] 7\n\n# Removing elements\nmylist[-3]\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 4\n\n[[4]]\n[1] 5\n\n[[5]]\n[1] 7\n\nmylist # Returns 1 2 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n[[6]]\n[1] 7\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidyaml-2023-01-13/index.html",
    "href": "posts/weekly-rtip-tidyaml-2023-01-13/index.html",
    "title": "The building of {tidyAML}",
    "section": "",
    "text": "Introduction\nYesterday I posted on An Update to {tidyAML} where I was discussing some of my thought process and how things could potentially work for the package.\nToday I want to showcase how the function fast_regression_parsnip_spec_tbl() and it’s complimentary function fast_classification_parsnip_spec_tbl() actually work or maybe don’t work for that matter.\nWe are going to pick on fast_regression_parsnip_spec_tbl() in today’s post. The point of it is that it creates a tibble of parsnip regression model specifications. This will create a tibble of 46 different regression model specifications which can be filtered. The model specs are created first and then filtered out. This will only create models for regression problems. To find all of the supported models in this package you can visit the parsnip search page\n\n\nFunction\nFirst let’s take a look at the function call itself.\n\nfast_regression_parsnip_spec_tbl(\n  .parsnip_fns = \"all\", \n  .parsnip_eng = \"all\"\n  )\n\nNow let’s take a look at the arguments:\n\n.parsnip_fns - The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(\"linear_reg\",\"cubist_rules\")\n.parsnip_eng - The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c('lm', 'glm')\n\nThe workhorse to this function is the internal_make_spec_tbl() function. This is the one that will be the subject of the post. Let’s take a look at it’s inner workings, afterall this is open source.\n\ninternal_make_spec_tbl <- function(.data){\n\n  # Checks ----\n  df <- dplyr::as_tibble(.data)\n\n  nms <- unique(names(df))\n\n  if (!\".parsnip_engine\" %in% nms | !\".parsnip_mode\" %in% nms | !\".parsnip_fns\" %in% nms){\n    rlang::abort(\n      message = \"The model tibble must come from the class/reg to parsnip function.\",\n      use_cli_format = TRUE\n    )\n  }\n\n  # Make tibble ----\n  mod_spec_tbl <- df %>%\n    dplyr::mutate(\n      model_spec = purrr::pmap(\n        dplyr::cur_data(),\n        ~ match.fun(..3)(mode = ..2, engine = ..1)\n      )\n    ) %>%\n    # add .model_id column\n    dplyr::mutate(.model_id = dplyr::row_number()) %>%\n    dplyr::select(.model_id, dplyr::everything())\n\n  # Return ----\n  return(mod_spec_tbl)\n\n}\n\nLet’s examine this (and it is currently changing form in a github issue). Firstly, we are taking in a data.frame/tibble that has to have certain names in it (this is going to change and look for a class instead). Once this determination is TRUE we then proceed to the meat and potatoes of it. The internal mod_spec_tbl is made using mutate, pmap, cur_data and match.fun. What this does essentially is the following:\n\nmutate a column called model_spec\nUse the {purrr} function pmap which maps over several columns in parallel to create the model spec.\nInside of the pmap we use cur_data() to get the current line where we match the function using match.fun (which takes a character string of the function, this means the library needs to be loaded) we supply the column it is in and then we supply the arguments we want.\nWe give it a numeric model id\nWe then ensure that the .model_id column is first.\n\n\n\nExample\nLet’s see it in action!\n\nlibrary(tidyAML) # Not yet available, you can install from GitHub though\n\nfast_regression_parsnip_spec_tbl()\n\n# A tibble: 46 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n# … with 36 more rows\n\n\nSo we see we get a nicely generated tibble of output that matchs a model spec to the .model_id and to the appropriate parsnip engine and mode\nWe can also choose the models we may want by giving either arguments to the .parsnip_engine parameter or .parsnip_fns or both.\n\nlibrary(dplyr)\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n11        11 stan_glmer      regression    linear_reg   <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 glm             regression    linear_reg   <spec[+]> \n3         3 glm             regression    poisson_reg  <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = \"glm\") %>%\n  pull(model_spec)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n[[2]]\nPoisson Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "",
    "text": "Many times someone may want to see a summary or cumulative statistic for a given set of data or even from several simulations of data. I went over bootstrap plotting earlier this month, and this is a form of what we will go over today although slightly more restrictive.\nI have decided to make today my weekly r-tip because tomorrow is Thanksgiving here in the US and I am taking an extended holiday so I won’t be back until Monday.\nToday’s function and weekly tip is on tidy_stat_tbl(). It is meant to be used with a tidy_ distribution function. Let’s take a look."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Single Simulation",
    "text": "Single Simulation\nLet’s go over some examples. Firstly, we will go over all the different .return_type’s of a single simulation of tidy_normal() using the quantile function.\nVector Output BE CAREFUL IT USES SAPPLY\n\nlibrary(TidyDensity)\n\nset.seed(123)\ntn <- tidy_normal()\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = quantile,\n  na.rm = TRUE,\n  probs = c(0.025, 0.5, 0.975)\n  )\n\n      sim_number_1\n2.5%   -1.59190149\n50%    -0.07264039\n97.5%   1.77074730\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n$sim_number_1\n       2.5%         50%       97.5% \n-1.59190149 -0.07264039  1.77074730 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nNow let’s take a look with multiple simulations."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Multiple Simulations",
    "text": "Multiple Simulations\nLet’s set our simulation count to 5. While this is not a large amount it will serve as a good illustration on the outputs.\n\nns <- 5\nf  <- quantile\nnr <- TRUE\np  <- c(0.025, 0.975)\n\nOk let’s run the same simulations but with the updated params.\nVector Output BE CAREFUL IT USES SAPPLY\n\nset.seed(123)\ntn <- tidy_normal(.num_sims = ns)\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = f,\n  na.rm = nr,\n  probs = p\n  )\n\n      sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n2.5%     -1.591901    -1.474945    -1.656679    -1.258156    -1.309749\n97.5%     1.770747     1.933653     1.894424     2.098923     1.943384\n\ntidy_stat_tbl(\n  tn, y, .return_type = \"vector\",\n  .fns = f, na.rm = nr\n)\n\n     sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n0%    -1.96661716   -2.3091689   -2.0532472  -1.31080153   -1.3598407\n25%   -0.55931702   -0.3612969   -0.9505826  -0.49541417   -0.7140627\n50%   -0.07264039    0.1525789   -0.3048700  -0.07675993   -0.2240352\n75%    0.69817699    0.6294358    0.2900859   0.55145766    0.5287605\n100%   2.16895597    2.1873330    2.1001089   3.24103993    2.1988103\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\n$sim_number_2\n        0%        25%        50%        75%       100% \n-2.3091689 -0.3612969  0.1525789  0.6294358  2.1873330 \n\n$sim_number_3\n        0%        25%        50%        75%       100% \n-2.0532472 -0.9505826 -0.3048700  0.2900859  2.1001089 \n\n$sim_number_4\n         0%         25%         50%         75%        100% \n-1.31080153 -0.49541417 -0.07675993  0.55145766  3.24103993 \n\n$sim_number_5\n        0%        25%        50%        75%       100% \n-1.3598407 -0.7140627 -0.2240352  0.5287605  2.1988103 \n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr, \n  probs = p\n)\n\n$sim_number_1\n     2.5%     97.5% \n-1.591901  1.770747 \n\n$sim_number_2\n     2.5%     97.5% \n-1.474945  1.933653 \n\n$sim_number_3\n     2.5%     97.5% \n-1.656679  1.894424 \n\n$sim_number_4\n     2.5%     97.5% \n-1.258156  2.098923 \n\n$sim_number_5\n     2.5%     97.5% \n-1.309749  1.943384 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <chr>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <chr> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <fct>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <fct> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nOk, now that we have shown that, let’s ratchet up the simulations so we can see the true difference in using the .use_data_tbl parameter when simulations are large. We are going to use {rbenchmark} for"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Benchmarking",
    "text": "Benchmarking\nHere we go. We are going to make a tidy_bootstrap() of the mtcars$mpg data which will produce 2000 simulations, we will replicate this 25 times.\n\nlibrary(rbenchmark)\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# Get the interesting vector, well for this anyways\nx <- mtcars$mpg\n\n# Bootstrap the vector (2k simulations is default)\ntb <- tidy_bootstrap(x) %>%\n  bootstrap_unnest_tbl()\n\nbenchmark(\n  \"tibble\" = {\n    tidy_stat_tbl(tb, y, IQR, \"tibble\")\n  },\n  \"data.table\" = {\n    tidy_stat_tbl(tb, y, IQR, .use_data_table = TRUE, type = 7)\n  },\n  \"sapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"vector\")\n  },\n  \"lapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"list\")\n  },\n  replications = 25,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) %>%\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1 data.table           25    4.11    1.000      3.33     0.11\n2     lapply           25   24.14    5.873     20.02     0.38\n3     sapply           25   25.11    6.109     21.01     0.28\n4     tibble           25   33.18    8.073     27.45     0.51\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html",
    "title": "Updates to {healthyverse} packages",
    "section": "",
    "text": "I have made several updates to {healthyverse}, this has resulted in new releases to CRAN for {healthyR.ai}, {healthyR.ts}, and {TidyDesnsity}."
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "title": "Updates to {healthyverse} packages",
    "section": "TidyDensity",
    "text": "TidyDensity\nFor TidyDensity a new distribution was added, welcome tidy_bernoulli(). This distribution also comes with the standard util_distname_param_estimate() and the util_distname_stats_tbl() functions. Let’s take a look at the function calls.\n\ntidy_bernoulli(.n = 50, .prob = 0.1, .num_sims = 1)\n\nutil_bernoulli_param_estimate(.x, .auto_gen_empirical = TRUE)\n\nutil_bernoulli_stats_tbl(.data)\n\nLet’s see them in use.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntb <- tidy_bernoulli()\n\ntb\n\n# A tibble: 50 × 7\n   sim_number     x     y      dx     dy     p     q\n   <fct>      <int> <int>   <dbl>  <dbl> <dbl> <dbl>\n 1 1              1     0 -0.338  0.0366   0.9     0\n 2 1              2     0 -0.304  0.0866   0.9     0\n 3 1              3     0 -0.270  0.187    0.9     0\n 4 1              4     0 -0.236  0.369    0.9     0\n 5 1              5     0 -0.201  0.663    0.9     0\n 6 1              6     0 -0.167  1.09     0.9     0\n 7 1              7     0 -0.133  1.63     0.9     0\n 8 1              8     1 -0.0988 2.22     1       1\n 9 1              9     0 -0.0646 2.76     0.9     0\n10 1             10     0 -0.0304 3.14     0.9     0\n# … with 40 more rows\n\nutil_bernoulli_param_estimate(tb$y)\n\n$combined_data_tbl\n# A tibble: 100 × 8\n   sim_number     x     y      dx     dy     p     q dist_type\n   <fct>      <int> <dbl>   <dbl>  <dbl> <dbl> <dbl> <fct>    \n 1 1              1     0 -0.338  0.0366  0.92     0 Empirical\n 2 1              2     0 -0.304  0.0866  0.92     0 Empirical\n 3 1              3     0 -0.270  0.187   0.92     0 Empirical\n 4 1              4     0 -0.236  0.369   0.92     0 Empirical\n 5 1              5     0 -0.201  0.663   0.92     0 Empirical\n 6 1              6     0 -0.167  1.09    0.92     0 Empirical\n 7 1              7     0 -0.133  1.63    0.92     0 Empirical\n 8 1              8     1 -0.0988 2.22    1        0 Empirical\n 9 1              9     0 -0.0646 2.76    0.92     0 Empirical\n10 1             10     0 -0.0304 3.14    0.92     0 Empirical\n# … with 90 more rows\n\n$parameter_tbl\n# A tibble: 1 × 8\n  dist_type samp_size   min   max  mean variance sum_x  prob\n  <chr>         <int> <dbl> <dbl> <dbl>    <dbl> <dbl> <dbl>\n1 Bernoulli        50     0     1  0.08   0.0736     4  0.08\n\nutil_bernoulli_stats_tbl(tb) %>%\n  glimpse()\n\nRows: 1\nColumns: 18\n$ tidy_function      <chr> \"tidy_bernoulli\"\n$ function_call      <chr> \"Bernoulli c(0.1)\"\n$ distribution       <chr> \"Bernoulli\"\n$ distribution_type  <chr> \"discrete\"\n$ points             <dbl> 50\n$ simulations        <dbl> 1\n$ mean               <dbl> 0.1\n$ mode               <chr> \"0\"\n$ coeff_var          <dbl> 0.09\n$ skewness           <dbl> 2.666667\n$ kurtosis           <dbl> 5.111111\n$ mad                <dbl> 0.5\n$ entropy            <dbl> 0.325083\n$ fisher_information <dbl> 11.11111\n$ computed_std_skew  <dbl> 3.096281\n$ computed_std_kurt  <dbl> 10.58696\n$ ci_lo              <dbl> 0\n$ ci_hi              <dbl> 1"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ai",
    "text": "healthyR.ai\nThis was a minor patch release that exported some previously internal only functions and fixed an error with the custom recipe steps. One of the functions that has been exported is hai_data_impute()\nLet’s take a look.\n\nhai_data_impute(\n  .recipe_object = NULL,\n  ...,\n  .seed_value = 123,\n  .type_of_imputation = \"mean\",\n  .number_of_trees = 25,\n  .neighbors = 5,\n  .mean_trim = 0,\n  .roll_statistic,\n  .roll_window = 5\n)\n\nLet’s take a look at an example of it’s use.\n\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(healthyR.ai)\n\ndate_seq <- seq.Date(from = as.Date(\"2013-01-01\"), length.out = 100, by = \"month\")\nval_seq <- rep(c(rnorm(9), NA), times = 10)\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 NA     \n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nhai_data_impute(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_imputation = \"roll\",\n  .roll_statistic = median\n)$impute_rec_obj %>%\n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 -0.322 \n# … with 90 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ts",
    "text": "healthyR.ts\nThis was a minor patch release fixing the function ts_lag_correlation() when the column that was the value was not explicitly called…value.\nThank you!"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "",
    "text": "Many times in the real world we have a data set which is actually a sample as we typically do not know what the actual population is. This is where bootstrapping tends to come into play. It allows us to get a hold on what the possible parameter values are by taking repeated samples of the data that is available to us.\nAt it’s core it is a resampling method with replacement where it assigns measures of accuracy to the sample estimates. Here is the Wikipedia Article for bootstrapping.\nIn this post I am going to go over how to use the bootstrap function set with {TidyDensity}. You can find the pkgdown site with all function references here: TidyDensity"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Mean",
    "text": "Cumulative Mean\n\ntb %>%\n  bootstrap_stat_plot(.value = y)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE)\n\n\n\n\n\nYou can see from this output that the statistic you choose is printed in the chart title and on the y axis, the caption will also tell you how many simulations are present. Lets look at skewness as another example."
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Skewness",
    "text": "Cumulative Skewness\n\nsc <- \"cskewness\"\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE,\n                      .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .stat = sc,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE,\n                      .show_groups = TRUE,\n                      .stat = sc)\n\n\n\n\n\nVolia!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Steve On Data",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nI am using this as a site to host all of the tips and tricks for R/SQL and data that I will post to my LinkedIn, Twitter and Telegram channels."
  },
  {
    "objectID": "posts/rtip-2023-01-26/index.html",
    "href": "posts/rtip-2023-01-26/index.html",
    "title": "Transforming Your Data: A Guide to Popular Methods and How to Implement Them with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nTransforming data refers to the process of changing the scale or distribution of a variable in order to make it more suitable for analysis. There are many different methods for transforming data, and each has its own specific use case.\n\nBox-Cox: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses a power transformation to adjust the scale of the data.\nBasis Spline: This is a type of non-parametric regression that uses splines (piecewise polynomials) to model the relationship between a dependent variable and one or more independent variables.\nLog: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses the logarithm function to adjust the scale of the data.\nLogit: This is a method for transforming binary data (i.e., data with only two possible values) into a continuous scale. It uses the logistic function to adjust the scale of the data.\nNatural Spline: This is a type of non-parametric regression that uses splines (piecewise polynomials) to model the relationship between a dependent variable and one or more independent variables, where the splines are chosen to be as smooth as possible.\nRectified Linear Unit (ReLU): This is a type of activation function used in artificial neural networks. It is used to introduce non-linearity in the output of a neuron.\nSquare Root: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses the square root function to adjust the scale of the data.\nYeo-Johnson: This is a power transformation that works well for data that is positively or negatively skewed. It is a generalization of the Box-Cox transformation and handles zero and negative data.\n\nThe R library {healthyR.ai} provides a function called hai_data_transform() that allows users to easily apply any of these transforms to their data. The function takes in the data and the type of transformation as arguments, and returns the transformed data. This makes it easy for users to experiment with different transformations and see which one works best for their data.\n\n\nFunction\nLet’s take a look at the full function call.\n\nhai_data_transform(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"log\",\n  .bc_limits = c(-5, 5),\n  .bc_num_unique = 5,\n  .bs_deg_free = NULL,\n  .bs_degree = 3,\n  .log_base = exp(1),\n  .log_offset = 0,\n  .logit_offset = 0,\n  .ns_deg_free = 2,\n  .rel_shift = 0,\n  .rel_reverse = FALSE,\n  .rel_smooth = FALSE,\n  .yj_limits = c(-5, 5),\n  .yj_num_unique = 5\n)\n\nNow let’s go over the arguments to the parameters.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“boxcox”\n“bs”\n“log”\n“logit”\n“ns”\n“relu”\n“sqrt”\n“yeojohnson\n\n.bc_limits - A length 2 numeric vector defining the range to compute the transformation parameter lambda.\n.bc_num_unique - An integer to specify minimum required unique values to evaluate for a transformation\n.bs_deg_free - The degrees of freedom for the spline. As the degrees of freedom for a spline increase, more flexible and complex curves can be generated. When a single degree of freedom is used, the result is a rescaled version of the original data.\n.bs_degree - Degree of polynomial spline (integer).\n.log_base - A numeric value for the base.\n.log_offset - An optional value to add to the data prior to logging (to avoid log(0))\n.logit_offset - A numeric value to modify values of the columns that are either one or zero. They are modifed to be x - offset or offset respectively.\n.ns_deg_free - The degrees of freedom for the natural spline. As the degrees of freedom for a natural spline increase, more flexible and complex curves can be generated. When a single degree of freedom is used, the result is a rescaled version of the original data.\n.rel_shift - A numeric value dictating a translation to apply to the data.\n.rel_reverse - A logical to indicate if the left hinge should be used as opposed to the right hinge.\n.rel_smooth - A logical indicating if hte softplus function, a smooth approximation to the rectified linear transformation, should be used.\n.yj_limits - A length 2 numeric vector defining the range to compute the transformation parameter lambda.\n.yj_num_unique - An integer where data that have less possible values will not be evaluated for a transformation.\n\n\n\nExamples\nLet’s look over some examples. For an example data set we are going to pick on the mtcars data set as the histogram will prove to be skewed which makes it a good candidate to test these transformations on.\n\ninstall.packages(\"healthyR.ai\")\n\nNow that we have {healthyR.ai} installed we can get to work. It does use the {recipes} package underneath so you will need to have that installed as well. Let’s look at the histogram of mtcars now.\n\nmpg_vec <- mtcars$mpg\n\nhist(mpg_vec)\n\n\n\nplot(density(mpg_vec))\n\n\n\n\nFirst up, Box-Cox\n\nlibrary(healthyR.ai)\nlibrary(recipes)\n\nro <- recipe(mpg ~ wt, data = mtcars)\n\nboxcox_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"boxcox\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(boxcox_vec))\n\n\n\n\nBasis Spline\n\nbs_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"bs\"\n)$scale_rec_obj %>%\n  get_juiced_data()\n\nplot(density(bs_vec$mpg_bs_1))\n\n\n\nplot(density(bs_vec$mpg_bs_2))\n\n\n\nplot(density(bs_vec$mpg_bs_3))\n\n\n\n\nLog\n\nlog_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"log\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(log_vec))\n\n\n\n\nYeo-Johnson\n\nyj_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"yeojohnson\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(yj_vec))\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyr-2023-01-27/index.html",
    "href": "posts/weekly-rtip-healthyr-2023-01-27/index.html",
    "title": "Service Line Grouping with {healthyR}",
    "section": "",
    "text": "Introduction\nHealthcare data analysis can be a complex and time-consuming task, but it doesn’t have to be. Meet {healthyR}, your new go-to R package for all things healthcare data analysis. With {healthyR}, you can easily and efficiently analyze your healthcare data, and make sense of the information it contains.\nOne of the key features of {healthyR} is the service_line_augment() function. This function is designed to help you quickly and easily append a vector to a data.frame or tibble that is passed to the .data parameter. In order to use this function, you will need a data.frame or tibble with a principal diagnosis column, a principal procedure column, and a column for the DRG number. These are needed so that the function can join the dx_cc_mapping and px_cc_mapping columns to provide the service line.\nThe service_line_augment() function is especially useful for analyzing healthcare data that is coded using ICD Version 10. This version of the ICD coding system is widely used in the healthcare industry, and the service_line_augment() function is specifically designed to work with it. With this function, you can quickly and easily append a vector to your data.frame or tibble that provides the service line for each visit.\nIn addition to the service_line_augment() function, {healthyR} also includes a wide range of other useful tools and functions for healthcare data analysis. Whether you’re looking to analyze claims data, clinical data, or any other type of healthcare data, {healthyR} has you covered.\nSo why wait? Download {healthyR} today and start making sense of your healthcare data! With {healthyR}, you can easily and efficiently analyze your healthcare data, and make sense of the information it contains.\n\n\nFunction\nLet’s take a look at the full function call.\n\nservice_line_augment(.data, .dx_col, .px_col, .drg_col)\n\nNow let’s look at the arguments to the parameters.\n\n.data - The data being passed that will be augmented by the function.\n.dx_col - The column containing the Principal Diagnosis for the discharge.\n.px_col - The column containing the Principal Coded Procedure for the discharge. It is possible that this could be blank.\n.drg_col - The DRG Number coded to the inpatient discharge.\n\nNow for some examples.\n\n\nExample\nFirst if you have not already, install {healthyR}\n\ninstall.packages(\"healthyR\")\n\nHere we go.\n\nlibrary(healthyR)\n\ndf <- data.frame(\n  dx_col = \"F10.10\",\n  px_col = NA,\n  drg_col = \"896\"\n)\n\nservice_line_augment(\n  .data = df,\n  .dx_col = dx_col,\n  .px_col = px_col,\n  .drg_col = drg_col\n)\n\n# A tibble: 1 × 4\n  dx_col px_col drg_col service_line \n  <chr>  <lgl>  <chr>   <chr>        \n1 F10.10 NA     896     alcohol_abuse\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-30/index.html",
    "href": "posts/rtip-2023-01-30/index.html",
    "title": "{healthyR.ts}: The New and Improved Library for Time Series Analysis",
    "section": "",
    "text": "Introduction\nAre you looking for a powerful and efficient library for time series analysis? Look no further than {healthyR.ts}! This library has recently been updated with new functions and improvements, making it easier for you to analyze and visualize your time series data.\nOne of the new functions in {healthyR.ts} is ts_geometric_brownian_motion(). This function allows you to generate multiple Brownian motion simulations at once, saving you time and effort. With this feature, you can easily generate multiple simulations to compare and analyze different scenarios.\nAnother new function, ts_brownian_motion_augment(), enables you to add a Brownian motion to a time series that you provide. This is a great tool for analyzing the impact of random variations on your data.\nThe ts_geometric_brownian_motion_augment() function generates a geometric Brownian motion, allowing you to study the effects of compounding growth or decay in your time series data. And, with the ts_brownian_motion_plot() function, you can easily plot both augmented and non-augmented Brownian motion plots, giving you a visual representation of your data.\nIn addition to the new functions, {healthyR.ts} has also made several minor fixes and improvements. For example, the ts_brownian_motion() function has been updated and optimized, resulting in a 49x speedup due to vectorization. Additionally, all Brownian motion functions now have an attribute of .motion_type, making it easier to track and identify your data.\nWith all of these new features and improvements, {healthyR.ts} is the ideal library for anyone looking to analyze and visualize time series data. So, if you want to take your time series analysis to the next level, install {healthyR.ts} today!\n\n\nFunction\nLet’s take a look at the new functions.\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\nIts arguments.\n\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.initial_value - Integer representing the initial value.\n.delta_time - Time step size.\n.return_tibble - The default is TRUE. If set to FALSE then an object of class matrix will be returned.\n\n\nts_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = NULL\n)\n\nIts arguments.\n\n.data - The data.frame/tibble being augmented.\n.date_col - The column that holds the date.\n.value_col - The value that is going to get augmented. The last value of this column becomes the initial value internally.\n.time - How many time steps ahead.\n.num_sims - How many simulations should be run.\n.delta_time - Time step size.\n\n\nts_geometric_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .num_sims = 10,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .delta_time = 1/365\n)\n\nIts arguments.\n\n.data - The data you are going to pass to the function to augment.\n.date_col - The column that holds the date\n.value_col - The column that holds the value\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.delta_time - Time step size.\n\n\nts_brownian_motion_plot(\n  .data, \n  .date_col, \n  .value_col, \n  .interactive = FALSE\n)\n\nIts arguments.\n\n.data - The data you are going to pass to the function to augment.\n.date_col - The column that holds the date\n.value_col - The column that holds the value\n.interactive - The default is FALSE, TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nFirst make sure you install {healthyR.ts} if you do not yet already have it, otherwise update it to gain th enew functionality.\n\ninstall.packages(\"healthyR.ts\")\n\nNow let’s take a look at ts_geometric_brownian_motion().\n\nlibrary(healthyR.ts)\n\nts_geometric_brownian_motion()\n\n# A tibble: 2,600 × 3\n   sim_number        t     y\n   <fct>         <int> <dbl>\n 1 sim_number 1      1   100\n 2 sim_number 2      1   100\n 3 sim_number 3      1   100\n 4 sim_number 4      1   100\n 5 sim_number 5      1   100\n 6 sim_number 6      1   100\n 7 sim_number 7      1   100\n 8 sim_number 8      1   100\n 9 sim_number 9      1   100\n10 sim_number 10     1   100\n# … with 2,590 more rows\n\n\nNow let’s take a look at ts_brownian_motion_augment().\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value\n)\n\n# A tibble: 1,041 × 3\n   sim_number  date_col    value\n   <fct>       <date>      <dbl>\n 1 actual_data 2022-01-01 -0.303\n 2 actual_data 2022-01-02 -1.17 \n 3 actual_data 2022-01-03 -1.44 \n 4 actual_data 2022-01-04 -0.682\n 5 actual_data 2022-01-05 -2.31 \n 6 actual_data 2022-01-06 -1.19 \n 7 actual_data 2022-01-07 -0.454\n 8 actual_data 2022-01-08 -1.83 \n 9 actual_data 2022-01-09  0.659\n10 actual_data 2022-01-10 -0.150\n# … with 1,031 more rows\n\n\nNow ts_geometric_brownian_motion_augment().\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_geometric_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value\n)\n\n# A tibble: 291 × 3\n   sim_number  date_col    value\n   <fct>       <date>      <dbl>\n 1 actual_data 2022-01-01 -1.47 \n 2 actual_data 2022-01-02 -1.63 \n 3 actual_data 2022-01-03  1.01 \n 4 actual_data 2022-01-04  1.44 \n 5 actual_data 2022-01-05 -1.05 \n 6 actual_data 2022-01-06 -0.599\n 7 actual_data 2022-01-07 -0.393\n 8 actual_data 2022-01-08  1.06 \n 9 actual_data 2022-01-09 -0.121\n10 actual_data 2022-01-10 -0.349\n# … with 281 more rows\n\n\nNow for ts_brownian_motion_plot().\n\nts_geometric_brownian_motion() %>%\n  ts_brownian_motion_plot(.date_col = t, .value_col = y)\n\n\n\n\n\nts_brownian_motion() %>%\n  ts_brownian_motion_plot(t, y, .interactive = TRUE)\n\n\n\n\n\nAnd with the augmenting functions\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value,\n  .time = 30,\n  .num_sims = 30\n) %>%\n  ts_brownian_motion_plot(date_col, value, TRUE)\n\n\n\n\n\nAnd with a static ggplot2 plot.\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value,\n  .time = 30,\n  .num_sims = 30\n) %>%\n  ts_brownian_motion_plot(date_col, value)\n\n\n\n\nThank you for reading, and Voila!"
  },
  {
    "objectID": "posts/rtip-2023-01-31/index.html",
    "href": "posts/rtip-2023-01-31/index.html",
    "title": "Median: A Simple Way to Detect Excess Events Over Time with {healthyR}",
    "section": "",
    "text": "Introduction\nAs we collect data over time, it’s important to look for patterns and trends that can help us understand what’s happening. One common way to do this is to look at the median of the data. The median is the middle value of a set of numbers, and it can be a useful tool for detecting whether there is an excess of events, either positive or negative, occurring over time.\nBenefits of Looking at Median:\n\nShows the central tendency: The median gives us a good idea of the central tendency of the data. This can help us understand what’s typical and what’s not.\nResistant to outliers: Unlike the mean, the median is not affected by outliers. This means that if there are a few extreme values in the data, the median will not be skewed by them.\nEasy to understand: The median is easy to understand, even for people who are not familiar with statistics.\n\nUsing the R Library {healthyR} provides a convenient way to perform median analysis. The function ts_median_excess_plt() can be used to plot the median of an event over time and detect any excess events that may be occurring. This function is designed to be user-friendly, so even if you’re not an expert in statistics, you can still use it to gain valuable insights into your data.\nIn conclusion, looking at the median of an event over time can be a useful tool for detecting excess events, either positive or negative. The R library {healthyR} provides a convenient way to perform this analysis with the function ts_median_excess_plt(). Give it a try and see what insights you can uncover in your own data!\n\n\nFunction\nHere is the full function call.\n\nts_median_excess_plt(\n  .data,\n  .date_col,\n  .value_col,\n  .x_axis,\n  .ggplot_group_var,\n  .years_back\n)\n\nHere are its arguments.\n\n.data - The data that is being analyzed, data must be a tibble/data.frame.\n.date_col - The column of the tibble that holds the date.\n.value_col - The column that holds the value of interest.\n.x_axis - What is the be the x-axis, day, week, etc.\n.ggplot_group_var - The variable to group the ggplot on.\n.years_back - How many yeas back do you want to go in order to compute the median value.\n\n\n\nExample\nFirst make sure you have the package installed.\n\ninstall.packages(\"healthyR\")\n\nNow for an example. The data is required to be in a certain format, this function is dated, meaning it was one of the first ones I wrote so I will be taking time to improve it in the future. We are using data from my {healthyR.data]} package.\n\nlibrary(healthyR.data)\nlibrary(lubridate)\nlibrary(healthyR)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(ggplot2)\n\ndf <- healthyR_data %>%\n  filter_by_time(\n    .date_var = visit_start_date_time,\n    .start_date = \"2012\",\n    .end_date = \"2019\"\n  ) %>%\n  filter(ip_op_flag == \"I\") %>%\n  select(visit_id, visit_start_date_time) %>%\n  mutate(\n    visit_start_date_time = as.Date(visit_start_date_time, \"%Y%M%D\"),\n    record = 1\n    ) %>%\n  summarise_by_time(\n    .date_var = visit_start_date_time,\n    visits = sum(record)\n  ) %>%\n  ts_signature_tbl(\n    .date_col = visit_start_date_time\n  )\n\nOk now that we have our data, let’s take a look at it using glimpse()\n\nglimpse(df)\n\nRows: 2,922\nColumns: 30\n$ visit_start_date_time <date> 2012-01-01, 2012-01-02, 2012-01-03, 2012-01-04,…\n$ visits                <dbl> 34, 52, 53, 44, 46, 55, 42, 29, 50, 55, 50, 43, …\n$ index.num             <dbl> 1325376000, 1325462400, 1325548800, 1325635200, …\n$ diff                  <dbl> NA, 86400, 86400, 86400, 86400, 86400, 86400, 86…\n$ year                  <int> 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ year.iso              <int> 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ half                  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ quarter               <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ month                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ month.xts             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ month.lbl             <ord> January, January, January, January, January, Jan…\n$ day                   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ hour                  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ minute                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ second                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hour12                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ am.pm                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ wday                  <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, …\n$ wday.xts              <int> 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, …\n$ wday.lbl              <ord> Sunday, Monday, Tuesday, Wednesday, Thursday, Fr…\n$ mday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ qday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ yday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ mweek                 <int> 5, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, …\n$ week                  <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, …\n$ week.iso              <int> 52, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3,…\n$ week2                 <int> 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ week3                 <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, …\n$ week4                 <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, …\n$ mday7                 <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, …\n\n\nNow to visualize it.\n\ndf %>%\n  ts_median_excess_plt(\n    .date_col = visit_start_date_time,\n    .value_col = visits,\n    .x_axis = month.lbl,\n    .ggplot_group_var = year,\n    .years_back = 3\n  ) +\n  labs(\n    y = \"Excess Visits\",\n    title = \"Excess Visits by Month YoY\"\n  ) + \n  theme(axis.text.x=element_text(angle = -90, hjust = 0))\n\n\n\n\nSo from here what we can see is that looking back in time over the visits data that the current year (the max year in the data) shows that it is significantly under previous years median values by month.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-01/index.html",
    "href": "posts/rtip-2023-02-01/index.html",
    "title": "Attributes in R Functions: An Overview",
    "section": "",
    "text": "R is a powerful programming language that is widely used for data analysis, visualization, and machine learning. One of the features of R that makes it versatile and flexible is the ability to assign attributes to functions. Attributes are metadata associated with an object in R, and they can be used to store additional information about the function or to modify the behavior of the function.\nIn this blog post, we will discuss what attributes are, how they can be useful, and how they can be used inside R functions.\n\n\nAttributes are pieces of information that are stored alongside an object in R. Functions are objects in R, and they can have attributes associated with them. Some of the common attributes associated with functions in R include:\n\nformals: This attribute stores the arguments of the function and their default values.\nsrcref: This attribute stores the source code of the function, including the line numbers of the code.\nenvironment: This attribute stores the environment in which the function was defined.\n\n\n\n\nAttributes can be useful in R functions in several ways, including:\n\nDebugging: Attributes can be used to store information that can be used to debug functions. For example, the srcref attribute can be used to retrieve the source code of the function and the line numbers of the code, which can be useful when trying to identify the source of an error.\nMetadata: Attributes can be used to store metadata about the function, such as the author, version, and date of creation. This information can be used to keep track of the function and to provide information about its purpose and usage.\nModifying Function Behavior: Attributes can be used to modify the behavior of the function. For example, the environment attribute can be used to set the environment in which the function is executed. This can be useful when creating closures or when using functions in a specific context.\n\n\n\n\nTo access or modify the attributes of a function in R, you can use the attributes() function. For example, to retrieve the formals attribute of a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\nformals(f)\n\n$x\n\n\n$y\n\n\nTo add an attribute to a function, you can use the attr() function. For example, to add a version attribute to a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattr(f, \"version\") <- \"1.0\"\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\n$version\n[1] \"1.0\"\n\n\nTo remove an attribute from a function, you can use the attributes() function with the NULL value. For example, to remove the version attribute from a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattr(f, \"version\") <- \"1.0\"\nattributes(f)$version <- NULL\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\n\nConclusion\nAttributes are a useful feature in R functions that can be used to store additional information about the function, to debug the function, and to modify its behavior. By using attributes, you can make your functions more versatile, flexible, and easier to work with."
  },
  {
    "objectID": "posts/rtip-2023-02-02/index.html",
    "href": "posts/rtip-2023-02-02/index.html",
    "title": "Diverging Lollipop Chart: A Visual Tool for Comparing Data with {healthyR}",
    "section": "",
    "text": "Introduction\nA diverging lollipop chart is a useful tool for comparing data that falls into two categories, usually indicated by different colors. This type of chart is particularly well-suited for comparing the differences between two data sets and for identifying which data points are contributing most to the differences.\nThe R package {healthyR} offers a function called diverging_lollipop_plt() that can be used to create a diverging lollipop chart. This function has several parameters that can be used to customize the chart to meet your specific needs.\nIn conclusion, the diverging lollipop chart is a useful tool for comparing data sets and can provide insights into the differences between two sets of data. The diverging_lollipop_plt() function from the {healthyR} package is a great option for creating this type of chart, as it offers a range of customization options to meet your specific needs. Whether you’re working with data related to business, finance, or any other field, a diverging lollipop chart can be a valuable tool in your visual analysis toolkit.\n\n\nFunction\nLet’s take a look at the full function call.\n\ndiverging_lollipop_plt(\n  .data,\n  .x_axis,\n  .y_axis,\n  .plot_title = NULL,\n  .plot_subtitle = NULL,\n  .plot_caption = NULL,\n  .interactive = FALSE\n)\n\nNow lets see the arguments that get provided to the parameters.\n\n.data - The data to pass to the function, must be a tibble/data.frame.\n.x_axis - The data that is passed to the x-axis. This will also be the x and xend parameters of the geom_segment\n.y_axis - The data that is passed to the y-axis. This will also equal the parameters of yend and label\n.plot_title - Default is NULL\n.plot_subtitle - Default is NULL\n.plot_caption - Default is NULL\n.interactive - Default is FALSE. TRUE returns a plotly plot\n\n\n\nExample\nLet’s see an example.\n\nlibrary(healthyR)\n\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata(\"mtcars\")\nmtcars$car_name <- rownames(mtcars)\nmtcars$mpg_z <- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), 2)\nmtcars$mpg_type <- ifelse(mtcars$mpg_z < 0, \"below\", \"above\")\nmtcars <- mtcars[order(mtcars$mpg_z), ]  # sort\nmtcars$car_name <- factor(mtcars$car_name, levels = mtcars$car_name)\n\ndiverging_lollipop_plt(\n  .data = mtcars, \n  .x_axis = car_name,\n  .y_axis = mpg_z\n)\n\n\n\n\nNow let’s also see the interactive chart.\n\ndiverging_lollipop_plt(\n  .data = mtcars, \n  .x_axis = car_name,\n  .y_axis = mpg_z,\n  .interactive = TRUE\n)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-03/index.html",
    "href": "posts/rtip-2023-02-03/index.html",
    "title": "The Argument Matcher: A Function for Selecting the Right Arguments {tidyAML}",
    "section": "",
    "text": "Introduction\nI am working on finishing up a few things with my new R package {tidyAML} before I release it to CRAN. One of those things is the ability of a user to build a model using a command that might be something like generate_model(). One of the things that is necessary to do is to match the function arguments from the generate_model() to the actual parsnip call.\nThis is where and argument matcher of sorts may come in handy. I am doing this because it will take one most step of abstraction away, and instead of say calling linear_reg() or mars() or something like that, you can just instead use generate_model() and type in your engine or the parsnip function call there.\nNow I am not one hundred percent certain that I’ll actually implement this or not, but the exercise was fun enough that I decided to share it. So let’s get into it.\n\n\nFunction\nHere is the current state of the function.\n\nargument_matcher <- function(.f = \"linear_reg\", .args = list()){\n  \n  # TidyEval ----\n  fns <- as.character(.f)\n  \n  fns_args <- formalArgs(fns)\n  fns_args_list <- as.list(fns_args)\n  names(fns_args_list) <- fns_args\n  \n  arg_list <- .args\n  arg_list_names <- unique(names(arg_list))\n  \n  l <- list(arg_list, fns_args_list)\n  \n  arg_idx <- which(arg_list_names %in% fns_args_list)\n  bad_arg_idx <- which(!arg_list_names %in% fns_args_list)\n  \n  bad_args <- arg_list[bad_arg_idx]\n  bad_arg_names <- unique(names(bad_args))\n  \n  final_args <- arg_list[arg_idx]\n  \n  # Return ----\n  if (length(bad_arg_names > 0)){\n    rlang::inform(\n      message = paste0(\"bad arguments passed: \", bad_arg_names),\n      use_cli_format = TRUE\n    )\n  }\n\n  return(final_args)\n}\n\nWhen working with R functions, it’s not uncommon to encounter a situation where you need to pass arguments to another function. This can be especially challenging when the arguments are not properly matched. Fortunately, the argument_matcher function provides an elegant solution to this problem.\nThe argument_matcher function takes two arguments: .f and .args. The .f argument is a string that specifies the name of the function you want to pass arguments to, while the .args argument is a list that contains the arguments you want to pass to the specified function.\nThe argument_matcher function first uses the formalArgs function to extract the formal arguments of the specified function and store them in fns_args. The names of the formal arguments are then used to create a list, fns_args_list.\nNext, the function extracts the names of the arguments in .args and stores them in arg_list_names. It then checks if the names of the arguments in .args match the names of the formal arguments of the specified function, and stores the matching arguments in final_args. Any arguments that don’t match the formal arguments are stored in bad_args, and a warning message is printed indicating that bad arguments were passed.\nThe final step is to return the final_args list, which contains only the arguments that match the formal arguments of the specified function.\nIn conclusion, the argument_matcher function is a useful tool for ensuring that arguments are properly matched when passed to another function. Whether you’re working with linear regression models or any other type of function, the argument_matcher function will help you select the right arguments and avoid common errors.\n\n\nExample\nLet’s see a simple example.\n\nsuppressPackageStartupMessages(library(tidymodels))\n\nargument_matcher(\n  .args = list(\n    mode = \"regression\", \n    engine = \"lm\",\n    cost = 0.5,\n    trees = 1, \n    mtry = 1\n    )\n  )\n\nbad arguments passed: cost\nbad arguments passed: trees\nbad arguments passed: mtry\n\n\n$mode\n[1] \"regression\"\n\n$engine\n[1] \"lm\"\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-06/inex.html",
    "href": "posts/rtip-2023-02-06/inex.html",
    "title": "Cumulative Measurement Functions with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re looking for an easy-to-use package to calculate cumulative statistics in R, you may want to check out the TidyDensity package. This package offers several functions to calculate cumulative measurements, including mean, median, standard deviation, variance, skewness, kurtosis, harmonic mean, and geometric mean.\n\ncgmean(): Cumulative Geometric Mean\n\nThe cgmean() function calculates the cumulative geometric mean of a set of values. This is the nth root of the product of the first n elements of the set. It’s a useful measurement for sets of values that are multiplied together, such as growth rates.\n\nchmean(): Cumulative Harmonic Mean\n\nThe chmean() function calculates the cumulative harmonic mean of a set of values. This is the inverse of the arithmetic mean of the reciprocals of the values. It’s commonly used for sets of values that represent rates, such as speeds.\n\nckurtosis(): Cumulative Kurtosis\n\nThe ckurtosis() function calculates the cumulative kurtosis of a set of values. Kurtosis is a measure of the peakedness of a distribution, relative to a normal distribution. The cumulative kurtosis calculates the kurtosis of a set of values up to a specific point in the set.\n\ncmean(): Cumulative Mean\n\nThe cmean() function calculates the cumulative mean of a set of values. It’s a measure of the average of the values up to a specific point in the set.\n\ncmedian(): Cumulative Median\n\nThe cmedian() function calculates the cumulative median of a set of values. It’s the value that separates the lower half of the set from the upper half, up to a specific point in the set.\n\ncsd(): Cumulative Standard Deviation\n\nThe csd() function calculates the cumulative standard deviation of a set of values. Standard deviation is a measure of the spread of values in a set. The cumulative standard deviation calculates the standard deviation up to a specific point in the set.\n\ncskewness(): Cumulative Skewness\n\nThe cskewness() function calculates the cumulative skewness of a set of values. Skewness is a measure of the asymmetry of a distribution. The cumulative skewness calculates the skewness up to a specific point in the set.\n\ncvar(): Cumulative Variance\n\nThe cvar() function calculates the cumulative variance of a set of values. Variance is a measure of the spread of values in a set. The cumulative variance calculates the variance up to a specific point in the set.\nIn conclusion, the {TidyDensity} package offers several functions for calculating cumulative statistics, including mean, median, standard deviation, variance, skewness, kurtosis, harmonic mean, and geometric mean. These functions make it easy to calculate cumulative statistics for sets of values in R.\n\n\nFunctions\nAll of the functions perform work strictly on a vector. Because of this I will not go over the function calls separately because they all follow the vectorized for of fun(.x) where .x is the argument passed to the cumulative function.\n\n\nExamples\nHere I will go over some examples of each function use the AirPassengers data set.\n\nlibrary(TidyDensity)\n\nv <- AirPassengers\n\nLet’s start at the top.\nCumulative Geometric Mean:\n\nhead(cgmean(v))\n\n[1] 112.0000 114.9609 120.3810 122.4802 122.1827 124.2311\n\ntail(cgmean(v))\n\n[1] 249.6135 251.1999 252.4577 253.5305 254.2952 255.2328\n\nplot(cgmean(v), type = \"l\")\n\n\n\n\nCumulative Harmonic Mean:\n\nhead(chmean(v))\n\n[1] 112.00000  57.46087  40.03378  30.55222  24.39304  20.66000\n\ntail(chmean(v))\n\n[1] 1.636832 1.632423 1.627194 1.621471 1.614757 1.608744\n\nplot(chmean(v), type = \"l\")\n\n\n\n\nCumulative Kurtosis:\n\nhead(ckurtosis(v))\n\n[1]      NaN 1.000000 1.500000 1.315839 1.597316 1.597850\n\ntail(ckurtosis(v))\n\n[1] 2.668951 2.795314 2.733117 2.674195 2.649894 2.606228\n\nplot(ckurtosis(v), type = \"l\")\n\n\n\n\nCumulative Mean:\n\nhead(cmean(v))\n\n[1] 112.0000 115.0000 120.6667 122.7500 122.4000 124.5000\n\ntail(cmean(v))\n\n[1] 273.1367 275.5143 277.1631 278.4577 279.2378 280.2986\n\nplot(cmean(v), type = \"l\")\n\n\n\n\nCumulative Median:\n\nhead(cmedian(v))\n\n[1] 112.0 115.0 118.0 123.5 121.0 125.0\n\ntail(cmedian(v))\n\n[1] 259.0 261.5 264.0 264.0 264.0 265.5\n\nplot(cmedian(v), type = \"l\")\n\n\n\n\nCumulative Standard Deviation:\n\nhead(csd(v))\n\n[1]        NA  4.242641 10.263203  9.358597  8.142481  8.916277\n\ntail(csd(v))\n\n[1] 115.0074 117.9956 119.1924 119.7668 119.7083 119.9663\n\nplot(csd(v), type = \"l\")\n\n\n\n\nCumulative Skewness:\n\nhead(cskewness(v))\n\n[1]         NaN  0.00000000  0.44510927 -0.14739157 -0.02100016 -0.18544758\n\ntail(cskewness(v))\n\n[1] 0.5936970 0.6471651 0.6349071 0.6145579 0.5972102 0.5770682\n\nplot(cskewness(v), type = \"l\")\n\n\n\n\nCumulative Variance:\n\nhead(cvar(v))\n\n[1]        NA  18.00000 105.33333  87.58333  66.30000  79.50000\n\ntail(cvar(v))\n\n[1] 13226.70 13922.96 14206.84 14344.08 14330.07 14391.92\n\nplot(cvar(v), type = \"l\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-07/index.html",
    "href": "posts/rtip-2023-02-07/index.html",
    "title": "Subsetting Named Lists in R",
    "section": "",
    "text": "Introduction\nIn R, lists are a fundamental data structure that allows us to store multiple objects of different data types under a single name. Often times, we want to extract certain elements of a list based on their names, and this can be accomplished through the use of the subset function. In this blog post, we will take a look at how to use the grep function to subset named lists in R.\nFirst, we will create a list object as follows:\n\nasc_list &lt;- list(\n  Facility = 1:10,\n  State = 11:20,\n  National = 21:30\n)\n\nWe now have a list with three elements, each with a different name. Next, we want to make sure that our list does not contain any 0 length items. This can be achieved by using the lapply function and the length function:\n\nasc_list &lt;- asc_list[lapply(asc_list, length) &gt; 0]\n\nThe lapply function applies the length function to each element of the list, and returns a logical vector indicating whether each element is of length greater than 0. By using the square bracket operator, we can extract only those elements for which the logical value is TRUE.\nNext, we create a character vector of possible items that we want to match on:\n\npatterns &lt;- c(\"state\",\"faci\")\n\nWe can now pass this vector of patterns to the grep function, along with the names of our list and the ignore.case argument set to TRUE. The grep function returns the indices of the elements in our list that match the given pattern:\n\nasc_list[grep(\n  paste(patterns, collapse = \"|\"),\n  names(asc_list),\n  ignore.case = TRUE\n  )]\n\n$Facility\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$State\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nThe result of this code is a new list that contains only the elements of our original list whose names match either “state” or “faci”. The paste function is used to join the patterns in the vector into a single string, with the | character separating each pattern. This allows us to search for multiple patterns at once.\nIn conclusion, the grep function is a powerful tool for sub-setting named lists in R, especially when we have multiple patterns that we want to match on. By combining the grep function with other R functions such as lapply and length, we can extract specific elements from our lists with ease."
  },
  {
    "objectID": "posts/rtip-2023-02-08/index.html",
    "href": "posts/rtip-2023-02-08/index.html",
    "title": "Creating an R Project Directory",
    "section": "",
    "text": "Introduction\nWhen working in R I find it best to create a new project when working on something. This keeps all of the data and scripts in one location. This also means that if you are not careful the directory you have your project in can become quite messy. This used to happen to me with regularity, then I got smart and wrote a script that would standardize how projects are built for me.\nI find it important to have different fodlers for different parts of a project. This does not mean I will use them all for every project but that is fine, you can either comment that portion out or just delete the files that are created.\n\n\nFunction\nHere is what I do broken down into different steps. First, I see if the package {fs} is installed, and if not, then install it, and finally load it.\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\nNext we create a character vector of folder paths that will exist inside of the main project folder itself.\n\nfolders <- c(\n  \"00_Scripts\"\n  , \"00_Data\"\n  , \"01_Queries\"\n  , \"02_Data_Manipulation\"\n  , \"03_Viz\"\n  , \"04_TS_Modeling\"\n  , \"99_Automations\"\n)\n\nNow that the folders we want are spelt out, we can create them.\n\nfs::dir_create(\n  path = folders\n)\n\nNow that is done, it’s off to creating a few files that I personally almost always use. I do a lot of work out of a data warehouse so a connection file is needed. We also need a disconnection function.\n\n# DSS Connection \ndb_connect <- function() {\n  db_con <- LICHospitalR::db_connect()\n  \n  return(db_con)\n  \n}\n\n# Disconnect from Database\ndb_disconnect <- function(.connection) {\n  \n  DBI::dbDisconnect(\n    conn = db_connect()\n  )\n  \n}\n\nNow, let’s load in the typical libraries. You can modify this to suit your own needs.\n\n# Library Load\n\nlibrary_load <- function(){\n  \n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"DBI\"\n    , \"odbc\"\n    , \"janitor\"\n    , \"dplyr\"\n    , \"tibble\"\n    , \"tidyr\"\n    , \"LICHospitalR\"\n    , \"modeltime\"\n  )\n  \n}\n\nOk so now the functions have been created, let’s dump them!\n\ndb_funs <- c(\"db_connect\",\"db_disconnect\")\ndump(\n  list = db_funs,\n  file = \"00_Scripts/db_con_obj.R\"\n)\n\nlib_funs <- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Scripts/library_load.R\"\n)\n\n\n\nExample\nHere is the full script!\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\nfolders <- c(\n  \"00_Scripts\"\n  , \"00_Data\"\n  , \"01_Queries\"\n  , \"02_Data_Manipulation\"\n  , \"03_Viz\"\n  , \"04_TS_Modeling\"\n  , \"99_Automations\"\n)\n\nfs::dir_create(\n  path = folders\n)\n\n\nfile_create(\"01_Queries/query_functions.R\")\nfile_create(\"02_Data_Manipulation/data_functions.R\")\nfile_create(\"03_Viz/viz_functions.R\")\nfile_create(\"04_TS_Modeling/ts_functions.R\")\n\n# DSS Connection \ndb_connect <- function() {\n  db_con <- LICHospitalR::db_connect()\n  \n  return(db_con)\n  \n}\n\n# Disconnect from Database\ndb_disconnect <- function(.connection) {\n  \n  DBI::dbDisconnect(\n    conn = db_connect()\n  )\n  \n}\n\n# Library Load\n\nlibrary_load <- function(){\n  \n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"DBI\"\n    , \"odbc\"\n    , \"janitor\"\n    , \"dplyr\"\n    , \"tibble\"\n    , \"tidyr\"\n    , \"LICHospitalR\"\n    , \"modeltime\"\n  )\n  \n}\n\ndb_funs <- c(\"db_connect\",\"db_disconnect\")\ndump(\n  list = db_funs,\n  file = \"00_Scripts/db_con_obj.R\"\n)\n\nlib_funs <- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Scripts/library_load.R\"\n)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-09/index.html",
    "href": "posts/rtip-2023-02-09/index.html",
    "title": "Creating and Predicting Fast Regression Parsnip Models with {tidyAML}",
    "section": "",
    "text": "Introduction\nI am almost ready for a first release of my R package {tidyAML}. The purpose of this is to act as a way of quickly generating models using the parsnip package and keeping things inside of the tidymodels framework allowing users to seamlessly create models in tidyAML but pluck and move them over to tidymodels should they prefer. This is because I believe that software should be interchangeable and work well with other libraries. Today I am going to showcase how the function fast_regression()\n\n\nFunction\nLet’s take a look at the function.\n\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL\n)\n\nHere are the arguments to the function:\n\n.data - The data being passed to the function for the regression problem\n.rec_obj - The recipe object being passed.\n.parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported.\n.parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported.\n.split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample\n.split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type.\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(tidyAML)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(purrr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfast_reg_tbl <- fast_regression(\n  .data = mtcars,\n  .rec_obj = rec_obj,\n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(fast_reg_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nLet’s take a look at the model spec.\n\nfast_reg_tbl %>% slice(1) %>% pull(model_spec) %>% pluck(1)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the wflw column.\n\nfast_reg_tbl %>% slice(1) %>% pull(wflw) %>% pluck(1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe Fitted workflow.\n\nfast_reg_tbl %>% slice(1) %>% pull(fitted_wflw) %>% pluck(1)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n -15.077267     1.107474     0.001161    -0.001014     4.010199    -1.280324  \n       qsec           vs           am         gear         carb  \n   0.512318    -0.488014     2.430052     4.353568    -2.546043  \n\n\nAnd lastly tne predicted workflow column.\n\nfast_reg_tbl %>% slice(1) %>% pull(pred_wflw) %>% pluck(1)\n\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.7\n 2  28.2\n 3  18.9\n 4  12.0\n 5  14.8\n 6  15.4\n 7  14.7\n 8  20.0\n 9  11.2\n10  19.1\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrdata-2023-02-10/index.html",
    "href": "posts/weekly-rtip-healthyrdata-2023-02-10/index.html",
    "title": "Get the Current Hospital Data Set from CMS with {healthyR.data}",
    "section": "",
    "text": "Introduction\nGetting data for health care in the US can sometimes be hard. With my R package {healthyR.data} I am hoping to alleviate some of that pain.\nRight now the package is bring actively developed from what was a simple yet sleepy simulated administrative data set is getting supercharged into a a full blow package that will retrieve data from outside sources. One such source is CMS.\nAt the start, and this is going to be a long road, I have started to build some functionality around getting the current hospital data from CMS. Let’s take a look at how it works.\n\n\nFunction\nHere is the function which has no parameters. This function will download the current and the official hospital data sets from the CMS.gov website.\nThe function makes use of a temporary directory and file to save and unzip the data. This will grab the current Hospital Data Files, unzip them and return a list of tibbles with each tibble named after the data file.\nThe function returns a list object with all of the current hospital data as a tibble. It does not save the data anywhere so if you want to save it you will have to do that manually.\nThis also means that you would have to store the data as a variable in order to access the data later on. It does have a given attributes and a class so that it can be piped into other functions.\n\ncurrent_hosp_data()\n\nNow let’s see it in action.\n\n\nExample\nWe will download the current hospital data sets and take a look.\n\nlibrary(healthyR.data)\nlibrary(dplyr)\n\ncurrent_hospital_dataset <- current_hosp_data()\n\nThis function downloads 70 files. Let’s see which ones have been downloaded.\n\nnames(current_hospital_dataset)\n\n [1] \"ASC_Facility.csv\"                                                \n [2] \"ASC_National.csv\"                                                \n [3] \"ASC_State.csv\"                                                   \n [4] \"ASCQR_OAS_CAHPS_BY_ASC.csv\"                                      \n [5] \"ASCQR_OAS_CAHPS_NATIONAL.csv\"                                    \n [6] \"ASCQR_OAS_CAHPS_STATE.csv\"                                       \n [7] \"CJR_PY6_Quality_Reporting_July_2022_Production_File.csv\"         \n [8] \"CMS_PSI_6_decimal_file.csv\"                                      \n [9] \"Complications_and_Deaths_Hospital.csv\"                           \n[10] \"Complications_and_Deaths_National.csv\"                           \n[11] \"Complications_and_Deaths_State.csv\"                              \n[12] \"Data_Updates_January_2023.csv\"                                   \n[13] \"Footnote_Crosswalk.csv\"                                          \n[14] \"FY_2023_HAC_Reduction_Program_Hospital.csv\"                      \n[15] \"FY_2023_Hospital_Readmissions_Reduction_Program_Hospital.csv\"    \n[16] \"FY2021_Distribution_of_Net_Change_in_Base_Op_DRG_Payment_Amt.csv\"\n[17] \"FY2021_Net_Change_in_Base_Op_DRG_Payment_Amt.csv\"                \n[18] \"FY2021_Percent_Change_in_Medicare_Payments.csv\"                  \n[19] \"FY2021_Value_Based_Incentive_Payment_Amount.csv\"                 \n[20] \"HCAHPS_Hospital.csv\"                                             \n[21] \"HCAHPS_National.csv\"                                             \n[22] \"HCAHPS_State.csv\"                                                \n[23] \"Healthcare_Associated_Infections_Hospital.csv\"                   \n[24] \"Healthcare_Associated_Infections_National.csv\"                   \n[25] \"Healthcare_Associated_Infections_State.csv\"                      \n[26] \"Hospital_General_Information.csv\"                                \n[27] \"HOSPITAL_QUARTERLY_MSPB_6_DECIMALS.csv\"                          \n[28] \"hvbp_clinical_outcomes.csv\"                                      \n[29] \"hvbp_efficiency_and_cost_reduction.csv\"                          \n[30] \"hvbp_person_and_community_engagement.csv\"                        \n[31] \"hvbp_safety.csv\"                                                 \n[32] \"hvbp_tps.csv\"                                                    \n[33] \"IPFQR_QualityMeasures_Facility.csv\"                              \n[34] \"IPFQR_QualityMeasures_National.csv\"                              \n[35] \"IPFQR_QualityMeasures_State.csv\"                                 \n[36] \"Maternal_Health_Hospital.csv\"                                    \n[37] \"Maternal_Health_National.csv\"                                    \n[38] \"Maternal_Health_State.csv\"                                       \n[39] \"Measure_Dates.csv\"                                               \n[40] \"Medicare_Hospital_Spending_by_Claim.csv\"                         \n[41] \"Medicare_Hospital_Spending_Per_Patient_Hospital.csv\"             \n[42] \"Medicare_Hospital_Spending_Per_Patient_National.csv\"             \n[43] \"Medicare_Hospital_Spending_Per_Patient_State.csv\"                \n[44] \"OAS_CAHPS_Footnotes.csv\"                                         \n[45] \"OQR_OAS_CAHPS_BY_HOSPITAL.csv\"                                   \n[46] \"OQR_OAS_CAHPS_NATIONAL.csv\"                                      \n[47] \"OQR_OAS_CAHPS_STATE.csv\"                                         \n[48] \"Outpatient_Imaging_Efficiency_Hospital.csv\"                      \n[49] \"Outpatient_Imaging_Efficiency_National.csv\"                      \n[50] \"Outpatient_Imaging_Efficiency_State.csv\"                         \n[51] \"Payment_National.csv\"                                            \n[52] \"Payment_State.csv\"                                               \n[53] \"Payment_and_Value_of_Care_Hospital.csv\"                          \n[54] \"PCH_HCAHPS_HOSPITAL.csv\"                                         \n[55] \"PCH_HCAHPS_NATIONAL.csv\"                                         \n[56] \"PCH_HCAHPS_STATE.csv\"                                            \n[57] \"PCH_HEALTHCARE_ASSOCIATED_INFECTIONS_HOSPITAL.csv\"               \n[58] \"PCH_ONCOLOGY_CARE_MEASURES_HOSPITAL.csv\"                         \n[59] \"PCH_OUTCOMES_HOSPITAL.csv\"                                       \n[60] \"PCH_OUTCOMES_NATIONAL.csv\"                                       \n[61] \"Timely_and_Effective_Care_Hospital.csv\"                          \n[62] \"Timely_and_Effective_Care_National.csv\"                          \n[63] \"Timely_and_Effective_Care_State.csv\"                             \n[64] \"Unplanned_Hospital_Visits_Hospital.csv\"                          \n[65] \"Unplanned_Hospital_Visits_National.csv\"                          \n[66] \"Unplanned_Hospital_Visits_State.csv\"                             \n[67] \"VA_IPF.csv\"                                                      \n[68] \"VA_TE.csv\"                                                       \n[69] \"Value_of_Care_National.csv\"                                      \n[70] \"Veterans_Health_Administration_Provider_Level_Data.csv\"          \n\n\nMore to come in the future!"
  },
  {
    "objectID": "posts/rtip-2023-02-13/index.html",
    "href": "posts/rtip-2023-02-13/index.html",
    "title": "Off to CRAN! {tidyAML}",
    "section": "",
    "text": "Introduction\nAre you tired of spending hours tuning and testing different machine learning models for your regression or classification problems? The new R package {tidyAML} is here to simplify the process for you! tidyAML is a simple interface for automatic machine learning that fits the tidymodels framework, making it easier for you to solve regression and classification problems.\nThe tidyAML package has been designed with the goal of providing a simple API that automates the entire machine learning pipeline, from data preparation to model selection, training, and prediction. This means that you no longer have to spend hours tuning and testing different models; tidyAML will do it all for you, saving you time and effort.\nIn this initial release (version 0.0.1), tidyAML introduces a number of new features and minor fixes to improve the overall user experience. Here are some of the updates in this release:\nNew Features:\n\nmake_regression_base_tbl() and make_classification_base_tbl() functions for creating base tables for regression and classification problems, respectively.\ninternal_make_spec_tbl() function for making the specification table for the machine learning pipeline.\ninternal_set_args_to_tune() function for setting arguments to tune the models. This has not yet been implemented in a true working fashion but might be useful for feedback in this initial release.\ncreate_workflow_set() function for creating a set of workflows to test different models.\nget_model(), extract_model_spec(), extract_wflw(), extract_wflw_fit(), and extract_wflw_pred() functions for extracting different parts of the machine learning pipeline.\nmatch_args() function for matching arguments between the base and specification tables.\n\nMinor Fixes and Improvements:\n\nUpdates to fast_classification_parsnip_spec_tbl() and fast_regression_parsnip_spec_tbl() to use the make_regression and make_classification functions and the internal_make_spec_tbl() function.\nAddition of a class for the base table functions and using that class in internal_make_spec_tbl().\nUpdate to the DESCRIPTION for R >= 3.4.0.\n\nIn conclusion, tidyAML is a game-changer for those looking to automate the machine learning pipeline. It provides a simple API that eliminates the need for manual tuning and testing of different models. With the updates in this initial release, the tidyAML package is sure to make your machine learning journey easier and more efficient.\n\n\nFunction\nThere are too many functions to go over in this post so you can find them all here\n\n\nExamples\nEven though there are many functions to go over, we can showcase some with a small useful example. So let’s get at it!\n\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(dplyr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\n\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n  )\n\nglimpse(frt_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nNow let’s go through the extractors.\nThe get_model() function.\n\nget_model(frt_tbl, 2) |>\n  glimpse()\n\nRows: 1\nColumns: 8\n$ .model_id       <int> 2\n$ .parsnip_engine <chr> \"glm\"\n$ .parsnip_mode   <chr> \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, glm, TRUE…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>]\n\n\nThe extract_model_spec() function.\n\nextract_model_spec(frt_tbl, 1)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOr do multiples:\n\nextract_model_spec(frt_tbl, 1:2)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nThe extract_wflw() function.\n\nextract_wflw(frt_tbl, 1)\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOr do multiples:\n\nextract_wflw(frt_tbl, c(1, 2))\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n[[2]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nThe extract_wflw_fit() function.\n\nextract_wflw_fit(frt_tbl, 1)\n\n[[1]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\n\nOr do multiples:\n\nextract_wflw_fit(frt_tbl, 1:2)\n\n[[1]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\n\n[[2]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::gaussian, data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\nDegrees of Freedom: 23 Total (i.e. Null);  13 Residual\nNull Deviance:      935.1 \nResidual Deviance: 121.5    AIC: 131\n\n\nFinally the extract_wflw_pred() function.\n\nextract_wflw_pred(frt_tbl, 2)\n\n[[1]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n\nOr do multiples:\n\nextract_wflw_pred(frt_tbl, 1:2)\n\n[[1]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n[[2]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-14/index.html",
    "href": "posts/rtip-2023-02-14/index.html",
    "title": "An example of using {box}",
    "section": "",
    "text": "Introduction\nToday I am going to make a short post on the R package {box} which was showcased to me quite nicely by Michael Miles. It was informative and I was able to immediately see the usefulness of the {box} library.\nSo what is ‘box’? Well here is the description straight from their site:\n\n‘box’ allows organising R code in a more modular way, via two mechanisms:\n\nIt enables writing modular code by treating files and folders of R code as independent (potentially nested) modules, without requiring the user to wrap reusable code into packages.\nIt provides a new syntax to import reusable code (both from packages and from modules) which is more powerful and less error-prone than library or require, by limiting the number of names that are made available.\n\n\nSo let’s see how it all works.\n\n\nFunction\nThe main portion of the script looks like this:\n\n# Main script\n\n# Script setup --------------------------------------\n\n# Load box modules\nbox::use(. / box / global_options / global_options)\nbox::use(. / box / io / imports)\nbox::use(. / box / io / exports)\nbox::use(. / box / mod / mod)\n\n# Load global options\nglobal_options$set_global_options() \n\n\n# Main script ---------------------------------------\n\n# Load data, process it, and export results\nall_data <- getOption('data_dir') |> \n  \n  # Load all data\n  imports$load_all() |> \n  \n  # Modify dataset\n  mod$modify_data() |> \n  \n  # Export data\n  exports$export_data()\n\nSo what does this do? Well it is grabbing data from a predefined location, modifying it and then re-exporting it. Now let’s look at all the code that is behind it, which allows us to do these things and then you will see the power of using box\n\n\nExample\nLet’s take a look at the global options settings.\n\n# Set global options\n#' @export\nset_global_options <- function() {\n  options(\n    look_ups = 'look-ups/',\n    data_dir = 'data/input/'\n  )\n}\n\nOk 6 lines, boxed down to one.\nNow the import function.\n\n# Function for importing data\n\n#' @export\nload_all <- function(file_path) {\n  \n  box::use(purrr)\n  box::use(vroom)\n  \n  file_path |> \n    \n    # Get all csv files from folder\n    list.files(full.names = TRUE) |> \n    \n    # Set list names\n    purrr$set_names(\\(file) basename(file)) |> \n    \n    # Load all csvs into list\n    purrr$map(\\(file) vroom$vroom(file))\n\n}\n\nNow the modify_data function.\n\n# Function for modifying data\n\n#' @export\nmodify_data <- function(df_list) {\n  \n  box::use(dplyr)\n  box::use(purrr)\n  \n  map_fun <- function(df) {\n    \n    df |> \n      dplyr$select(name:mass) |> \n      dplyr$mutate(lol = height * mass) |> \n      dplyr$filter(lol > 1500)\n  }\n  \n  # Apply mapping function to list\n  purrr$map(df_list, map_fun)\n  \n}\n\nOk again, a big savings here, instead of the above we simply call mod$modify_data() which makes things clearner and also modular in that we can go to a very specific spot in our proejct to fix an error or add/subtract functionality.\nLastly the export.\n\n# Function for exporting data\n\n#' @export\nexport_data <- function(df_list) {\n  \n  box::use(vroom)\n  box::use(purrr)\n  \n  # Export data\n  purrr$map2(.x = df_list,\n             .y = names(df_list),\n             ~vroom$vroom_write(x = .x,\n                               file = paste0('data/output/', \n                                             .y),\n                               delim = ','))\n  \n}\n\nVoila! I think to even a fresh user, the power of boxing your functions is fairly apparent and to the advanced user, eyes are most likely glowing!"
  },
  {
    "objectID": "posts/rtip-2023-02-15/index.html",
    "href": "posts/rtip-2023-02-15/index.html",
    "title": "Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nAre you interested in visualizing time series data in a clear and concise way? The R package {healthyR.ts} provides a variety of tools for time series analysis and visualization, including the ts_ma_plot() function.\nThe ts_ma_plot() function is designed to help you quickly and easily create moving average plots for time series data. This function takes several arguments, including the data you want to visualize, the date column from your data, the value column from your data, and the frequency of the aggregation.\nOne of the great features of ts_ma_plot() is that it can handle both weekly and monthly data frequencies, making it a flexible tool for analyzing a variety of time series data. If you pass in a frequency other than “weekly” or “monthly”, the function will default to weekly, so it’s important to ensure that your data is aggregated at the appropriate frequency.\nWith ts_ma_plot(), you can create a variety of plots to help you better understand your time series data. The function allows you to add up to three different titles to your plot, helping you to organize and communicate your findings effectively. The main_title argument sets the title for the main plot, while the secondary_title and tertiary_title arguments set the titles for the second and third plots, respectively.\nIf you’re interested in using ts_ma_plot() for your own time series data, you’ll first need to preprocess your data so that it’s in the appropriate format for this function. Once you’ve done that, though, ts_ma_plot() can help you to quickly identify trends and patterns in your data that might not be immediately apparent from a raw data set.\nIn summary, ts_ma_plot() is a powerful and flexible tool for visualizing time series data. Whether you’re working with weekly or monthly data, this function can help you to quickly and easily create moving average plots that can help you to better understand your data. If you’re interested in time series analysis, be sure to check out {healthyR.ts} and give ts_ma_plot() a try!\n\n\nFunction\nHere is the full function call.\n\nts_ma_plot(\n  .data,\n  .date_col,\n  .value_col,\n  .ts_frequency = \"monthly\",\n  .main_title = NULL,\n  .secondary_title = NULL,\n  .tertiary_title = NULL\n)\n\nNow for the arguments to the parameters.\n\n.data: the data you want to visualize, which should be pre-processed and the aggregation should match the .frequency argument.\n.date_col: the data column from the .data argument that contains the dates for your time series.\n.value_col: the data column from the .data argument that contains the values for your time series.\n.ts_frequency: the frequency of the aggregation, which should be quoted as “weekly” or “monthly”. If not specified, the function defaults to weekly.\n.main_title: the title of the main plot.\n.secondary_title: the title of the second plot.\n.tertiary_title: the title of the third plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndata_tbl <- ts_to_tbl(AirPassengers) |>\n  select(-index)\n\noutput <- ts_ma_plot(\n  .data = data_tbl,\n  .date_col = date_col,\n  .value_col = value\n)\n\nLet’s take a look at each piece of the output.\n\noutput$data_trans_xts |> head()\n\n           value ma12\n1949-01-01   112   NA\n1949-02-01   118   NA\n1949-03-01   132   NA\n1949-04-01   129   NA\n1949-05-01   121   NA\n1949-06-01   135   NA\n\n\n\noutput$data_diff_xts_a |> head()\n\n              diff_a\n1949-01-01        NA\n1949-02-01  5.357143\n1949-03-01 11.864407\n1949-04-01 -2.272727\n1949-05-01 -6.201550\n1949-06-01 11.570248\n\n\n\noutput$data_diff_xts_b |> head()\n\n           diff_b\n1949-01-01     NA\n1949-02-01     NA\n1949-03-01     NA\n1949-04-01     NA\n1949-05-01     NA\n1949-06-01     NA\n\n\n\noutput$data_summary_tbl\n\n# A tibble: 144 × 5\n   date_col   value  ma12 diff_a diff_b\n   <date>     <dbl> <dbl>  <dbl>  <dbl>\n 1 1949-01-01   112    NA   0         0\n 2 1949-02-01   118    NA   5.36      0\n 3 1949-03-01   132    NA  11.9       0\n 4 1949-04-01   129    NA  -2.27      0\n 5 1949-05-01   121    NA  -6.20      0\n 6 1949-06-01   135    NA  11.6       0\n 7 1949-07-01   148    NA   9.63      0\n 8 1949-08-01   148    NA   0         0\n 9 1949-09-01   136    NA  -8.11      0\n10 1949-10-01   119    NA -12.5       0\n# … with 134 more rows\n\n\n\noutput$pgrid\n\n\n\n\n\noutput$xts_plt"
  },
  {
    "objectID": "posts/rtip-2023-02-16/index.html",
    "href": "posts/rtip-2023-02-16/index.html",
    "title": "Officially on CRAN {tidyAML}",
    "section": "",
    "text": "Introduction\nI’m excited to announce that the R package {tidyAML} is now officially available on CRAN! This package is designed to make it easy for users to perform automated machine learning (AutoML) using the tidymodels ecosystem. With a simple and intuitive interface, tidyAML allows users to quickly generate high-quality machine learning models without worrying about the underlying details.\nOne of the key features of tidyAML is its ability to generate regression models on the fly, without the need to build a full specification or tune hyper-parameters. This makes it ideal for users who want to quickly build a machine learning model without spending a lot of time on the setup process.\ntidyAML is also designed to be easy to use, with a set of functions that are straightforward and can generate many models and predictions at once. And because it’s built on top of the tidymodels ecosystem, users don’t need to worry about setting up additional packages or dependencies.\nWe’re also happy to announce that tidyAML will be added to the R package {healthyverse} and pushed to CRAN this week. This means that users who install {healthyverse} will automatically get access to tidyAML, as well as other popular packages like ggplot2, dplyr, and tidyr.\nWhether you’re a beginner or an experienced machine learning practitioner, tidyAML is a powerful tool that can help you quickly generate high-quality models with minimal setup. We hope you’ll give it a try and let us know what you think!"
  },
  {
    "objectID": "posts/rtip-2023-02-17/index.html",
    "href": "posts/rtip-2023-02-17/index.html",
    "title": "Converting a {tidyAML} tibble to a {workflowsets}",
    "section": "",
    "text": "The {tidyAML} package is an R package that provides a set of tools for building regression/classification models on the fly with minimal input required. In this post we will discuss the create_workflow_set() function.\nThe create_workflow_set function is a function in the tidyAML package that is used to create a workflowset object from the workflowsets package. A workflow is a sequence of tasks that can be executed in a specific order, and is often used in data analysis and machine learning to automate data processing and model fitting. The create_workflow_set function takes as input a YAML specification of a set of workflows, and returns a list of workflow objects that can be executed using the tidymodels package and its associated packages.\nThe create_workflow_set function is particularly useful when working with the tidymodels package and the parsnip framework. The tidymodels package is a collection of packages for modeling and machine learning in R that provides a consistent interface for building, tuning, and evaluating machine learning models. The parsnip package is part of the tidymodels ecosystem and provides a way to specify a wide range of models in a consistent manner.\n\n\nTo use the create_workflow_set function with tidymodels andparsnip, you will need to provide a recipe or recipes as a list to the .recipe_list parameter and a model_spec tibble that you would get from something like fast_regression_parsnip_spec_tbl(), other classes will be supported in the future.\nThe reason this was done was because I did not want to force users to remain inside of tidyAML perhaps and most likely there are other packages out there that are more suited to an end users specific problem at hand."
  },
  {
    "objectID": "posts/rtip-2023-02-22/index.html",
    "href": "posts/rtip-2023-02-22/index.html",
    "title": "Calibrate and Plot a Time Series with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nIn time series analysis, it is common to split the data into training and testing sets to evaluate the accuracy of a model. However, it is important to ensure that the model is calibrated on the training set before evaluating its performance on the testing set. The {healthyR.ts} library provides a function called calibrate_and_plot() that simplifies this process.\n\n\nFunction\nHere is the full function call:\n\ncalibrate_and_plot(\n  ...,\n  .type = \"testing\",\n  .splits_obj,\n  .data,\n  .print_info = TRUE,\n  .interactive = FALSE\n)\n\nHere are the arguments to the parameters:\n\n... - The workflow(s) you want to add to the function.\n.type - Either the training(splits) or testing(splits) data.\n.splits_obj - The splits object.\n.data - The full data set.\n.print_info - The default is TRUE and will print out the calibration accuracy tibble and the resulting plotly plot.\n.interactive - The defaults is FALSE. This controls if a forecast plot is interactive or not via plotly.\n\n\n\nExample\nBy default, calibrate_and_plot() will print out a calibration accuracy tibble and a resulting plotly plot. This can be controlled with the print_info argument, which is set to TRUE by default. If you prefer a non-interactive forecast plot, you can set the interactive argument to FALSE.\nHere’s an example of how to use the calibrate_and_plot() function:\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(workflows)\nlibrary(rsample)\n\n# Get the Data\ndata <- ts_to_tbl(AirPassengers) |>\n  select(-index)\n\n# Split the data into training and testing sets\nsplits <- time_series_split(\n   data\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\n# Make the recipe object\nrec_obj <- recipe(value ~ ., data = training(splits))\n\n# Make the Model\nmodel_spec <- linear_reg(\n   mode = \"regression\"\n   , penalty = 0.5\n   , mixture = 0.5\n) |>\n   set_engine(\"lm\")\n\n# Make the workflow object\nwflw <- workflow() |>\n   add_recipe(rec_obj) |>\n   add_model(model_spec) |>\n   fit(training(splits))\n\n# Get our output\noutput <- calibrate_and_plot(\n  wflw\n  , .type = \"training\"\n  , .splits_obj = splits\n  , .data = data\n  , .print_info = FALSE\n  , .interactive = TRUE\n )\n\nThe resulting output will include a calibration accuracy tibble and a plotly plot showing the original time series data along with the fitted values for the training set.\nLet’s take a look at the output.\n\noutput$calibration_tbl\n\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc .type .calibration_data \n      <int> <list>     <chr>       <chr> <list>            \n1         1 <workflow> LM          Test  <tibble [132 × 4]>\n\n\n\noutput$model_accuracy\n\n# A tibble: 1 × 9\n  .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq\n      <int> <chr>       <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 LM          Test   31.4  12.0  1.31  11.9  41.7 0.846\n\n\nAnd…\n\noutput$plot\n\n\n\n\n\nOverall, the calibrate_and_plot() function is a useful tool for simplifying the process of calibrating time series models on a training set and evaluating their performance on a testing set.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-23/index.html",
    "href": "posts/rtip-2023-02-23/index.html",
    "title": "Data Preppers with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nThere are many different methods that one can choose from in order to model their data. This brings with it a fundamental issue of how to prepare your data for the specified algorithm. With the [{healthyR.ai}] package there are many different functions in this family that will help solve this issue for some algorithms but of course not all, that would be utterly exhausting for me to do on my own.\nIn healthyR.ai I call these Data Preppers because they prep the data you supply to the format necessary for the algorithm to function properly.\nLet’s take a look at one.\n\n\nFunction\nHere we are going to use the hai_c50_data_prepper(.data, .recipe_formula) function.\n\nhai_c50_data_prepper(.data, .recipe_formula)\n\nHere are the simple arguments:\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::recipe() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the iris data then the formula would most likely be something like Species ~ .\n\n\n\nExample\nHere is a small example:\n\nlibrary(healthyR.ai)\n\nhai_c50_data_prepper(.data = Titanic, .recipe_formula = Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\n\nrec_obj <- hai_c50_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(rec_obj)\n\n# A tibble: 32 × 5\n   Class Sex    Age       n Survived\n   <fct> <fct>  <fct> <dbl> <fct>   \n 1 1st   Male   Child     0 No      \n 2 2nd   Male   Child     0 No      \n 3 3rd   Male   Child    35 No      \n 4 Crew  Male   Child     0 No      \n 5 1st   Female Child     0 No      \n 6 2nd   Female Child     0 No      \n 7 3rd   Female Child    17 No      \n 8 Crew  Female Child     0 No      \n 9 1st   Male   Adult   118 No      \n10 2nd   Male   Adult   154 No      \n# … with 22 more rows\n\n\nHere are the rest of the data-preppers at the time of writing this article:\n\nhai_c50_data_prepper()\nhai_cubist_data_prepper()\nhai_earth_data_prepper()\nhai_glmnet_data_prepper()\nhai_knn_data_prepper()\nhai_ranger_data_prepper()\nhai_svm_poly_data_prepper()\nhai_svm_rbf_data_prepper()\nhai_xgboost_data_prepper()\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-27/index.html",
    "href": "posts/rtip-2023-02-27/index.html",
    "title": "Quickly Generate Nested Time Series Models",
    "section": "",
    "text": "Introduction\nThere are many approaches to modeling time series data in R. One of the types of data that we might come across is a nested time series. This means the data is grouped simply by one or more keys. There are many methods in which to accomplish this task. This will be a quick post, but if you want a longer more detailed and quite frankly well written out one, then this is a really good article\n\n\nExampmle\nLet’s just get to it with a very simple example, the motivation here isn’t to be all encompassing, but rather to just showcase it is possible for those who may not know it is.\n\nlibrary(healthyR.data)\nlibrary(dplyr)\nlibrary(timetk)\n\nts_tbl <- healthyR_data |> \n  filter(ip_op_flag == \"I\") |> \n  select(visit_end_date_time, service_line, length_of_stay) |>\n  mutate(visit_end_date_time = as.Date(visit_end_date_time)) |>\n  group_by(service_line) |>\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by = \"month\",\n    los = mean(length_of_stay)\n  ) |>\n  ungroup()\n\nglimpse(ts_tbl)\n\nRows: 2,148\nColumns: 3\n$ service_line        <chr> \"Alcohol Abuse\", \"Alcohol Abuse\", \"Alcohol Abuse\",…\n$ visit_end_date_time <date> 2011-09-01, 2011-10-01, 2011-11-01, 2011-12-01, 2…\n$ los                 <dbl> 3.666667, 3.181818, 4.380952, 3.464286, 3.677419, …\n\n\n\nlibrary(forecast)\nlibrary(broom)\nlibrary(tidyr)\n\nglanced_models <- ts_tbl |> \n  nest_by(service_line) |> \n  mutate(AA = list(auto.arima(data$los))) |> \n  mutate(perf = list(glance(AA))) |> \n  unnest(cols = c(perf))\n\nglanced_models |>\n  select(-data)\n\n# A tibble: 23 × 7\n# Groups:   service_line [23]\n   service_line                  AA         sigma logLik   AIC   BIC  nobs\n   <chr>                         <list>     <dbl>  <dbl> <dbl> <dbl> <int>\n 1 Alcohol Abuse                 <fr_ARIMA> 2.22  -241.   493.  506.   109\n 2 Bariatric Surgery For Obesity <fr_ARIMA> 0.609  -80.1  168.  178.    88\n 3 CHF                           <fr_ARIMA> 0.963 -152.   309.  314.   110\n 4 COPD                          <fr_ARIMA> 0.987 -155.   315.  320.   110\n 5 CVA                           <fr_ARIMA> 1.50  -201.   407.  412.   110\n 6 Carotid Endarterectomy        <fr_ARIMA> 6.27  -166.   335.  339.    51\n 7 Cellulitis                    <fr_ARIMA> 1.07  -163.   329.  335.   110\n 8 Chest Pain                    <fr_ARIMA> 0.848 -139.   281.  287.   110\n 9 GI Hemorrhage                 <fr_ARIMA> 1.21  -179.   361.  366.   111\n10 Joint Replacement             <fr_ARIMA> 1.65  -196.   396.  401.   102\n# … with 13 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-28/index.html",
    "href": "posts/rtip-2023-02-28/index.html",
    "title": "Open a File Folder in R",
    "section": "",
    "text": "Inroduction\nWhen writing a function, it is possible that you may want to ask the user where they want the data stored and if they want to open the file folder after the download has taken place. Well we can do this in R by invoking the shell.exec() command where we use a variable like f_path that is the path to the folder. We are going to go over a super simple example.\n\n\nFunction\nHere is the function:\n\nshell.exec(file)\n\nHere are the arguments.\n\nfile - file, directory or URL to be opened.\n\nNow let’s go over a simple example\n\n\nExample\nHere we go.\n\n# Create a temporary file to store the zip file\nf_path <- utils::choose.dir()\n\n# Open file folder?\nif (.open_folder){\n    shell.exec(f_path)\n}\n\nIf in our function creation we make a variable .open_folder and set it equal to TRUE then the if statement will execute and shell.exec(f_path) will open the specified path set by utils::choose.dir()\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-01/index.html",
    "href": "posts/rtip-2023-03-01/index.html",
    "title": "Text Processing Made Easy with {healthyR}’s sql_left(), sql_mid(), and sql_right() Functions in R",
    "section": "",
    "text": "Introduction\nAre you tired of manually manipulating text data in R? Do you find yourself frequently needing to extract substrings from long strings or to grab just the first few characters of a string? If so, you’re in luck! The {healthyR} library has three functions that will make your text processing tasks much easier: sql_left(), sql_mid(), and sql_right().\n\n\nFunction\nHere are the function calls, I will also make the source avilable in the same cell so steal this code!!\n\n# LEFT\nsql_left(\"text\", 3)\n\nsql_left <- function(.text, .num_char) {\n    base::substr(.text, 1, .num_char)\n}\n\n# MID\nsql_mid(\"this is some text\", 6, 2)\n\nsql_mid <- function(.text, .start_num, .num_char) {\n    base::substr(.text, .start_num, .start_num + .num_char - 1)\n}\n\n# RIGHT\nsql_right(\"this is some more text\", 3)\n\nsql_right <- function(.text, .num_char) {\n    base::substr(.text, base::nchar(.text) - (.num_char-1), base::nchar(.text))\n}\n\n\n\nExample\nLet’s start with sql_left(). This function is similar to the LEFT() function in SQL and Excel, in that it returns the specified number of characters from the beginning of a string. For example, if we have the string “Hello, world!”, and we want to grab just the first three characters, we can use sql_left() like this:\n\nlibrary(healthyR)\nsql_left(\"Hello, world!\", 3)\n\n[1] \"Hel\"\n\n\nThis will return the string “Hel”.\nNext up is sql_mid(). This function is similar to the SUBSTRING() and MID() functions in SQL and Excel, in that it returns a specified portion of a string. The first argument is the string itself, the second argument is the starting position of the substring, and the third argument is the length of the substring. For example, if we have the string “This is some text”, and we want to grab the two characters starting at position six, we can use sql_mid() like this:\n\nsql_mid(\"This is some text\", 6, 2)\n\n[1] \"is\"\n\n\nThis will return the string “is”.\nFinally, we have sql_right(). This function is similar to the RIGHT() function in SQL and Excel, in that it returns the specified number of characters from the end of a string. For example, if we have the string “This is some more text”, and we want to grab just the last three characters, we can use sql_right() like this:\n\nsql_right(\"This is some more text\", 3)\n\n[1] \"ext\"\n\n\nThis will return the string “ext”.\nThese three functions can be extremely helpful when working with text data in R. They can save you time and effort, and make your code more concise and readable. So next time you find yourself needing to manipulate text data, remember to reach for sql_left(), sql_mid(), and sql_right()!\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-02/index.html",
    "href": "posts/rtip-2023-03-02/index.html",
    "title": "Forecasting Timeseries in a list with R",
    "section": "",
    "text": "Introduction\nIn this article, we will discuss how to perform an ARIMA forecast on nested data or data that is in a list using R programming language. This is a common scenario in which we have data stored in a list format, where each element of the list corresponds to a different time series. We will use the R programming language, specifically the “forecast” package, to perform the ARIMA forecast.\nFirst, we will need to load the required packages and data. For this example, we will use the “AirPassengers” dataset which is included in the “datasets” package. This dataset contains the number of international airline passengers per month from 1949 to 1960. We will then create a list containing subsets of this data for each year.\n\nlibrary(forecast)\n\nyearly_data <- split(AirPassengers, f = ceiling(seq_along(AirPassengers)/12))\n\nyearly_data\n\n$`1`\n [1] 112 118 132 129 121 135 148 148 136 119 104 118\n\n$`2`\n [1] 115 126 141 135 125 149 170 170 158 133 114 140\n\n$`3`\n [1] 145 150 178 163 172 178 199 199 184 162 146 166\n\n$`4`\n [1] 171 180 193 181 183 218 230 242 209 191 172 194\n\n$`5`\n [1] 196 196 236 235 229 243 264 272 237 211 180 201\n\n$`6`\n [1] 204 188 235 227 234 264 302 293 259 229 203 229\n\n$`7`\n [1] 242 233 267 269 270 315 364 347 312 274 237 278\n\n$`8`\n [1] 284 277 317 313 318 374 413 405 355 306 271 306\n\n$`9`\n [1] 315 301 356 348 355 422 465 467 404 347 305 336\n\n$`10`\n [1] 340 318 362 348 363 435 491 505 404 359 310 337\n\n$`11`\n [1] 360 342 406 396 420 472 548 559 463 407 362 405\n\n$`12`\n [1] 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nIn the above code, we use the “split” function to split the data into yearly subsets. The “f” parameter is used to specify the grouping variable which, in this case, is the sequence of numbers from 1 to the length of the dataset divided by 12, rounded up to the nearest integer. This creates a list of 12 elements, one for each year.\n\n\nFunction\nNext, we will define a function that takes a single element of the list, fits an ARIMA model, and generates a forecast.\n\narima_forecast <- function(x){\n  fit <- auto.arima(x)\n  forecast(fit)\n}\n\nThis function takes a single argument “x” which is one of the elements of the list. We use the “auto.arima” function from the “forecast” package to fit an ARIMA model to the data. The “forecast” function is then used to generate a forecast based on this model.\n\n\nExample\nWe can now use the “lapply” function to apply this function to each element of the list.\n\nforecasts <- lapply(yearly_data, arima_forecast)\n\nThe “lapply” function applies the “arima_forecast” function to each element of the “yearly_data” list and returns a list of forecasts.\nFinally, we can extract and plot the forecasts for a specific year.\n\nplot(forecasts[[5]])\n\n\n\n\nNow lets take a look at them all.\n\npar(mfrow = c(2,1))\n\npurrr::map(forecasts, plot)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$`1`\n$`1`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 132.2237 126.4744 126.4744 126.4744 126.4744 126.4744 126.4744 126.4744\n [9] 126.4744 126.4744\n\n$`1`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 120.1608 113.7751\n14 110.0828 101.4056\n15 110.0828 101.4056\n16 110.0828 101.4056\n17 110.0828 101.4056\n18 110.0828 101.4056\n19 110.0828 101.4056\n20 110.0828 101.4056\n21 110.0828 101.4056\n22 110.0828 101.4056\n\n$`1`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 144.2865 150.6722\n14 142.8660 151.5432\n15 142.8660 151.5432\n16 142.8660 151.5432\n17 142.8660 151.5432\n18 142.8660 151.5432\n19 142.8660 151.5432\n20 142.8660 151.5432\n21 142.8660 151.5432\n22 142.8660 151.5432\n\n\n$`2`\n$`2`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 153.8708 139.5919 139.5919 139.5919 139.5919 139.5919 139.5919 139.5919\n [9] 139.5919 139.5919\n\n$`2`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 136.3778 127.1175\n14 115.8789 103.3260\n15 115.8789 103.3260\n16 115.8789 103.3260\n17 115.8789 103.3260\n18 115.8789 103.3260\n19 115.8789 103.3260\n20 115.8789 103.3260\n21 115.8789 103.3260\n22 115.8789 103.3260\n\n$`2`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 171.3638 180.6240\n14 163.3048 175.8577\n15 163.3048 175.8577\n16 163.3048 175.8577\n17 163.3048 175.8577\n18 163.3048 175.8577\n19 163.3048 175.8577\n20 163.3048 175.8577\n21 163.3048 175.8577\n22 163.3048 175.8577\n\n\n$`3`\n$`3`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 173.6413 170.0479 170.0479 170.0479 170.0479 170.0479 170.0479 170.0479\n [9] 170.0479 170.0479\n\n$`3`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 153.5404 142.8995\n14 146.6452 134.2565\n15 146.6452 134.2565\n16 146.6452 134.2565\n17 146.6452 134.2565\n18 146.6452 134.2565\n19 146.6452 134.2565\n20 146.6452 134.2565\n21 146.6452 134.2565\n22 146.6452 134.2565\n\n$`3`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 193.7423 204.3831\n14 193.4506 205.8393\n15 193.4506 205.8393\n16 193.4506 205.8393\n17 193.4506 205.8393\n18 193.4506 205.8393\n19 193.4506 205.8393\n20 193.4506 205.8393\n21 193.4506 205.8393\n22 193.4506 205.8393\n\n\n$`4`\n$`4`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 194.0074 194.0119 194.0147 194.0164 194.0174 194.0180 194.0184 194.0186\n [9] 194.0187 194.0188\n\n$`4`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 169.7973 156.9812\n14 165.6741 150.6730\n15 164.2944 148.5614\n16 163.8005 147.8051\n17 163.6201 147.5288\n18 163.5539 147.4272\n19 163.5296 147.3898\n20 163.5207 147.3761\n21 163.5175 147.3711\n22 163.5163 147.3692\n\n$`4`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 218.2176 231.0336\n14 222.3497 237.3509\n15 223.7350 239.4680\n16 224.2322 240.2276\n17 224.4146 240.5059\n18 224.4821 240.6088\n19 224.5071 240.6469\n20 224.5165 240.6611\n21 224.5200 240.6664\n22 224.5213 240.6684\n\n\n$`5`\n$`5`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 206.8929 210.7977 213.3851 215.0996 216.2356 216.9884 217.4872 217.8178\n [9] 218.0368 218.1819\n\n$`5`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 178.2600 163.1026\n14 176.4492 158.2662\n15 176.8082 157.4455\n16 177.5860 157.7275\n17 178.3181 158.2458\n18 178.8949 158.7294\n19 179.3167 159.1104\n20 179.6134 159.3893\n21 179.8176 159.5856\n22 179.9562 159.7208\n\n$`5`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 235.5258 250.6831\n14 245.1461 263.3291\n15 249.9620 269.3246\n16 252.6131 272.4716\n17 254.1531 274.2255\n18 255.0819 275.2475\n19 255.6578 275.8641\n20 256.0221 276.2462\n21 256.2559 276.4879\n22 256.4076 276.6430\n\n\n$`6`\n$`6`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 245.0709 240.0400 240.0400 240.0400 240.0400 240.0400 240.0400 240.0400\n [9] 240.0400 240.0400\n\n$`6`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 212.6687 195.5160\n14 196.9893 174.1996\n15 196.9893 174.1996\n16 196.9893 174.1996\n17 196.9893 174.1996\n18 196.9893 174.1996\n19 196.9893 174.1996\n20 196.9893 174.1996\n21 196.9893 174.1996\n22 196.9893 174.1996\n\n$`6`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 277.4731 294.6259\n14 283.0907 305.8803\n15 283.0907 305.8803\n16 283.0907 305.8803\n17 283.0907 305.8803\n18 283.0907 305.8803\n19 283.0907 305.8803\n20 283.0907 305.8803\n21 283.0907 305.8803\n22 283.0907 305.8803\n\n\n$`7`\n$`7`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 278.0001 278.0001 278.0002 278.0002 278.0002 278.0002 278.0002 278.0002\n [9] 278.0002 278.0002\n\n$`7`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 236.8903 215.1282\n14 228.5879 202.4307\n15 225.3145 197.4243\n16 223.9224 195.2953\n17 223.3147 194.3659\n18 223.0466 193.9559\n19 222.9278 193.7742\n20 222.8751 193.6936\n21 222.8516 193.6577\n22 222.8412 193.6418\n\n$`7`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 319.1098 340.8720\n14 327.4123 353.5695\n15 330.6859 358.5760\n16 332.0780 360.7051\n17 332.6857 361.6345\n18 332.9538 362.0445\n19 333.0726 362.2262\n20 333.1254 362.3069\n21 333.1488 362.3427\n22 333.1592 362.3587\n\n\n$`8`\n$`8`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 349.0540 373.2678 369.7906 348.0549 325.4487 315.1915 319.8599 332.7645\n [9] 344.2812 348.1670\n\n$`8`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 315.6225 297.9249\n14 322.1404 295.0752\n15 314.7795 285.6584\n16 292.8344 263.6024\n17 266.5768 235.4118\n18 252.9822 220.0505\n19 257.0954 223.8699\n20 269.7958 236.4622\n21 280.1875 246.2583\n22 283.2781 248.9280\n\n$`8`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 382.4855 400.1831\n14 424.3952 451.4604\n15 424.8018 453.9229\n16 403.2754 432.5074\n17 384.3206 415.4855\n18 377.4009 410.3325\n19 382.6243 415.8498\n20 395.7332 429.0668\n21 408.3750 442.3042\n22 413.0559 447.4061\n\n\n$`9`\n$`9`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 378.9729 406.5723 408.7509 392.6048 372.9147 361.5778 362.0569 370.2398\n [9] 379.1516 383.6927\n\n$`9`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 336.2126 313.5766\n14 342.0963 307.9648\n15 339.3660 302.6358\n16 323.1265 286.3469\n17 300.2319 261.7560\n18 285.7363 245.5882\n19 285.5516 245.0521\n20 293.6654 253.1294\n21 301.8675 260.9558\n22 305.8147 264.5885\n\n$`9`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 421.7333 444.3692\n14 471.0482 505.1797\n15 478.1359 514.8660\n16 462.0831 498.8627\n17 445.5975 484.0734\n18 437.4193 477.5674\n19 438.5622 479.0617\n20 446.8142 487.3503\n21 456.4356 497.3473\n22 461.5707 502.7968\n\n\n$`10`\n$`10`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 391.9249 381.5489 381.5489 381.5489 381.5489 381.5489 381.5489 381.5489\n [9] 381.5489 381.5489\n\n$`10`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 331.8921 300.1126\n14 304.6704 263.9734\n15 304.6704 263.9734\n16 304.6704 263.9734\n17 304.6704 263.9734\n18 304.6704 263.9734\n19 304.6704 263.9734\n20 304.6704 263.9734\n21 304.6704 263.9734\n22 304.6704 263.9734\n\n$`10`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 451.9577 483.7372\n14 458.4274 499.1244\n15 458.4274 499.1244\n16 458.4274 499.1244\n17 458.4274 499.1244\n18 458.4274 499.1244\n19 458.4274 499.1244\n20 458.4274 499.1244\n21 458.4274 499.1244\n22 458.4274 499.1244\n\n\n$`11`\n$`11`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 408.4203 410.7762 412.3990 413.5168 414.2868 414.8171 415.1824 415.4340\n [9] 415.6074 415.7268\n\n$`11`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 342.2241 307.1820\n14 330.3960 287.8452\n15 326.1006 280.4170\n16 324.5481 277.4509\n17 324.0788 276.3255\n18 324.0270 275.9656\n19 324.1175 275.9106\n20 324.2390 275.9632\n21 324.3506 276.0422\n22 324.4407 276.1168\n\n$`11`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 474.6165 509.6586\n14 491.1565 533.7072\n15 498.6974 544.3810\n16 502.4855 549.5827\n17 504.4948 552.2480\n18 505.6072 553.6686\n19 506.2474 554.4543\n20 506.6291 554.9049\n21 506.8641 555.1726\n22 507.0128 555.3367\n\n\n$`12`\n$`12`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 502.9998 476.0531 476.0531 476.0531 476.0531 476.0531 476.0531 476.0531\n [9] 476.0531 476.0531\n\n$`12`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 437.2687 402.4728\n14 387.1722 340.1214\n15 387.1722 340.1214\n16 387.1722 340.1214\n17 387.1722 340.1214\n18 387.1722 340.1214\n19 387.1722 340.1214\n20 387.1722 340.1214\n21 387.1722 340.1214\n22 387.1722 340.1214\n\n$`12`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 568.7308 603.5267\n14 564.9341 611.9848\n15 564.9341 611.9848\n16 564.9341 611.9848\n17 564.9341 611.9848\n18 564.9341 611.9848\n19 564.9341 611.9848\n20 564.9341 611.9848\n21 564.9341 611.9848\n22 564.9341 611.9848\n\ndev.off()\n\nnull device \n          1 \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-03/index.html",
    "href": "posts/rtip-2023-03-03/index.html",
    "title": "Simple examples of pmap() from {purrr}",
    "section": "",
    "text": "Introduction\nThe pmap() function in R is part of the purrr library, which is a package designed to make it easier to work with functions that operate on vectors, lists, and other types of data structures.\nThe pmap() function is used to apply a function to a list of arguments, where each element in the list contains the arguments for a single function call. The function is applied in parallel, meaning that each call is executed concurrently, which can help speed up computations when working with large datasets.\nHere is the basic syntax of the pmap() function:\n\npmap(.l, .f, ...)\n\nwhere:\n\n.l - is a list of arguments, where each element of the list contains the arguments for a single function call.\n.f - is the function to apply to the arguments in .l.\n... - is used to pass additional arguments to .f.\n\nThe pmap() function returns a list, where each element of the list contains the output of a single function call.\nLet’s define a function for an example.\n\n\nFunction\n\nmy_function <- function(a, b, c) {\n  # do something with a, b, and c\n  return(a + b + c)\n}\n\nA very simple function that just adds up the elements passed.\nNow let’s go over a couple simple examples.\n\n\nExample\n\nlibrary(purrr)\nlibrary(TidyDensity)\n\n\n# create a list of vectors with your arguments\nmy_args <- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(7, 8, 9)\n)\n\n# apply your function to each combination of arguments in parallel\nresults <- pmap(my_args, my_function)\n\n# print the results\nprint(results)\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 18\n\n\nNow lets see a couple more examples.\n\nargsl <- list(\n  c(100, 100, 100, 100), # this is .n\n  c(0,1,2,3),            # this is .mean\n  c(4,3,2,1),            # this is .sd\n  c(10,10,10,10)         # this is .num_sims\n)\n\npmap(argsl, tidy_normal)\n\n[[1]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx        dy     p      q\n   <fct>      <int>  <dbl> <dbl>     <dbl> <dbl>  <dbl>\n 1 1              1  3.56  -15.0 0.0000353 0.814  3.56 \n 2 1              2 -0.433 -14.6 0.0000679 0.457 -0.433\n 3 1              3 -1.93  -14.3 0.000125  0.315 -1.93 \n 4 1              4  1.68  -14.0 0.000219  0.663  1.68 \n 5 1              5  4.18  -13.7 0.000369  0.852  4.18 \n 6 1              6  0.805 -13.4 0.000596  0.580  0.805\n 7 1              7  7.99  -13.1 0.000922  0.977  7.99 \n 8 1              8 -1.61  -12.8 0.00137   0.344 -1.61 \n 9 1              9  1.83  -12.5 0.00195   0.676  1.83 \n10 1             10  6.66  -12.1 0.00267   0.952  6.66 \n# … with 990 more rows\n\n[[2]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx        dy      p      q\n   <fct>      <int>  <dbl> <dbl>     <dbl>  <dbl>  <dbl>\n 1 1              1 -0.335 -9.02 0.0000814 0.328  -0.335\n 2 1              2  2.00  -8.82 0.000162  0.630   2.00 \n 3 1              3 -0.238 -8.62 0.000304  0.340  -0.238\n 4 1              4  1.17  -8.41 0.000544  0.523   1.17 \n 5 1              5  1.50  -8.21 0.000921  0.567   1.50 \n 6 1              6  4.68  -8.01 0.00148   0.890   4.68 \n 7 1              7  4.59  -7.81 0.00227   0.884   4.59 \n 8 1              8 -1.18  -7.61 0.00331   0.233  -1.18 \n 9 1              9  2.35  -7.40 0.00460   0.673   2.35 \n10 1             10 -3.73  -7.20 0.00610   0.0574 -3.73 \n# … with 990 more rows\n\n[[3]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx       dy     p      q\n   <fct>      <int>  <dbl> <dbl>    <dbl> <dbl>  <dbl>\n 1 1              1  4.42  -3.98 0.000118 0.886  4.42 \n 2 1              2  2.24  -3.86 0.000211 0.547  2.24 \n 3 1              3 -0.207 -3.73 0.000369 0.135 -0.207\n 4 1              4  3.32  -3.61 0.000622 0.745  3.32 \n 5 1              5  0.999 -3.48 0.00101  0.308  0.999\n 6 1              6  4.08  -3.36 0.00160  0.851  4.08 \n 7 1              7  5.81  -3.23 0.00244  0.972  5.81 \n 8 1              8  6.11  -3.11 0.00362  0.980  6.11 \n 9 1              9  2.30  -2.98 0.00518  0.560  2.30 \n10 1             10  0.231 -2.86 0.00718  0.188  0.231\n# … with 990 more rows\n\n[[4]]\n# A tibble: 1,000 × 7\n   sim_number     x     y      dx       dy       p     q\n   <fct>      <int> <dbl>   <dbl>    <dbl>   <dbl> <dbl>\n 1 1              1 3.41  -0.635  0.000128 0.658   3.41 \n 2 1              2 0.415 -0.557  0.000243 0.00487 0.415\n 3 1              3 3.24  -0.479  0.000440 0.593   3.24 \n 4 1              4 3.73  -0.401  0.000758 0.768   3.73 \n 5 1              5 4.22  -0.324  0.00124  0.889   4.22 \n 6 1              6 3.70  -0.246  0.00193  0.757   3.70 \n 7 1              7 4.35  -0.168  0.00288  0.911   4.35 \n 8 1              8 1.50  -0.0899 0.00408  0.0672  1.50 \n 9 1              9 2.58  -0.0120 0.00551  0.336   2.58 \n10 1             10 3.41   0.0658 0.00713  0.661   3.41 \n# … with 990 more rows\n\npmap(argsl, tidy_normal) |>\n  map(tidy_autoplot)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-06/index.html",
    "href": "posts/rtip-2023-03-06/index.html",
    "title": "Simple examples of imap() from {purrr}",
    "section": "",
    "text": "Introduction\nThe imap() function is a powerful tool for iterating over a list or a vector while also keeping track of the index or names of the elements. This function applies a given function to each element of a list, along with the name or index of that element, and returns a new list with the results.\nThe imap() function takes two main arguments: x and .f. x is the list or vector to iterate over, and .f is the function to apply to each element. The .f function takes two arguments: x and i, where x is the value of the element and i is the index or name of the element.\n\n\nFunction\nHere is the imap() function.\n\nimap(.x, .f, ...)\n\nHere is the documentation from the function page:\n\n.x - A list or atomic vector.\n.f - A function, specified in one of the following ways:\n\nA named function, e.g. paste.\nAn anonymous function, e.g. (x, idx) x + idx or function(x, idx) x + idx.\nA formula, e.g. ~ .x + .y. You must use .x to refer to the current element and .y to refer to the current index. Only recommended if you require backward compatibility with older versions of R.\n\n... - Additional arguments passed on to the mapped function. We now generally recommend against using … to pass additional (constant) arguments to .f. Instead use a shorthand anonymous function:\n\n\n# Instead of\nx |> map(f, 1, 2, collapse = \",\")\n# do:\nx |> map(\\(x) f(x, 1, 2, collapse = \",\"))\n\nThis makes it easier to understand which arguments belong to which function and will tend to yield better error messages.\n\n\nExample\nHere’s an example of using imap() with a simple list of integers:\n\nlibrary(purrr)\n\n# create a list of integers\nmy_list <- list(1, 2, 3, 4, 5)\n\n# define a function to apply to each element of the list\nmy_function <- function(x, i) {\n  paste(\"The element at index\", i, \"is\", x)\n}\n\n# apply the function to each element of the list using imap()\nresult <- imap(my_list, my_function)\n\n# print the result\nprint(result)\n\n[[1]]\n[1] \"The element at index 1 is 1\"\n\n[[2]]\n[1] \"The element at index 2 is 2\"\n\n[[3]]\n[1] \"The element at index 3 is 3\"\n\n[[4]]\n[1] \"The element at index 4 is 4\"\n\n[[5]]\n[1] \"The element at index 5 is 5\"\n\n\nIn this example, we create a list of integers called my_list. We define a function called my_function that takes two arguments: x, which is the value of each element in the list, and i, which is the index of that element. We then use imap() to apply my_function to each element of my_list, passing both the value and the index of the element as arguments. The result is a new list where each element contains the output of my_function applied to the corresponding element of my_list.\nNow let’s take a look at a slightly more complex example. In this case, we will use imap() to iterate over a list of data frames, apply a function to each data frame that subsets the data to include only certain columns, and return a new list of data frames with the subsetted data.\n\n# create a list of data frames\nmy_list <- list(\n  data.frame(x = 1:5, y = c(\"a\", \"b\", \"c\", \"d\", \"e\")),\n  data.frame(x = 6:10, y = c(\"f\", \"g\", \"h\", \"i\", \"j\")),\n  data.frame(x = 11:15, y = c(\"k\", \"l\", \"m\", \"n\", \"o\"))\n)\n\n# define a function to apply to each element of the list\nmy_function <- function(df, i) {\n  # subset the data to include only the x column\n  df_subset <- df[, \"x\", drop = FALSE]\n  # rename the column to include the index of the element\n  colnames(df_subset) <- paste(\"x_\", i, sep = \"\")\n  # return the subsetted data frame\n  return(df_subset)\n}\n\n# apply the function to each element of the list using imap\nresult <- imap(my_list, my_function)\n\n# print the result\nprint(result)\n\n[[1]]\n  x_1\n1   1\n2   2\n3   3\n4   4\n5   5\n\n[[2]]\n  x_2\n1   6\n2   7\n3   8\n4   9\n5  10\n\n[[3]]\n  x_3\n1  11\n2  12\n3  13\n4  14\n5  15\n\n\nIn this example, we create a list of three data frames called my_list. We define a function called my_function that takes two arguments: df, which is the value of each element in the list (a data frame), and i, which is the index of that element. The function subsets the data frame to include only the x column, renames the column to include the index of the element, and returns the subsetted data frame.\nWe use imap() to apply my_function to each element of my_list, passing both the data frame and the index of the element as arguments. The result is a new list of data frames, where each data frame contains only the x column from the original data frame, with a new name that includes the index of the element.\nAs you can see, the output is a list of three data frames, each containing only the x column from the corresponding original data frame, with a new name that includes the index of the element.\nIn summary, the imap() function from the R library purrr is a useful tool for iterating over a list or a vector while also keeping track of the index or names of the elements. The function takes a list or a vector as its first argument, and a function as its second argument, which takes two arguments: the value of each element, and the index or name of that element. The function returns a new list or vector with the results of applying the function to each element of the original list or vector. This function is particularly useful for complex data structures, where the index or name of each element is important for further data analysis or processing.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-07/index.html",
    "href": "posts/rtip-2023-03-07/index.html",
    "title": "tidy_bernoulli() with {data.table}",
    "section": "",
    "text": "Introduction\nSo I was challanged by Adrian Antico to learn data.table, so yesterday I started with a single function from my package {TidyDensity} called tidy_bernoulli().\nSo let’s see how I did (hint, works but needs a lot of improvement, so I’ll learn it.)\n\n\nFunction\nLet’s see the function in data.table\n\nlibrary(data.table)\nlibrary(tidyr)\nlibrary(stats)\nlibrary(purrr)\n\nnew_func &lt;- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data &lt;- data.table(sim_number = factor(seq(1, num_sims, 1)))\n  \n  # Group the data by sim_number and add columns for x and y\n  sim_data[, `:=` (\n    x = list(1:n),\n    y = list(stats::rbinom(n = n, size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, `:=` (\n    d = list(density(unlist(y), n = n)[c(\"x\", \"y\")] |&gt;\n      set_names(\"dx\", \"dy\") |&gt;\n      as_tibble())\n  ), by = sim_number]\n  \n  # Compute the p-values for the y values and add a column for p\n  sim_data[, `:=` (\n    p = list(stats::pbinom(unlist(y), size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, `:=` (\n    q = list(stats::qbinom(unlist(p), size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Unnest the columns for x, y, d, p, and q\n  sim_data &lt;- sim_data[, \n                       unnest(\n                         .SD, \n                         cols = c(\"x\", \"y\", \"d\", \"p\", \"q\")\n                         ), \n                       by = sim_number]\n  \n  # Remove the grouping\n  sim_data[, sim_number := as.factor(sim_number)]\n  \n  return(sim_data)\n}\n\n\n\nExample\nNow, let’s see the output of the original function tidy_bernoulli() and new_func().\n\nlibrary(TidyDensity)\nn &lt;- 50\npr &lt;- 0.1\nsims &lt;- 5\n\nset.seed(123)\ntb &lt;- tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n\nset.seed(123)\nnf &lt;- new_func(n = n, num_sims = sims, pr = pr)\n\nprint(tb)\n\n# A tibble: 250 × 7\n   sim_number     x     y      dx     dy     p     q\n   &lt;fct&gt;      &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1              1     0 -0.405  0.0292   0.9     0\n 2 1              2     0 -0.368  0.0637   0.9     0\n 3 1              3     0 -0.331  0.129    0.9     0\n 4 1              4     0 -0.294  0.243    0.9     0\n 5 1              5     1 -0.258  0.424    1       1\n 6 1              6     0 -0.221  0.688    0.9     0\n 7 1              7     0 -0.184  1.03     0.9     0\n 8 1              8     0 -0.147  1.44     0.9     0\n 9 1              9     0 -0.110  1.87     0.9     0\n10 1             10     0 -0.0727 2.25     0.9     0\n# ℹ 240 more rows\n\nprint(nf)\n\n     sim_number  x y         dx          dy   p q\n  1:          1  1 0 -0.4053113 0.029196114 0.9 0\n  2:          1  2 0 -0.3683598 0.063683226 0.9 0\n  3:          1  3 0 -0.3314083 0.129227066 0.9 0\n  4:          1  4 0 -0.2944568 0.242967496 0.9 0\n  5:          1  5 1 -0.2575054 0.424395426 1.0 1\n ---                                             \n246:          5 46 0  1.2575054 0.057872104 0.9 0\n247:          5 47 0  1.2944568 0.033131931 0.9 0\n248:          5 48 1  1.3314083 0.017621873 1.0 1\n249:          5 49 1  1.3683598 0.008684076 1.0 1\n250:          5 50 0  1.4053113 0.003981288 0.9 0\n\n\nOk so at least the output is identical which is a good sign. Now let’s benchmark the two solutions.\n\nlibrary(rbenchmark)\nlibrary(dplyr)\n\nbenchmark(\n  \"original\" = {\n    tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n  },\n  \"data.table\" = {\n    new_func(n = n, pr = pr, num_sims = sims)\n  },\n  replications = 100,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) |&gt;\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1   original          100    3.18    1.000      2.89     0.12\n2 data.table          100    4.89    1.538      4.49     0.11\n\n\nYeah, needs some work but it’s a start."
  },
  {
    "objectID": "posts/rtip-2023-03-08/index.html",
    "href": "posts/rtip-2023-03-08/index.html",
    "title": "Getting NYS Home Heating Oil Prices with {rvest}",
    "section": "",
    "text": "Introduction\nIf you live in New York and rely on heating oil to keep your home warm during the colder months, you know how important it is to keep track of heating oil prices. Fortunately, with a bit of R code, you can easily access the latest heating oil prices in New York.\nThe code uses the {dplyr} package to clean and manipulate the data, as well as the {timetk} package to plot the time series. Here’s a breakdown of what the code does:\n\nFirst, it loads the necessary packages and sets the URL for the data source.\nNext, it reads the HTML from the URL using the read_html function from the xml2 package.\nIt then uses the html_node function from the rvest package to extract the HTML node that contains the data table.\n\nThe resulting data table is then cleaned and transformed using dplyr functions such as html_table, as_tibble, set_names, select, mutate, and arrange.\nFinally, the resulting time series data is plotted using plot_time_series from the timetk package.\nTo run this code, you will need to have these packages installed on your machine. You can install them using the install.packages function in R. Here’s how you can install the packages:\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"xml2\")\ninstall.packages(\"rvest\")\ninstall.packages(\"tibble\")\ninstall.packages(\"purrr\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"timetk\")\n\nOnce you have installed the packages, you can copy and paste the code into your R console or RStudio and run it to get the latest heating oil prices in New York.\nIn conclusion, the code above provides a simple and efficient way to access and visualize heating oil prices in New York using R. By keeping track of these prices, you can make informed decisions about when to buy heating oil and how much to purchase, ultimately saving you money on your heating bills.\n\n\nExample\nNow let’s run it!\n\nurl  <- \"https://www.eia.gov/opendata/qb.php?sdid=PET.W_EPD2F_PRS_SNY_DPG.W\"\npage <- xml2::read_html(url)\nnode <- rvest::html_node(\n    x = page\n    , xpath = \"/html/body/div[1]/section/div/div/div[2]/div[1]/table\"\n)\nny_tbl <- node |>\n    rvest::html_table() |>\n    tibble::as_tibble() |>\n    purrr::set_names('series_name','period','frequency','value','units') |>\n    dplyr::select(period, frequency, value, units, series_name) |>\n    dplyr::mutate(period = lubridate::ymd(period)) |>\n    dplyr::arrange(period)\n\nny_tbl |>\n    timetk::plot_time_series(.date_var = period, .value = value)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-09/index.html",
    "href": "posts/rtip-2023-03-09/index.html",
    "title": "Multiple Solutions to speedup tidy_bernoulli() with {data.table}",
    "section": "",
    "text": "Introduction\nI had just recently posted on making an attempt to speedup computations with my package {TidyDensity} using a purely data.table solution, yes of course I can use {dtplyr} or {tidytable} but that not the challenge put to me.\nMy original attempt was worse than the original solution of tidy_bernoulli(). After I posted on Mastadon, LinkedIn and Reddit, I recieved potential solutions from each site by users. Let’s check them out below.\n\n\nFunction\nFirst let’s load in the necessary libraries.\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(rbenchmark)\nlibrary(TidyDensity)\n\nNow let’s look at the different solutions.\n\n# My original new function\nnew_func &lt;- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data &lt;- data.table(sim_number = factor(seq(1, num_sims, 1)))\n\n  # Group the data by sim_number and add columns for x and y\n  sim_data[, `:=` (\n    x = list(1:n),\n    y = list(stats::rbinom(n = n, size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, `:=` (\n    d = list(density(unlist(y), n = n)[c(\"x\", \"y\")] |&gt;\n               set_names(\"dx\", \"dy\") |&gt;\n               as_tibble())\n  ), by = sim_number]\n\n  # Compute the p-values for the y values and add a column for p\n  sim_data[, `:=` (\n    p = list(stats::pbinom(unlist(y), size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, `:=` (\n    q = list(stats::qbinom(unlist(p), size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Unnest the columns for x, y, d, p, and q\n  sim_data &lt;- sim_data[,\n                       unnest(\n                         .SD,\n                         cols = c(\"x\", \"y\", \"d\", \"p\", \"q\")\n                       ),\n                       by = sim_number]\n\n  # Remove the grouping\n  sim_data[, sim_number := as.factor(sim_number)]\n\n  return(sim_data)\n}\n\nreddit_func &lt;- function(num_sims, n, pr) {\n  sim_dat &lt;- data.table(sim_number = rep(1:num_sims,each=n),\n                        x          = rep(1:n,num_sims))\n\n  sim_dat[, y := stats::rbinom(n = n, size = 1, prob = pr), by=sim_number]\n  sim_dat[, c(\"dx\",\"dy\") := density(y,n=n)[c(\"x\",\"y\")]    , by=sim_number]\n  sim_dat[, p := stats::pbinom(y, size = 1, prob = pr)    , by=sim_number]\n  sim_dat[, q := stats::qbinom(p, size = 1, prob = pr)    , by=sim_number]\n  \n  return(sim_dat)\n}\n\nmastadon_func &lt;- function(num_sims, n, pr){\n  sim_data &lt;- data.table(sim_number = 1:num_sims\n  )[, `:=`( x = .(1:n), y= .(rbinom(n = n, size = 1, prob = pr))), sim_number\n  ][, `:=`( d = .(density(unlist(y), n = n)[c('x','y')] |&gt; \n                    as.data.table() |&gt; \n                    setnames(c('dx','dy'))\n                  )\n            ), sim_number\n  ][, `:=`( p = .(pbinom(unlist(y), size = 1, prob = pr))), sim_number\n  ][, `:=`( q = .(qbinom(unlist(p), size = 1, prob = pr))), sim_number]\n\n    cbind(\n      sim_data[, lapply(.SD, unlist), by = sim_number, .SDcol = c('x','y','p','q')],\n      rbindlist(sim_data$d)\n    ) |&gt;\n    setcolorder(c('sim_number','x','y','dx','dy'))\n    \n    return(sim_data)\n}\n\nlinkedin_func &lt;- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data &lt;- CJ(sim_number = factor(1:num_sims), x = 1:n)\n\n  # Group the data by sim_number and add columns for x and y\n  sim_data[, y := stats::rbinom(n = .N, size = 1, prob = pr)]\n\n\n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, c(\"dx\", \"dy\") := density(y, n = n)[c(\"x\", \"y\")], by = sim_number]\n\n  # Compute the p-values for the y values and add a column for p\n  sim_data[, p := stats::pbinom(y, size = 1, prob = pr)]\n\n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, q := stats::qbinom(p, size = 1, prob = pr)]\n  setkey(sim_data, NULL) # needed only to compare with new_func\n  return(sim_data)\n}\n\nAll of the functions work in the same set of three arguments as input: * num_sims: an integer value that specifies the number of simulations to run * n: an integer value that specifies the sample size * pr: a numeric value that specifies the probability of success\nThe functions use the data.table package to create a data table named sim_dat/sim_data. The data table has two columns: sim_number and x. The sim_number column represents the simulation number, and x column represents the observation number.\nThe functions then generate random binary data using the rbinom function from the stats package. The function generates n binary data points for each simulation number (sim_number) using the input parameter pr as the probability of success. The resulting binary data points are stored in the y column of sim_dat/data.\nNext, the function calculates the density of y using the density function from the stats package. The function calculates the density separately for each simulation number (sim_number) and stores the resulting values in the dx and dy columns of sim_dat/data.\nThe functions then calculate the cumulative probability (p) of each binary data point using the pbinom function from the stats package. The function calculates the cumulative probability separately for each simulation number (sim_number) and stores the resulting values in the p column of sim_dat.\nFinally, the functions calculate the inverse of the cumulative probability (q) using the qbinom function from the stats package. The function calculates the inverse of the cumulative probability separately for each simulation number (sim_number) and stores the resulting values in the q column of sim_dat.\nThe functions then return the data table containing the results of the simulations.\n\n\nExample\nHow do they stack up to each other? Lets see!\n\nn &lt;- 50\npr &lt;- 0.1\nnum_sims &lt;- sims &lt;- 5\n\nbenchmark(\n  \"tidy_bernoulli()\" = {\n    tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n  },\n  \"my.first.attempt\" = {\n    new_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"linkedin.attempt\" = {\n    linkedin_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"mastadon.attempt\" = {\n    mastadon_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"reddit.attempt\" = {\n    reddit_func(n = n, pr = pr, num_sims = sims)\n  },\n  replications = 200,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) |&gt;\n  arrange(relative)\n\n              test replications elapsed relative user.self sys.self\n1 linkedin.attempt          200    0.95    1.000      0.92     0.02\n2   reddit.attempt          200    1.03    1.084      1.00     0.03\n3 mastadon.attempt          200    1.60    1.684      1.52     0.06\n4 tidy_bernoulli()          200    5.83    6.137      5.40     0.37\n5 my.first.attempt          200    8.66    9.116      8.17     0.14\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-10/index.html",
    "href": "posts/rtip-2023-03-10/index.html",
    "title": "Use of the apply family of functions",
    "section": "",
    "text": "In this post I will talk about the use of the R functions apply(), lapply(), sapply(), tapply(), and vapply() with examples.\nThese functions are all designed to help users apply a function to a set of data in R, but they differ in their input and output types, as well as in the way they handle missing values and other complexities. By using the right function for your particular problem, you can make your code more efficient and easier to read.\nLet’s start with the basics.\n\n\nBefore we dive into the details of each function, let’s define some terms:\n\nA vector is a one-dimensional array of data, like a list of numbers or strings.\nA matrix is a two-dimensional array of data, like a table of numbers.\nA data frame is a two-dimensional object that can hold different types of data, like a spreadsheet.\nA list is a collection of objects, which can be of different types, like a shopping bag full of different items.\n\nEach of the five functions we’ll discuss here takes a list as input (although some can also take vectors or matrices). Let’s create a list object to use in our examples:\n\nmy_list <- list(\n  a = c(1, 2, 3),\n  b = matrix(1:6, nrow = 2),\n  c = data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\")),\n  d = c(4, NA, 6),\n  e = list(\"foo\", \"bar\", \"baz\")\n)\n\nThis list contains five elements:\n\nA vector of numbers (a)\nA matrix of numbers (b)\nA data frame with two columns (c)\nA vector of numbers with a missing value (d)\nA list of character strings (e)\n\nNow that we have our data, let’s look at each of the functions in turn.\n\n\n\napply()\nThe apply() function applies a function to the rows or columns of a matrix or array. It is most commonly used with matrices, but can also be used with higher-dimensional arrays. The function takes three arguments:\n\nThe matrix or array to apply the function to\nThe margin (1 for rows, 2 for columns, or a vector of dimensions)\nThe function to apply\n\nLet’s apply the mean() function to the columns of our matrix in my_list$b:\n\napply(my_list$b, 2, mean)\n\n[1] 1.5 3.5 5.5\n\n\nThis will return a vector of means for each column of the matrix\nlapply()\nThe lapply() function applies a function to each element of a list and returns a list of the results. It takes two arguments:\n\nThe list to apply the function to\nThe function to apply\n\nLet’s apply the class() function to each element of our list:\n\nlapply(my_list, class)\n\n$a\n[1] \"numeric\"\n\n$b\n[1] \"matrix\" \"array\" \n\n$c\n[1] \"data.frame\"\n\n$d\n[1] \"numeric\"\n\n$e\n[1] \"list\"\n\n\nThis will return a list of the classes of each element.\nsapply()\nThe sapply() function is similar to lapply(), but it simplifies the output to a vector or matrix if possible. It takes the same two arguments as lapply():\n\nThe list to apply the function\nThe function to apply\n\nLet’s apply the length() function to each element of our list using sapply():\n\nsapply(my_list, length)\n\na b c d e \n3 6 2 3 3 \n\n\nThis will return a vector of lengths for each element.\ntapply()\nThe tapply() function applies a function to subsets of a vector or data frame, grouped by one or more factors. It takes three arguments:\n\nThe vector or data frame to apply the function to\nThe factor(s) to group the data by\nThe function to apply\n\nLet’s apply the mean() function to the elements of our vector my_list$d, grouped by whether they are missing or not:\n\ntapply(my_list$d, !is.na(my_list$d), mean)\n\nFALSE  TRUE \n   NA     5 \n\n\nThis will return a vector of means for each group where they are NOT NA.\nvapply()\nThe vapply() function is similar to sapply(), but allows the user to specify the output type and length, making it more efficient and less prone to errors. It takes four arguments:\n\nThe list to apply the function to\nThe function to apply\nThe output type of the function\nThe length of the output vector or matrix\n\nLet’s apply the length() function to each element of our list, specifying that the output type is an integer and the length is 1:\n\nvapply(my_list, length, integer(1))\n\na b c d e \n3 6 2 3 3 \n\n\nThis will return a matrix of lengths for each element, with 1 row:"
  },
  {
    "objectID": "posts/rtip-2023-03-17/index.html",
    "href": "posts/rtip-2023-03-17/index.html",
    "title": "Use of the apply family of functions",
    "section": "",
    "text": "I am thrilled to announce that the R universe of packages {healthyverse} has surpassed 60,000 downloads! Thank you to everyone who has downloaded and used these packages, your support is greatly appreciated.\nFor those who are not familiar, the {healthyverse} package is a collection of R packages focused on data science and analysis with an emphasis on healthcare. These packages are ever evolving and for the most part still in an experimental stage, but are maturing. These packages are:\n\n{healthyR}\n{healthyR.ts}\n{healthyR.ai}\n{healthyR.data}\n{TidyDensity}\n{tidyAML}\n\nI am continuously working on updates and improvements to the {healthyverse} package, and I hope to release them soon. Some of the updates include bug fixes, new functionality, and enhancements to existing functions.\nAdditionally, I want to encourage users who are interested in contributing to the {healthyverse} package to submit pull requests. Contributions can be in the form of bug fixes, new functions, or enhancements to existing ones. I am always open to feedback and suggestions on how to improve these packages.\nOnce again, thank you to everyone who has downloaded and used the {healthyverse} package. Your support motivates me to continue working on this project and making it the best it can be.\n\n\n\n60k"
  },
  {
    "objectID": "posts/rtip-2023-03-21/index.html",
    "href": "posts/rtip-2023-03-21/index.html",
    "title": "Getting the CCI30 Index Current Makeup",
    "section": "",
    "text": "Introduction\nThe CCI30 Crypto Index is a cryptocurrency index that tracks the performance of the top 30 cryptocurrencies by market capitalization. It was created in 2017 by a team of researchers and analysts from the CryptoCompare and MVIS indices.\nThe CCI30 Crypto Index is designed to provide a broad-based and representative measure of the cryptocurrency market’s overall performance. It includes a diverse range of cryptocurrencies, such as Bitcoin, Ethereum, Litecoin, Ripple, and many others. The index is weighted by market capitalization, with each cryptocurrency’s weight determined by its market capitalization relative to the total market capitalization of all 30 cryptocurrencies.\nThe CCI30 Crypto Index has become a popular benchmark for the cryptocurrency market, as it offers a comprehensive view of the market’s performance, rather than just focusing on one particular cryptocurrency. It is often used by investors, traders, and researchers to analyze trends and make investment decisions.\nOne notable feature of the CCI30 Crypto Index is that it is rebalanced every quarter. This means that the composition of the index is adjusted to reflect changes in the market capitalization of the constituent cryptocurrencies. This helps to ensure that the index remains representative of the overall cryptocurrency market.\nOverall, the CCI30 Crypto Index provides a useful tool for tracking the performance of the cryptocurrency market. It is a valuable resource for investors, traders, and researchers who are interested in this exciting and rapidly evolving field.\n\n\nCode Explanation\nLet’s break it down step by step:\n\nThe first line of the code loads the “dplyr” package, which provides a set of functions for data manipulation.\nThe second line of the code reads the HTML code from the website “https://cci30.com/” using the “read_html” function from the “xml2” package.\nThe next two blocks of code extract two tables from the HTML document using the “html_node” function from the “rvest” package. The tables are located at two different XPaths in the HTML document.\nThe extracted tables are then converted into tibbles using the “as_tibble” function from the “tibble” package. The tibbles are further transformed by selecting only the columns from the second to the fifth column using the “select” function from the “dplyr” package.\nThe column names of the tibbles are then set using the “set_names” function from the “purrr” package.\nFinally, the two tibbles are combined using the “union” function from the “dplyr” package, and the resulting tibble is printed to the console.\n\nIn summary, the code is extracting two tables from a website, transforming them into tibbles, selecting a subset of columns, renaming the columns, and combining them into a single tibble.\n\n\nExample\n\ncci30 <- xml2::read_html(\"https://cci30.com/\")\n\ntbl1 <- cci30 |>\n    rvest::html_node(xpath = \"/html/body/div[2]/div/div/div/div[2]/div[1]/table\") |>\n    rvest::html_table(header = 1) |>\n    tibble::as_tibble() |>\n    dplyr::select(2:5) |>\n    purrr::set_names(\n        \"Coin\",\"Price\",\"Mkt Cap\",\"Daily Change\"\n    )\n\ntbl2 <- cci30 |>\n    rvest::html_node(xpath = \"/html/body/div[2]/div/div/div/div[2]/div[2]/table\") |>\n    rvest::html_table(header = 1) |>\n    tibble::as_tibble() |>\n    dplyr::select(2:5) |>\n    purrr::set_names(\n        \"Coin\",\"Price\",\"Mkt Cap\",\"Daily Change\"\n    )\n\ntbl <- tbl1 |>\n  dplyr::union(tbl2) |>\n  knitr::kable()\n\ntbl\n\n\n\n\nCoin\nPrice\nMkt Cap\nDaily Change\n\n\n\n\nBitcoin\n$27,767.24\n$536,553,055,078\n0.17%\n\n\nEthereum\n$1,735.32\n$212,357,972,798\n0.04%\n\n\nBNB\n$332.92\n$52,565,516,823\n0.13%\n\n\nXRP\n$0.37\n$19,087,613,742\n0.13%\n\n\nCardano\n$0.33\n$11,547,419,916\n0.05%\n\n\nPolygon\n$1.10\n$9,643,536,324\n0.17%\n\n\nDogecoin\n$0.07\n$9,484,198,878\n0.34%\n\n\nSolana\n$22.18\n$8,507,167,040\n0.12%\n\n\nPolkadot\n$6.10\n$7,119,808,610\n0.08%\n\n\nShiba Inu\n$0.00\n$6,169,390,592\n0.24%\n\n\nTRON\n$0.07\n$5,936,468,687\n0.14%\n\n\nLitecoin\n$78.42\n$5,685,409,727\n0.40%\n\n\nAvalanche\n$16.64\n$5,418,625,875\n0.06%\n\n\nUniswap\n$6.19\n$4,716,487,304\n0.19%\n\n\nChainlink\n$7.06\n$3,649,558,739\n0.06%\n\n\nCosmos\n$11.56\n$3,309,216,299\n0.03%\n\n\nUNUS SED LEO\n$3.35\n$3,195,413,769\n-0.55%\n\n\nToncoin\n$2.38\n$2,907,590,168\n-0.62%\n\n\nMonero\n$151.58\n$2,767,118,876\n-0.01%\n\n\nEthereum Classic\n$19.58\n$2,741,016,944\n-6.84%\n\n\nOKB\n$44.34\n$2,660,455,327\n0.56%\n\n\nBitcoin Cash\n$130.60\n$2,526,173,508\n-0.48%\n\n\nStellar\n$0.09\n$2,293,156,708\n-0.04%\n\n\nCronos\n$0.07\n$1,787,408,658\n1.05%\n\n\nNEAR Protocol\n$2.00\n$1,728,135,015\n0.26%\n\n\nVeChain\n$0.02\n$1,665,251,562\n0.33%\n\n\nQuant\n$126.33\n$1,525,183,149\n0.31%\n\n\nInternet Computer\n$5.11\n$1,516,076,264\n0.19%\n\n\nAlgorand\n$0.21\n$1,498,361,340\n0.41%\n\n\nApeCoin\n$4.06\n$1,496,070,125\n0.12%"
  },
  {
    "objectID": "posts/rtip-2023-03-22/index.html",
    "href": "posts/rtip-2023-03-22/index.html",
    "title": "Some Examples of Cumulative Mean with {TidyDensity}",
    "section": "",
    "text": "Introduction\nCumulative mean is a statistical measure that calculates the mean of a set of numbers up to a certain point in time or after a certain number of observations. It is also known as a running average or moving average.\nCumulative mean can be useful in a variety of contexts. For example:\n\nTracking progress: Cumulative mean can be used to track progress over time. For instance, a teacher might use it to track the average test scores of her students throughout the school year.\nAnalyzing trends: Cumulative mean can help identify trends in data. For example, a business might use it to track the average revenue generated by a new product over the course of several months.\nSmoothing data: Cumulative mean can be used to smooth out fluctuations in data. For instance, a meteorologist might use it to calculate the average temperature over the course of a year, which would help to smooth out the effects of daily temperature fluctuations.\n\nIn summary, cumulative mean is a useful statistical measure that can help track progress, analyze trends, and smooth out fluctuations in data.\n\n\nFunction\nThe function we will review is cmean() from the {TidyDensity} R package. Let’s take a look at it.\n\ncmean()\n\nThe only argument is .x which is a numeric vector as this is a vectorized function. Let’s see it in use.\n\n\nExample\nFirst let’s load in TidyDensity\n\nlibrary(TidyDensity)\n\nOk now let’s make some data. For this we are going to use the simple rnorm() function.\n\nx <- rnorm(100)\n\nhead(x)\n\n[1] -0.8293250 -1.2983499  2.2782337 -0.1521549  0.6859169  0.3809020\n\n\nOk, now that we have our vector, let’s run it through the function and see what it outputs and then we will graph it.\n\ncmx <- cmean(x)\nhead(cmx)\n\n[1] -0.8293249774 -1.0638374319  0.0501862766 -0.0003990095  0.1368641726\n[6]  0.1775371452\n\n\nNow let’s graph it.\n\nplot(cmx, type = \"l\")\n\n\n\n\nOk nice, so can we do this on grouped data or lists of data? Of course! First let’s use a for loop to generate a list of rnorm() values.\n\n# Initialize an empty list to store the generated values\nmy_list <- list()\n\n# Generate values using rnorm(5) in a for loop and store them in the list\nfor (i in 1:5) {\n  my_list[[i]] <- rnorm(100)\n}\n\n# Print the generated list\npurrr::map(my_list, head)\n\n[[1]]\n[1] -0.8054353 -0.4596541 -0.2362475  1.1486398 -0.7242154  0.5184610\n\n[[2]]\n[1]  0.3243327  0.7170802 -0.5963424 -1.0307104  0.3388504  0.5717486\n\n[[3]]\n[1]  1.7360816 -1.0359467 -0.3206138 -1.2157684 -0.8841356  0.1856481\n\n[[4]]\n[1] -1.1401642 -0.4437817 -0.2555245 -0.1809040 -0.2131763 -0.1251750\n\n[[5]]\n[1]  0.08835903 -1.79153379 -2.15010900  0.67344844  1.06125849  0.99848796\n\n\nNow that we have our list object let’s go ahead and plot the values out after we pass the data through cmean().\n\nlibrary(purrr)\n\nmy_list |>\n  map(\\(x) x |> cmean() |> plot(type = \"l\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n\nFrom here I think it is easy to see how one could do this on gruoped data as well with dplyr’s group_by()."
  },
  {
    "objectID": "posts/rtip-2023-03-24/index.html",
    "href": "posts/rtip-2023-03-24/index.html",
    "title": "How fast do the files read in?",
    "section": "",
    "text": "I will demonstrate how to generate a 1,000 row and column matrix with random numbers in R, and then save it in different file formats. I will also show how to get the file size of each saved object and benchmark how long it takes to read in each file using different functions.\n\n\nTo generate a 1,000 row and column matrix with random numbers, we can use the matrix() function and the runif() function in R. Here’s the code to generate the matrix:\n\n# set seed for reproducibility\nset.seed(123)\n\n# number of rows/columns in matrix\nn <- 1000\n\n# generate matrix with random normal values\nmat <- matrix(runif(n^2), nrow = n) \n\nThis code sets the random number generator seed to ensure that the same random numbers are generated every time the code is run. It then generates a vector of 1,000^2 random numbers using the runif() function, and creates a matrix with 1,000 columns using the matrix() function.\n\n\n\nWe can save the generated matrix in different file formats using different functions in R. Here are the functions we will use for each file format:\n\nCSV: write.csv()\nRDS: saveRDS()\nFST: write_fst()\nArrow: write_feather()\n\nHere’s the code to save the matrix in each file format:\n\nlibrary(fst)\nlibrary(arrow)\n\n# Save matrix in different file formats\nwrite.csv(mat, \"matrix.csv\", row.names=FALSE)\nsaveRDS(mat, \"matrix.rds\")\nwrite_fst(as.data.frame(mat), \"matrix.fst\")\nwrite_feather(as_arrow_table(as.data.frame(mat)), \"matrix.arrow\")\n\nThis code saves the matrix in each file format using the corresponding function, with the file name specified as the second argument. Getting the file size of each saved object\nTo get the file size of each saved object, we can use the file.size() function in R. Here’s the code to get the file size of each saved object:\n\n# Get file size of each saved object\ncsv_size <- file.size(\"matrix.csv\")  / (1024^2)\nrds_size <- file.size(\"matrix.rds\") / (1024^2)\nfst_size <- file.size(\"matrix.fst\") / (1024^2)\narrow_size <- file.size(\"matrix.arrow\") / (1024^2)\n\n# Print file size in human-readable format\nprint(paste(\"CSV file size in MB:\", format(csv_size, units=\"auto\")))\n\n[1] \"CSV file size in MB: 17.17339\"\n\nprint(paste(\"RDS file size in MB:\", format(rds_size, units=\"auto\")))\n\n[1] \"RDS file size in MB: 5.079627\"\n\nprint(paste(\"FST file size in MB:\", format(fst_size, units=\"auto\")))\n\n[1] \"FST file size in MB: 7.700841\"\n\nprint(paste(\"Arrow file size in MB:\", format(arrow_size, units=\"auto\")))\n\n[1] \"Arrow file size in MB: 6.705355\"\n\n\nThis code uses the file.size() function to get the file size of each object, and stores the file size of each object in a separate variable.\nFinally, it prints the file size of each object in a human-readable format using the format() function with the units=“auto” argument. The units=“auto” argument automatically chooses the most appropriate unit (e.g., KB, MB, GB) based on the file size.\n\n\n\nTo benchmark how long it takes to read in each file, we can use the {rbenchmark} package in R. In this example, we will compare the read times for the CSV file using four different functions: read.csv(), read_csv() from the {readr} package, fread() from the {data.table} package, and vroom() from the {vroom} package. We will also benchmark the read times for the RDS file using readRDS(), the FST file using read_fst(), and the Arrow file using read_feather().\nHere’s the code to benchmark the read times:\n\n# Load rbenchmark package\nlibrary(rbenchmark)\nlibrary(readr)\nlibrary(data.table)\nlibrary(vroom)\nlibrary(dplyr)\n\nn = 30\n\n# Benchmark read times for CSV file\nbenchmark(\n  # CSV File\n  \"read.csv\" = {\n    a <- read.csv(\"matrix.csv\")\n  },\n  \"read_csv\" = {\n    b <- read_csv(\"matrix.csv\")\n  },\n  \"fread\" = {\n    c <- fread(\"matrix.csv\")\n  },\n  \"vroom alltrep false\" = {\n    d <- vroom(\"matrix.csv\")\n  },\n  \"vroom alltrep true\" = {\n    dd <- vroom(\"matrix.csv\", altrep = TRUE)\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               fread           30    1.35    1.000      0.90     0.20\n2  vroom alltrep true           30    6.59    4.881      3.58     1.71\n3 vroom alltrep false           30    6.62    4.904      3.43     1.62\n4            read.csv           30   33.86   25.081     26.15     0.22\n5            read_csv           30   82.39   61.030     20.39     3.47\n\n# RDS File\nbenchmark(\n  # RDS File\n  \"readRDS\" = {\n    e <- readRDS(\"matrix.rds\")\n  },\n  \"read_rds\" = {\n    f <- read_rds(\"matrix.rds\")\n  },\n  \n  # Repications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n      test replications elapsed relative user.self sys.self\n1 read_rds           30    0.95    1.000      0.74     0.01\n2  readRDS           30    0.97    1.021      0.74     0.02\n\n# FST / Arrow\nbenchmark(\n  # FST\n  \"read_fst\" = {\n    g <- read_fst(\"matrix.fst\")\n  },\n  \n  # Arrow\n  \"arrow\" = {\n    h <- read_feather(\"matrix.arrow\")\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n      test replications elapsed relative user.self sys.self\n1 read_fst           30    0.21    1.000      0.05     0.12\n2    arrow           30    3.00   14.286      1.60     0.11\n\n\nThis code loads the {rbenchmark} package, and uses the benchmark() function to compare the read times for each file format. We specify the function to use for each file format, and set the number of replications to 10. Conclusion\nIn this blog post, we demonstrated how to generate a large matrix with random numbers in R, and how to save it in different file formats. We also showed how to get the file size of each saved object, and benchmarked the read times for each file format using different functions.\nThis example demonstrates the importance of choosing the appropriate file format and read function for your data. Depending on the size of your data and the requirements of your analysis, some file formats and functions may be more efficient than others."
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html",
    "href": "posts/rtip-2023-03-27/index.html",
    "title": "How fast does a compressed file in?",
    "section": "",
    "text": "I received an email over the weekend in regards to my last post not containing the reading in of gz compressed file(s) for the benchmarking. While this was not an over site per-se it was a good reminder that people would probably be interested in seeing this as well.\nBenchmarking is the process of measuring and comparing the performance of different programs, tools, or configurations in order to identify which one is the most efficient for a specific task. It is a critical step in software development that can help developers identify performance bottlenecks and improve the overall performance of their applications.\nIn this post I create a square matrix and then convert it to a data.frame (2,000 rows by 2,000 columns) and then saved it as a gz compressed csv file. The benchmark compares different R packages and functions, including base R, data.table, vroom, and readr, and measures their relative speeds based on the time it takes to read in the .csv.gz file.\nHere are some pro’s of trying things different ways and properly benchmarking them:\n\nIdentify the most efficient solution: Benchmarking can help you identify the most efficient solution for a specific task. By measuring the relative speeds of different programs or tools, you can determine which one is the fastest and use it to improve the performance of your application.\nOptimize resource utilization: Benchmarking can help you optimize resource utilization by identifying programs or tools that consume more resources than others. By choosing the most resource-efficient solution, you can reduce the cost of running your application and improve its scalability.\nAvoid premature optimization: Benchmarking can help you avoid premature optimization by measuring the performance of different programs or tools before you start optimizing them. By identifying the slowest parts of your application, you can focus your optimization efforts on the most critical areas and avoid wasting time optimizing code that doesn’t need it.\nKeep up with technology: Benchmarking can help you keep up with technology by comparing the performance of different tools and libraries. By staying up to date with the latest technologies, you can improve the performance of your application and stay ahead of your competitors.\nImprove code quality: Benchmarking can help you improve the quality of your code by identifying performance bottlenecks and areas for optimization. By optimizing your code, you can improve its maintainability, reliability, and readability.\n\nIn conclusion, benchmarking is an essential tool for software developers that can help them identify the most efficient solutions for their applications. By measuring the relative speeds of different programs or tools, developers can optimize resource utilization, avoid premature optimization, keep up with technology, and improve the quality of their code."
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#base-r",
    "href": "posts/rtip-2023-03-27/index.html#base-r",
    "title": "How fast does a compressed file in?",
    "section": "Base R",
    "text": "Base R\n\nread.csv()\nread.table()"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#data.table",
    "href": "posts/rtip-2023-03-27/index.html#data.table",
    "title": "How fast does a compressed file in?",
    "section": "data.table",
    "text": "data.table\n\nfread"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#vroom",
    "href": "posts/rtip-2023-03-27/index.html#vroom",
    "title": "How fast does a compressed file in?",
    "section": "vroom",
    "text": "vroom\n\nvroom() with altrep = FALSE\nvroom() with altrep = TRUE"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#readr",
    "href": "posts/rtip-2023-03-27/index.html#readr",
    "title": "How fast does a compressed file in?",
    "section": "readr",
    "text": "readr\n\nread_csv()"
  },
  {
    "objectID": "posts/rtip-2023-03-27/index.html#benchmarking",
    "href": "posts/rtip-2023-03-27/index.html#benchmarking",
    "title": "How fast does a compressed file in?",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nlibrary(rbenchmark)\nlibrary(data.table)\nlibrary(readr)\nlibrary(vroom)\nlibrary(dplyr)\n\nn <- 30\n\nbenchmark(\n  # Base R\n  \"read.table\" = {\n    a <- read.table(\"matrix.csv.gz\", sep = \",\")\n  },\n  \"read.csv\" = {\n    b <- read.csv(\"matrix.csv.gz\", sep = \",\")\n  },\n  \n  # data.table\n  \"fread\" = {\n    c <- fread(\"matrix.csv.gz\", sep = \",\")\n  },\n  \n  # vroom\n  \"vroom alltrep false\" = {\n    d <- vroom(\"matrix.csv.gz\", delim = \",\")\n  },\n  \"vroom alltrep true\" = {\n    e <- vroom(\"matrix.csv.gz\", delim = \",\", altrep = TRUE)\n  },\n  \n  # readr\n  \"readr\" = {\n    f <- read_csv(\"matrix.csv.gz\")\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               fread           30   19.44    1.000     13.56     1.59\n2  vroom alltrep true           30   22.06    1.135     10.54     2.63\n3 vroom alltrep false           30   24.75    1.273     10.22     2.84\n4          read.table           30   94.34    4.853     79.02     0.64\n5            read.csv           30  143.28    7.370    115.64     0.74\n6               readr           30  177.61    9.136     50.37    10.05\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html",
    "href": "posts/rtip-2023-03-28/index.html",
    "title": "How fast does a compressed file in Part 2",
    "section": "",
    "text": "Yesterday I posted on performing a benchmark on reading in a compressed .csv.gz file of a 2,000 by 2,000 data.frame. It was brought to my attention by someone on Mastadon (@mariviere@fediscience.org - https://fediscience.org/@mariviere) that I should also use {duckdb} and {arrow} so I will perform the same analysis as yesterday but I will also add in the two aforementioned packages."
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html#make-the-data",
    "href": "posts/rtip-2023-03-28/index.html#make-the-data",
    "title": "How fast does a compressed file in Part 2",
    "section": "Make the Data",
    "text": "Make the Data\nLet’s make that dataset again:\n\nlibrary(R.utils)\n\n# create a 1000 x 1000 matrix of random numbers\ndf <- matrix(rnorm(2000000), nrow = 2000, ncol = 2000) |>\n  as.data.frame()\n\n# Make and save gzipped file\nwrite.csv(df, \"df.csv\")\ngzip(\n  filename = \"df.csv\", \n  destname = \"df.csv.gz\",\n  overwrite = FALSE, \n  remove = TRUE\n)"
  },
  {
    "objectID": "posts/rtip-2023-03-28/index.html#benchmarking",
    "href": "posts/rtip-2023-03-28/index.html#benchmarking",
    "title": "How fast does a compressed file in Part 2",
    "section": "Benchmarking",
    "text": "Benchmarking\nTime to benchmark\n\nlibrary(rbenchmark)\nlibrary(data.table)\nlibrary(readr)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(vroom)\nlibrary(dplyr)\nlibrary(DBI)\n\nn <- 30\n\nbenchmark(\n  # Base R\n  \"read.table\" = {\n    a <- read.table(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \"read.csv\" = {\n    b <- read.csv(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \n  # data.table\n  \"fread\" = {\n    c <- fread(\n      \"df.csv.gz\", \n      sep = \",\", \n      colClasses = list(numeric = 1:2000)\n    )\n  },\n  \n  # vroom\n  \"vroom alltrep false\" = {\n    d <- vroom(\"df.csv.gz\", delim = \",\", col_types = \"d\")\n  },\n  \"vroom alltrep true\" = {\n    e <- vroom(\"df.csv.gz\", delim = \",\", altrep = TRUE, col_types = \"d\")\n  },\n  \n  # readr\n  \"readr\" = {\n    f <- read_csv(\"df.csv.gz\", col_types = \"d\")\n  },\n  \n  # Arrow\n  \"arrow\" = {\n    g <- open_csv_dataset(\"df.csv.gz\")\n  },\n  \n  # DuckDB\n  \"duckdb\" = {\n    con <- dbConnect(duckdb())\n    h <- duckdb_read_csv(\n      conn = con,\n      name = \"df\",\n      files = \"C:\\\\Users\\\\ssanders\\\\Documents\\\\GitHub\\\\steveondata\\\\posts\\\\rtip-2023-03-28\\\\df.csv.gz\"\n    )\n    dbDisconnect(con)\n  },\n  \n  # Replications\n  replications = n,\n  \n  # Columns\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |>\n  arrange(relative)\n\n                 test replications elapsed relative user.self sys.self\n1               arrow           30    3.01    1.000      5.04     0.25\n2               fread           30   28.28    9.395     19.56     4.30\n3 vroom alltrep false           30   31.89   10.595     26.25    10.75\n4  vroom alltrep true           30   33.72   11.203     25.75    10.67\n5              duckdb           30   94.09   31.259     90.70     2.77\n6               readr           30   98.28   32.651    113.05    45.12\n7          read.table           30  109.97   36.535    107.78     1.24\n8            read.csv           30  153.79   51.093    152.44     0.56\n\n\nImportant note is the session info on the pc I am using to write this:\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] dplyr_1.1.1       vroom_1.6.1       arrow_11.0.0.3    duckdb_0.7.1-1   \n [5] DBI_1.1.3         readr_2.1.4       data.table_1.14.8 rbenchmark_1.0.0 \n [9] R.utils_2.12.2    R.oo_1.25.0       R.methodsS3_1.8.2\n\nloaded via a namespace (and not attached):\n [1] pillar_1.9.0      compiler_4.2.3    tools_4.2.3       digest_0.6.31    \n [5] bit_4.0.5         jsonlite_1.8.4    evaluate_0.20     lifecycle_1.0.3  \n [9] tibble_3.2.1      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] rstudioapi_0.14   parallel_4.2.3    yaml_2.3.7        xfun_0.38        \n[17] fastmap_1.1.1     knitr_1.42        generics_0.1.3    vctrs_0.6.1      \n[21] htmlwidgets_1.6.2 hms_1.1.3         bit64_4.0.5       tidyselect_1.2.0 \n[25] glue_1.6.2        R6_2.5.1          fansi_1.0.4       rmarkdown_2.21   \n[29] tzdb_0.3.0        purrr_1.0.1       magrittr_2.0.3    htmltools_0.5.5  \n[33] assertthat_0.2.1  utf8_1.2.3        crayon_1.5.2     \n\n Sys.info() |> \n   as.data.frame() |> \n   tibble::rownames_to_column() |> \n   as_tibble() |> \n   slice(1,2,3,5)\n\n# A tibble: 4 × 2\n  rowname `Sys.info()`\n  <chr>   <chr>       \n1 sysname Windows     \n2 release 10 x64      \n3 version build 19045 \n4 machine x86-64      \n\n memory.profile() |>\n   as.data.frame()\n\n            memory.profile()\nNULL                       1\nsymbol                 24303\npairlist              642504\nclosure                11189\nenvironment             4009\npromise                22963\nlanguage              189766\nspecial                   47\nbuiltin                  701\nchar                 2039073\nlogical                18866\ninteger               108132\ndouble                 20060\ncomplex                    5\ncharacter             160381\n...                       21\nany                        0\nlist                   58500\nexpression                 5\nbytecode               41555\nexternalptr            12382\nweakref                13860\nraw                    10113\nS4                      1362\n\n gc()\n\n           used  (Mb) gc trigger  (Mb) max used  (Mb)\nNcells  3363479 179.7    5830931 311.5  5830931 311.5\nVcells 32950395 251.4   81254422 620.0 81254324 620.0"
  },
  {
    "objectID": "posts/rtip-2023-03-29/index.html",
    "href": "posts/rtip-2023-03-29/index.html",
    "title": "A Bootstrapped Time Series Model with auto.arima() from {forecast}",
    "section": "",
    "text": "Introduction\nTime series analysis is a powerful tool for understanding and predicting patterns in data that vary over time. In this tutorial, we will use the AirPassengers dataset to create a bootstrapped timeseries model in R.\nThe AirPassengers dataset The AirPassengers dataset contains data on the number of passengers traveling on international flights per month from 1949 to 1960. To begin, we will load the dataset into R and plot it to get an idea of the data’s structure and any underlying patterns.\n\nlibrary(forecast)\n\ndata(AirPassengers)\nplot(AirPassengers, main = \"International Airline Passengers 1949-1960\")\n\n\n\n\nFrom the plot, we can see that there is a clear upward trend in the data, as well as some seasonality.\n\n\nCreating a bootstrapped timeseries model\nNow that we have an idea of the structure of the data, we can create a bootstrapped timeseries model using the auto.arima() function from the {forecast} package. The auto.arima() function uses an automated algorithm to determine the best model for a given timeseries.\n\nset.seed(123)\nn <- length(AirPassengers)\nn_boot <- 1000\n\n# create bootstrap sample indices\nboot_indices <- replicate(n_boot, sample(1:n, replace = TRUE))\n\n# create list to store models\nmodels <- list()\n\n# create bootstrapped models\nfor(i in 1:n_boot) {\n  boot_data <- AirPassengers[boot_indices[, i]]\n  models[[i]] <- auto.arima(boot_data)\n}\n\nmodels[[1]]\n\nSeries: boot_data \nARIMA(0,0,0) with non-zero mean \n\nCoefficients:\n          mean\n      275.5347\ns.e.    9.5443\n\nsigma^2 = 13209:  log likelihood = -887.01\nAIC=1778.02   AICc=1778.1   BIC=1783.96\n\n\nIn the code above, we first set a seed to ensure reproducibility of our results. We then specify the length of the timeseries and the number of bootstrap iterations we want to run. We create a list to store the models and a set of bootstrap sample indices.\nWe then loop through each bootstrap iteration, creating a new dataset from the original timeseries by sampling with replacement using the boot_indices. We use the auto.arima() function to create a timeseries model for each bootstrap sample and store it in our models list.\n\n\nSummarizing and plotting residuals\nNow that we have created our bootstrapped timeseries models, we can summarize and plot the residuals of each model to get an idea of how well our models fit the data.\n\n# create list to store residuals\nresiduals <- list()\n\n# create residuals for each model\nfor(i in 1:n_boot) {\n  boot_data <- AirPassengers[boot_indices[, i]]\n  residuals[[i]] <- residuals(models[[i]])\n}\n\n# summarize residuals\nresidual_means <- sapply(residuals, mean)\nresidual_sd <- sapply(residuals, sd)\n\n# plot residuals\npar(mfrow = c(2, 1))\n\nhist(\n  residual_means, \n  main = \"Bootstrapped Model Residuals\", \n  xlab = \"Mean Residuals\"\n  )\nhist(\n  residual_sd, \n  col = \"red\", \n  main = \"\", \n  xlab = \"SD Residuals\"\n  )\n\n\n\npar(mfrow = c(1,1))\n\nIn the code above, we create a list to store the residuals for each model, loop through each model to create residuals using the residuals() function, and summarize the residuals by taking the mean and standard deviation of each set of residuals.\nWe then plot the mean residuals and standard deviations for each model using the plot() function and add a legend to indicate the meaning of the two lines.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html",
    "href": "posts/rtip-2023-04-03/index.html",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "",
    "text": "In this blog post, we will be discussing how to create a Shiny application in R that will download and extract data from a zip file and allow users to choose which data they would like to see presented to them in the app from a selection drop-down menu. We will be using the current_hosp_data() function to obtain and read in the data. This function is in the upcoming release for the {healthyR.data} package."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#install",
    "href": "posts/rtip-2023-04-03/index.html#install",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Install",
    "text": "Install\n\ninstall.packages(c(\"shiny\",\"shinythemes\"))"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#current-hospital-data",
    "href": "posts/rtip-2023-04-03/index.html#current-hospital-data",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Current Hospital Data",
    "text": "Current Hospital Data\nHere is the current_hospital_data() function:\n\ncurrent_hosp_data <- function() {\n\n  # URL for file\n  url <- \"https://data.cms.gov/provider-data/sites/default/files/archive/Hospitals/current/hospitals_current_data.zip\"\n\n  # Create a temporary directory to process the zip file\n  tmp_dir <- tempdir()\n  download_location <- file.path(tmp_dir, \"download.zip\")\n  extract_location <- file.path(tmp_dir, \"extract\")\n\n  # Download the zip file to the temporary location\n  utils::download.file(\n    url = url,\n    destfile = download_location\n  )\n\n  # Unzip the file\n  utils::unzip(download_location, exdir = extract_location)\n\n  # Read the csv files into a list\n  csv_file_list <- list.files(\n    path = extract_location,\n    pattern = \"\\\\.csv$\",\n    full.names = TRUE\n  )\n\n  # make named list\n  csv_names <-\n    stats::setNames(\n      object = csv_file_list,\n      nm =\n        csv_file_list |>\n        basename() |>\n        gsub(pattern = \"\\\\.csv$\", replacement = \"\") |>\n        janitor::make_clean_names()\n    )\n\n  # Process CSV Files\n  parse_csv_file <- function(file) {\n    # Normalize the path to use C:/path/to/file structure\n    normalizePath(file, \"/\") |>\n      # read in the csv file and use check.names = FALSE because some of\n      # the names are very long\n      utils::read.csv(check.names = FALSE) |>\n      dplyr::as_tibble() |>\n      # clean the field names\n      janitor::clean_names()\n  }\n\n  list_of_tables <- lapply(csv_names, parse_csv_file)\n\n  unlink(tmp_dir, recursive = TRUE)\n\n  # Return the tibbles\n  # Add and attribute and a class type to the object\n  attr(list_of_tables, \".list_type\") <- \"current_hosp_data\"\n  class(list_of_tables) <- c(\"current_hosp_data\", class(list_of_tables))\n\n  list_of_tables\n}"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#app-file",
    "href": "posts/rtip-2023-04-03/index.html#app-file",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "App File",
    "text": "App File\nNext, let’s create a new file called app.R. In this file, we will create the Shiny app. The app will have a user interface (UI) and a server.\nThe UI is responsible for creating the layout of the app, while the server is responsible for processing the data and responding to user input.\nFirst, let’s create the UI. The UI will consist of a drop-down menu that will allow users to choose which data they would like to see presented to them in the app.\n\nlibrary(shiny)\nlibrary(shinythemes)\n\nhosp_data <- current_hosp_data()\n\nui <- fluidPage(theme = shinytheme(\"cerulean\"),\n                \n                # Set up the dropdown menu\n                selectInput(inputId = \"table\", \n                            label = \"Select a table:\", \n                            choices = names(hosp_data), \n                            selected = NULL),\n                \n                # Set up the table output\n                tableOutput(outputId = \"table_output\")\n)\n\nThe fluidPage() function creates a new Shiny app page. We also specify the theme using the {shinythemes} package. The selectInput() function creates the drop-down menu, which allows users to select which data they would like to see presented to them in the app. The choices argument is set to the names of the tables in the current_hosp_data() object. The tableOutput() function creates the output for the selected table."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#server",
    "href": "posts/rtip-2023-04-03/index.html#server",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Server",
    "text": "Server\nNext, let’s create the server. The server will be responsible for processing the data and generating the output based on user input.\n\nserver <- function(input, output) {\n    \n    # Load the data into a reactive object\n    data <- reactive(hosp_data)\n    \n    # Set up the table output\n    output$table_output <- renderTable({\n        # Get the selected table\n        table_selected <- input$table\n        \n        # Get the table from the data object\n        table_data <- data()[[table_selected]]\n        \n        # Return the table data\n        table_data\n    })\n}\n\nThe reactive() function is used to create a reactive object that will load the data when the app starts. The renderTable() function generates the output for the selected table. It does this by getting the selected table from the drop-down menu, getting the table data from the reactive data object, and returning the table data."
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#shiny-app",
    "href": "posts/rtip-2023-04-03/index.html#shiny-app",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Shiny App",
    "text": "Shiny App\nFinally, we need to run the appl using the shinyApp() function:\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/rtip-2023-04-03/index.html#pros-and-cons",
    "href": "posts/rtip-2023-04-03/index.html#pros-and-cons",
    "title": "A sample Shiny App to view CMS Healthcare Data",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nPros:\n\nThe app is easy to use, and users can quickly select which data they would like to see presented to them in the app.\nThe current_hosp_data() function is only called once when the app starts, which can save time and resources if the function is time-consuming or resource-intensive.\n\nCons:\n\nThe app will not update if the data in the zip file changes. Users will need to restart the app to see the updated data.\nThe app loads all the data into memory when it starts, which can be an issue if the data is large and memory-intensive."
  },
  {
    "objectID": "posts/rtip-2023-04-04/index.html",
    "href": "posts/rtip-2023-04-04/index.html",
    "title": "A sample Shiny App to view Forecasts on the AirPassengers Data",
    "section": "",
    "text": "Hello! In this code, we are making a program that will help us predict the number of air passengers in the future. Let me explain what each part of the code does, step by step.\nFirst, we need to load some tools that will help us create the program. These tools are called “packages.” We use the library() function to load them. The packages we need are called shiny, forecast, and ggplot2.\n\n\n\nlibrary(shiny)\nlibrary(forecast)\nlibrary(ggplot2)\n\n\n\n\nNext, we need some data to work with. We will use a dataset of the number of air passengers each month from 1949 to 1960. We load this dataset using the data() function.\n\ndata(AirPassengers)\n\n\n\n\nNow, we need to create the user interface, or UI. This is what the user will see and interact with. In this case, we will create a simple app with a title, a dropdown menu to choose a forecasting model, and a plot and table to display the forecast results. We use the fluidPage() function to create the UI, and we define the UI elements inside it.\n\nui <- fluidPage(\n  titlePanel(\"AirPassengers Forecast\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"model\", label = \"Choose a model:\",\n                  choices = c(\"auto.arima\", \"ets\", \"holtwinters\"))\n    ),\n    mainPanel(\n      plotOutput(outputId = \"forecast_plot\"),\n      tableOutput(outputId = \"forecast_table\")\n    )\n  )\n)\n\n\n\n\nNow, we need to define the server. The server is where the program does the calculations and generates the output based on what the user selects in the UI. We define the server inside the function(input, output) argument.\n\nserver <- function(input, output) {\n\nInside the server, we need to create a reactive expression that generates the forecast based on the model the user selects. We use an if statement to check which model the user selected, and then we use the corresponding function to generate the forecast.\n\n\n\nforecast_data <- reactive({\n    if (input$model == \"auto.arima\") {\n      fit <- auto.arima(AirPassengers)\n      forecast(fit)\n    } else if (input$model == \"ets\") {\n      fit <- ets(AirPassengers)\n      forecast(fit)\n    } else {\n      fit <- hw(AirPassengers)\n      forecast(fit)\n    }\n  })\n\n\n\n\nThe renderPlot() function tells the program to create a plot based on the reactive expression we defined earlier. We use the plotOutput() function in the UI to display the plot.\n\noutput$forecast_plot <- renderPlot({\n    plot(forecast_data())\n  })\n\nSimilarly, the renderTable() function tells the program to create a table based on the reactive expression we defined earlier. We use the tableOutput() function in the UI to display the table.\n\noutput$forecast_table <- renderTable({\n    forecast_data()$mean\n  })\n\nFinally, we run the app using the shinyApp() function, with the UI and server arguments.\n\nshinyApp(ui = ui, server = server)\n\nAnd that’s it! This program allows the user to choose a forecasting model, and then generates a plot and table with the predicted number of air passengers based on that model.\nHere is the Full code block”\n\n# Load required packages\nlibrary(shiny)\nlibrary(forecast)\nlibrary(ggplot2)\n\n# Load AirPassengers dataset\ndata(AirPassengers)\n\n# Define UI\nui <- fluidPage(\n  \n  # Title of the app\n  titlePanel(\"AirPassengers Forecast\"),\n  \n  # Sidebar with input controls\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"model\", label = \"Choose a model:\",\n                  choices = c(\"auto.arima\", \"ets\", \"holtwinters\"))\n    ),\n    \n    # Output plot and table\n    mainPanel(\n      plotOutput(outputId = \"forecast_plot\"),\n      tableOutput(outputId = \"forecast_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Reactive expression to create forecast based on selected model\n  forecast_data <- reactive({\n    if (input$model == \"auto.arima\") {\n      fit <- auto.arima(AirPassengers)\n      forecast(fit)\n    } else if (input$model == \"ets\") {\n      fit <- ets(AirPassengers)\n      forecast(fit)\n    } else {\n      fit <- hw(AirPassengers)\n      forecast(fit)\n    }\n  })\n  \n  # Output plot\n  output$forecast_plot <- renderPlot({\n    plot(forecast_data())\n    #checkresiduals(forecast_data())\n  })\n  \n  # Output table\n  output$forecast_table <- renderTable({\n    forecast_data()$mean\n  })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-05/index.html",
    "href": "posts/rtip-2023-04-05/index.html",
    "title": "Looking at Daily Log Returns with tidyquant, TidyDensity, and Shiny",
    "section": "",
    "text": "In this blog post, we’ll walk through how to create a shiny application that allows users to analyze the weekly returns of FAANG stocks (AAPL, AMZN, FB, GOOGL, and NFLX) using the {tidyquant} and {TidyDensity} packages in R.\n\n\nThe first section of the code sets up the necessary R packages and creates the UI for the shiny app. The packages we’ll be using are:\n\nshiny: for creating interactive web applications in R\ntidyquant: for easily getting and analyzing financial data in R\nTidyDensity: for computing and visualizing probability distributions in a tidy way\ndplyr: for manipulating data in a tidy way\nDT: for creating interactive and scrollable data tables\n\nAnalysts assemble your packages!\n\nlibrary(shiny)\nlibrary(tidyquant)\nlibrary(TidyDensity)\nlibrary(dplyr)\nlibrary(DT)\n\nThe UI consists of a title panel, a sidebar panel, and a main panel. The sidebar panel contains a select input that allows users to choose which FAANG stock to analyze, as well as a numeric input for the number of simulations to run. The main panel contains two sections: one for the tidy_autoplot() output (a plot of the stock returns), and one for the tidy_empirical() output (a table of the log returns).\n\n\n\nThe second section of the code defines the server function for the shiny app. The server function takes the input values from the UI (i.e. the selected stock and number of simulations) and uses them to get and analyze the stock data.\nTo get the stock data, we use the tq_get() function from the tidyquant package to retrieve the adjusted stock prices for the selected security from January 1, 2010 to the present. We then use the tq_transmute() function to compute the weekly log returns of the stock and rename the resulting column to “log_return”.\nThe tidy_empirical() function from the TidyDensity package is used to compute the empirical distribution of the log returns. The resulting table is displayed using the renderDT() function from the DT package, which creates a scrollable data table that can be sorted and filtered.\nThe tidy_autoplot() function is used to create a plot of the log returns, which is displayed using the renderPlot() function.\n\n\n\nThe final section of the code runs the shiny app using the ui and server functions.\nOverall, this shiny app provides a simple and interactive way for users to analyze the weekly returns of FAANG stocks using tidyquant and TidyDensity in R. By allowing users to choose which stock to analyze and how many simulations to run, the app provides a customizable way to explore the empirical distributions of the log returns."
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html",
    "href": "posts/rtip-2023-04-06/index.html",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "",
    "text": "BRVM"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_ticker_desc-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_ticker_desc-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_ticker_desc() function",
    "text": "The BRVM_ticker_desc() function\nIt receives no argument and returns BRVM tickers information such as its full name, sector and country.\n\n# Display tickers of BRVM\ntickers <- BRVM_ticker_desc()\ntickers\n\n\n\nWarning: package 'kableExtra' was built under R version 4.2.3\n\n\n\n\n \n  \n    Ticker \n    Company name \n    Sector \n    Country \n  \n \n\n  \n    ABJC \n    SERVAIR ABIDJAN  COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    BICC \n    BICI COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    BNBC \n    BERNABE COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    BOAB \n    BANK OF AFRICA BENIN \n    FINANCE \n    BENIN \n  \n  \n    BOABF \n    BANK OF AFRICA BURKINA FASO \n    FINANCE \n    BURKINA FASO \n  \n  \n    BOAC \n    BANK OF AFRICA COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    BOAM \n    BANK OF AFRICA MALI \n    FINANCE \n    MALI \n  \n  \n    BOAN \n    BANK OF AFRICA NIGER \n    FINANCE \n    NIGER \n  \n  \n    BOAS \n    BANK OF AFRICA SENEGAL \n    FINANCE \n    SENEGAL \n  \n  \n    CABC \n    SICABLE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    CBIBF \n    CORIS BANK INTERNATIONAL BURKINA FASO \n    FINANCE \n    BURKINA FASO \n  \n  \n    CFAC \n    CFAO MOTORS COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    CIEC \n    CIE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    ECOC \n    ECOBANK COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    ETIT \n    Ecobank Transnational Incorporated TOGO \n    FINANCE \n    TOGO \n  \n  \n    FTSC \n    FILTISAC COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    NEIC \n    NEI-CEDA COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    NSBC \n    NSIA BANQUE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    NTLC \n    NESTLE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    ONTBF \n    ONATEL BURKINA FASO \n    PUBLIC SERVICE \n    BURKINA FASO \n  \n  \n    ORAC \n    ORANGE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    ORGT \n    ORAGROUP TOGO \n    FINANCE \n    TOGO \n  \n  \n    PALC \n    PALM COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    PRSC \n    TRACTAFRIC MOTORS COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    SAFC \n    SAFCA COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SCRC \n    SUCRIVOIRE COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SDCC \n    SODE COTE D'IVOIRE \n    PUBLIC SERVICE \n    IVORY COAST \n  \n  \n    SDSC \n    BOLLORE TRANSPORT & LOGISTICS COTE D'IVOIRE \n    TRANSPORT \n    IVORY COAST \n  \n  \n    SEMC \n    CROWN SIEM COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SGBC \n    SOCIETE GENERALE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SHEC \n    VIVO ENERGY COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    SIBC \n    SOCIETE IVOIRIENNE DE BANQUE COTE D'IVOIRE \n    FINANCE \n    IVORY COAST \n  \n  \n    SICC \n    SICOR COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SIVC \n    AIR LIQUIDE COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SLBC \n    SOLIBRA COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SMBC \n    SMB COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SNTS \n    SONATEL SENEGAL \n    PUBLIC SERVICE \n    SENEGAL \n  \n  \n    SOGC \n    SOGB COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    SPHC \n    SAPH COTE D'IVOIRE \n    AGRICULTURE \n    IVORY COAST \n  \n  \n    STAC \n    SETAO COTE D'IVOIRE \n    OTHER \n    IVORY COAST \n  \n  \n    STBC \n    SITAB COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    SVOC \n    MOVIS COTE D'IVOIRE \n    TRANSPORT \n    IVORY COAST \n  \n  \n    TTLC \n    TOTAL COTE D'IVOIRE \n    DISTRIBUTION \n    IVORY COAST \n  \n  \n    TTLS \n    TOTAL SENEGAL \n    DISTRIBUTION \n    SENEGAL \n  \n  \n    TTRC \n    TRITURAF Ste en Liquid \n    INDUSTRY \n    IVORY COAST \n  \n  \n    UNLC \n    UNILEVER COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST \n  \n  \n    UNXC \n    UNIWAX COTE D'IVOIRE \n    INDUSTRY \n    IVORY COAST"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_index-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_index-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_index() function :",
    "text": "The BRVM_index() function :\nIt receives no argument and returns a table of updated data (with as table header: indexes, previous closing, closing, change (%), Year to Date Change) on all the indices available on the BRVM exchange.\n\n\n\n\n \n  \n    Indexes \n    Previous closing \n    Closing \n    Change (%) \n    Year to Date Change \n  \n \n\n  \n    BRVM-30 \n    99.71 \n    99.75 \n    0.04 \n    0.00 \n  \n  \n    BRVM - AGRICULTURE \n    281.76 \n    281.25 \n    -0.18 \n    -0.66 \n  \n  \n    BRVM - OTHER SECTOR \n    1295.58 \n    1357.27 \n    4.76 \n    -7.32 \n  \n  \n    BRVM - COMPOSITE \n    199.37 \n    199.46 \n    0.05 \n    0.85 \n  \n  \n    BRVM - DISTRIBUTION \n    346.02 \n    345.33 \n    -0.20 \n    0.69 \n  \n  \n    BRVM - FINANCE \n    74.53 \n    75.03 \n    0.67 \n    -0.66 \n  \n  \n    BRVM - INDUSTRY \n    98.33 \n    98.10 \n    -0.23 \n    0.92 \n  \n  \n    BRVM - PRESTIGE \n    102.61 \n    102.56 \n    -0.05 \n    0.00 \n  \n  \n    BRVM - PRINCIPAL \n    94.56 \n    94.62 \n    0.06 \n    0.00 \n  \n  \n    BRVM - PUBLIC SERVICES \n    480.97 \n    479.60 \n    -0.28 \n    2.23 \n  \n  \n    BRVM - TRANSPORT \n    345.28 \n    341.70 \n    -1.04 \n    0.35"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_get.symbol-.from-.to-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_get.symbol-.from-.to-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_get(“.symbol”, “.from”, “.to”) function",
    "text": "The BRVM_get(“.symbol”, “.from”, “.to”) function\nThis function will get the data of the companies listed on the BVRM stock exchange in Rich Bourse website. The function takes a single parameter, .symbol (which represents the “Ticker”). The function will automatically format tickers you enter in uppercase using toupper() and then ensure that the passed ticker is in a Google spreadsheet of allowed tickers.\n\n.symbol : A vector of symbols, like: c(“BICC”,“XOM”,“SlbC”) ;\n.from : A quoted start date, ie. “2020-01-01” or “2020/01/01”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD” ;\n.to : A quoted end date, ie. “2022-01-31” or “2022/01/31”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”.\n\n\n#' Displaying data of SONATEL Senegal stock\nBRVM_get(.symbol = \"snts\")\n\n[1] \"SNTS\"\n\n\n# A tibble: 251 × 6\n   Date        Open  High   Low Close Volume\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1 2022-04-06 15800 15895 15750 15800   7436\n 2 2022-04-07 15800 15900 15750 15900   1265\n 3 2022-04-08 15900 15995 15800 15900   1164\n 4 2022-04-11 15895 15900 15800 15800   4252\n 5 2022-04-12 15800 15800 15780 15800   6561\n 6 2022-04-13 15800 15865 15795 15850   5409\n 7 2022-04-14 15855 15900 15850 15900  16957\n 8 2022-04-15 15995 15995 15900 15900    791\n 9 2022-04-19 15900 15995 15895 15900  31217\n10 2022-04-20 15900 15995 15895 15990  32322\n# ℹ 241 more rows\n\nsymbols <- c(\"BiCc\",\"XOM\",\"SlbC\")   # We use here three tickers\ndata_tbl <- BRVM_get(.symbol = symbols, .from = \"2020-01-01\", .to = Sys.Date() - 1)\n\n[1] \"BICC\" \"SLBC\"\n\n# Display the first twenty observations of the table\nhead(data_tbl, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2020-01-10  6500  6500  6500  6500     24 BICC  \n 2 2020-01-13  6370  6500  6370  6500     29 BICC  \n 3 2020-01-14  6495  6495  6495  6495     10 BICC  \n 4 2020-01-29  6010  6010  6010  6010     24 BICC  \n 5 2020-01-30  6000  6000  6000  6000     50 BICC  \n 6 2020-02-04  5800  5800  5800  5800     12 BICC  \n 7 2020-02-07  5650  5650  5650  5650      5 BICC  \n 8 2020-02-10  5500  5500  5500  5500      5 BICC  \n 9 2020-02-14  5300  5300  5300  5300      9 BICC  \n10 2020-02-17  4910  4910  4910  4910    210 BICC  \n11 2020-02-18  4910  4910  4910  4910     50 BICC  \n12 2020-02-20  4895  4895  4895  4895      5 BICC  \n13 2020-02-21  4895  4895  4890  4890     13 BICC  \n14 2020-02-25  4525  4525  4525  4525     16 BICC  \n15 2020-02-26  4435  4435  4430  4430     21 BICC  \n16 2020-02-27  4345  4760  4335  4760   1809 BICC  \n17 2020-03-03  4745  4750  4745  4750     11 BICC  \n18 2020-03-05  4700  4700  4700  4700      5 BICC  \n19 2020-03-06  4695  4695  4695  4695      6 BICC  \n20 2020-03-11  4345  4450  4345  4450    135 BICC  \n\n# Display the last twenty elements of the table\ntail(data_tbl, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2023-02-15 80000 80000 79000 79000      2 SLBC  \n 2 2023-02-17 78000 78000 78000 78000      5 SLBC  \n 3 2023-02-21 80000 80000 80000 80000      5 SLBC  \n 4 2023-02-23 80000 80000 80000 80000     18 SLBC  \n 5 2023-02-24 80000 80000 80000 80000      6 SLBC  \n 6 2023-02-27 80000 80000 80000 80000     98 SLBC  \n 7 2023-02-28 80000 80000 80000 80000     11 SLBC  \n 8 2023-03-02 80000 80000 80000 80000     11 SLBC  \n 9 2023-03-08 80000 80000 80000 80000      2 SLBC  \n10 2023-03-09 80000 80000 80000 80000      2 SLBC  \n11 2023-03-13 80005 80005 80000 80000     12 SLBC  \n12 2023-03-14 80000 80000 80000 80000      1 SLBC  \n13 2023-03-20 80000 80000 80000 80000      3 SLBC  \n14 2023-03-21 80000 80000 80000 80000      4 SLBC  \n15 2023-03-27 78000 80000 78000 80000    169 SLBC  \n16 2023-03-28 80000 80000 80000 80000    435 SLBC  \n17 2023-03-30 80000 80000 80000 80000      3 SLBC  \n18 2023-03-31 80000 80000 80000 80000      1 SLBC  \n19 2023-04-04 80000 86000 80000 86000      3 SLBC  \n20 2023-04-05 85950 86000 85950 86000      6 SLBC"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm_get1ticker-period-from-to-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm_get1ticker-period-from-to-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM_get1(“ticker”, “Period”, “from”, “to”) function",
    "text": "The BRVM_get1(“ticker”, “Period”, “from”, “to”) function\nThis function will get data of the companies listed on the BVRM stock exchange through the sikafinance site. The function takes in a single parameter of ticker and will auto-format the tickers you input into all upper case by using toupper()\n\nticker : A vector of ticker, like: c(“BICC”,“XOM”,“SlbC”, “BRvm10”);\nPeriod : Numeric number indicating time period. Valid entries are 0, 1, 5, 30, 91, and 365 representing respectively ‘daily’, ‘one year’, ‘weekly’, ‘monthly’, ‘quarterly’ and ‘yearly’;\nfrom : A quoted start date, ie. “2020-01-01” or “2020/01/01”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”;\nto : A quoted end date, ie. “2022-01-31” or “2022/01/31”. The date must be in ymd format “YYYY-MM-DD” or “YYYY/MM/DD”\n\n** NB : There is a small difference between the BRVM_get and BRVM_get1 functions. * With BRVM_get it is only possible to download tickers’ daily data. * But with BRVM_get1, you can download daily, weekly, monthly, annual tickers’ data, indices and even market capitalization.\n\n#' Displaying data of SONATEL Senegal stock\nBRVM_get1(\"snts\")\n\n[1] \"Make sure you have an active internet connection\"\n\n# Get daily data of all indexes\nall_ind <- BRVM_get1(\"ALL INDEXES\", Period = 0, from = \"2020-01-04\", to = \"2023-03-24\") \n\n[1] \"We obtained BRVM10 data from 2019-12-26 to 2023-01-04\"\n[1] \"We obtained BRVMAG data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMC data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMAS data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMDI data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMFI data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMIN data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMSP data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMTR data from 2019-12-26 to 2023-03-24\"\n[1] \"We obtained BRVMPR data from 2023-01-01 to 2023-03-24\"\n[1] \"We obtained BRVMPA data from 2023-01-04 to 2023-03-24\"\n[1] \"We obtained BRVM30 data from 2023-01-01 to 2023-03-24\"\n[1] \"We obtained CAPIB data from 2020-01-02 to 2023-03-24\"\n\n# display the first two tens elements of the table\nhead(all_ind, 20)\n\n# A tibble: 20 × 7\n   Date        Open  High   Low Close Volume Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr> \n 1 2022-12-26  169.  169.  169.  169.      0 BRVM10\n 2 2022-12-27  169.  169.  169.  169.      0 BRVM10\n 3 2022-12-28  167.  167.  167.  167.      0 BRVM10\n 4 2022-12-29  167.  167.  167.  167.      0 BRVM10\n 5 2022-12-30  166.  166.  166.  166.      0 BRVM10\n 6 2023-01-02  166.  166.  166.  166.      0 BRVM10\n 7 2023-01-03  166.  166.  166.  166.      0 BRVM10\n 8 2023-01-04  166.  166.  166.  166.      0 BRVM10\n 9 2022-09-26  163.  163.  163.  163.      0 BRVM10\n10 2022-09-27  162.  162.  162.  162.      0 BRVM10\n11 2022-09-28  162.  162.  162.  162.      0 BRVM10\n12 2022-09-29  163.  163.  163.  163.      0 BRVM10\n13 2022-09-30  164.  164.  164.  164.      0 BRVM10\n14 2022-10-03  162.  162.  162.  162.      0 BRVM10\n15 2022-10-04  162.  162.  162.  162.      0 BRVM10\n16 2022-10-05  161.  161.  161.  161.      0 BRVM10\n17 2022-10-06  161.  161.  161.  161.      0 BRVM10\n18 2022-10-07  161.  161.  161.  161.      0 BRVM10\n19 2022-10-10  160.  160.  160.  160.      0 BRVM10\n20 2022-10-11  160.  160.  160.  160.      0 BRVM10\n\n# display the two tens of the last elements of the table\ntail(all_ind, 20)\n\n# A tibble: 20 × 7\n   Date          Open    High     Low   Close Volume Ticker\n   <date>       <dbl>   <dbl>   <dbl>   <dbl>  <dbl> <chr> \n 1 2020-02-26 4281311 4281311 4281311 4281311      0 CAPIB \n 2 2020-02-27 4314933 4314933 4314933 4314933      0 CAPIB \n 3 2020-02-28 4346515 4346515 4346515 4346515      0 CAPIB \n 4 2020-03-02 4424073 4424073 4424073 4424073      0 CAPIB \n 5 2020-03-03 4379647 4379647 4379647 4379647      0 CAPIB \n 6 2020-03-04 4369550 4369550 4369550 4369550      0 CAPIB \n 7 2020-03-05 4342229 4342229 4342229 4342229      0 CAPIB \n 8 2020-03-06 4359879 4359879 4359879 4359879      0 CAPIB \n 9 2020-03-09 4338293 4338293 4338293 4338293      0 CAPIB \n10 2020-03-10 4357221 4357221 4357221 4357221      0 CAPIB \n11 2020-03-11 4332656 4332656 4332656 4332656      0 CAPIB \n12 2020-03-12 4318096 4318096 4318096 4318096      0 CAPIB \n13 2020-03-13 4318112 4318112 4318112 4318112      0 CAPIB \n14 2020-03-16 4285184 4285184 4285184 4285184      0 CAPIB \n15 2020-03-17 4301727 4301727 4301727 4301727      0 CAPIB \n16 2020-03-18 4288582 4288582 4288582 4288582      0 CAPIB \n17 2020-03-19 4207231 4207231 4207231 4207231      0 CAPIB \n18 2020-03-20 4209788 4209788 4209788 4209788      0 CAPIB \n19 2020-03-23 4154445 4154445 4154445 4154445      0 CAPIB \n20 2020-03-24 4144325 4144325 4144325 4144325      0 CAPIB \n\n# To get yearly data\nyearly_data <- BRVM_get1(c(\"brvmtr\", \"BiCc\", \"BOAS\"), Period = 365 ) \n# display the first two tens elements of the table\nhead(yearly_data, 20) \n\n# A tibble: 20 × 6\n   Date         Open   High    Low  Close Ticker\n   <date>      <dbl>  <dbl>  <dbl>  <dbl> <chr> \n 1 2003-04-11   74.0   88.6   73.6   88.6 BRVMTR\n 2 2004-01-02   88.6   89.2   72.9   89.2 BRVMTR\n 3 2005-01-03   89.2  107.    70.7  104.  BRVMTR\n 4 2006-01-02  104.   158.   104.   153.  BRVMTR\n 5 2007-01-02  153.   275.   149.   249.  BRVMTR\n 6 2008-01-02  249.   386.   226.   296.  BRVMTR\n 7 2009-01-02  275.   296.   227.   236.  BRVMTR\n 8 2010-01-04  236.   259.   224.   238.  BRVMTR\n 9 2011-01-03  238.   249.   204.   239   BRVMTR\n10 2012-01-02  239    349.   201.   349.  BRVMTR\n11 2013-01-02  349.   794.   339.   789.  BRVMTR\n12 2014-01-02  789.  1213.   601.  1213.  BRVMTR\n13 2015-01-02 1213.  1525.   653.  1525.  BRVMTR\n14 2016-01-04 1525.  1525.  1216.  1432.  BRVMTR\n15 2017-01-02 1432.  1433.   764.  1203.  BRVMTR\n16 2018-01-02 1114.  1193.   966.   966.  BRVMTR\n17 2019-06-03  403.   429.   311.   367.  BRVMTR\n18 2020-01-01  367.   475.   292.   379.  BRVMTR\n19 2021-01-04  376.   622.   325    622.  BRVMTR\n20 2022-01-03  667.   667.   295.   342.  BRVMTR\n\n# display the two tens of the last elements of the table\ntail(yearly_data, 20) \n\n# A tibble: 20 × 6\n   Date        Open  High   Low Close Ticker\n   <date>     <dbl> <dbl> <dbl> <dbl> <chr> \n 1 2014-01-02  5650  7848  5650  7800 BICC  \n 2 2015-01-02  8385 10750  7800 10100 BICC  \n 3 2016-01-04 10000 10700  8566  9890 BICC  \n 4 2017-01-05  9750 10000  6440  8490 BICC  \n 5 2018-01-02  8700  8750  3795  7900 BICC  \n 6 2019-01-04  7550  7550  3710  6800 BICC  \n 7 2020-01-01  6800  6890  2855  6680 BICC  \n 8 2021-01-04  6680  7525  4280  7400 BICC  \n 9 2022-01-03  7250  7250  5550  6850 BICC  \n10 2023-01-02  6500  6850  5785  6275 BICC  \n11 2014-12-10  1613  3225  1613  3225 BOAS  \n12 2015-01-02  3370  4300  2900  3950 BOAS  \n13 2016-01-04  3700  4101  2000  2350 BOAS  \n14 2017-01-02  2325  3875  2035  2500 BOAS  \n15 2018-01-02  2400  3250  1700  2020 BOAS  \n16 2019-01-02  1900  2000  1500  1545 BOAS  \n17 2020-01-01  1550  1700  1295  1495 BOAS  \n18 2021-01-04  1480  2750  1340  2350 BOAS  \n19 2022-01-03  2350  2780  2200  2450 BOAS  \n20 2023-01-02  2580  2585  2175  2265 BOAS"
  },
  {
    "objectID": "posts/rtip-2023-04-06/index.html#the-brvm.index-function",
    "href": "posts/rtip-2023-04-06/index.html#the-brvm.index-function",
    "title": "A New Package for the African Stock Market {BRVM}",
    "section": "The BRVM.index() function :",
    "text": "The BRVM.index() function :\nIt receives no argument and returns the name of all indexes available on BRVM Stock Exchange.\n\nBRVM.index()\n\n [1] \"BRVMAG\" \"BRVMC\"  \"BRVMAS\" \"BRVMDI\" \"BRVMFI\" \"BRVMIN\" \"BRVMSP\" \"BRVMTR\"\n [9] \"BRVMPR\" \"BRVMPA\" \"BRVM30\"\n\n\nAuthors : \n\nKoffi Frederic Sessie (koffisessie@gmail.com),\nAbdoul Oudouss Diakité (abdouloudoussdiakite@gmail.com),\nSanderson Steven(spsanderson@gmail.com)\n\nCreator : Koffi Frederic Sessie \ncph (Copyright Holder) : Koffi Frederic Sessie \nLicense : MIT 2023, BRVM authors. All rights reserved."
  },
  {
    "objectID": "posts/rtip-2023-04-07/index.html",
    "href": "posts/rtip-2023-04-07/index.html",
    "title": "Reading in Multiple Excel Sheets with lapply and {readxl}",
    "section": "",
    "text": "Intruduction\nReading in an Excel file with multiple sheets can be a daunting task, especially for users who are not familiar with the process. In this blog post, we will walk through a sample function that can be used to read in an Excel file with multiple sheets using the R programming language.\n\n\nFunction\nThe function we will be using is called excel_sheet_reader(). This function takes one argument: filename, which is the name of the Excel file we want to read in. This function, since it is using the {readxl} package will automatically read that data to a tibble.\n\n\nExample\nHere is the function:\n\nexcel_sheet_reader <- function(filename) {\n  sheets <- excel_sheets(filename)\n  x <- lapply(sheets, function(X) read_excel(filename, sheet = X))\n  names(x) <- sheets\n  x\n}\n\nThe first thing the excel_sheet_reader() function does is to determine the names of all the sheets in the Excel file using the excel_sheets function from the readxl package. This function returns a character vector containing the names of all the sheets in the Excel file.\n\nsheets <- excel_sheets(filename)\n\nNext, the function uses the lapply function to loop through all the sheet names and read in each sheet using the read_excel() function, also from the readxl package. This function takes two arguments: filename, which is the name of the Excel file, and sheet, which is the name of the sheet we want to read in. The lapply function returns a list containing all the sheets.\n\nx <- lapply(sheets, function(X) read_excel(filename, sheet = X))\n\nFinally, the function uses the names function to assign the sheet names to the list of sheets and returns the list.\n\nnames(x) <- sheets\nx\n\nNow that we have explained the excel_sheet_reader() function, let’s use it to read in the iris and mtcars datasets.\n\nlibrary(healthyR)\nlibrary(dplyr)\nlibrary(writexl)\nlibrary(readxl)\n\niris |>\n  named_item_list(Species) |>\n  write_xlsx(path = \"iris.xlsx\")\n\nmtcars |>\n  named_item_list(cyl) |>\n  write_xlsx(path = \"mtcars.xlsx\")\n\niris_sheets <- excel_sheet_reader(\"iris.xlsx\")\nmtcars_sheets <- excel_sheet_reader(\"mtcars.xlsx\")\n\nNow lets see the structure of each file.\n\niris_sheets\n\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <chr>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <chr>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# ℹ 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <chr>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# ℹ 40 more rows\n\n\nNow mtcars_sheets\n\nmtcars_sheets\n\n$`4`\n# A tibble: 11 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  22.8     4 108      93  3.85  2.32  18.6     1     1     4     1\n 2  24.4     4 147.     62  3.69  3.19  20       1     0     4     2\n 3  22.8     4 141.     95  3.92  3.15  22.9     1     0     4     2\n 4  32.4     4  78.7    66  4.08  2.2   19.5     1     1     4     1\n 5  30.4     4  75.7    52  4.93  1.62  18.5     1     1     4     2\n 6  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n 7  21.5     4 120.     97  3.7   2.46  20.0     1     0     3     1\n 8  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1\n 9  26       4 120.     91  4.43  2.14  16.7     0     1     5     2\n10  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n11  21.4     4 121     109  4.11  2.78  18.6     1     1     4     2\n\n$`6`\n# A tibble: 7 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n3  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n4  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n5  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n6  17.8     6  168.   123  3.92  3.44  18.9     1     0     4     4\n7  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6\n\n$`8`\n# A tibble: 14 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 2  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 3  16.4     8  276.   180  3.07  4.07  17.4     0     0     3     3\n 4  17.3     8  276.   180  3.07  3.73  17.6     0     0     3     3\n 5  15.2     8  276.   180  3.07  3.78  18       0     0     3     3\n 6  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n 7  10.4     8  460    215  3     5.42  17.8     0     0     3     4\n 8  14.7     8  440    230  3.23  5.34  17.4     0     0     3     4\n 9  15.5     8  318    150  2.76  3.52  16.9     0     0     3     2\n10  15.2     8  304    150  3.15  3.44  17.3     0     0     3     2\n11  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4\n12  19.2     8  400    175  3.08  3.84  17.0     0     0     3     2\n13  15.8     8  351    264  4.22  3.17  14.5     0     1     5     4\n14  15       8  301    335  3.54  3.57  14.6     0     1     5     8\n\n\nAnd that’s it! Hope this has been helpful!"
  },
  {
    "objectID": "posts/rtip-2023-04-10/index.html",
    "href": "posts/rtip-2023-04-10/index.html",
    "title": "Styling Tables for Excel with {styledTables}",
    "section": "",
    "text": "Introduction\nIn the analytics realm whether some like it or not, Excel is huge and maybe King. This is due to the fact of the shear volume of people using it. Microsoft has positioned Excel well in this situation, but, that does not mean we cannot extend Excel with R. In fact we can do just that. I will be focusing new posts on this topic as I gear up to collaborate on a new project focusing on this issue.\nFor this post we are going to discuss the {styledTable} R package that can be installed from GitHub. Here are a few ways in which the styledTable package can help.\n\nCreating visually appealing tables: Excel is a powerful tool for data analysis and visualization, but it can be limited in terms of formatting options. With the ‘styledtable’ package, users can create tables with a wide range of formatting options, such as bold text, colored cells, and borders. This can make the tables more visually appealing and easier to read, which can be helpful when presenting data to others.\nAutomating data analysis: The ‘styledtable’ package can be used in combination with other R packages to automate data analysis tasks. For example, users can use R to clean and transform data, and then use the ‘styledtable’ package to create formatted tables for reporting or sharing with others. This can save time and reduce errors associated with manual data entry and formatting.\nIntegrating with other R packages: R has a large ecosystem of packages for data analysis, visualization, and reporting. The ‘styledtable’ package can be used in conjunction with other R packages to extend the functionality of Excel. For example, users can use R to perform statistical analysis on data, and then use the ‘styledtable’ package to create formatted tables for reporting the results in Excel.\nFacilitating collaboration: Sharing Excel files can be challenging when working with multiple users or teams. With the ‘styledtable’ package, users can export styled tables to Excel format, which can be shared with others. This can facilitate collaboration and streamline the process of sharing data and analysis results.\n\nThe styledtable package in R, which allows users to create styled tables in R Markdown documents. The package can help to create tables with various formatting options such as bold text, colored cells, and borders. It also has functionality on how to port these to Excel itself.\nThe package offers a simple syntax that allows users to specify formatting options using HTML and CSS. The resulting table can be customized by changing the CSS file or by using the ‘styler’ function to apply custom styles to individual cells or rows.\nOverall, the styledtable package provides a useful tool for creating visually appealing tables in R Markdown documents, and the ability to export these tables to Excel format makes it easier to share and analyze data with others.\n\n\nFunctions\n\n\nExamples\n\n# Install development version from GitHub\ndevtools::install_github('R-package/styledTables', build_vignettes = TRUE)"
  },
  {
    "objectID": "posts/rtip-2023-04-11/index.html",
    "href": "posts/rtip-2023-04-11/index.html",
    "title": "Styling Tables for Excel with {styledTables}",
    "section": "",
    "text": "Introduction\nIn the analytics realm whether some like it or not, Excel is huge and maybe King. This is due to the fact of the shear volume of people using it. Microsoft has positioned Excel well in this situation, but, that does not mean we cannot extend Excel with R. In fact we can do just that. I will be focusing new posts on this topic as I gear up to collaborate on a new project focusing on this issue.\nFor this post we are going to discuss the {styledTable} R package that can be installed from GitHub. Here are a few ways in which the styledTable package can help.\n\nCreating visually appealing tables: Excel is a powerful tool for data analysis and visualization, but it can be limited in terms of formatting options. With the ‘styledtable’ package, users can create tables with a wide range of formatting options, such as bold text, colored cells, and borders. This can make the tables more visually appealing and easier to read, which can be helpful when presenting data to others.\nAutomating data analysis: The ‘styledtable’ package can be used in combination with other R packages to automate data analysis tasks. For example, users can use R to clean and transform data, and then use the ‘styledtable’ package to create formatted tables for reporting or sharing with others. This can save time and reduce errors associated with manual data entry and formatting.\nIntegrating with other R packages: R has a large ecosystem of packages for data analysis, visualization, and reporting. The ‘styledtable’ package can be used in conjunction with other R packages to extend the functionality of Excel. For example, users can use R to perform statistical analysis on data, and then use the ‘styledtable’ package to create formatted tables for reporting the results in Excel.\nFacilitating collaboration: Sharing Excel files can be challenging when working with multiple users or teams. With the ‘styledtable’ package, users can export styled tables to Excel format, which can be shared with others. This can facilitate collaboration and streamline the process of sharing data and analysis results.\n\nThe styledtable package in R, which allows users to create styled tables in R Markdown documents. The package can help to create tables with various formatting options such as bold text, colored cells, and borders. It also has functionality on how to port these to Excel itself.\nThe package offers a simple syntax that allows users to specify formatting options using HTML and CSS. The resulting table can be customized by changing the CSS file or by using the ‘styler’ function to apply custom styles to individual cells or rows.\nOverall, the styledtable package provides a useful tool for creating visually appealing tables in R Markdown documents, and the ability to export these tables to Excel format makes it easier to share and analyze data with others.\n\n\nExamples\n\n# Install development version from GitHub\ndevtools::install_github('R-package/styledTables', build_vignettes = TRUE)\n\n\nlibrary(styledTables)\nlibrary(dplyr)\nlibrary(xlsx)\n\ndf <- mtcars |>\n  select(mpg, cyl, am)\n\ndf\n\n                     mpg cyl am\nMazda RX4           21.0   6  1\nMazda RX4 Wag       21.0   6  1\nDatsun 710          22.8   4  1\nHornet 4 Drive      21.4   6  0\nHornet Sportabout   18.7   8  0\nValiant             18.1   6  0\nDuster 360          14.3   8  0\nMerc 240D           24.4   4  0\nMerc 230            22.8   4  0\nMerc 280            19.2   6  0\nMerc 280C           17.8   6  0\nMerc 450SE          16.4   8  0\nMerc 450SL          17.3   8  0\nMerc 450SLC         15.2   8  0\nCadillac Fleetwood  10.4   8  0\nLincoln Continental 10.4   8  0\nChrysler Imperial   14.7   8  0\nFiat 128            32.4   4  1\nHonda Civic         30.4   4  1\nToyota Corolla      33.9   4  1\nToyota Corona       21.5   4  0\nDodge Challenger    15.5   8  0\nAMC Javelin         15.2   8  0\nCamaro Z28          13.3   8  0\nPontiac Firebird    19.2   8  0\nFiat X1-9           27.3   4  1\nPorsche 914-2       26.0   4  1\nLotus Europa        30.4   4  1\nFord Pantera L      15.8   8  1\nFerrari Dino        19.7   6  1\nMaserati Bora       15.0   8  1\nVolvo 142E          21.4   4  1\n\n\nOk, now we have our data that we are going to work with, so let’s check out some features.\nFirst we will just apply the styled_table() function and inspect the output.\n\nstl_df <- df |>\n  styled_table(keep_header = TRUE)\n\nclass(stl_df)\n\n[1] \"StyledTable\"\nattr(,\"package\")\n[1] \"styledTables\"\n\n\nNow let’s apply some simple formatting.\n\nstl_df <- stl_df |>\n  set_border_position(\"all\", row_id = 1) |>\n  set_bold(row_id = 1) |>\n  set_fill_color(\"#00FF00\", col_id = 2, condition = X == \"6\")\n\nWrite out to excel.\n\nwb <- createWorkbook()\nsheet <- createSheet(wb, \"mtcars_tbl\")\n\n# Insert table\nwrite_excel(sheet, stl_df)\n\n# Save workbook\nsaveWorkbook(wb, \"test.xlsx\")\n\nHere is the test output:\n\n\n\nTest Output"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html",
    "href": "posts/rtip-2023-04-18/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "",
    "text": "Shiny is an R package that allows you to build interactive web applications using R code. TidyDensity is an R package that provides a tidyverse-style interface for working with probability density functions. In this tutorial, we’ll use these two packages to build a Shiny app that allows users to interact with TidyDensity functions."
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#required-packages",
    "href": "posts/rtip-2023-04-18/index.html#required-packages",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "Required Packages",
    "text": "Required Packages\nBefore we dive into the code, let’s go over the packages that we’ll be using in this app:\n\nShiny: As mentioned earlier, Shiny is an R package for building interactive web applications. It provides a variety of input controls and output elements that allow you to create user interfaces for your R code.\nTidyDensity: TidyDensity is an R package that provides a tidyverse-style interface for working with probability density functions. It provides a set of functions for generating density functions, as well as a tidy_autoplot() function for creating visualizations.\ntidyverse: Tidyverse is a collection of R packages designed for data science. It includes many popular packages such as ggplot2, dplyr, and tidyr. We’ll be using some functions from the tidyverse packages in our Shiny app.\nDT: DT is an R package for creating interactive tables in RMarkdown documents, Shiny apps, and RStudio. We’ll be using the DT::datatable() function to create a table of output data in our app.\n\nLoad them up!\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(tidyverse)\nlibrary(TidyDensity)"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#the-ui-object",
    "href": "posts/rtip-2023-04-18/index.html#the-ui-object",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "The UI Object",
    "text": "The UI Object\nThe UI object is the first argument of the shinyApp() function, and it defines the layout and appearance of the app. In our TidyDensity Shiny app, we’ll use a sidebar layout with two input controls and two output elements:\n\nSelect Function Input: A selectInput() control that allows users to select one of four TidyDensity functions: tidy_normal(), tidy_bernoulli(), tidy_beta(), and tidy_gamma().\nNumber of Simulations Input: A numericInput() control that allows users to specify the number of simulations to use in the TidyDensity function.\nSample Size Input: A numericInput() control that allows users to specify the sample size to use in the TidyDensity function.\nDensity Plot Output: A plotOutput() element that displays the density plot generated by tidy_autoplot().\nData Table Output: A dataTableOutput() element that displays the output data from the TidyDensity function.\n\nHere’s the code for the UI object:\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"function\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                    )\n                  ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200)\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      dataTableOutput(\"data_table\")\n    )\n  )\n)"
  },
  {
    "objectID": "posts/rtip-2023-04-18/index.html#the-server-object",
    "href": "posts/rtip-2023-04-18/index.html#the-server-object",
    "title": "Exploring Distributions with {shiny} and {TidyDensity}",
    "section": "The Server Object",
    "text": "The Server Object\nThe server object is the second argument of the shinyApp() function, and it defines the behavior and output of the app. In our TidyDensity Shiny app, the server object consists of two reactive expressions that generate the output elements based on the user inputs:\n\nData Reactive Expression: A reactive expression that generates the output data for the selected TidyDensity function based on the user inputs. We use match.fun() to convert the selected function name into an R function, and we pass the num_sims and n arguments from the input controls.\nDensity Plot Reactive Expression: A reactive expression that generates the density plot using tidy_autoplot() and the output data from the data reactive expression.\nData Table Output: We use DT::renderDataTable() to generate the data table output element based on the output data from the data reactive expression.\n\nHere’s the code for the server object:\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$function)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() %>%\n      tidy_autoplot()\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n}"
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html",
    "href": "posts/rtip-2023-04-19/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "",
    "text": "Shiny is an R package that allows you to create interactive web applications from R code. In this blog post, we’ll explore the different components of a Shiny application and show how they work together to create an interactive data visualization app. This is a part 2 with a small enhancement."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-ui",
    "href": "posts/rtip-2023-04-19/index.html#the-ui",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The UI",
    "text": "The UI\nThe user interface (UI) is the visual part of the app that the user interacts with. In our app, the UI is defined using the fluidPage() function from the shiny package. It consists of a title panel, a sidebar layout, and a main panel.\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      )\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\nThe title panel displays the app name, while the sidebar layout contains the input controls for the user. In this case, we have four input elements:\n\nselectInput() allows the user to choose a statistical distribution to generate data from.\nnumericInput() allows the user to set the number of simulations.\nAnother numericInput() allows the user to set the sample size.\nselectInput() allows the user to choose the type of plot to display.\n\nThe main panel contains the output elements for the app, in this case a plot and a table."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-server",
    "href": "posts/rtip-2023-04-19/index.html#the-server",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The Server",
    "text": "The Server\nThe server is the backend of the app that handles the logic and generates the output based on user input. In our app, the server is defined using the server() function from the shiny package.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n}\n\nThe server() function takes two arguments, input and output. These arguments allow the server to interact with the user interface.\nFirst, we create a reactive data object data, which takes in the user’s input for the function, number of simulations, and sample size, and passes it to the appropriate function using match.fun().\nNext, we create the density_plot output. We use the renderPlot() function to create a reactive plot of the data using the tidy_autoplot() function from the {TidyDensity} package. The tidy_autoplot() function allows the user to choose from several plot types, including density, quantile, probability, qq, and mcmc. We then print the plot using the print() function.\nFinally, we create the data_table output using the DT::renderDataTable() function. This output displays the reactive data as a table using the DT::datatable() function."
  },
  {
    "objectID": "posts/rtip-2023-04-19/index.html#the-shiny-app",
    "href": "posts/rtip-2023-04-19/index.html#the-shiny-app",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 2",
    "section": "The Shiny App",
    "text": "The Shiny App\nFinally, we run the Shiny app using the shinyApp() function, which takes the ui and server functions as arguments:\n\nshinyApp(ui = ui, server = server)\n\nThis launches the app and displays the user interface. The user can interact with the app by selecting a function, specifying the number of simulations and sample size, and viewing the resulting density plot and data table. The app provides a simple and interactive way to explore the TidyDensity package and its functionality."
  },
  {
    "objectID": "posts/rtip-2023-04-20/index.html",
    "href": "posts/rtip-2023-04-20/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 3",
    "section": "",
    "text": "Introduction\nIn the previous post we allowed users to choose a distribution and a plot type. Now, we want to allow users to download a .csv file of the data that is generated.\nIn the UI, we added a downloadButton with outputId = \"download_data\" and label = \"Download Data\". In the server, we added a downloadHandler that takes a filename and content function. The filename function returns the name of the file to be downloaded (in this case, we used the selected function name as the file name with “.csv” extension). The content function writes the reactive data to a CSV file using the write.csv function. The downloadHandler returns the file to be downloaded when the button is clicked.\nSee here: \n\n\nUI Section\nHere is the update to the UI Section\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      # Download the data\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n\n\nServer Section\nHere is the update to the Server section.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      paste0(input$functions, \".csv\")\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\n\n\nConclusion\nWith these changes, the user can now export the data to a .csv file by clicking the “Export Data” button and selecting where to save the file.\nI hope this update to the TidyDensity app will make it more useful for your data analysis needs. If you have any questions or feedback, please feel free to let me know, and as usual…Steal this Code!! Modify for yourself and see what you come up with.\nHere is the entire script:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"functions\",\n                  label = \"Select Function\",\n                  choices = c(\n                    \"tidy_normal\", \n                    \"tidy_bernoulli\", \n                    \"tidy_beta\", \n                    \"tidy_gamma\"\n                  )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input\n    match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    p <- data() |>\n      tidy_autoplot(.plot_type = input$plot_type)\n    \n    print(p)\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    DT::datatable(data())\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      paste0(input$functions, \".csv\")\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-21/index.html",
    "href": "posts/rtip-2023-04-21/index.html",
    "title": "Exploring Distributions with {shiny} and {TidyDensity} Part 4",
    "section": "",
    "text": "Introduction\nIf you’re new to data science or statistics, you may have heard about probability distributions. Probability distributions are mathematical functions that help us understand the probability of a random variable taking on a certain value. For example, if we’re rolling a fair six-sided die, we know that each number has an equal chance of being rolled (1/6 or about 17% chance). We can represent this using a probability distribution, specifically a discrete uniform distribution.\nHowever, not all probability distributions are as simple as a uniform distribution. Many real-world phenomena, such as the heights of people, the number of cars passing through a toll booth in a day, or the amount of rainfall in a particular area, are continuous and can’t be represented using a discrete distribution. Instead, we use continuous probability distributions, which describe the probability of a continuous variable taking on a range of values.\nThere are many different types of continuous probability distributions, each with their own properties and use cases. For example, the normal distribution, also known as the bell curve, is commonly used to model many natural phenomena, such as human heights and weights. The beta distribution is used to model proportions or percentages, such as the proportion of voters who support a particular candidate. The gamma distribution is used to model the time between events in a Poisson process, such as the time between customers arriving at a store.\nThe sample TidyDensity App is a tool that helps us explore and visualize these different types of probability distributions. It’s a web application built using the R programming language and the Shiny framework, which allows us to create interactive web applications with R.\nLet’s break down the different components of the TidyDensity App.\n\n\nUser Interface\nThe user interface, or UI for short, is what the user sees and interacts with when they use the app. It’s built using HTML, CSS, and JavaScript, and it’s the first thing the user sees when they open the app.\nThe TidyDensity App has a simple UI that allows the user to select from four different probability distributions: normal, Bernoulli, beta, and gamma. Each of these distributions has its own properties and use cases, and the user can select which one they want to explore using a dropdown menu.\nIn addition, the user can specify the number of simulations they want to run, which determines how many times the probability distribution is sampled to generate data. They can also specify the sample size, which determines how many data points are generated in each simulation.\nFinally, the user can select which type of plot they want to see, such as a density plot, a quantile plot, a probability plot, or a QQ plot. Each of these plots shows a different aspect of the data generated from the probability distribution, and the user can choose which one to explore.\nHere is the code:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n\n\n\nHere is the new addition to the UI\n\n\n\n\nServer\nThe server is the back-end of the TidyDensity App. It’s responsible for generating the data based on the user’s inputs, and for creating the plots and tables that the user sees on the UI.\nThe server is written in R, and it uses several R packages to generate the data and create the plots. For example, the TidyDensity package is used to generate data from the selected probability distribution, and the ggplot2 package is used to create the plots.\nThe server is also responsible for handling user inputs, such as which probability distribution to use, how many simulations to run, and which plot type to show. It then generates the appropriate data and plot based on these inputs and sends them back to the UI for display.\nThe first thing we do is create a reactive variable data that will store the output of the match.fun() function, which is called with the arguments .num_sims and .n obtained from the user interface. We use the reactive variable because it will update automatically whenever the inputs are changed.\nThe output$density_plot object is created with renderPlot(), which takes the reactive variable data() and passes it to tidy_autoplot() with the plot type selected by the user in the input$plot_type object. The resulting plot is then printed to the user interface.\nThe output$data_table object is created with DT::renderDataTable(), which takes the reactive variable data() and returns a table to the user interface using the DT::datatable() function.\nFinally, the output$download_data object is created using downloadHandler(), which creates a download button for the user to download a .csv file of the data. The filename argument specifies the name of the file, and the content argument writes the data to a .csv file.\nHere is the code:\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n\n\nData Table\nThe data table is a table that shows the data generated from the probability distribution. It’s displayed on.\nOverall, this app is designed to allow users to generate various types of probability density plots and accompanying data tables based on user input. By allowing users to select different functions, sample sizes, and plot types, this app provides a flexible and customizable tool for exploring and visualizing probability distributions.\n\n\nFull Shiny App\nHere is the full script:\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      plotOutput(\"density_plot\"),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-24/index.html",
    "href": "posts/rtip-2023-04-24/index.html",
    "title": "Exploring Distributions with {shiny}, {TidyDensity} and {plotly} Part 5",
    "section": "",
    "text": "Introduction\nI have been writing about using the {TidyDensity} package with shiny for the last few posts, and this one is the last. This post will go over the app and discuss how to change the output of the graph from a ggplot2 object into a plotly object. So we will end up with something like this in the menu panel:\n\n\n\nPlotly Output\n\n\nAnd here is the difference between the plots, first the ggplot2 plot: \nAnd the plotly_plot: \nFirst, the required libraries are loaded: shiny, TidyDensity, tidyverse, DT, and plotly.\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(plotly)\n\n\n\nUI\nThe user interface (UI) is defined using the fluidPage() function from the shiny package. The UI consists of a title panel, a sidebar panel, and a main panel. The title panel simply displays the title of the app, while the sidebar panel contains user input elements such as radio buttons, text inputs, and numeric inputs. The main panel displays the plot, data table, and download button.\n\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      # user input elements\n    ),\n    mainPanel(\n      # plot, data table, and download button\n    )\n  )\n)\n\nNext, the server is defined using the server() function from the shiny package. The server is responsible for generating the output based on the user inputs. The first step is to create reactive data using the reactive() function. The reactive data is created based on the user inputs for the distribution function or the entered data. The match.fun() function is used to match the selected function with the corresponding function in the TidyDensity package. The tidy_empirical() function is used if the user entered their own data.\n\n\nServer\n\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n\nAfter the reactive data is created, the output is generated. The output consists of the density plot, data table, and download button. The renderPlot() and renderPlotly() functions are used to generate the plot output. The renderDataTable() function is used to generate the data table output. The downloadHandler() function is used to generate the download button.\n\n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n\nNext, we define the server function, which contains the code that will run in response to user input. We start by creating a reactive data object called data. This object will store the data that will be used to generate the plots and tables in the app.\nThe data that data stores depends on the user’s input. If the user selects “Enter Data” in the sidebar, then data will be set to a tidy_empirical() object generated from the user-entered data. Otherwise, if the user selects “Select Function”, then data will be set to a tidy_ function object generated using the user’s choices for number of simulations and sample size.\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  ...\n}\n\nThe tidy_empirical() function is used to generate a density plot of the empirical distribution of the user-entered data. This function takes the user-entered data as input and returns a tidy data frame that can be used to create a density plot.\nThe tidy_ functions are used to simulate data from various distributions and generate plots based on that data. These functions take the number of simulations and sample size as input and return a tidy data frame that can be used to create various types of plots.\nNext, we define the code for generating the density plot. This code uses the data object that was created earlier to generate a plot. The tidy_autoplot() function is used to generate the plot based on the user’s selected plot type. If the user selects the “Use Plotly” option, then the plot is generated using the ggplotly() function from the plotly package.\n\n # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n\nThe ggplotly() function is used to generate an interactive version of the plot that can be zoomed in and out of and hovered over to see details about specific data points.\nNext, we define the code for generating the data table. This code simply displays the data object as a table using the datatable() function from the DT package.\n\n# Create data table\noutput$data_table <- DT::renderDataTable({\n  # Return reactive data as a data table\n  if (!is.null(data())) {\n    DT::datatable(data())\n  }\n})\n\nFinally, we define the code for downloading the data as a CSV file. This code uses the downloadHandler() function to generate a file download link that, when clicked, will download the data as a CSV file. The name of the CSV file depends on the user’s input.\n\n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n\nFinally, here is the script in it’s entirety, steal it and see what you can come up with!!\n\nlibrary(shiny)\nlibrary(TidyDensity)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(plotly)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"TidyDensity App\"),\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(inputId = \"data_input_type\",\n                   label = \"Data Input Type:\",\n                   choices = c(\"Select Function\", \"Enter Data\"),\n                   selected = \"Select Function\"),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Enter Data'\",\n        textInput(inputId = \"data\",\n                  label = \"Enter data as a comma-separated list of numeric values\")\n      ),\n      conditionalPanel(\n        condition = \"input.data_input_type == 'Select Function'\",\n        selectInput(inputId = \"functions\",\n                    label = \"Select Function\",\n                    choices = c(\n                      \"tidy_normal\", \n                      \"tidy_bernoulli\", \n                      \"tidy_beta\", \n                      \"tidy_gamma\"\n                    )\n        )\n      ),\n      numericInput(inputId = \"num_sims\",\n                   label = \"Number of simulations:\",\n                   value = 1,\n                   min = 1,\n                   max = 15),\n      numericInput(inputId = \"n\",\n                   label = \"Sample size:\",\n                   value = 50,\n                   min = 30,\n                   max = 200),\n      selectInput(inputId = \"plot_type\",\n                  label = \"Select plot type\",\n                  choices = c(\n                    \"density\",\n                    \"quantile\",\n                    \"probability\",\n                    \"qq\",\n                    \"mcmc\"\n                  )\n      ),\n      selectInput(inputId = \"plotly_option\",\n                  label = \"Use Plotly\",\n                  choices = c(\"TRUE\", \"FALSE\"),\n                  selected = \"FALSE\"\n      ),\n      downloadButton(outputId = \"download_data\", label = \"Download Data\")\n    ),\n    mainPanel(\n      conditionalPanel(\n        condition = \"input.plotly_option == 'TRUE'\",\n        plotlyOutput(\"density_plotly\")\n      ),\n      conditionalPanel(\n        condition = \"input.plotly_option == 'FALSE'\",\n        plotOutput(\"density_plot\")\n      ),\n      DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n# Define server\nserver <- function(input, output) {\n  \n  # Create reactive data\n  data <- reactive({\n    # Call selected function with user input or tidy_empirical if user entered data\n    if (input$data_input_type == \"Enter Data\") {\n      data <- input$data\n      if (is.null(data) || data == \"\") {\n        return(NULL)\n      }\n      data <- as.numeric(strsplit(data, \",\")[[1]])\n      tidy_empirical(data)\n    } else {\n      match.fun(input$functions)(.num_sims = input$num_sims, .n = input$n)\n    }\n  })\n  \n  # Create density plot\n  output$density_plot <- renderPlot({\n    # Call autoplot on reactive data\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      print(p)\n      \n      #ifelse(input$plotly_option == \"TRUE\", ggplotly(p), p)\n    }\n  })\n  \n  output$density_plotly <- renderPlotly({\n    if (!is.null(data())) {\n      p <- data() |>\n        tidy_autoplot(.plot_type = input$plot_type)\n      \n      ggplotly(p)\n    }\n  })\n  \n  # Create data table\n  output$data_table <- DT::renderDataTable({\n    # Return reactive data as a data table\n    if (!is.null(data())) {\n      DT::datatable(data())\n    }\n  })\n  \n  # Download data handler\n  output$download_data <- downloadHandler(\n    filename = function() {\n      if (input$data_input_type == \"Enter Data\") {\n        paste0(\"tidy_empirical.csv\")\n      } else {\n        paste0(input$functions, \".csv\")\n      }\n    },\n    content = function(file) {\n      write.csv(data(), file, row.names = FALSE)\n    }\n  )\n  \n}\n\n# Run\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-25/index.html",
    "href": "posts/rtip-2023-04-25/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 1",
    "section": "",
    "text": "Introduction\nWelcome to the {tidyAML} Model Builder, a Shiny web application that allows you to build predictive models using the tidyAML and Parsnip packages in R.\nLet’s dive into the code to understand how it works!\n\n\nLoad Libraries\nFirst, we load the necessary packages:\n\nshiny\ntidyAML\nrecipes\nDT\nglmnet.\n\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\n\n\n\nUI\nNext, we define the user interface (UI) of the Shiny app using the fluidPage() function from the shiny package. The UI consists of a title panel, a sidebar panel, and a main panel.\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\n        \"dataset\", \n        \"Choose a built-in dataset:\", \n        choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\n        \"predictor_col\", \n        \"Select the predictor column:\", \n        choices = NULL\n      ),\n      selectInput(\n        \"model_type\", \n        \"Select a model type:\", \n        choices = c(\"regression\", \"classification\")\n      ),\n      selectInput(\n        \"model_fn\", \n        \"Select a model function:\", \n         choices = c(\"lm\", \"glm\", \"glmnet\")\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\")\n    )\n  )\n)\n\nThe sidebarPanel() contains several input elements that allow the user to specify the dataset, the predictor column, the type of model, and the model function. There is also an input element that allows the user to upload their own data file. The actionButton() is used to trigger the model building process. Finally, the verbatimTextOutput() element is used to display the output of the model building process.\nThe mainPanel() contains a single verbatimTextOutput() element that displays the output of the model building process.\nNext, we define the server function, which is responsible for handling the user inputs and building the predictive models. The server function takes three arguments:input, output, and session.\n\nserver <- function(input, output, session){\n  ...\n}\n\nWe start by defining a reactive expression called data. This expression reads in the user-specified dataset or data file and updates the predictor_col select input with the names of the columns of the dataset.\n\n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n\nThe first reactive expression, data, reads in the data file uploaded by the user or selects a built-in dataset, depending on which option the user chooses. If the user uploads a file, the read.csv() function is used to read the data file into a data frame. If the user selects a built-in dataset, the get() function is used to retrieve the data frame associated with that dataset. In both cases, the column names of the data frame are used to update the choices in the predictor_col select input, so that the user can select which column to use as the predictor variable.\nThe next reactive expression, recipe_obj, creates a recipe object based on thepredictor_col selected by the user and the data frame returned by data(). The as.formula() function is used to create a formula that specifies the predictor column as the response variable and all other columns as the predictors. The resulting formula is passed to the recipe() function, along with the data frame. The step_normalize() function is then used to standardize all numeric predictors (except for the outcome variable) to have a mean of 0 and a standard deviation of 1. The resulting recipe object is returned by the reactive expression.\n\n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    \n    return(rec)\n  })\n\nThe model_fn reactive expression uses a switch() statement to determine which model function to use based on the model_fn select input. The available options are \"lm\" (for linear regression), \"glm\" (for generalized linear models), and \"glmnet\" (for regularized linear models).\n\n  model_fn <- reactive({\n    switch(\n      input$model_fn,\n      \"lm\" = \"lm\",\n      \"glm\" = \"glm\",\n      \"glmnet\" = \"glmnet\"\n    )\n  })\n\nThe last reactive expression, model, uses the fast_regression() or fast_classification() functions from the tidyAML package to build a regression or classification model based on the data, recipe, and model function selected by the user. The resulting model object is returned by the reactive expression.\n\n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(\n        .data = data(),\n        .rec_obj = recipe_obj(),\n        .parsnip_eng = model_fn()\n      )\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(\n        .data = data(),\n        .rec_obj = recipe_obj(),\n        .parsnip_eng = model_fn()\n      )\n    }\n    return(mod)\n  })\n\nFinally we output the summary of the recipe_obj and print the resulting tibble of model(s) to the screen.\n\n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n\nAnd of course, we cannot serve our app until we run the following line:\n\nshinyApp(ui = ui, server = server)\n\nI hope you have enjoyed this post. Please steal this code and see what you can do with it. I am trying to figure out how to print the tibble using the DT package so maybe in another post.\n\n\nFull Shiny App\nHere are some pictures \n\n\n\nMaking a recipe change\n\n\n\n\n\nSingle Model Output\n\n\n\n\n\nTwo Model Output with one successful failure\n\n\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n                  ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n                  ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_fn\", \"Select a model function:\", \n                  choices = c(\"lm\", \"glm\", \"glmnet\")\n                  ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n                  ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_fn <- reactive({\n    switch(input$model_fn,\n           \"lm\" = \"lm\",\n           \"glm\" = \"glm\",\n           \"glmnet\" = \"glmnet\")\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_fn())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_fn())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-26/index.html",
    "href": "posts/rtip-2023-04-26/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 2",
    "section": "",
    "text": "Introduction\nYesterday I spoke about building tidymodels models using my package {tidyAML} and {shiny}. I have made an update to it, and will continue to make updates to it this week.\nI have added all of the supported engines for regression problems only, NOT classification yet, that will be tomorrow’s work. I will then add a drop down for users to pick which backend function they want to use from {parsnp} like linear_reg().\nHere are some pictures of the udpates.\n\n\n\nNew Drop Down Additions\n\n\n\n\n\nreactable Error, not sure on how to fix yet\n\n\n\n\n\nreactable output\n\n\nHere is the full application, please steal this code and modify for yourself, you never know what you might come up with!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"TidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n                  ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n                  ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_fn\", \"Select a model function:\", \n                  choices = c(\"all\",\"lm\",\"brulee\",\"gee\",\"glm\",\n                              \"glmer\",\"glmnet\",\"gls\",\"lme\",\n                              \"lmer\",\"stan\",\"stan_glmer\",\n                              \"Cubist\",\"hurdle\",\"zeroinfl\",\"earth\",\n                              \"rpart\",\"dbarts\",\"xgboost\",\"lightgbm\",\n                              \"partykit\",\"mgcv\",\"nnet\",\"kknn\",\"ranger\",\n                              \"randomForest\",\"xrf\",\"LiblineaR\",\"kernlab\"\n                            )\n                  ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n        )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n        )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n                  ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_fn <- reactive({\n    switch(input$model_fn,\n           \"all\" = \"all\",\n           \"lm\" = \"lm\",\n           \"brulee\" = \"brulee\",\n           \"gee\" = \"gee\",\n           \"glm\" = \"glm\",\n           \"glmer\" = \"glmer\",\n           \"glmnet\" = \"glmnet\",\n           \"gls\" = \"gls\",\n           \"lme\" = \"lme\",\n           \"lmer\" = \"lmer\",\n           \"stan\" = \"stan\",\n           \"stan_glmer\" = \"stan_glmer\",\n           \"Cubist\" = \"Cubist\",\n           \"hurdle\" = \"hurdle\",\n           \"zeroinfl\" = \"zeroinfl\",\n           \"earth\" = \"earth\",\n           \"rpart\" = \"rpart\",\n           \"dbarts\" = \"dbarts\",\n           \"xgboost\" = \"xgboost\"          ,\n           \"lightgbm\" = \"lightgbm\",\n           \"partykit\" = \"partykit\",\n           \"mgcv\" = \"mgcv\",\n           \"nnet\" = \"nnet\",\n           \"kknn\" = \"kknn\",\n           \"ranger\" = \"ranger\",\n           \"randomForest\" = \"randomForest\",\n           \"xrf\" = \"xrf\",\n           \"LiblineaR\" = \"LiblineaR\",\n           \"kernlab = kernlab\")\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_fn())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_fn())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/rtip-2023-04-27/index.html",
    "href": "posts/rtip-2023-04-27/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 3",
    "section": "",
    "text": "Introduction\nAs data science continues to be a sought-after field, creating a reliable and accurate model is essential. While there are various machine learning algorithms available, the process of selecting the correct algorithm can be complex. The {tidyAML} package, part of the tidymodels suite, offers an easy-to-use, consistent interface for building machine learning models. In this post, we will explore a Shiny application that utilizes tidyAML to build a machine learning model.\nToday I have updated the tidyAML shiny app to include the ability to set the parameter of the fast_regression() function .parsnip_fns and this is things like linear_reg.\nHere is a full list of what is available:\n\nlibrary(tidyAML)\nlibrary(dplyr)\n\nc(\"all\",\n  make_regression_base_tbl() |> \n    pull(.parsnip_fns) |> \n    unique()\n  )\n\n [1] \"all\"              \"linear_reg\"       \"cubist_rules\"     \"poisson_reg\"     \n [5] \"bag_mars\"         \"bag_tree\"         \"bart\"             \"boost_tree\"      \n [9] \"decision_tree\"    \"gen_additive_mod\" \"mars\"             \"mlp\"             \n[13] \"nearest_neighbor\" \"rand_forest\"      \"rule_fit\"         \"svm_linear\"      \n[17] \"svm_poly\"         \"svm_rbf\"         \n\n\nI have updated the UI to reflect using that method as well. Here is the UI changes:\n\n      selectInput(\"model_engine\", \"Select a model engine:\", \n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_engine) |> \n                                unique()\n                  )\n      ),\n      selectInput(\"model_fns\", \"Select a model function:\",\n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_fns) |> \n                                unique()\n                              )\n\nHere are some pictures showing the changes:\n\n\n\nUI Change\n\n\n\n\n\nUI Change 2\n\n\n\n\n\nOutput\n\n\nSo what this means is that we can just pick a function like parsnip::linear_reg() and leave the engine set to \"all\" and it will build models for all engines supported that work with linear_reg().\n\n\nThe Shiny Application\nThe Shiny application is a graphical user interface (GUI) that allows users to select a dataset, predictor column, model type, and engine, and then build a machine learning model. The user can upload a CSV or TXT file or choose one of two built-in datasets: “mtcars” or “iris”. The user can select the predictor column, which is the variable used to predict the outcome, and then choose the model type, either “regression” or “classification”. Next, the user can select a model engine and a model function to use in building the model. Once the user has made all the selections, they can click the “Build Model” button to create the model.\nThe code for the Shiny application can be broken down into two parts, the User Interface (UI) and the Server. Let’s take a closer look at each of these parts.\n\n\nThe UI\nThe UI is created using the fluidPage() function from the shiny package. The titlePanel() function creates the title of the application. The sidebarLayout() function creates the sidebar and main panel. The sidebar contains input controls such as file input, select input, and an action button. The main panel displays the outputs generated by the model.\nThe fileInput() function creates a widget that allows the user to upload a data file. The selectInput() function creates dropdown menus for the user to select the dataset, predictor column, model type, model engine, and model function. The actionButton() function creates a button that the user clicks to build the model. The verbatimTextOutput() function and reactableOutput() function display the output generated by the model.\n\n\nThe Server\nThe Server is where the input data is processed, the model is built, and the output is generated. The Server is created using the server() function from the shiny package.\nThe reactive() function is used to create a reactive object called data that reads in the data file or built-in dataset selected by the user. The eventReactive() function is used to create a reactive object called recipe_obj that creates a recipe for preprocessing the data. The recipe includes steps to normalize the numeric variables and remove the outcome variable from the recipe.\nTwo other reactive objects, model_engine and model_fns, are created using the switch() function. These objects contain a list of available engines and model functions for the user to choose from.\nFinally, the eventReactive() function is used to create a reactive object called model that builds the machine learning model. The fast_regression() and fast_classification() functions from the tidyAML package are used to build the regression and classification models, respectively.\n\n\nConclusion\nIn this post, we explored a Shiny application that uses tidyAML to build a machine learning model. The application allows users to select a dataset, predictor column, model type, engine, and function to build a machine learning model. The Shiny application is an excellent tool for those who are new to machine learning or those who want to streamline the rapid prototyping process.\n\n\nFull Application\nThis is a work in progress, and I want you to steal this code and see what you can come up with!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\", \n                  \"Select the predictor column:\", \n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")),\n      selectInput(\"model_engine\", \"Select a model engine:\", \n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_engine) |> \n                                unique()\n                  )\n      ),\n      selectInput(\"model_fns\", \"Select a model function:\",\n                  choices = c(\"all\",\n                              make_regression_base_tbl() |> \n                                pull(.parsnip_fns) |> \n                                unique()\n                              )\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n      )\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n    ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_engine <- reactive({\n    switch(input$model_engine,\n           \"all\" = \"all\",\n           \"lm\" = \"lm\",\n           \"brulee\" = \"brulee\",\n           \"gee\" = \"gee\",\n           \"glm\" = \"glm\",\n           \"glmer\" = \"glmer\",\n           \"glmnet\" = \"glmnet\",\n           \"gls\" = \"gls\",\n           \"lme\" = \"lme\",\n           \"lmer\" = \"lmer\",\n           \"stan\" = \"stan\",\n           \"stan_glmer\" = \"stan_glmer\",\n           \"Cubist\" = \"Cubist\",\n           \"hurdle\" = \"hurdle\",\n           \"zeroinfl\" = \"zeroinfl\",\n           \"earth\" = \"earth\",\n           \"rpart\" = \"rpart\",\n           \"dbarts\" = \"dbarts\",\n           \"xgboost\" = \"xgboost\"          ,\n           \"lightgbm\" = \"lightgbm\",\n           \"partykit\" = \"partykit\",\n           \"mgcv\" = \"mgcv\",\n           \"nnet\" = \"nnet\",\n           \"kknn\" = \"kknn\",\n           \"ranger\" = \"ranger\",\n           \"randomForest\" = \"randomForest\",\n           \"xrf\" = \"xrf\",\n           \"LiblineaR\" = \"LiblineaR\",\n           \"kernlab = kernlab\")\n  })\n  \n  model_fns <- reactive({\n    switch(input$model_fns,\n           \"all\" = \"all\",\n           \"linear_reg\" = \"linear_reg\",\n           \"cubist_rules\" = \"cubist_rules\",\n           \"poisson_reg\" = \"poisson_reg\",\n           \"bag_mars\" = \"bag_mars\",\n           \"bag_tree\" = \"bag_tree\",\n           \"bart\" = \"bart\",\n           \"boost_tree\" = \"boost_tree\",\n           \"decision_tree\" = \"decision_tree\",\n           \"gen_additive_mod\" = \"gen_additive_mod\",\n           \"mars\" = \"mars\",\n           \"mlp\" = \"mlp\",\n           \"nearest_neighbor\" = \"nearest_neighbor\",\n           \"rand_forest\" = \"rand_forest\",\n           \"rule_fit\" = \"rule_fit\",\n           \"svm_linear\" = \"svm_linear\",\n           \"svm_poly\" = \"svm_poly\",\n           \"svm_rbf\" = \"svm_rbf\"\n    )\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_engine(),\n                             .parsnip_fns = model_fns())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_engine(),\n                                 .parsnip_fns = model_fns())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-04-29/index.html",
    "href": "posts/rtip-2023-04-29/index.html",
    "title": "Building models with {shiny} and {tidyAML} Part 4",
    "section": "",
    "text": "Introduction\nThis is a Shiny app for building models using the {tidyAML} which is based on the tidymodels package in R. The app allows you to upload your own data or choose from one of two built-in datasets (mtcars or iris) and select the type of model you want to build (regression or classification).\nLet’s take a closer look at the code.\nFirst, the necessary packages are loaded:\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\nThe tidymodels_prefer() function is called to set some default options for the tidymodels package, and load_deps() from tidyAML is called to make sure all the necessary packages are loaded, you can also separately run install_deps() to make sure they all get installed.\n\ntidymodels_prefer()\nload_deps()\n\nNext, the user interface (UI) is defined using the fluidPage() function. The UI consists of a title panel and a sidebar layout with various input elements, such as file input and select input. There are also two conditional panels that are shown depending on the selected model type (regression or classification). The UI also includes an action button and some output elements, such as verbatimTextOutput and reactableOutput.\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\",\n                  \"Select the predictor column:\",\n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")\n      ),\n      conditionalPanel(\n        condition = \"input.model_type == 'regression'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique()))\n      ),\n      \n      conditionalPanel(\n        condition = \"input.model_type == 'classification'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique())),\n        checkboxInput(\"predictor_factor\",\n                      \"Convert predictor column to factor?\",\n                      value = TRUE)\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nAfter defining the UI, the server function is defined. The server function handles the reactive behavior of the app.\nThe first reactive element is data, which reads in the data file if one is uploaded or loads the selected built-in dataset if one is chosen. It also converts the predictor column to a factor if the classification model type is selected.\nIn the server function, we first define a reactive expression data() that will read the data file uploaded by the user or one of the built-in datasets (mtcars or iris). If the user has uploaded a file, the function read.csv is used to read the data, and if it’s a classification problem, the predictor column is converted to a factor variable. The updateSelectInput function is then called to update the predictor_col select input with the names of the columns in the data. If the user has chosen one of the built-in datasets, it is loaded using the get function, and the same preprocessing is performed.\nNext, we define an event reactive recipe_obj() that creates a recipes object based on the selected predictor column and normalizes the numeric variables in the data. The step_normalize function standardizes all numeric variables (except the outcome variable) to have mean 0 and standard deviation 1. This is a common preprocessing step in machine learning pipelines that can improve model performance.\nTwo reactive expressions, model_engine() and model_fns(), are then defined to generate the available model engines and functions based on the selected model type. For regression models, the make_regression_base_tbl functions are used, and for classification models, the make_classification_base_tbl functions are used. These functions return a table with information about the available model engines and functions for a given problem type. The pull function is used to extract the relevant columns from the table, and unique is used to remove duplicate values. The c function is used to concatenate the “all” choice with the available model engines or functions.\nFinally, an event reactive model() is defined that builds the model based on the selected parameters. If the model type is regression, the fast_regression function from the tidyAML package is used, and if the model type is classification, the fast_classification function is used. These functions take as inputs the data, the recipes object, the selected model engine and function, and any additional model parameters.\nThere are three output functions defined in the server: output$recipe_output, output$model_table, and output$model_reactable. The first output function output$recipe_output renders a summary of the recipes object created by recipe_obj() if the predictor_col input is not null. The second output function output$model_table prints the model object returned by model() if the build_model button has been clicked. The third output function output$model_reactable renders a reactive table using the reactable function from the reactable package if the build_model button has been clicked. This table displays the tidyaml_model_tbl.\nOverall, this code creates a Shiny web application that allows users to build machine learning models using the tidymodels framework via {tidyAML}. Users can upload their own data or use one of the built-in datasets, select a predictor column, choose a model type, select a model engine and function, and build the model. The output is displayed in a table that provides insights into the model’s performance and coefficients. This code is useful for data scientists and analysts who want to quickly build and evaluate machine learning models without having to write code from scratch.\n\n\nFull Application\nAs usual, steal this code and make it your own! See what you can do too!\n\nlibrary(shiny)\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(DT)\nlibrary(glmnet)\nlibrary(rules)\nlibrary(tidymodels)\nlibrary(reactable)\n\ntidymodels_prefer()\n\nui <- fluidPage(\n  titlePanel(\"tidyAML Model Builder\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Upload your data file (csv or txt):\"),\n      selectInput(\"dataset\", \n                  \"Choose a built-in dataset:\", \n                  choices = c(\"mtcars\", \"iris\")\n      ),\n      selectInput(\"predictor_col\",\n                  \"Select the predictor column:\",\n                  choices = NULL\n      ),\n      selectInput(\"model_type\", \n                  \"Select a model type:\", \n                  choices = c(\"regression\", \"classification\")\n      ),\n      conditionalPanel(\n        condition = \"input.model_type == 'regression'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_regression_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique()))\n      ),\n      \n      conditionalPanel(\n        condition = \"input.model_type == 'classification'\",\n        selectInput(\"model_engine\", \n                    \"Select a model engine:\", \n                    choices = c(\"all\", \n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_engine) %>% \n                                  unique())),\n        selectInput(\"model_fns\", \n                    \"Select a model function:\", \n                    choices = c(\"all\",\n                                make_classification_base_tbl() %>% \n                                  pull(.parsnip_fns) %>% \n                                  unique())),\n        checkboxInput(\"predictor_factor\",\n                      \"Convert predictor column to factor?\",\n                      value = TRUE)\n      ),\n      actionButton(\"build_model\", \"Build Model\"),\n      verbatimTextOutput(\"recipe_output\")\n    ),\n    mainPanel(\n      verbatimTextOutput(\"model_table\"),\n      reactableOutput(\"model_reactable\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  data <- reactive({\n    if (!is.null(input$file)) {\n      df <- read.csv(\n        input$file$datapath, \n        header = TRUE, \n        stringsAsFactors = FALSE\n      )\n      if (input$model_type == \"classification\") {\n        df[[input$predictor_col]] <- as.factor(df[[input$predictor_col]])\n      }\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    } else if (!is.null(input$dataset)) {\n      df <- get(input$dataset)\n      if (input$model_type == \"classification\") {\n        df[[input$predictor_col]] <- as.factor(df[[input$predictor_col]])\n      }\n      updateSelectInput(\n        session, \n        \"predictor_col\", \n        choices = names(df)\n      )\n      return(df)\n    }\n  })\n  \n  recipe_obj <- eventReactive(input$predictor_col, {\n    rec <- recipe(as.formula(paste(input$predictor_col, \"~ .\")), \n                  data = data()\n    ) |>\n      step_normalize(all_numeric(), -all_outcomes())\n    return(rec)\n  })\n  \n  model_engine <- reactive({\n    if (input$model_type == \"regression\") {\n      c(\"all\", \n        make_regression_base_tbl() %>% \n          pull(.parsnip_engine) %>% \n          unique())\n    } else if (input$model_type == \"classification\") {\n      c(\"all\", \n        make_classification_base_tbl() %>% \n          pull(.parsnip_engine) %>% \n          unique())\n    }\n  })\n  \n  model_fns <- reactive({\n    if (input$model_type == \"regression\") {\n      c(\"all\", \n        make_regression_base_tbl() %>% \n          pull(.parsnip_fns) %>% \n          unique())\n    } else if (input$model_type == \"classification\") {\n      c(\"all\", \n        make_classification_base_tbl() %>% \n          pull(.parsnip_fns) %>% \n          unique())\n    }\n  })\n  \n  model <- eventReactive(input$build_model, {\n    if (input$model_type == \"regression\") {\n      mod <- fast_regression(.data = data(),\n                             .rec_obj = recipe_obj(),\n                             .parsnip_eng = model_engine(),\n                             .parsnip_fns = model_fns())\n    } else if (input$model_type == \"classification\") {\n      mod <- fast_classification(.data = data(),\n                                 .rec_obj = recipe_obj(),\n                                 .parsnip_eng = model_engine(),\n                                 .parsnip_fns = model_fns())\n    }\n    return(mod)\n  })\n  \n  output$recipe_output <- renderPrint({\n    if (!is.null(input$predictor_col)) {\n      summary(recipe_obj())\n    }\n  })\n  \n  output$model_table <- renderPrint({\n    if (input$build_model > 0) {\n      print(model())\n    }\n  })\n  \n  output$model_reactable <- renderReactable({\n    if (input$build_model > 0) {\n      reactable(model())\n    }\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-01/index.html",
    "href": "posts/rtip-2023-05-01/index.html",
    "title": "Extracting a model call from a fitted workflow in {tidymodels}",
    "section": "",
    "text": "Introduction\nIn this post, we are using a package called tidymodels, which provides a suite of tools for modeling and machine learning.\nNow, let’s take a closer look at the code itself and how we extract a model call from a fitted workflow object.\n\nlibrary(tidymodels)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nrec_obj\n\nThe first line loads the tidymodels package. Then, we create a “recipe” object called rec_obj using the recipe() function. A recipe is a set of instructions for preparing data for modeling. In this case, we are telling the recipe to use the mpg variable as the outcome or dependent variable, and all other variables in the mtcars dataset as the predictors or independent variables.\n\nmodel_spec <- linear_reg(mode = \"regression\", engine = \"lm\")\nmodel_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNext, we create a “model specification” object called model_spec using the linear_reg() function. This specifies the type of model we want to use, which is a linear regression model in this case. We also specify that the model is a regression (i.e., we are predicting a continuous outcome variable) and that the model engine is “lm”, which stands for “linear model”.\n\nwflw <- workflow() |>\n  add_recipe(rec_obj) |>\n  add_model(model_spec)\nwflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nIn the next section of code, we create a “workflow” object called wflw using the workflow() function. A workflow is a way of organizing the steps involved in building a machine learning model. In this case, we are using a “pipe” (|>) to sequentially add the recipe and model specification to the workflow. This means that we first add the recipe to the workflow using the add_recipe() function, and then add the model specification using the add_model() function.\n\nwflw_fit <- fit(wflw, data = mtcars)\nwflw_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   12.30337     -0.11144      0.01334     -0.02148      0.78711     -3.71530  \n       qsec           vs           am         gear         carb  \n    0.82104      0.31776      2.52023      0.65541     -0.19942  \n\n\nFinally, we fit the workflow to the data using the fit() function, which takes the workflow object (wflw) and the data (mtcars) as input. This creates a new object called wflw_fit, which is the fitted model object. This object contains various pieces of information about the fitted model, such as the model coefficients and the R-squared value.\n\nwflw_fit$fit$fit$fit$call\n\nstats::lm(formula = ..y ~ ., data = data)\n\n\nThe last line of code extracts the actual function call that was used to fit the model. This can be useful for reproducing the analysis later on.\nOverall, the code you shared shows how to build a simple linear regression model using the tidymodels package in R. We start by creating a recipe that specifies the outcome variable and predictor variables, then create a model specification for a linear regression model, and finally combine these into a workflow and fit the model to the data."
  },
  {
    "objectID": "posts/rtip-2023-05-03/index.html",
    "href": "posts/rtip-2023-05-03/index.html",
    "title": "How to Download a File from the Internet using download.file()",
    "section": "",
    "text": "Introduction\nThe download.file() function in R is used to download files from the internet and save them onto your computer. Here’s a simple explanation of how to use it:\n\nSpecify the URL of the file you want to download.\nSpecify the file name and the location where you want to save the file on your computer.\nCall the download.file() function, passing in the URL and file name/location as arguments.\n\nHere’s an example:\n\n# Specify the URL of the file you want to download\nurl <- \"https://example.com/data.csv\"\n\n# Specify the file name and location where you want to save the file on your computer\nfile_name <- \"my_data.csv\"\nfile_path <- \"/path/to/save/folder/\"\n\n# Call the download.file() function, passing in the URL and file name/location as arguments\ndownload.file(url, paste(file_path, file_name, sep = \"\"), mode = \"wb\")\n\nIn this example, we’re downloading a CSV file from “https://example.com/data.csv”, and saving it as “my_data.csv” in the “/path/to/save/folder/” directory on our computer.\nThe mode = “wb” argument specifies that we want to download the file in binary mode.\nOnce you run this code, the file will be downloaded from the URL and saved to your specified file location.\nLet’s try a working example.\n\n\nExample\nWe are going to download the Measure Dates file from the following location: {https://data.cms.gov/provider-data/dataset/4j6d-yzce}\n\nurl <- \"https://data.cms.gov/provider-data/sites/default/files/resources/49244993de5a948bcb0d69bf5cc778bd_1681445112/Measure_Dates.csv\"\n\nfile_name <- \"measure_dates.csv\"\nfile_path <- \"C:\\\\Downloads\\\\\"\n\ndownload.file(url = url, destfile = paste0(file_path, file_name, sep = \"\"))\n\nNow let’s read in the file in order to make sure it actually downloaded.\n\nmeasure_dates_df <- read.csv(file = paste0(file_path, file_name))\n\ndplyr::glimpse(measure_dates_df)\n\nRows: 170\nColumns: 6\n$ Measure.ID            <chr> \"ASC_11\", \"ASC_12\", \"ASC_13\", \"ASC_14\", \"ASC_17\"…\n$ Measure.Name          <chr> \"Percentage of patients who had cataract surgery…\n$ Measure.Start.Quarter <chr> \"1Q2021\", \"1Q2019\", \"1Q2021\", \"1Q2021\", \"3Q2020\"…\n$ Start.Date            <chr> \"01/01/2021\", \"01/01/2019\", \"01/01/2021\", \"01/01…\n$ Measure.End.Quarter   <chr> \"4Q2021\", \"4Q2021\", \"4Q2021\", \"4Q2021\", \"4Q2021\"…\n$ End.Date              <chr> \"12/31/2021\", \"12/31/2021\", \"12/31/2021\", \"12/31…\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-11/index.html",
    "href": "posts/rtip-2023-01-11/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2022, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2023!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2022\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nfp <- \"linkedin_content.xlsx\"\n\nengagement_tbl <- read_excel(fp, sheet = \"ENGAGEMENT\") %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ntop_posts_tbl <- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %>%\n  clean_names()\n\nfollowers_tbl <- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ndemographics_tbl <- read_excel(fp, sheet = \"DEMOGRAPHICS\") %>%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 362\nColumns: 4\n$ date              <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 202…\n$ impressions       <dbl> 3088, 3911, 3303, 3134, 1118, 799, 3068, 1954, 2663,…\n$ engagements       <dbl> 31, 56, 51, 42, 8, 4, 43, 20, 33, 43, 14, 41, 5, 17,…\n$ `Engagement Rate` <dbl> 1.0038860, 1.4318589, 1.5440509, 1.3401404, 0.715563…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 5\n$ post_url_1  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ engagements <dbl> 241, 136, 123, 117, 117, 115, 107, 106, 104, 104, 95, 81, …\n$ x3          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_4  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ impressions <dbl> 52300, 33903, 30752, 29887, 25953, 24139, 23769, 18522, 18…\n\nglimpse(followers_tbl)\n\nRows: 362\nColumns: 2\n$ date          <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 2022-01…\n$ new_followers <dbl> 10, 10, 12, 5, 12, 13, 9, 8, 11, 4, 9, 6, 7, 9, 10, 11, …\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics <chr> \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            <chr> \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       <chr> \"0.054587073624134064\", \"0.035217467695474625\", \"0.02…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %>%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\n\nYou will notice that I placed a blue line where I started my telegram channel @steveondata and a red line where I started this blog. So far, not bad, it looks like the telegram channel helped a little bit but writing on the blog seems to maybe been helping the most.\nLet’s look at a cumulative view of things.\n\nengagement_tbl %>%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %>%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %>%\n  slice(1:12) %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %>%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %>%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %>%\n  slice(1:12) %>%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html",
    "href": "posts/rtip-2023-05-04/index.html",
    "title": "Maps with {shiny}",
    "section": "",
    "text": "The code is used to create a Shiny app that allows the user to search for a type of amenity (such as a pharmacy) in a particular city, state, and country, and then display the results on a map. Here is a step-by-step explanation of how the code works."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#concatenating-the-address",
    "href": "posts/rtip-2023-05-04/index.html#concatenating-the-address",
    "title": "Maps with {shiny}",
    "section": "Concatenating the Address",
    "text": "Concatenating the Address\nThe first thing that the observeEvent function does is concatenate the user inputs for city, state, and country into a single string. This is done using the paste function. The sep argument specifies that the words should be separated by a comma and space. The resulting string is the address that will be used to search for the specified amenity."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#obtaining-the-bounding-box",
    "href": "posts/rtip-2023-05-04/index.html#obtaining-the-bounding-box",
    "title": "Maps with {shiny}",
    "section": "Obtaining the Bounding Box",
    "text": "Obtaining the Bounding Box\nNext, the code uses the getbb function from the osmdata library to obtain the bounding box for the specified address. A bounding box is a rectangle that contains the entire area of interest (in this case, the specified city, state, and country). The bounding box is necessary to limit the search for the specified amenity to only the specified area."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#creating-the-query",
    "href": "posts/rtip-2023-05-04/index.html#creating-the-query",
    "title": "Maps with {shiny}",
    "section": "Creating the Query",
    "text": "Creating the Query\nThe code then creates a query object using the opq function from the osmdata library. The bbox argument specifies the bounding box that was obtained in the previous step. The add_osm_feature function is then used to specify the amenity that the user is searching for. The key argument specifies that we are searching for an “amenity”, and the value argument specifies the specific type of amenity that the user entered (e.g., pharmacy)."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#obtaining-the-results",
    "href": "posts/rtip-2023-05-04/index.html#obtaining-the-results",
    "title": "Maps with {shiny}",
    "section": "Obtaining the Results",
    "text": "Obtaining the Results\nThe osmdata_sf function is used to retrieve the results of the query. This function returns a sf object that contains the spatial data for the points that match the specified amenity. The resulting sf object is then passed to the mapview function from the mapview library, which creates an interactive map of the results."
  },
  {
    "objectID": "posts/rtip-2023-05-04/index.html#displaying-the-map",
    "href": "posts/rtip-2023-05-04/index.html#displaying-the-map",
    "title": "Maps with {shiny}",
    "section": "Displaying the Map",
    "text": "Displaying the Map\nFinally, the renderLeaflet function is used to display the map in the UI. The m@map argument specifies that we want to display the map that was created by the mapview function. The resulting map is displayed in the leafletOutput that was defined in the UI."
  },
  {
    "objectID": "posts/rtip-2023-05-05/index.html",
    "href": "posts/rtip-2023-05-05/index.html",
    "title": "Maps with {shiny} Pt 2",
    "section": "",
    "text": "Introduction\nThe code provided at the end of this post is an example of how to create a simple Shiny app in R that utilizes the OpenStreetMap (OSM) API to create a map of amenities in a specific location.\nThe app has two main parts: the user interface (UI) and the server.\nThe UI section is defined using the fluidPage function from the Shiny library, which creates a responsive, fluid layout for the app. It includes a title panel, a sidebar panel with text input fields for the city, state, country, and amenity type, and a submit button. The main panel of the UI includes a leafletOutput object, which will display the map of amenities.\nThe server section is defined using the server function from the Shiny library. This function is responsible for processing the inputs from the UI, performing any necessary calculations, and rendering the output.\nThe observeEvent function is used to capture the click event of the submit button. When the button is clicked, the function getbb from the osmdata library is used to retrieve the bounding box (bbox) for the specified location.\nNext, the opq function from the osmdata library is used to create a query object that searches for amenities of the specified type (input$amenity) within the retrieved bbox.\nThe assign function is used to set a variable has_internet_via_proxy to TRUE in the curl environment. This is necessary to ensure that the osmdata_sf function, which downloads the OSM data, works properly.\nThe osmdata_sf function is then called with the created query object as its argument. This function downloads the OSM data and converts it to an sf object. The resulting sf object contains a data frame with information about the amenities found in the specified location.\nA mapview object is then created using the osm_points part of the sf object. This object is assigned to the variable m.\nFinally, the renderLeaflet function is used to display the resulting map. The mapview object m is accessed and its @map attribute is used as the input to the renderLeaflet function. This displays the map of amenities in the specified location.\nThere is also some commented out code in the server section that provides an alternative way to display the map using the leaflet library instead of the mapview library. This code creates a leaflet object, adds tiles to the map, and then adds circle markers to represent the amenities found in the specified location. The popup argument specifies what information is displayed in the popups that appear when the user clicks on a marker.\nOverall, this code demonstrates how to use the Shiny library to create an interactive web application that utilizes the OSM API to display maps of amenities in specific locations.\n\n\nFull Application\nAs usual, here is the full code. Please take it and see what you can do with it.\n\nlibrary(shiny)\nlibrary(osmdata)\nlibrary(mapview)\nlibrary(leaflet)\nlibrary(htmltools)\n\nui <- fluidPage(\n  titlePanel(\"Mapping with Shiny\"),\n  sidebarLayout(\n    sidebarPanel(\n      textInput(\"city\", \"City\", placeholder = \"e.g. Queens\"),\n      textInput(\"state\", \"State\", placeholder = \"e.g. New York\"),\n      textInput(\"country\", \"Country\", placeholder = \"e.g. USA\"),\n      textInput(\"amenity\", \"Amenity Type\", placeholder = \"e.g. pharmacy\"),\n      actionButton(\"submit\", \"Submit\")\n    ),\n    mainPanel(\n      leafletOutput(\"map\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  observeEvent(input$submit, {\n    # Concatenate city, state, and country inputs into a single string\n    address <- paste(input$city, input$state, input$country, sep = \", \")\n    \n    bbox <- getbb(address)\n    \n    query <- opq(bbox = bbox) |>\n      add_osm_feature(key = \"amenity\", value = input$amenity)\n    \n    assign(\"has_internet_via_proxy\", TRUE, environment(curl::has_internet))\n    sf_obj <- osmdata_sf(query)\n    \n    m <- mapview(sf_obj$osm_points)\n    output$map <- renderLeaflet({\n      m@map\n    })\n    \n    # output$map <- renderLeaflet({\n    #   leaflet(sf_obj$osm_points) |>\n    #     addTiles() |>\n    #     addCircleMarkers(\n    #       radius = 3, \n    #       popup = ~as.character(\n    #         paste(\n    #           \"Name: \", name, \"<br/>\",\n    #           \"OSM ID: \", osm_id, \"<br/>\"\n    #         )\n    #       ),\n    #       opacity = 0.3\n    #     )\n    # })\n  })\n}\n\nshinyApp(ui, server)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-08/index.html",
    "href": "posts/rtip-2023-05-08/index.html",
    "title": "Updates to {healthyR.data}",
    "section": "",
    "text": "Introduction\nIntroducing the Updated {healthyR.data} Package: Your Ultimate Health Data Companion\nIf you’re a healthcare professional or a data enthusiast, you’re probably familiar with the healthyR.data package. This R package has been an invaluable resource for accessing and analyzing public health data. With its latest release, version 1.0.3, the package has undergone some significant changes, including the addition of several new functions and a requirement for R version 3.4.0. In this post, we’ll take a closer look at the updates and how they can help you work with health data more efficiently.\n\n\nBreaking Changes\nIn keeping with tidyverse practices, healthyR.data now requires R version 3.4.0. This change may affect some users who haven’t updated their R version recently, but it’s an important step to keep the package up-to-date and compatible with other tidyverse packages.\n\n\nNew Functions\nOne of the main highlights of the new version is the addition of several new functions. Let’s take a look at each one and how it can help you work with health data:\n\ndl_hosp_data_dict(): This function downloads the data dictionary for the Hospital Compare dataset. This information can be crucial when working with health data, as it provides a clear understanding of the variables and their definitions.\ncurrent_hosp_data(): This function retrieves the most recent Hospital Compare dataset, which includes information on hospital quality, patient experience, and more.\ncurrent_asc_data(): This function retrieves the most recent Ambulatory Surgical Center (ASC) dataset, which includes information on ASC quality measures.\ncurrent_asc_oas_cahps_data(): This function retrieves the most recent ASC Outpatient and Ambulatory Surgery Consumer Assessment of Healthcare Providers and Systems (OAS CAHPS) dataset, which includes patient experience measures for ASCs.\ncurrent_comp_death_data(): This function retrieves the most recent data on hospital mortality rates for conditions such as heart attack, pneumonia, and stroke.\ncurrent_hai_data(): This function retrieves the most recent Healthcare-Associated Infections (HAI) dataset, which includes information on infections acquired during hospitalization.\ncurrent_hcahps_data(): This function retrieves the most recent Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS) dataset, which includes patient experience measures for hospitals.\ncurrent_hvbp_data(): This function retrieves the most recent Hospital Value-Based Purchasing (HVBP) dataset, which includes information on hospital quality and payment incentives.\ncurrent_ipfqr_data(): This function retrieves the most recent Inpatient Psychiatric Facility Quality Reporting (IPFQR) dataset, which includes information on psychiatric facility quality measures.\ncurrent_maternal_data(): This function retrieves the most recent Maternal and Infant Health Care Quality dataset, which includes information on maternal and infant health outcomes.\ncurrent_medicare_hospital_spending_data(): This function retrieves the most recent Medicare Hospital Spending by Claim dataset, which includes information on Medicare payments for hospital services.\ncurrent_opqr_data(): This function retrieves the most recent Outpatient Prospective Payment System (OPPS) Quality Reporting (OPQR) dataset, which includes information on outpatient facility quality measures.\ncurrent_imaging_efficiency_data(): This function retrieves the most recent Radiology Imaging Efficiency (RIE) dataset, which includes information on the appropriateness of imaging studies.\ncurrent_unplanned_hospital_visits_data(): This function retrieves the most recent Unplanned Hospital Visits dataset, which includes information on hospital readmissions and emergency department visits.\ncurrent_payments_data(): This function retrieves the most recent Provider-Level Payments dataset, which includes information on payments to healthcare providers.\ncurrent_pch_hcahps_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) dataset.\ncurrent_pch_hai_hospital_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Healthcare-Associated Infections (HAI) Hospital dataset, which includes information on healthcare-associated infections in PCMH hospitals.\ncurrent_pch_oncology_measures_hospital_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Oncology Measures Hospital dataset, which includes information on oncology measures in PCMH hospitals.\ncurrent_pch_outcomes_data(): This function retrieves the most recent Patient-Centered Medical Home (PCMH) Outcomes dataset, which includes information on outcomes for PCMH practices.\ncurrent_timely_and_effective_care_data(): This function retrieves the most recent Timely and Effective Care dataset, which includes information on hospital performance on timely and effective care measures.\ncurrent_va_data(): This function retrieves the most recent Veterans Affairs (VA) dataset, which includes information on VA hospital quality measures.\n\nAll of these functions provide valuable access to important health data, allowing users to perform detailed analyses and gain insights into various aspects of healthcare quality and outcomes.\n\n\nOther Improvements\nIn addition to the new functions, healthyR.data version 1.0.3 also includes several bug fixes and improvements. For example, the logic in the current_hosp_data() function has been confirmed by user feedback.\n\n\nConclusion\nThe healthyR.data package has long been a valuable resource for anyone working with health data. With the latest release, version 1.0.3, the package has become even more powerful and versatile, thanks to the addition of many new functions and improvements. If you’re a healthcare professional, researcher, or data enthusiast, healthyR.data is a must-have tool in your arsenal. Give it a try and see how it can help you gain new insights into the world of healthcare quality and outcomes."
  },
  {
    "objectID": "posts/rtip-2023-05-09/index.html",
    "href": "posts/rtip-2023-05-09/index.html",
    "title": "VBA to R and Back Again: Running R from VBA",
    "section": "",
    "text": "Introduction\nToday I am going to briefly go over an extremely simple example of running some R code via Excel VBA.\nLet’s start by discussing each line of code one by one:\nSub CallRnorm()\nThis line defines a subroutine called “CallRnorm”. A subroutine is a block of code that can be executed repeatedly from any part of the code, and it starts with the “Sub” keyword followed by the subroutine name and any arguments in parentheses.\nDim R As Variant\nDim result As Variant\nThese two lines declare two variables named “R” and “result” as “Variant” data type. “Variant” is a data type that can store any type of data.\nColumns(\"A\").Delete\nThis line deletes the entire column A from the active worksheet.\nR = \"library(stats);rnorm(10) |&gt; as.data.frame()\"\nThis line assigns a string of R code to the variable “R”. The code will load the “stats” package and generate 10 random numbers from a normal distribution using the “rnorm()” function, and then convert the result to a data frame using the pipe operator “|&gt;” and the “as.data.frame()” function.\nresult = VBA.CreateObject(\"WScript.Shell\").Exec(\"C:\\Program Files\\R\\R-4.1.2\\bin\\x64\\Rscript.exe -e \"\"\" & R & \"\"\"\").StdOut.ReadAll\nThis line uses the “CreateObject” method to create a new object of the “WScript.Shell” class, which allows us to execute commands in the Windows command shell. It then uses the “Exec” method to execute the R code stored in the “R” variable using the “Rscript.exe” command-line tool, which runs R scripts from the command line. The result of the command is stored in the “result” variable by reading the output of the command using the “StdOut” property of the “Exec” object and the “ReadAll” method.\nresult = Split(result, vbCrLf)\nFor i = 0 To UBound(result)\n    ActiveSheet.Range(\"A1\").Offset(i, 0).Value = result(i)\nNext i\nThese two lines split the result of the R code execution into an array of strings using the “Split” function and the newline character (vbCrLf) as the delimiter. It then loops through the array using a “For” loop and assigns each element to a cell in the active worksheet, starting from cell A1 and offsetting each cell by one row using the “Offset” method.\nSo, in summary, this VBA code creates a subroutine that deletes column A from the active worksheet, executes a block of R code that generates 10 random numbers from a normal distribution and converts the result to a data frame, captures the output of the R code execution, splits the output into an array of strings, and pastes the result into column A of the active worksheet.\n\n\n\nVBA to R and Back Again"
  },
  {
    "objectID": "posts/rtip-2023-05-10/index.html",
    "href": "posts/rtip-2023-05-10/index.html",
    "title": "VBA to R and Back Again: Running R from VBA Pt 2",
    "section": "",
    "text": "Introduction\nYesterday I posted on using VBA to execute R code that is written inside of the VBA script. So today, I will go over a simple example on executing an R script from VBA. So let’s get into the code and what it does.\nFirst, let’s look at the Function called “Run_R_Script”. This function takes four arguments, where the first two are mandatory, and the last two are optional.\n\nsRApplicationPath - This is the path to the R application that you want to use to run your script. It is a required argument, and you need to provide the full path to the Rscript.exe file on your machine.\nsRFilePath - This is the path to the R script file that you want to execute. It is also a required argument, and you need to provide the full path to your R script file.\niStyle - This is an optional argument that specifies how the script will be executed. By default, it is set to 1, which means that the script will run in a minimized window.\nbWaitTillComplete - This is another optional argument that specifies whether the function should wait until the script has finished running before returning control to the caller. By default, it is set to True, which means that the function will not return control until the script has completed execution.\n\nThe first line inside the Function defines two variables: sPath and shell.\n\nsPath - This variable will hold the path to the Rscript.exe file and the path to the R script file, which will be used later to run the script.\nshell - This variable is used to create an instance of the WScript.Shell object.\n\nNext, we wrap the R path with double quotations to avoid any issues with spaces in the path.\nAfter that, the script deletes Column A.\nThen, instead of using the “shell.Run” function, the code uses the “shell.Exec” function to execute the R script. This function returns an object that has a “StdOut” property, which contains the output of the script.\nThe output is then read using the “ReadAll” method, and the resulting string is split into an array using the “Split” function. The array is then iterated using a “For” loop, and each element of the array is written to Column A, starting at cell A1.\nFinally, the Function returns an Integer value, which is the result of the “shell.Run” function.\nThe Subroutine called “Demo” just demonstrates how to use the “Run_R_Script” function by calling it with the appropriate parameters.\n\n\nFull Code\nHere is the R Script\n\ndata.frame(\n    x = 1:10,\n    y = rnorm(10)\n)\n\nlist(\n    data.frame(\n        x = 1:10,\n        y = rnorm(10)\n    ),\n    data.frame(\n        x = 1:10,\n        y = rnorm(10)\n    )\n)\n\nFull VBA\nFunction Run_R_Script(sRApplicationPath As String, _\n                        sRFilePath As String, _\n                        Optional iStyle As Integer = 1, _\n                        Optional bWaitTillComplete As Boolean = True) As Integer\n\n    Dim sPath As String\n    Dim shell As Object\n\n    'Define shell object\n    Set shell = VBA.CreateObject(\"WScript.Shell\")\n\n    'Wrap the R path with double quotations\n    sPath = \"\"\"\" & sRApplicationPath & \"\"\"\"\n    sPath = sPath & \" \"\n    sPath = sPath & sRFilePath\n\n    'Delete Coumn A\n    Columns(\"A\").Delete\n    \n    'Get Result\n    result = shell.Exec(sPath).StdOut.ReadAll\n    result = Split(result, vbCrLf)\n    For i = 0 To UBound(result)\n        ActiveSheet.Range(\"A1\").Offset(i, 0).Value = result(i)\n    Next i\n    \nEnd Function\n\nSub Demo()\n    Dim iEerrorCode As Integer\n    iEerrorCode = Run_R_Script(\"C:\\Program Files\\R\\R-4.1.2\\bin\\x64\\Rscript.exe\", \"C:\\Users\\ssanders\\Desktop\\test.R\")\nEnd Sub\n\n\nPicture\n\n\n\nExample Output, VBA, and R\n\n\n\n\nReference\nhttps://stackoverflow.com/a/54816881"
  },
  {
    "objectID": "posts/rtip-2023-05-12/index.html",
    "href": "posts/rtip-2023-05-12/index.html",
    "title": "Working with Dates and Times Pt 1",
    "section": "",
    "text": "Introduction\nIn this post, we will cover the basics of handling dates and times in R using the as.Date, as.POSIXct, and as.POSIXlt functions. We will use the example code below to explain each line in simple terms. Let’s get started!\nHere is the script we are going to look at:\n\n# the date object\nSteve_online &lt;- as.Date(\"1981-02-25\")\n\nstr(Steve_online) #Date[1:1], format: \"1981-02-25\n\n Date[1:1], format: \"1981-02-25\"\n\nclass(Steve_online) #Date\n\n[1] \"Date\"\n\nas.numeric(Steve_online) # stored as number of days since 1970-01-01\n\n[1] 4073\n\nas.numeric(as.Date(\"1970-01-01\")) # equals zero\n\n[1] 0\n\nas.Date(as.Date(\"1970-01-01\") + 4073) # produces 1981-02-25 -- our original date\n\n[1] \"1981-02-25\"\n\n# vectors can contain multiple dates\nSteve_online &lt;- as.Date(c(\"1981-02-25\", \"1997-01-12\"))\nstr(Steve_online)\n\n Date[1:2], format: \"1981-02-25\" \"1997-01-12\"\n\nSteve_online[2]\n\n[1] \"1997-01-12\"\n\n# what about POSIX?\n# POSIXct stores date time as integer == # seconds since 1970-01-01 UTC\nSteve_online &lt;- as.POSIXct(\"1981-02-25 02:25:00\", tz = \"US/Mountain\")\nas.integer(`Steve_online`)\n\n[1] 351941100\n\n# POSIXlt stores date time as list:sec, min, hour, mday, mon, year, wday, yday, isdst, zone, gmtoff\nSteve_online &lt;- as.POSIXlt(\"1981-02-25 02:25:00\", tz = \"US/Mountain\")\nas.integer(Steve_online) # no longer an integer\n\nWarning: NAs introduced by coercion\n\n\n [1]  0 25  2 25  1 81  3 55  0 NA NA\n\nunclass(Steve_online) # this shows the components of the list\n\n$sec\n[1] 0\n\n$min\n[1] 25\n\n$hour\n[1] 2\n\n$mday\n[1] 25\n\n$mon\n[1] 1\n\n$year\n[1] 81\n\n$wday\n[1] 3\n\n$yday\n[1] 55\n\n$isdst\n[1] 0\n\n$zone\n[1] \"MST\"\n\n$gmtoff\n[1] NA\n\nattr(,\"tzone\")\n[1] \"US/Mountain\"\n\nmonth.name[Steve_online$mon + 1] # equals February\n\n[1] \"February\"\n\n\nThe first line of code creates a date object called Steve_online with the value of February 25, 1981, using the as.Date function. This function is used to convert a character string to a date object. The str function is then used to show the structure of the Steve_online object, which is of class Date.\nThe as.numeric function is used to convert the Steve_online object to the number of days since January 1, 1970 (known as the Unix epoch). This is a common way of representing dates in programming languages, and is useful for calculations involving dates. We also demonstrate that as.numeric(as.Date(\"1970-01-01\")) returns zero, since this is the starting point of the Unix epoch.\nWe then show how to add or subtract days from a date object by adding or subtracting the desired number of days (as an integer) to the as.Date function with the reference date of January 1, 1970. In this case, we add 4073 days to January 1, 1970, resulting in the date of February 25, 1981 (our original date).\nNext, we demonstrate how to create a vector of date objects by passing a character vector of dates to the as.Date function. The str function is used again to show the structure of the Steve_online object, which is now a vector of two date objects. We then show how to access the second element of the vector using indexing (Steve_online[2]).\nMoving on to POSIX objects, we introduce the as.POSIXct function, which creates a POSIXct object that stores date time as an integer equal to the number of seconds since January 1,"
  },
  {
    "objectID": "posts/rtip-2023-05-15/index.html",
    "href": "posts/rtip-2023-05-15/index.html",
    "title": "Working with Dates and Times Pt 2: Finding the Next Mothers Day with Simplicity",
    "section": "",
    "text": "Introduction\nMother’s Day is a special occasion to honor and appreciate the incredible women in our lives. As programmers, we can use our coding skills to make our lives easier when it comes to important dates like Mother’s Day. In this blog post, we’ll walk through a simple and engaging R code that helps us find the next Mother’s Day. So grab your coding hats, and let’s get started!\n\n# if you aren't using times, use the Date class; it's simpler\nNextMothersDay &lt;- as.Date(\n  c(\n    startMothersDay = \"2024-05-14\", \n    endMothersDay =\"2024-05-14\"\n    )\n  )\n\nNextMothersDay\n\nstartMothersDay   endMothersDay \n   \"2024-05-14\"    \"2024-05-14\" \n\n\nIn the first part of our code, we use the as.Date() function to find the next Mother’s Day. Since we don’t need to consider specific times, we can simply use the Date class, which simplifies the process. We create a vector with two elements: startMothersDay and endMothersDay, both set to “2024-05-14”. This represents the range of Mother’s Day for the year 2024. Finally, we store the result in the variable NextMothersDay and print it to the console. Voilà! We have the next Mother’s Day date.\n\n# if you have times, then use POSIX.\nNextMothersDay_ct &lt;- as.POSIXct(\n  c(\n    startMothersDay = \"2024-05-15 10:00\", # Let Mommy Sleep!\n    endMothersDay =\"2024-05-15 23:59\"\n    ),\n  tz = \"GMT\"\n  )\n\nNextMothersDay_ct\n\n          startMothersDay             endMothersDay \n\"2024-05-15 10:00:00 GMT\" \"2024-05-15 23:59:00 GMT\" \n\n\nNow, let’s say we want to consider specific times for Mother’s Day celebrations. We can use the as.POSIXct() function to handle dates and times together. We create another vector with two elements: startMothersDay and endMothersDay, but this time with specific times. The start time is set to “2024-05-15 10:00” (because let’s let Mommy sleep in!) and the end time is set to “2024-05-15 23:59”. We also specify the time zone as “GMT” using the tz argument. The result is stored in the variable NextMothersDay_ct, and when we print it, we get the range of Mother’s Day with times included.\n\n# converting from one POSIX to another is easy\nNextMothersDay_lt &lt;- as.POSIXlt(NextMothersDay_ct)\nunclass(NextMothersDay_lt)\n\n$sec\n[1] 0 0\n\n$min\n[1]  0 59\n\n$hour\n[1] 10 23\n\n$mday\n[1] 15 15\n\n$mon\n[1] 4 4\n\n$year\nstartMothersDay   endMothersDay \n            124             124 \n\n$wday\n[1] 3 3\n\n$yday\n[1] 135 135\n\n$isdst\n[1] 0 0\n\nattr(,\"tzone\")\n[1] \"GMT\"\n\n\nNow, let’s explore how to convert a POSIXct object to a POSIXlt object. We use the as.POSIXlt() function to convert NextMothersDay_ct into a POSIXlt object. This conversion allows us to access more detailed components of the date and time, such as the day of the week, hour, minute, and second. Finally, we use the unclass() function to remove the class attributes from the object and print the result to the console.\n\n\nConclusion\nWith just a few lines of code, we have learned how to find the next Mother’s Day using R. Whether you need a simple date or a specific time range, R provides us with convenient functions to handle both scenarios. So the next time you want to plan a special surprise for your mom, you can rely on your coding skills to\n\n\nFull Script\n\n# if you aren't using times, use the Date class it's simpler\nNextMothersDay &lt;- as.Date(\n  c(\n    startMothersDay = \"2024-05-14\", \n    endMothersDay =\"2024-05-14\"\n    )\n  )\n\nNextMothersDay\n\n# if you have times, then use POSIX.\nNextMothersDay_ct &lt;- as.POSIXct(\n  c(\n    startMothersDay = \"2024-05-15 10:00\", # Let Mommy Sleep!\n    endMothersDay =\"2024-05-15 23:59\"\n    ),\n  tz = \"GMT\"\n  )\n\nNextMothersDay_ct\n\n# converting from one POSIX to another is easy\nNextMothersDay_lt &lt;- as.POSIXlt(NextMothersDay_ct)\nunclass(NextMothersDay_lt)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-05-16/index.html",
    "href": "posts/rtip-2023-05-16/index.html",
    "title": "Working with Dates and Times Pt 3",
    "section": "",
    "text": "Introduction\nDates and times are essential components in many programming tasks, and R provides various functions and packages to handle them effectively. In this post, we’ll explore some common operations using both the base R functions and the lubridate package, comparing their simplicity and ease of understanding.\nLet’s dive right in!\n\n# What class does as.Date() produce?\nclass(as.Date(\"1881/10/25\"))\n\n[1] \"Date\"\n\n# be sure lubridate \n# install.packages(\"lubridate\")\nlibrary(lubridate)\n\n# Which do you find easier to understand? base or lubridate?\ntoday() # today() = Sys.Date()\n\n[1] \"2023-05-16\"\n\nnow() # now() = Sys.time()\n\n[1] \"2023-05-16 08:27:52 EDT\"\n\n# as_date and as.Date produce the same class\nclass(as_date(\"1881/10/25\")) # lubridate\n\n[1] \"Date\"\n\nclass(as.Date(\"1881/10/25\")) # base\n\n[1] \"Date\"\n\n# simpler strptime\nstrptime(\"2014-07-13 16:00:00 -0300\", \"%Y-%m-%d %H:%M:%S %z\") # time zone is messed up\n\n[1] \"2014-07-13 15:00:00\"\n\nparse_date_time(\"2014-07-13 16:00:00 -0300\", \"ymd HMS z\") # time zone works\n\n[1] \"2014-07-13 19:00:00 UTC\"\n\n# lubridate takes it one step further\nymd(\"2014-07-13 16:00:00 -0300\")\n\n[1] NA\n\nymd_hms(\"2014-07-13 16:00:00 -0300\")\n\n[1] \"2014-07-13 19:00:00 UTC\"\n\nmdy_hm(\"July 13, 2014 4:00 pm\")\n\n[1] \"2014-07-13 16:00:00 UTC\"\n\n\n1️⃣ Determining the Class of a Date: The first line of code checks the class produced by the as.Date() function when given the input “1881/10/25.” By using the class() function, we can identify that the output is of class “Date.” This means that the as.Date() function converts the input into a date format.\n2️⃣ Base R vs. lubridate: Before we proceed further, we need to ensure that the lubridate package is installed. If not, the code installs it using the install.packages() function. We then load the package using the library() function.\nNext, we compare the ease of use between base R and lubridate for working with dates and times.\n\nToday’s Date and Current Time: The today() function, equivalent to Sys.Date(), gives you the current date. Similarly, now() returns the current date and time using Sys.time(). These functions make it straightforward to obtain the current date or date and time in R.\nClass Comparison: We compare the classes of dates produced by as_date() from lubridate and as.Date() from base R. Using the class() function on each result, we observe that both functions produce the same “Date” class output. Hence, both methods are equivalent in this regard.\n\n3️⃣ Simplifying Date and Time Parsing: Parsing date and time strings can sometimes be tricky, especially when dealing with time zones. However, lubridate provides simplified functions to handle such scenarios.\n\nBase R’s strptime(): The strptime() function is a base R function that parses a date and time string based on a given format. In this case, we try to parse “2014-07-13 16:00:00 -0300” with the format “%Y-%m-%d %H:%M:%S %z.” However, we encounter a problem with the time zone, as it does not parse correctly.\nlubridate’s parse_date_time(): To overcome the time zone issue, lubridate offers the parse_date_time() function. We provide the same date and time string along with the format “ymd HMS z.” This time, the time zone is parsed correctly, resulting in a valid date and time object.\n\n4️⃣ Going the Extra Mile with lubridate: lubridate takes date and time manipulation a step further with its intuitive functions.\n\nymd(): The ymd() function converts a character string of the form “2014-07-13 16:00:00 -0300” into a date object. It handles various date formats and automatically infers the year, month, and day information.\nymd_hms(): Similar to ymd(), the ymd_hms() function converts a character string into a date-time object, considering the year, month, day, hour, minute, and second components.\nmdy_hm(): The mdy_hm() function allows us to parse a character string like “July 13, 2014 4:00 pm” into\n\na date-time object. It handles different date formats and automatically extracts the month, day, year, hour, and minute information.\nBy leveraging these functions, lubridate simplifies the process of working with dates and times, offering a more intuitive and concise syntax compared to base R.\nIn conclusion, understanding how to handle dates and times in R is crucial for many programming tasks. While base R provides essential functions, the lubridate package offers additional capabilities and a more straightforward syntax, making it an attractive choice for working with dates and times in R."
  },
  {
    "objectID": "posts/rtip-2023-05-17/index.html",
    "href": "posts/rtip-2023-05-17/index.html",
    "title": "Working with Dates and Times Pt 4",
    "section": "",
    "text": "Introduction\nFormatting dates is an essential task in data analysis and programming. In R, there are various ways to manipulate and present dates according to specific requirements. In this blog post, we will explore the world of date formatting in R, uncovering the power of the strftime() function. We will walk through practical examples using the provided code snippet, demonstrating how to format dates in a clear and concise manner. So, let’s dive in and uncover the secrets of date formatting in R!\n#Understanding the strftime() Function:\nIn R, the strftime() function allows us to format dates and times based on a set of predefined modifiers. These modifiers act as placeholders for different components of the date and time. By using these modifiers, we can customize the output format to suit our needs.\nLet’s analyze the code snippet provided to gain a better understanding of the strftime() function and its capabilities.\n\n# all of the modifiers\nfor (formatter in sort(c(letters, LETTERS))) {\n  modifier &lt;- paste0(\"%\", formatter)\n  print(\n    paste0(\n      modifier, \n      \" used on: \",\n      RightNow,\n      \" will give: \",\n      strftime(RightNow, modifier)\n    )\n  )\n}\n\nThe code snippet above iterates through a set of modifiers, both lowercase and uppercase letters, and applies each modifier to the RightNow variable. It then prints the modifier, the original RightNow value, and the formatted output. This allows us to see the effect of each modifier on the date and time representation.\n\n\nModifier Showcase:\nLet’s explore some commonly used modifiers and their corresponding output formats:\n%a - Abbreviated weekday name (e.g., \"Mon\").\n%A - Full weekday name (e.g., \"Monday\").\n%b - Abbreviated month name (e.g., \"Jan\").\n%B - Full month name (e.g., \"January\").\n%d - Day of the month (01-31).\n%H - Hour in 24-hour format (00-23).\n%I - Hour in 12-hour format (01-12).\n%m - Month (01-12).\n%M - Minute (00-59).\n%p - AM/PM indicator.\n%S - Second (00-59).\n%Y - Year with century (e.g., \"2023\").\n%y - Year without century (e.g., \"23\").\nFeel free to experiment with different modifiers and observe the changes in the output format.\nHere is a full example\n\nRightNow &lt;- Sys.time()\n\n# all of the modifiers\nfor (formatter in sort(c(letters, LETTERS))) {\n  modifier &lt;- paste0(\"%\", formatter)\n  print(\n    paste0(\n      modifier, \n      \" used on: \",\n      RightNow,\n      \" will give: \",\n      strftime(RightNow, modifier)\n    )\n  )\n}\n\n[1] \"%a used on: 2023-05-17 09:34:47 will give: Wed\"\n[1] \"%A used on: 2023-05-17 09:34:47 will give: Wednesday\"\n[1] \"%b used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%B used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%c used on: 2023-05-17 09:34:47 will give: Wed May 17 09:34:47 2023\"\n[1] \"%C used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%d used on: 2023-05-17 09:34:47 will give: 17\"\n[1] \"%D used on: 2023-05-17 09:34:47 will give: 05/17/23\"\n[1] \"%e used on: 2023-05-17 09:34:47 will give: 17\"\n[1] \"%E used on: 2023-05-17 09:34:47 will give: E\"\n[1] \"%f used on: 2023-05-17 09:34:47 will give: f\"\n[1] \"%F used on: 2023-05-17 09:34:47 will give: 2023-05-17\"\n[1] \"%g used on: 2023-05-17 09:34:47 will give: 23\"\n[1] \"%G used on: 2023-05-17 09:34:47 will give: 2023\"\n[1] \"%h used on: 2023-05-17 09:34:47 will give: May\"\n[1] \"%H used on: 2023-05-17 09:34:47 will give: 09\"\n[1] \"%i used on: 2023-05-17 09:34:47 will give: i\"\n[1] \"%I used on: 2023-05-17 09:34:47 will give: 09\"\n[1] \"%j used on: 2023-05-17 09:34:47 will give: 137\"\n[1] \"%J used on: 2023-05-17 09:34:47 will give: J\"\n[1] \"%k used on: 2023-05-17 09:34:47 will give:  9\"\n[1] \"%K used on: 2023-05-17 09:34:47 will give: K\"\n[1] \"%l used on: 2023-05-17 09:34:47 will give:  9\"\n[1] \"%L used on: 2023-05-17 09:34:47 will give: L\"\n[1] \"%m used on: 2023-05-17 09:34:47 will give: 05\"\n[1] \"%M used on: 2023-05-17 09:34:47 will give: 34\"\n[1] \"%n used on: 2023-05-17 09:34:47 will give: \\n\"\n[1] \"%N used on: 2023-05-17 09:34:47 will give: N\"\n[1] \"%o used on: 2023-05-17 09:34:47 will give: o\"\n[1] \"%O used on: 2023-05-17 09:34:47 will give: O\"\n[1] \"%p used on: 2023-05-17 09:34:47 will give: AM\"\n[1] \"%P used on: 2023-05-17 09:34:47 will give: am\"\n[1] \"%q used on: 2023-05-17 09:34:47 will give: q\"\n[1] \"%Q used on: 2023-05-17 09:34:47 will give: Q\"\n[1] \"%r used on: 2023-05-17 09:34:47 will give: 09:34:47 AM\"\n[1] \"%R used on: 2023-05-17 09:34:47 will give: 09:34\"\n[1] \"%s used on: 2023-05-17 09:34:47 will give: 1684330487\"\n[1] \"%S used on: 2023-05-17 09:34:47 will give: 47\"\n[1] \"%t used on: 2023-05-17 09:34:47 will give: \\t\"\n[1] \"%T used on: 2023-05-17 09:34:47 will give: 09:34:47\"\n[1] \"%u used on: 2023-05-17 09:34:47 will give: 3\"\n[1] \"%U used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%v used on: 2023-05-17 09:34:47 will give: 17-May-2023\"\n[1] \"%V used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%w used on: 2023-05-17 09:34:47 will give: 3\"\n[1] \"%W used on: 2023-05-17 09:34:47 will give: 20\"\n[1] \"%x used on: 2023-05-17 09:34:47 will give: 5/17/2023\"\n[1] \"%X used on: 2023-05-17 09:34:47 will give: 9:34:47 AM\"\n[1] \"%y used on: 2023-05-17 09:34:47 will give: 23\"\n[1] \"%Y used on: 2023-05-17 09:34:47 will give: 2023\"\n[1] \"%z used on: 2023-05-17 09:34:47 will give: -0400\"\n[1] \"%Z used on: 2023-05-17 09:34:47 will give: EDT\"\n\n\n\n\nConclusion\nIn this blog post, we explored the strftime() function in R, which provides powerful capabilities for formatting dates. By using the various modifiers available, we can easily customize the representation of dates and times to meet our specific requirements. Understanding date formatting is crucial for effective data analysis, visualization, and reporting.\nRemember to refer to the R documentation for strftime() to discover additional modifiers and advanced formatting options. With the knowledge gained from this blog post, you are now equipped to master date formatting in R and handle dates with confidence in your programming endeavors.\nHappy coding with R and may your dates always be formatted to perfection!"
  },
  {
    "objectID": "posts/rtip-2023-05-18/index.html",
    "href": "posts/rtip-2023-05-18/index.html",
    "title": "The which() Function in R",
    "section": "",
    "text": "Introduction:\nAs a programmer, one of the most important tasks is to extract valuable insights from data. To make this process efficient, it is crucial to have a reliable tool at your disposal. Enter the which() function in R. This versatile function allows you to locate specific elements within a vector or a data frame, helping you filter and analyze data with ease. In this blog post, we’ll explore the ins and outs of the which() function, discussing its syntax, common use cases, and providing practical examples to solidify your understanding.\n\n\nUnderstanding the Syntax:\nBefore diving into real-world examples, let’s grasp the basic syntax of the which() function. The general structure of the function is as follows:\n\nwhich(logical_vector, arr.ind = FALSE)\n\nThe logical_vector parameter represents the condition or logical expression you want to evaluate. It can be any expression that returns a logical vector, such as a comparison or logical operation. The optional arr.ind parameter, when set to TRUE, returns the result in array indices instead of a vector, that is to say that the which() function will return a vector of integers that correspond to the positions of the elements in the vector that satisfy the condition.\n\n\nExample 1: Locating Elements in a Numeric Vector\nSuppose we have a numeric vector called scores, representing test scores of students. We want to find the indices of scores greater than or equal to 90. Here’s how we can accomplish that using the which() function:\n\nscores &lt;- c(85, 92, 88, 94, 79, 91, 87, 98, 84, 90)\nindices &lt;- which(scores &gt;= 90)\nindices\n\n[1]  2  4  6  8 10\n\n\nIn this example, the which() function evaluates the logical expression scores &gt;= 90 and returns the indices of the elements satisfying the condition. The resulting indices vector will contain [2, 4, 6, 8, 10], indicating the positions of the scores that meet the criteria.\n\n\nExample 2: Filtering Data Frames\nData frames are widely used in data analysis. The which() function can be incredibly useful when working with data frames to filter rows based on specific conditions. Consider the following example:\n\ndata &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Charlie\", \"Dave\"),\n                   Age = c(25, 32, 28, 30),\n                   City = c(\"New York\", \"London\", \"Paris\", \"Sydney\"))\n\nselected_rows &lt;- which(data$Age &gt;= 30)\nfiltered_data &lt;- data[selected_rows, ]\nfiltered_data\n\n  Name Age   City\n2  Bob  32 London\n4 Dave  30 Sydney\n\n\nIn this case, we use the which() function to find the rows where the Age column is greater than or equal to 30. The selected_rows vector will hold the indices [2, 4], which we subsequently use to filter the original data frame. The resulting filtered_data will contain the rows corresponding to the selected indices, in this case, rows for Bob and Dave.\n\n\nExample 3: Using the arr.ind Parameter\nThe arr.ind parameter of the which() function comes in handy when working with multi-dimensional arrays. It allows you to obtain the indices as an array instead of a vector. Let’s illustrate this with an example:\n\nmatrix_data &lt;- matrix(1:12, nrow = 3, ncol = 4)\nselected_indices &lt;- which(matrix_data %% 3 == 0, arr.ind = TRUE)\nselected_indices\n\n     row col\n[1,]   3   1\n[2,]   3   2\n[3,]   3   3\n[4,]   3   4\n\n\nIn this example, we create a matrix called matrix_data and use the which() function to find the indices where the matrix elements are divisible by 3. By setting arr.ind = TRUE, we obtain a matrix of indices, where each row represents the position of an element satisfying the condition.\n\n\nConclusion:\nThe which() function in R proves to be an invaluable tool for data exploration and filtering. By allowing you to locate specific elements in vectors or data frames, it simplifies the process of extracting relevant information from your data. Throughout this blog post, we explored the syntax and various practical examples of using the which() function. Armed with this knowledge, you can now confidently apply the which() function to your own data analysis tasks in R, boosting your productivity and uncovering hidden insights with ease."
  },
  {
    "objectID": "posts/2023-05-19/index.html",
    "href": "posts/2023-05-19/index.html",
    "title": "Mastering File Manipulation with R’s list.files() Function",
    "section": "",
    "text": "Introduction\nWhen it comes to working with files in R, having a powerful tool at your disposal can make a world of difference. Enter the list.files() function, a versatile and handy utility that allows you to effortlessly navigate through directories, retrieve file names, and perform various file-related operations. In this blog post, we will delve into the intricacies of list.files() and explore real-world examples to help you harness its full potential.\n\nlist.files(\n  path, \n  all.files = FALSE, \n  full.names = FALSE, \n  recursive = FALSE, \n  pattern = NULL\n)\n\n\npath is a character vector specifying the directory to list. If no path is specified, the current working directory is used.\nall.files is a logical value specifying whether all files should be listed, including hidden files. The default value is FALSE, which only lists visible files.\nfull.names is a logical value specifying whether the full paths to the files should be returned. The default value is FALSE, which only returns the file names.\nrecursive is a logical value specifying whether subdirectories should be searched. The default value is FALSE, which only lists files in the specified directory.\npattern is a regular expression that can be used to filter the files that are listed. If no pattern is specified, all files are listed.\n\n\n\nUnderstanding the Basics\nBefore diving into the practical examples, let’s familiarize ourselves with the fundamental aspects of the list.files() function. In its simplest form, list.files() retrieves a character vector containing the names of files and directories within a specified directory. It takes in several optional arguments that provide flexibility and control over the file selection process.\n\n\nExample 1: Listing Files in a Directory\n\n# List all files in the current working directory\nfile_names &lt;- list.files()\nprint(file_names)\n\n[1] \"blank.txt\"       \"index.qmd\"       \"index.rmarkdown\"\n\n\nIn this example, the list.files() function is called without any arguments, resulting in the retrieval of all file names within the current working directory. The file_names variable will store the obtained character vector, which can then be printed or further processed.\n\n\nExample 2: Specifying a Directory\n\n# List all files in a specific directory\ndirectory &lt;- \"../rtip-2022-10-24/\"\nfile_names &lt;- list.files(path = directory)\nprint(file_names)\n\n[1] \"index.qmd\"\n\n\nHere, by setting the path argument to the desired directory, you can obtain the list of file names within that particular location. Remember to provide the appropriate path to the directory you wish to explore.\n\n\nExample 3: Selecting Files with a Pattern\n\n# List only files with a specific extension\npattern &lt;- \"\\\\.txt$\"\nfile_names &lt;- list.files(pattern = pattern)\nprint(file_names)\n\n[1] \"blank.txt\"\n\n\nIn this case, the pattern argument is used to filter the file names based on a regular expression. The example showcases the retrieval of only those files with a “.txt” extension. Customize the pattern as per your requirements, utilizing the power of regular expressions.\n\n\nExample 4: Recursive File Listing\n\n# List files recursively within a directory and its subdirectories\ndirectory &lt;- \"../rtip-2023-02-14/R/box/\"\nfile_names &lt;- list.files(path = directory, recursive = TRUE)\nprint(file_names)\n\n[1] \"global_options/global_options.R\" \"io/exports.R\"                   \n[3] \"io/imports.R\"                    \"mod/mod.R\"                      \n\n\nBy setting the recursive argument to TRUE, you can instruct list.files() to search for files not only in the specified directory but also in its subdirectories. This feature is particularly useful when dealing with nested file structures.\n\n\nExample 5: Excluding Directories\n\n# List only files and exclude directories\ndirectory &lt;- \"../rtip-2023-02-14/R/box/\"\nfile_names &lt;- list.files(path = directory, include.dirs = FALSE)\nprint(file_names)\n\n[1] \"global_options\" \"io\"             \"mod\"           \n\n\nIn scenarios where you only want to retrieve files and exclude directories, set the include.dirs argument to FALSE. This ensures that only the file names are included in the result, omitting any directory names.\nHere are some more examples:\n\n# List all files in the current working directory\nlist.files()\n\n# List all files in the current working directory, including hidden files\nlist.files(all.files = TRUE)\n\n# List all files in the current working directory with the .csv extension\nlist.files(pattern = \"\\\\.csv$\")\n\n# List all files in the /data directory\nlist.files(\"/data\")\n\n# List all files in the /data directory, including subdirectories\nlist.files(\"/data\", recursive = TRUE)\n\n\n\nConclusion\nThe list.files() function in R is an invaluable tool for file manipulation, enabling you to effortlessly retrieve file names, filter based on patterns, explore nested directories, and more. By mastering this function, you gain greater control over your file-handling tasks and can efficiently process and analyze data stored in files.\nRemember to consult R’s documentation for additional details on the various optional arguments and explore the wide range of possibilities offered by list.files(). With practice and experimentation, you’ll become a proficient file explorer in no time!\nHappy coding!"
  },
  {
    "objectID": "posts/2023-05-22/index.html",
    "href": "posts/2023-05-22/index.html",
    "title": "Update to {TidyDensity}",
    "section": "",
    "text": "Introduction\nTo effectively extract insights and communicate findings, you need powerful tools that simplify the process and present data in an engaging manner. If you’re a programmer with a penchant for data analysis, you’re in luck! The latest version of {TidyDensity}, the popular R package, has just been released, bringing you exciting new features and enhancements. In this blog post, we’ll explore the highlights of TidyDensity 1.2.5 and why you should download it today.\n\n\nNew Feature: Introducing util_burr_param_estimate()\nTidyDensity 1.2.5 introduces a new function: util_burr_param_estimate(). This function enables you to estimate parameters using the Burr distribution, expanding the possibilities of your data analysis. Whether you’re working with survival analysis, reliability modeling, or extreme value theory, util_burr_param_estimate() equips you with a powerful tool to tackle complex scenarios with ease. Say goodbye to manual calculations and embrace the simplicity and accuracy of TidyDensity.\n\n\nMinor Fixes and Improvements for Enhanced Workflow\nIn addition to the groundbreaking new feature, TidyDensity 1.2.5 addresses user feedback and provides several minor fixes and improvements. Let’s take a look at a couple of them:\n\nImproved Parameter Rounding: With the new version, you now have more control over the rounding of parameter estimates. The updated function tidy_distribution_comparison() includes a parameter called .round_to_place, allowing you to precisely control the rounding behavior of the parameter estimates passed to their corresponding distribution parameters. This enhancement ensures that your analysis remains accurate and aligned with your specific requirements.\n\n\n\nWhy Upgrade to TidyDensity 1.2.5?\n\nStay Ahead of the Curve: The world of data analysis is constantly evolving, and staying up to date with the latest tools and features is crucial to remain competitive. TidyDensity 1.2.5 empowers you with advanced capabilities, enabling you to analyze and visualize data more effectively than ever before.\nSimplify Complex Analysis: With the new util_burr_param_estimate() function, TidyDensity 1.2.5 simplifies complex data analysis tasks. Whether you’re a seasoned data scientist or a beginner, this feature allows you to explore a wider range of statistical distributions and unlock deeper insights from your data.\nFine-Tuned Precision: The improved parameter rounding in tidy_distribution_comparison() ensures that your analysis is not only powerful but also precise. This level of control over rounding provides you with the flexibility to align your analysis with your specific requirements.\n\n\n\nConclusion\nTidyDensity 1.2.5 is a significant update that brings you exciting new features and enhancements. From the introduction of util_burr_param_estimate() to the fine-tuned parameter rounding and polished visuals, this version is designed to empower you in your data analysis journey. By downloading TidyDensity1.2.5, you can stay at the forefront of data analysis, simplify complex tasks, and elevate the precision and user experience of your projects. Upgrade to TidyDensity 1.2.5 today!."
  },
  {
    "objectID": "posts/2023-05-23/index.html",
    "href": "posts/2023-05-23/index.html",
    "title": "What is the sink() function? Capturing Output to External Files",
    "section": "",
    "text": "Introduction\nThe sink() function in R is used to divert R output to an external connection. This can be useful for a variety of purposes, such as exporting data to a file, logging R output, or debugging R code.\nIn this blog post, we will explore the inner workings of the sink() function, understand its purpose, and provide practical examples using the popular datasets mtcars and iris.\nThe sink() function takes four arguments:\n\nfile: The name of the file to which R output will be diverted. If file is NULL, then R output will be diverted to the console.\nappend: A logical value indicating whether R output should be appended to the file (TRUE) or overwritten (FALSE). The default value is FALSE.\ntype: A character string. Either the output stream or the messages stream. The name will be partially match so can be abbreviated.\nsplit: logical: if TRUE, output will be sent to the new sink and the current output stream, like the Unix program tee.\n\n\n\nExamples\nHere are some examples of how to use the sink() function. To export the mtcars dataset to a file called “mtcars.csv”, you would use the following code:\n\nsink(\"mtcars.csv\")\nprint(mtcars)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nsink()\n\nTo log R output to a file called “r_output.log”, you would use the following code:\n\nsink(\"r_output.log\")\n# Your R code goes here\nsink()\n\nTo debug R code, you can use the sink() function to divert R output to a file. This can be helpful for tracking down errors in your code. For example, if you are trying to debug a function called my_function(), you could use the following code:\n\nsink(\"my_function.log\")\nmy_function()\nsink()\n\n\n\nCapturing Summary Statistics of mtcars Dataset\n\nsink(\"summary_output.txt\")  # Redirect output to the file\n\nsummary(mtcars)  # Generate summary statistics\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\nsink()  # Turn off redirection\n\nIn this example, the output of the summary(mtcars) command will be saved in the “summary_output.txt” file. We can later open the file to review the summary statistics of the mtcars dataset.\n\n\nSaving Regression Results of iris Dataset\n\nsink(\"regression_results.txt\")  # Redirect output to the file\n\nfit &lt;- lm(Sepal.Length ~ Sepal.Width, data = iris)  # Perform linear regression\n\nsummary(fit)  # Display regression summary\n\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5561 -0.6333 -0.1120  0.5579  2.2226 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.5262     0.4789   13.63   &lt;2e-16 ***\nSepal.Width  -0.2234     0.1551   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8251 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\nsink()  # Turn off redirection\n\nIn this example, the output of the summary(fit) command will be saved in the “regression_results.txt” file. By redirecting the output, we can analyze the regression results in detail without cluttering the console.\n\n\nAppending Output to a File\nBy default, calling sink() with a file name will overwrite any existing content in the file. However, if we want to append output to an existing file, we can pass the append = TRUE argument to sink().\n\nsink(\"output.txt\", append = TRUE)  # Append output to the existing file\n\ncat(\"Additional text\\n\")  # Append custom text\n\nAdditional text\n\nsink()  # Turn off redirection\n\nIn this example, the string “Additional text” will be appended to the “output.txt” file. This feature is useful when we want to continuously update a log file or add multiple output sections to a single file.\n\n\nConclusion\nThe sink() function is a handy tool in R that allows us to redirect output to external files. By using this function, we can save and review the output generated during data analysis, statistical modeling, or any other R programming tasks. In this blog post, we explored the basic usage of sink() and provided practical examples using the mtcars and iris datasets. By mastering sink(), you can efficiently manage your R output and ensure a more organized workflow."
  },
  {
    "objectID": "posts/2023-05-24/index.html",
    "href": "posts/2023-05-24/index.html",
    "title": "Exploring Data with TidyDensity: A Guide to Using tidy_empirical() and tidy_four_autoplot() in R",
    "section": "",
    "text": "Introduction\nYesterday I had the need to see data that had a grouping column in it. I wanted to use the tidy_four_autoplot() function on it from the {TidyDensity} library on it. This post will explain how I did it. The data in my session was called df_tbl. In this blog post, we will explore the steps involved in using the tidy_empirical() and tidy_four_autoplot() functions from the R library TidyDensity. These functions are incredibly useful when working with data, as they allow us to analyze and visualize empirical distributions efficiently. We will walk through a code snippet that demonstrates how to use these functions within a map() function, enabling us to analyze multiple subsets of data simultaneously.\n#Prerequisites\nTo follow along with this tutorial, it is assumed that you have a basic understanding of the R programming language, as well as familiarity with the dplyr, purrr, and TidyDensity libraries. Make sure you have these packages installed and loaded before proceeding.\nHere is the code that I used, the explanation will follow:\n\nlibrary(dplyr) # to use group_split()\nlibrary(purrr) # to use map()\nlibrary(TidyDensity) # to use tidy_empirical() and tidy_four_plot()\n\ndf_tbl |&gt;\n  group_split(SP_NAME) |&gt;\n  map(\\(run_time) pull(run_time) |&gt;\n        tidy_empirical() |&gt;\n        tidy_four_autoplot()\n      )\n\n\n\nCode Explanation\nLet’s break down the code step by step:\nImporting Required Libraries:\n\nTo access the necessary functions, we need to load the required libraries. In this case, we use library(dplyr) to utilize the group_split() function from the dplyr package, library(purrr) to use the map() function from the purrr package, and library(TidyDensity) to access the tidy_empirical() and tidy_four_autoplot() functions from the TidyDensity package.\n\nGrouping and Splitting the Data:\n\nThe first line of the code snippet takes a dataframe named df_tbl and uses the group_split() function from the dplyr library to split it into multiple subsets based on a variable called SP_NAME. This creates a list of dataframes, each representing a unique group based on SP_NAME.\n\nApplying Functions to Each Subset using map():\n\nThe second line of code utilizes the map() function from the purrr library to iterate over each subset of data created in the previous step. The map() function takes two arguments: the object to iterate over (in this case, the list of dataframes) and a function to apply to each element.\n\nAnonymous Function Inside map():\n\nWithin the map() function, an anonymous function (denoted by (run_time)) is defined. This function takes a single argument named run_time, representing each individual subset of data. The purpose of this anonymous function is to perform the necessary computations and visualizations on each subset of data.\n\nData Manipulation and Visualization:\n\nInside the anonymous function, the pull(run_time) function is used to extract the run_time column from each subset of data. This column is then passed to the tidy_empirical() function from the TidyDensity library, which calculates the empirical distribution of the data. The result is a tidy dataframe that contains information about the empirical distribution.\n\nTidy Four Autoplot:\n\nThe output of tidy_empirical() is then piped (|&gt;) into the tidy_four_autoplot() function from the TidyDensity library. This function generates a visualization called a “Tidy Four Plot,” which consists of four individual plots: empirical density, empirical cumulative density, QQ plot, and histogram.\n\nFinal Output:\n\nThe result of the tidy_four_autoplot() function is the final output of the anonymous function within map(). This output represents the visualization of the empirical distribution for each subset of data.\n\nHappy Coding!"
  },
  {
    "objectID": "posts/2023-05-25/index.html",
    "href": "posts/2023-05-25/index.html",
    "title": "Comparing R Packages for Writing Excel Files: An Analysis of writexl, openxlsx, and xlsx in R",
    "section": "",
    "text": "Introduction\nIn the realm of data analysis and manipulation, R has become a popular programming language due to its extensive collection of packages and libraries. One common task is exporting data to Excel files, which allows for easy sharing and presentation of results. In this blog post, we will explore three popular R packages for writing Excel files: writexl, openxlsx, and xlsx. We will compare their performance using the benchmarking package and analyze the results. So let’s dive in!\n\n\nSetting up the Environment\nBefore we proceed, make sure you have the necessary packages installed. We will be using the rbenchmark, nycflights13, and dplyr packages. The nycflights13 package provides a dataset named “flights,” which we will use for our benchmarking tests.\n\nlibrary(rbenchmark)\nlibrary(nycflights13)\nlibrary(dplyr)\n\n#Defining the Number of Replications\nTo ensure reliable performance measurements, we will repeat each test multiple times. The variable n represents the number of replications, and you can adjust its value depending on your requirements.\n\nn &lt;- 5\n\n\n\nBenchmarking the Packages\nNow, let’s move on to the actual benchmarking process. We will use the benchmark() function from the rbenchmark package to compare the performance of writexl, openxlsx, and xlsx.\n\nbenchmark(\n  \"writexl\" = {\n    writexl::write_xlsx(flights, tempfile())\n  },\n  \"openxlsx\" = {\n    openxlsx::write.xlsx(flights, tempfile())\n  },\n  \"xlsx\" = {\n    xlsx::write.xlsx(flights, paste0(tempfile(),\".xlsx\"))\n  },\n  replications = n,\n  columns = c(\n    \"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n)\n\nIn the code snippet above, we define three tests, each representing one package. We provide the code to execute for each test. For example, in the “writexl” test, we use the write_xlsx() function from the writexl package to write the “flights” dataset to a temporary Excel file.\nThe replications parameter specifies the number of times each test should be repeated. In our case, we set it to n, which we defined earlier as 5.\nThe columns parameter defines the columns to include in the benchmarking results. We specify “test” for the test name, “replications” for the number of replications, “elapsed” for the total time taken, “relative” for the relative performance compared to the fastest test, “user.self” for the CPU time used in user code, and “sys.self” for the CPU time used in system code.\n\n\nPrettifying the Results\nTo make the results more readable, we can use the arrange() function from the dplyr package to sort the results by the “relative” column in ascending order.\n\narrange(relative)\n\nThis will arrange the benchmarking results in ascending order of relative performance, allowing us to easily identify the most efficient package.\n\n\nBenchmark Output\n\ntest replications elapsed relative user.self sys.self\n1 writexl       5   0.034   1.000000   0.024   0.010\n2 openxlsx       5   0.055   1.617647   0.044   0.011\n3 xlsx          5   0.101   2.941176   0.078   0.023\n\n\n\nInterpretation of the Results\nThe results of the benchmark show that writexl is the fastest package for writing to Excel, followed by openxlsx and xlsx. The difference in performance between the three packages is not significant, but writexl is consistently faster than the other two packages.\n\n\nConclusion\nIn this blog post, we compared the performance of three R packages, writexl, openxlsx, and xlsx, for writing Excel files. We used the rbenchmark package to benchmark the packages, considering the number of replications, elapsed time, relative performance, user CPU time, and system CPU time. By arranging the results using the dplyr package, we obtained a sorted view of the relative performance. This analysis can help you choose the most suitable package for your specific needs, considering both performance and functionality.\nRemember, benchmarking can vary depending on the dataset and system specifications. So, it’s always a good idea to run your own benchmarks and evaluate the results in your specific context. Happy coding!"
  },
  {
    "objectID": "posts/2023-05-26/index.html",
    "href": "posts/2023-05-26/index.html",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "",
    "text": "When working with data, it is important to be aware of the file size of the data you are working with. This is especially true when you are working with large datasets, as the file size can have a significant impact on the performance of your code.\nIn R, there are a number of different ways to write data to files. Each method has its own advantages and disadvantages, and the file size of the output can vary depending on the method you use.\nIn this blog post, we will discuss why it is a good idea to check the file size output for different methods. We will also provide three examples of how to check the file size output using the R libraries writexl, openxlsx, and xlsx."
  },
  {
    "objectID": "posts/2023-05-26/index.html#writexl",
    "href": "posts/2023-05-26/index.html#writexl",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "writexl",
    "text": "writexl\nTo check the file size output of the writexl::write_xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the\n\nlibrary(writexl)\n\nwrite_xlsx(iris, tmp1 &lt;- tempfile())\n\nfile.info(tmp1)$size\n\n[1] 8497"
  },
  {
    "objectID": "posts/2023-05-26/index.html#openxlsx",
    "href": "posts/2023-05-26/index.html#openxlsx",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "openxlsx",
    "text": "openxlsx\nTo check the file size output of the openxlsx::write.xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the output:\n\nlibrary(openxlsx)\n\nwrite.xlsx(iris, tmp2 &lt;- tempfile())\n\nfile.info(tmp2)$size\n\n[1] 9631"
  },
  {
    "objectID": "posts/2023-05-26/index.html#xlsx",
    "href": "posts/2023-05-26/index.html#xlsx",
    "title": "Why Check File Size Output for Different Methods?",
    "section": "xlsx",
    "text": "xlsx\nTo check the file size output of the xlsx::write.xlsx() function, you can use the file.info() function. For example, the following code will write the iris dataset to a temporary file and then print the file size of the output:\n\nlibrary(xlsx)\n\nwrite.xlsx(iris, tmp3 &lt;- paste0(tempfile(), \".xlsx\"))\n\nfile.info(tmp3)$size\n\n[1] 7905"
  },
  {
    "objectID": "posts/2023-05-30/index.html",
    "href": "posts/2023-05-30/index.html",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "",
    "text": "Programming is often about making decisions based on certain conditions. In the world of R, there are numerous functions that can help us simplify our code and make it more efficient. One such function is any(). In this blog post, we’ll explore the any() function and learn how it can be used to streamline our logical operations. Whether you’re a beginner or an experienced programmer, this post aims to make the concept accessible to everyone. So, let’s dive in!"
  },
  {
    "objectID": "posts/2023-05-30/index.html#basic-examples",
    "href": "posts/2023-05-30/index.html#basic-examples",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Basic Examples",
    "text": "Basic Examples\nNow, let’s see some basic examples on how to use the any() function.\n\nx &lt;- c(1, 2, 3, 4, 5)\n\nany(x &gt; 10)\n\n[1] FALSE\n\n\n\nx &lt;- c(1, 2, NA, 4, 5)\n\nany(x &gt; 10)\n\n[1] NA\n\nany(x == 5)\n\n[1] TRUE\n\n\nNow, let’s explore some examples to see how any() can be utilized in various scenarios:"
  },
  {
    "objectID": "posts/2023-05-30/index.html#checking-for-the-presence-of-a-specific-value",
    "href": "posts/2023-05-30/index.html#checking-for-the-presence-of-a-specific-value",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Checking for the Presence of a Specific Value:",
    "text": "Checking for the Presence of a Specific Value:\nSuppose we have a vector of numbers, and we want to check if any of them are divisible by 5. We can use the any() function to accomplish this as follows:\n\nnumbers &lt;- c(2, 7, 12, 15, 21)\nis_divisible_by_5 &lt;- any(numbers %% 5 == 0)\n\nif (is_divisible_by_5) {\n  print(\"At least one number is divisible by 5.\")\n} else {\n  print(\"None of the numbers are divisible by 5.\")\n}\n\n[1] \"At least one number is divisible by 5.\"\n\n\nIn this example, we use the modulus operator (%%) to check if each number in the vector has a remainder of 0 when divided by 5. The any() function then returns TRUE if any such element is found, indicating the presence of at least one number divisible by 5."
  },
  {
    "objectID": "posts/2023-05-30/index.html#validating-user-input",
    "href": "posts/2023-05-30/index.html#validating-user-input",
    "title": "Simplifying Logical Operations with the R Function any()",
    "section": "Validating User Input:",
    "text": "Validating User Input:\nLet’s say we are building a program that requires the user to input a positive number. We can use the any() function to validate the input as shown below:\n\nuser_input &lt;- as.numeric(readline(prompt = \"Enter a positive number: \"))\n\nEnter a positive number: \n\n# Dummy input\nuser_input &lt;- 5\nis_positive &lt;- any(user_input &gt; 0)\n\nif (is_positive) {\n  print(\"Input is a positive number.\")\n} else {\n  print(\"Input is not a positive number.\")\n}\n\n[1] \"Input is a positive number.\"\n\n\nHere, we convert the user input to a numeric value using as.numeric() and then check if it is greater than zero. The any() function returns TRUE if any element satisfies this condition, confirming that the input is indeed a positive number."
  },
  {
    "objectID": "posts/2023-05-31/index.html",
    "href": "posts/2023-05-31/index.html",
    "title": "Demystifying Regular Expressions: A Programmer’s Guide for Beginners",
    "section": "",
    "text": "Introduction\nRegular expressions, often abbreviated as regex, are powerful tools used in programming to match and manipulate text patterns. While they might seem intimidating at first, regular expressions are incredibly useful for tasks like data validation, text parsing, and pattern matching. In this blog post, we’ll explore regular expressions in the context of R programming, breaking down the concepts step by step and providing practical examples along the way. By the end, you’ll have a solid understanding of regular expressions and be ready to apply them to your own projects.\n\n\nWhat are Regular Expressions?\nAt its core, a regular expression is a sequence of characters that define a search pattern. It allows you to search, extract, and manipulate text based on specific patterns of characters. Regular expressions are supported in many programming languages, including R, and they provide a concise and flexible way to work with text.\n\n\nHow do regular expressions work?\nRegular expressions work by matching patterns of characters in text. The basic syntax of a regular expression is a sequence of characters enclosed in delimiters, such as slashes (/). The characters in the regular expression can be literal characters, special characters, or character classes.\nLiteral characters are characters that match themselves. For example, the regular expression /a/ matches the letter a.\nSpecial characters are characters that have special meaning in regular expressions. For example, the special character . matches any character.\nCharacter classes are a way to specify a set of characters. For example, the character class [a-z] matches any lowercase letter.\n\n\nHow to use regular expressions in R\nRegular expressions can be used in R to search for, extract, and replace text. To use regular expressions in R, you can use the grep(), grepl(), sub(), and gsub() functions.\nThe grep() function is used to search for text that matches a regular expression. The grepl() function is similar to grep(), but it returns a logical vector indicating whether each element of a vector matches the regular expression. The sub() function is used to replace text that matches a regular expression. The gsub() function is similar to sub(), but it replaces all occurrences of the text that matches the regular expression.\n\n\nBasic Characters\n\n. | Matches any single character except a newline character.\n[] | Matches any character within the brackets. For example, [a-z] matches any lowercase letter.\n* | Matches zero or more occurrences of the preceding character. For example, a* matches any number of a characters, including zero.\n+ | Matches one or more occurrences of the preceding character. For example, a+ matches one or more a characters.\n? | Matches zero or one occurrences of the preceding character. For example, a? matches either one or zero a characters.\n^ | Matches the beginning of the string.\n$ | Matches the end of the string.\n\n\n\nSpecial Characters\nThe following are the special characters used in regular expressions:\n\n\\d | Matches a digit.\n\\s | Matches a whitespace character.\n\\w | Matches a word character (alphanumeric character or underscore).\n\\W | Matches a non-word character.\n\\n | Matches a newline character.\n\\r | Matches a carriage return character.\n\\t | Matches a tab character.\n\n\n\nExamples of regular expressions in R\nHere are some examples of regular expressions in R:\n\nTo search for all occurrences of the word “hello” in a string, you would use the following code:\n\n\ngrep(\"hello\", \"This is a string that contains the word 'hello'\")\n\n[1] 1\n\n\n\nTo extract all of the email addresses from a string, you would use the following code:\n\ngrepl(\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}\"), “This is a string that contains some email addresses”)\n\nTo replace all of the spaces in a string with underscores, you would use the following code:\n\n\nsub(\" \", \"_\", \"This is a string with some spaces\")\n\n[1] \"This_is a string with some spaces\"\n\n\n\nTo replace all of the occurrences of the word “hello” with the word “goodbye” in a string, you would use the following code:\n\n\ngsub(\"hello\", \"goodbye\", \"This is a string that contains the word 'hello'\")\n\n[1] \"This is a string that contains the word 'goodbye'\"\n\n\n\n\nMatching a Simple Pattern\nLet’s start with a simple example in R. Suppose we have a character vector called fruits that contains various fruit names:\n\nfruits &lt;- c(\"apple\", \"banana\", \"orange\", \"kiwi\", \"mango\")\n\nWe can use a regular expression to find all the fruits that start with the letter “a”. In R, the grep() function allows us to perform pattern matching. Here’s how we can achieve this:\n\npattern &lt;- \"^a\"  # ^ denotes the start of the line\nmatching_fruits &lt;- grep(pattern, fruits, value = TRUE)\nprint(matching_fruits)\n\n[1] \"apple\"\n\n\nThe output will be “apple”.\nIn this example, the pattern “^a” specifies that we want to match any fruit that starts with the letter “a”. The grep() function returns the matching fruit names, and we set value = TRUE to obtain the matched values instead of their indices.\n\n\nExtracting Digits from a String\nRegular expressions can be used to extract specific information from a string. Suppose we have a character vector called sentences containing sentences with numbers:\n\nsentences &lt;- c(\"I have 10 apples.\", \"The recipe calls for 2 cups of sugar.\", \"You are the 3rd winner.\")\n\nTo extract the digits from each sentence, we can use the gsub() function, which replaces specific patterns within a string:\n\npattern &lt;- \"\\\\D\"  # \\\\D matches any non-digit character\ndigits &lt;- gsub(pattern, \"\", sentences)\nprint(digits)\n\n[1] \"10\" \"2\"  \"3\" \n\n\nThe output will be “10” “2” “3”\nIn this example, the pattern “\\D” matches any non-digit character. By replacing these characters with an empty string, we effectively extract the digits from each sentence.\n\n\nConclusion\nRegular expressions are an invaluable tool for working with text patterns in programming. While they may seem daunting at first, breaking down the concepts and understanding their building blocks can help demystify them. In this blog post, we explored the basics of regular expressions in R, showcasing practical examples along the way. Armed with this knowledge, you can now confidently incorporate regular expressions into your programming projects, allowing you to manipulate and extract information from text efficiently.\nRemember, practice makes perfect when it comes to regular expressions. Experiment with different patterns, explore the rich set of metacharacters and operators available, and refer to the R documentation for more in-depth information. Regular expressions open up a whole new world of possibilities in text manipulation, so embrace their power and have fun exploring the endless patterns you can match!"
  },
  {
    "objectID": "posts/2023-06-01/index.html",
    "href": "posts/2023-06-01/index.html",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "",
    "text": "As a programmer, you’re always on the lookout for tools that can enhance your productivity and make your code more efficient. In the world of R programming, the do.call() function is one such gem. This often-overlooked function is a powerful tool that allows you to dynamically call other functions, opening up a world of possibilities for code organization, reusability, and flexibility. In this blog post, we will demystify the do.call() function in simple terms and provide you with practical examples that showcase its versatility."
  },
  {
    "objectID": "posts/2023-06-01/index.html#example-1-combining-multiple-vectors-with-rbind",
    "href": "posts/2023-06-01/index.html#example-1-combining-multiple-vectors-with-rbind",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "Example 1: Combining Multiple Vectors with rbind()",
    "text": "Example 1: Combining Multiple Vectors with rbind()\nLet’s say you have a list of vectors, and you want to combine them into a single matrix using the rbind() function. Instead of manually specifying the vectors one by one, you can leverage do.call() to dynamically generate the function call:\n\nvectors &lt;- list(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9))\ncombined_matrix &lt;- do.call(rbind, vectors)\ncombined_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nIn this example, do.call() dynamically constructs the function call rbind(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9)), resulting in a matrix that combines the vectors."
  },
  {
    "objectID": "posts/2023-06-01/index.html#example-2-applying-a-function-to-multiple-data-frames-with-lapply",
    "href": "posts/2023-06-01/index.html#example-2-applying-a-function-to-multiple-data-frames-with-lapply",
    "title": "The do.call() function in R: Unlocking Efficiency and Flexibility",
    "section": "Example 2: Applying a Function to Multiple Data Frames with lapply()",
    "text": "Example 2: Applying a Function to Multiple Data Frames with lapply()\nSuppose you have a list of data frames, and you want to apply a specific function to each of them, such as summarizing the mean of a column. Instead of writing repetitive code, you can use do.call() to apply the desired function dynamically:\n\ndata_frames &lt;- list(\n  data.frame(a = 1:3), \n  data.frame(a = 4:6), \n  data.frame(a = 7:9)\n  )\nmean_results &lt;- do.call(\n  rbind, \n  lapply(data_frames, function(df) mean(df$a))\n  )\nmean_results\n\n     [,1]\n[1,]    2\n[2,]    5\n[3,]    8\n\n\nIn this example, do.call() combines the results of applying the mean function to each data frame’s ‘a’ column into a single matrix."
  },
  {
    "objectID": "posts/2023-06-02/index.html",
    "href": "posts/2023-06-02/index.html",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "",
    "text": "In the realm of data analysis and programming, organizing and sorting data efficiently is crucial. In R, a programming language renowned for its data manipulation capabilities, we have three powerful functions at our disposal: order(), sort(), and rank(). In this blog post, we will delve into the intricacies of these functions, explore their applications, and understand their parameters. These R functions are all used to sort data, however, they each have different purposes and use different methods to sort the data."
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-order",
    "href": "posts/2023-06-02/index.html#parameters-of-order",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of order():",
    "text": "Parameters of order():\n\n... - Specify the vectors to be sorted."
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-sort",
    "href": "posts/2023-06-02/index.html#parameters-of-sort",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of sort():",
    "text": "Parameters of sort():\n\nx - The vector or matrix to be sorted.\ndecreasing - A logical value indicating whether the sorting should be in descending order. (Default is FALSE)"
  },
  {
    "objectID": "posts/2023-06-02/index.html#parameters-of-rank",
    "href": "posts/2023-06-02/index.html#parameters-of-rank",
    "title": "Sorting, Ordering, and Ranking: Unraveling R’s Powerful Functions",
    "section": "Parameters of rank():",
    "text": "Parameters of rank():\n\nx - The vector to be ranked.\nties.method - A string specifying the method to handle ties in ranking. (Options: “average”, “first”, “last”, “random”, “max”, “min”) (Default is “average”)"
  },
  {
    "objectID": "posts/2023-06-06/index.html",
    "href": "posts/2023-06-06/index.html",
    "title": "Simplifying Data Transformation with pivot_longer() in R’s tidyr Library",
    "section": "",
    "text": "Introduction\nIn the world of data analysis and manipulation, tidying and reshaping data is often an essential step. R’s tidyr library provides powerful tools to efficiently transform and reshape data. One such function is pivot_longer(). In this blog post, we’ll explore how pivot_longer() works and demonstrate its usage through several examples. By the end, you’ll have a solid understanding of how to use this function to make your data more manageable and insightful.\nThe tidyr library holds the function, so we are going to have to load it first.\n\nlibrary(tidyr)\n\n\n\nUnderstanding pivot_longer()\nThe pivot_longer() function is designed to reshape data from a wider format to a longer format. It takes columns that represent different variables and consolidates them into key-value pairs, making it easier to analyze and visualize the data.\nSyntax: The basic syntax of pivot_longer() is as follows:\npivot_longer(data, cols, names_to, values_to)\n\ndata: The data frame or tibble to be reshaped.\ncols: The columns to be transformed.\nnames_to: The name of the new column that will hold the variable names.\nvalues_to: The name of the new column that will hold the corresponding values.\n\n\n\nExample 1: Reshaping Wide Data to Long Data\nLet’s start with a simple example to demonstrate the usage of pivot_longer(). Suppose we have a data frame called students with columns representing subjects and their respective scores:\n\nstudents &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  math = c(90, 85, 92),\n  science = c(95, 88, 91),\n  history = c(87, 92, 78)\n)\n\nTo reshape this data from a wider format to a longer format, we can use pivot_longer() as follows:\n\nstudents_long &lt;- pivot_longer(\n  students, \n  cols = -name, \n  names_to = \"subject\", \n  values_to = \"score\"\n  )\n\nstudents_long\n\n# A tibble: 9 × 3\n  name    subject score\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 Alice   math       90\n2 Alice   science    95\n3 Alice   history    87\n4 Bob     math       85\n5 Bob     science    88\n6 Bob     history    92\n7 Charlie math       92\n8 Charlie science    91\n9 Charlie history    78\n\n\nThe resulting students_long data frame will have three columns: name, subject, and score, where each row represents a student’s score in a specific subject.\nExample 2: Handling Multiple Variables In many cases, data frames contain multiple variables that need to be pivoted simultaneously. Consider a data frame called sales with columns representing sales figures for different products in different regions:\n\nsales &lt;- data.frame(\n  region = c(\"North\", \"South\", \"East\"),\n  product_A = c(100, 120, 150),\n  product_B = c(80, 90, 110),\n  product_C = c(60, 70, 80)\n)\n\nTo reshape this data, we can specify multiple columns to pivot using pivot_longer():\n\nsales_long &lt;- pivot_longer(\n  sales, \n  cols = starts_with(\"product\"), \n  names_to = \"product\", \n  values_to = \"sales\"\n  )\n\nsales_long\n\n# A tibble: 9 × 3\n  region product   sales\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 North  product_A   100\n2 North  product_B    80\n3 North  product_C    60\n4 South  product_A   120\n5 South  product_B    90\n6 South  product_C    70\n7 East   product_A   150\n8 East   product_B   110\n9 East   product_C    80\n\n\nThe resulting sales_long data frame will have three columns: region, product, and sales, where each row represents the sales figure of a specific product in a particular region.\n\n\nExample 3: Handling Irregular Data\nSometimes, data frames contain irregular structures, such as missing values or uneven numbers of columns. pivot_longer() can handle such scenarios gracefully. Consider a data frame called measurements with columns representing different measurement types and their respective values:\n\nmeasurements &lt;- data.frame(\n  timestamp = c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"),\n  temperature = c(25.3, 27.1, 24.8),\n  humidity = c(65.2, NA, 68.5),\n  pressure = c(1013, 1012, NA)\n)\n\nTo reshape this data, we can use pivot_longer() and handle the missing values:\n\nmeasurements_long &lt;- pivot_longer(\n  measurements, \n  cols = -timestamp, \n  names_to = \"measurement\", \n  values_to = \"value\", \n  values_drop_na = TRUE\n  )\n\nmeasurements_long\n\n# A tibble: 7 × 3\n  timestamp  measurement  value\n  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n1 2022-01-01 temperature   25.3\n2 2022-01-01 humidity      65.2\n3 2022-01-01 pressure    1013  \n4 2022-01-02 temperature   27.1\n5 2022-01-02 pressure    1012  \n6 2022-01-03 temperature   24.8\n7 2022-01-03 humidity      68.5\n\n\nThe resulting measurements_long data frame will have three columns: timestamp, measurement, and value, where each row represents a specific measurement at a particular timestamp. The values_drop_na argument ensures that rows with missing values are dropped.\n\n\nConclusion\nIn this blog post, we explored the pivot_longer() function from the tidyr library, which allows us to reshape data from a wider format to a longer format. We covered the syntax and provided several examples to illustrate its usage. By mastering pivot_longer(), you’ll be equipped to tidy your data and unleash its true potential for analysis and visualization."
  },
  {
    "objectID": "posts/2023-06-08/index.html",
    "href": "posts/2023-06-08/index.html",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "",
    "text": "In R, the file.info() function is a useful tool for retrieving file information, such as file attributes and metadata. It allows programmers to gather details about files, including their size, permissions, and timestamps. In this post, we will explore the file.info() function and demonstrate how it can be used to list files by date.\n\n\nThe file.info() function returns a data frame with file information as its columns. Each row corresponds to a file, and the columns contain attributes such as the file size, permissions, and timestamps. This function accepts one or more file paths as its argument, providing flexibility in examining multiple files simultaneously. The following columns are returned in the data.frame that results from file.info():\n\nname: The name of the file.\nsize: The size of the file in bytes.\nmode: The mode of the file, which can be used to determine the file’s permissions.\nmtime: The modification time of the file.\nctime: The creation time of the file.\natime: The last access time of the file.\n\nIn order to get some data to work with, we will save the iris dataset as an excel file four times in a for loop, waiting 10 seconds between each save.\nlibrary(writexl)\n\n# Generate file names\nfile_prefix &lt;- \"iris\"\nfile_extension &lt;- \".xlsx\"\nnum_files &lt;- 4\n\n# Save iris dataset as Excel files\nfor (i in 1:num_files) {\n  file_name &lt;- paste0(file_prefix, \"_\", i, file_extension)\n  write_xlsx(iris, file_name)\n  cat(\"File\", file_name, \"saved successfully.\\n\")\n  Sys.sleep(10) # Sleep for 10 seconds then go again\n}"
  },
  {
    "objectID": "posts/2023-06-08/index.html#example-1-retrieving-file-information",
    "href": "posts/2023-06-08/index.html#example-1-retrieving-file-information",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "Example 1: Retrieving File Information",
    "text": "Example 1: Retrieving File Information\nLet’s begin by retrieving information about a single file. we have a file named “iris_1.xlsx” located in our working directory. We can use the file.info() function to obtain its attributes:\n\nfile_info &lt;- file.info(\"iris_1.xlsx\")\nprint(file_info)\n\n            size isdir mode               mtime               ctime\niris_1.xlsx 8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\n                          atime exe\niris_1.xlsx 2023-06-08 07:58:19  no\n\n\nThe output will display a data frame with the attributes of the “iris_1.xlsx” file, including the file size, permissions, and timestamps. This information can be valuable for tasks such as file management and quality control."
  },
  {
    "objectID": "posts/2023-06-08/index.html#example-2-listing-files-by-date",
    "href": "posts/2023-06-08/index.html#example-2-listing-files-by-date",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "Example 2: Listing Files by Date",
    "text": "Example 2: Listing Files by Date\nNow, let’s dive into listing files based on their dates. To achieve this, we will combine the file.info() function with other functions to extract and manipulate the timestamp information.\n\n# Obtain file information for all files in a directory\nfiles &lt;- list.files(full.names = TRUE, pattern = \"*.xlsx$\")\nfile_info &lt;- file.info(files)\nfile_info$file_name &lt;- rownames(file_info)\n\n# Sort files by modification date in ascending order\nsorted_files &lt;- files[order(file_info$mtime)]\n\n# Display the sorted file list\nprint(sorted_files)\n\n[1] \"./iris_1.xlsx\" \"./iris_2.xlsx\" \"./iris_3.xlsx\" \"./iris_4.xlsx\"\n\nfile_info[order(file_info$mtime), ]\n\n              size isdir mode               mtime               ctime\n./iris_1.xlsx 8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\n./iris_2.xlsx 8497 FALSE  666 2023-06-08 07:34:41 2023-06-08 07:34:41\n./iris_3.xlsx 8497 FALSE  666 2023-06-08 07:34:52 2023-06-08 07:34:52\n./iris_4.xlsx 8497 FALSE  666 2023-06-08 07:35:05 2023-06-08 07:35:05\n                            atime exe     file_name\n./iris_1.xlsx 2023-06-08 07:59:30  no ./iris_1.xlsx\n./iris_2.xlsx 2023-06-08 07:58:19  no ./iris_2.xlsx\n./iris_3.xlsx 2023-06-08 07:58:19  no ./iris_3.xlsx\n./iris_4.xlsx 2023-06-08 07:58:19  no ./iris_4.xlsx\n\n\nIn this example, we first specify the directory path where our target files are located. By using list.files(), we obtain a vector of file names within that directory. Setting full.names = TRUE ensures that the file paths include the directory path. We also used the pattern parameter to ensure that we only grab the Excel files.\nNext, we use file.info() on the vector of file names to retrieve the file information for all files in the directory. The resulting data frame, file_info, contains details about each file, including the modification timestamp (mtime).\nTo list the files by date, we sort the file names vector based on the modification timestamp, using order(file_info$mtime). The resulting sorted_files vector contains the file paths sorted in ascending order based on the modification date.\nFinally, we print the sorted file list to the console, providing an easy way to visualize the files listed by their modification date.\nLet’s go over some more examples. How about you want to see the files that were created in the last 24 hours, well, you could then do the following:\n\nfiles &lt;- file.info(list.files(), full.names = TRUE)\nfiles &lt;- files[files$mtime &gt;= Sys.time() - 24 * 60 * 60, ]\nprint(files)\n\n                size isdir mode               mtime               ctime\nindex.qmd       5161 FALSE  666 2023-06-08 07:59:28 2023-06-07 08:14:49\nindex.rmarkdown 5281 FALSE  666 2023-06-08 07:59:30 2023-06-08 07:59:30\niris_1.xlsx     8497 FALSE  666 2023-06-08 07:34:30 2023-06-08 07:34:29\niris_2.xlsx     8497 FALSE  666 2023-06-08 07:34:41 2023-06-08 07:34:41\niris_3.xlsx     8497 FALSE  666 2023-06-08 07:34:52 2023-06-08 07:34:52\niris_4.xlsx     8497 FALSE  666 2023-06-08 07:35:05 2023-06-08 07:35:05\nNA                NA    NA &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n                              atime  exe\nindex.qmd       2023-06-08 07:59:29   no\nindex.rmarkdown 2023-06-08 07:59:30   no\niris_1.xlsx     2023-06-08 07:59:30   no\niris_2.xlsx     2023-06-08 07:59:30   no\niris_3.xlsx     2023-06-08 07:59:30   no\niris_4.xlsx     2023-06-08 07:59:30   no\nNA                             &lt;NA&gt; &lt;NA&gt;\n\n\nThe file.infor() function can also be used to filter files by other criteria such as size. Lets say we want to find all files that are larger than 100MB, well we could do the following:\n\nfiles &lt;- file.info(list.files(), full.name = TRUE)\nfiles &lt;- files[files$size &gt; 100 * 1024^2, ]\nprint(files)\n\n   size isdir mode mtime ctime atime  exe\nNA   NA    NA &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n\n\nWe can see that we had no files greater than 100MB in the current directory."
  },
  {
    "objectID": "posts/2023-06-13/index.html",
    "href": "posts/2023-06-13/index.html",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "",
    "text": "As a programmer, you may come across various scenarios where you need to create complex model formulas in R. However, constructing these formulas can often be challenging and time-consuming. This is where the ‘reformulate()’ function comes to the rescue! In this blog post, we will explore the purpose and usage of the reformulate() function in R, and provide you with simple examples to help you grasp its power."
  },
  {
    "objectID": "posts/2023-06-13/index.html#example-1-linear-regression",
    "href": "posts/2023-06-13/index.html#example-1-linear-regression",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "Example 1: Linear Regression",
    "text": "Example 1: Linear Regression\nLet’s say we want to use the mtcars dataset containing information about cars, including their hp and number of cylinders. We want to perform a linear regression to predict the mpg of the car based upon hp and cyl. Here’s how we can use ‘reformulate()’ for this purpose:\n\nlibrary(stats)\n\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"hp\", \"cyl\"), response = \"mpg\")\n\n# Fitting a linear regression model\nmodel &lt;- lm(formula, data = mtcars)\n\nformula\n\nmpg ~ hp + cyl\n\nmodel\n\n\nCall:\nlm(formula = formula, data = mtcars)\n\nCoefficients:\n(Intercept)           hp          cyl  \n   36.90833     -0.01912     -2.26469  \n\n\nIn this example, the ‘reformulate()’ function creates a formula object that specifies the relationship between the response variable “mpg” and the predictor variables “hp” and “cyl”. This formula is then passed to the ‘lm()’ function for fitting a linear regression model."
  },
  {
    "objectID": "posts/2023-06-13/index.html#example-2-logistic-regression",
    "href": "posts/2023-06-13/index.html#example-2-logistic-regression",
    "title": "Simplifying Model Formulas with the R Function ‘reformulate()’",
    "section": "Example 2: Logistic Regression",
    "text": "Example 2: Logistic Regression\nConsider a scenario where we use the mtcars dataset. We use the mpg, hp, and disp variables, and whether the car is an automatic or manual. We want to perform a logistic regression to predict the probability of passing based on the mpg, hp, and disp. Here’s how ‘reformulate()’ can help us:\n\nlibrary(stats)\n\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"mpg\", \"hp\", \"disp\"), response = \"am\")\n\n# Fitting a logistic regression model\nmodel &lt;- glm(formula, data = mtcars, family = \"binomial\")\n\nformula\n\nam ~ mpg + hp + disp\n\nmodel\n\n\nCall:  glm(formula = formula, family = \"binomial\", data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           hp         disp  \n  -33.81283      1.28498      0.14936     -0.06545  \n\nDegrees of Freedom: 31 Total (i.e. Null);  28 Residual\nNull Deviance:      43.23 \nResidual Deviance: 10.15    AIC: 18.15\n\n\nIn this example, the ‘reformulate()’ function constructs a formula that defines the relationship between the response variable “am” and the predictor variables “mpg”, “hp”, and “disp”. The resulting formula is then passed to the glm() function for fitting a logistic regression model."
  },
  {
    "objectID": "posts/2023-06-14/index.html",
    "href": "posts/2023-06-14/index.html",
    "title": "Pulling a formula from a recipe object",
    "section": "",
    "text": "Introduction\nThe formula() function in R is a generic function that is used to create and manipulate formulas. Formulas are used to specify the relationship between variables in statistical models. The basic syntax for a formula is:\nresponse ~ predictors\nThe response is the variable that you are trying to predict, and the predictors are the variables that you are using to predict the response. You can use multiple predictors by separating them with + signs. For example, the following formula predicts the mpg (miles per gallon) of a car based on the wt (weight) and hp (horsepower) of the car:\nmpg ~ wt + hp\nThe formula() function can be used to create formulas from scratch, or it can be used to extract formulas from existing objects. For example, the following code creates a formula object called my_formula that predicts the mpg of a car based on the wt and hp of the car:\n\nmy_formula &lt;- formula(mpg ~ wt + hp)\nmy_formula\n\nmpg ~ wt + hp\n\n\nThe formula() function can also be used to manipulate formulas. For example, the following code adds a new predictor called drat (drive ratio) to the my_formula formula:\n\nmy_formula &lt;- update(my_formula, mpg ~ wt + hp + drat)\nmy_formula\n\nmpg ~ wt + hp + drat\n\n\nThe formula() function is a powerful tool that can be used to create, manipulate, and analyze formulas in R.\nHere are some additional things to know about the formula() function:\n\nFormulas are objects in R, and they have a number of methods that can be used to manipulate them. For example, you can use the summary() method to get a summary of a formula, or you can use the plot() method to plot a formula.\nFormulas can be used with a variety of statistical functions in R. For example, you can use the lm() function to fit a linear model to a formula, or you can use the glm() function to fit a generalized linear model to a formula.\nFormulas are a powerful tool for statistical analysis, and they can be used to solve a wide variety of problems. If you are working with data in R, it is important to understand how to use formulas.\n\nNow that we have a decent understanding of the function, I want to shift focus a little bit and show how we can use the generics function formula() in order to extract a formula from a recipe object.\nHere is the full code that we are going to look at:\n\nlibrary(recipes)\n\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\n\nsummary(rec_obj)\n\n# A tibble: 11 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 cyl      &lt;chr [2]&gt; predictor original\n 2 disp     &lt;chr [2]&gt; predictor original\n 3 hp       &lt;chr [2]&gt; predictor original\n 4 drat     &lt;chr [2]&gt; predictor original\n 5 wt       &lt;chr [2]&gt; predictor original\n 6 qsec     &lt;chr [2]&gt; predictor original\n 7 vs       &lt;chr [2]&gt; predictor original\n 8 am       &lt;chr [2]&gt; predictor original\n 9 gear     &lt;chr [2]&gt; predictor original\n10 carb     &lt;chr [2]&gt; predictor original\n11 mpg      &lt;chr [2]&gt; outcome   original\n\n# Get formula\nrec_obj |&gt; prep() |&gt; formula()\n\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n&lt;environment: 0x0000013e3255d2f0&gt;\n\n\nLet’s break down each line and understand what it does:\nlibrary(recipes)\nThe first line imports the recipes package, which is a powerful tool for preparing and preprocessing data in a structured and reproducible manner.\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\nHere, we create a recipe object named rec_obj. This object represents a set of instructions for data transformation. In this case, we specify the formula mpg ~ ., which means we want to predict the miles per gallon (mpg) using all other variables in the mtcars dataset.\nrec_obj |&gt; prep() |&gt; formula()\nThe next line leverages the magrittr pipe operator (|&gt;) to chain multiple operations. Let’s break it down:\n\nrec_obj is passed to the prep() function. This function performs data preparation steps specified in the recipe object, such as handling missing values, feature scaling, or encoding categorical variables.\nThe output of prep() is then piped to the formula() function, which extracts the formula representation from the preprocessed recipe object. The resulting formula can be used in subsequent modeling steps.\n\nThat’s it! With just a few lines of code, we have defined a recipe, prepared the data accordingly, and obtained the formula representation for further modeling.\nNow, let’s dive into a couple more examples to showcase the versatility of the recipes package:\n\nrec_obj &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_normalize(all_predictors())\n\nrec_obj |&gt; prep() |&gt; formula()\n\nSpecies ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\n&lt;environment: 0x0000013e2e8346f0&gt;\n\n\nIn this example, we create a recipe to predict the species (Species) using all other variables in the iris dataset. We then use the step_normalize() function to standardize all predictor variables in the recipe. This step ensures that variables are on a similar scale, which can be beneficial for certain machine learning algorithms.\nrec_obj &lt;- recipe(SalePrice ~ ., data = train_data) |&gt;\n  step_dummy(all_nominal(), -all_outcomes())\nHere, we define a recipe to predict the sale price (SalePrice) using all other variables in the train_data dataset. The step_dummy() function is used to convert all nominal variables in the recipe into dummy variables. The all_nominal() argument specifies that all variables should be considered, while the -all_outcomes() argument ensures that the outcome variable (SalePrice) is not transformed.\nThese examples provide a glimpse into the power and flexibility of the recipes package for data preprocessing in R. It enables you to define a clear and reproducible data transformation pipeline that can greatly simplify your machine learning workflows.\nHappy coding! 🚀"
  },
  {
    "objectID": "posts/2023-06-15/index.html",
    "href": "posts/2023-06-15/index.html",
    "title": "Introduction to Linear Regression in R: Analyzing the mtcars Dataset with lm()",
    "section": "",
    "text": "Introduction\nThe lm() function in R is used for fitting linear regression models. It stands for “linear model,” and it allows you to analyze the relationship between variables and make predictions based on the data.\nLet’s dive into the parameters of the lm() function:\n\nformula: This is the most important parameter, as it specifies the relationship between the variables. It follows a pattern: y ~ x1 + x2 + ..., where y is the response variable, and x1, x2, etc., are the predictor variables. For example, in the mtcars dataset, we can use the formula mpg ~ wt to predict the miles per gallon (mpg) based on the weight (wt) of the cars.\ndata: This parameter refers to the dataset you want to use for the analysis. In our case, we’ll use the mtcars dataset that comes with R.\n\nNow, let’s see some examples using the mtcars dataset\n\n\nExamples\nExample 1: Simple Linear Regression\n\n# Fit a linear regression model to predict mpg based on weight\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nExample 2: Multiple Linear Regression\n\n# Fit a linear regression model to predict mpg based on weight and horsepower\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nExample 3: Include Interaction Term\n\n# Fit a linear regression model to predict mpg based on weight, horsepower, and their interaction\nmodel &lt;- lm(mpg ~ wt + hp + wt:hp, data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp + wt:hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0632 -1.6491 -0.7362  1.4211  4.5513 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.80842    3.60516  13.816 5.01e-14 ***\nwt          -8.21662    1.26971  -6.471 5.20e-07 ***\nhp          -0.12010    0.02470  -4.863 4.04e-05 ***\nwt:hp        0.02785    0.00742   3.753 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.153 on 28 degrees of freedom\nMultiple R-squared:  0.8848,    Adjusted R-squared:  0.8724 \nF-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13\n\n\nThese examples demonstrate how to use the lm() function with different sets of predictor variables. After fitting the model, you can use the summary() function to get detailed information about the regression results, including coefficients, p-values, and R-squared values.\nI encourage you to try running these examples and explore different variables in the mtcars dataset. Feel free to modify the formulas and experiment with additional parameters to deepen your understanding of linear regression modeling in R!"
  },
  {
    "objectID": "posts/2023-06-08/index.html#explaining-the-file.info-function",
    "href": "posts/2023-06-08/index.html#explaining-the-file.info-function",
    "title": "Understanding the file.info() Function in R: Listing Files by Date",
    "section": "",
    "text": "The file.info() function returns a data frame with file information as its columns. Each row corresponds to a file, and the columns contain attributes such as the file size, permissions, and timestamps. This function accepts one or more file paths as its argument, providing flexibility in examining multiple files simultaneously. The following columns are returned in the data.frame that results from file.info():\n\nname: The name of the file.\nsize: The size of the file in bytes.\nmode: The mode of the file, which can be used to determine the file’s permissions.\nmtime: The modification time of the file.\nctime: The creation time of the file.\natime: The last access time of the file.\n\nIn order to get some data to work with, we will save the iris dataset as an excel file four times in a for loop, waiting 10 seconds between each save.\nlibrary(writexl)\n\n# Generate file names\nfile_prefix &lt;- \"iris\"\nfile_extension &lt;- \".xlsx\"\nnum_files &lt;- 4\n\n# Save iris dataset as Excel files\nfor (i in 1:num_files) {\n  file_name &lt;- paste0(file_prefix, \"_\", i, file_extension)\n  write_xlsx(iris, file_name)\n  cat(\"File\", file_name, \"saved successfully.\\n\")\n  Sys.sleep(10) # Sleep for 10 seconds then go again\n}"
  },
  {
    "objectID": "posts/2023-06-16/index.html",
    "href": "posts/2023-06-16/index.html",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, it’s crucial to have a deep understanding of the tools at your disposal. In the realm of data analysis and manipulation, R stands as a powerhouse. One function that proves to be invaluable in many scenarios is diff(). In this blog post, we will explore the ins and outs of the diff() function, showcasing its functionality and providing you with practical examples to enhance your programming skills."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-1-simple-vector",
    "href": "posts/2023-06-16/index.html#example-1-simple-vector",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 1: Simple Vector",
    "text": "Example 1: Simple Vector\nLet’s start with a straightforward example using a numeric vector:\n\n# Create a vector\nmy_vector &lt;- c(2, 5, 9, 12, 18)\n\n# Compute differences\ndiff_vector &lt;- diff(my_vector)\n\n# Display the result\ndiff_vector\n\n[1] 3 4 3 6\n\n\nIn this example, the diff() function calculates the differences between consecutive elements in my_vector. The resulting vector, diff_vector, shows the differences [3, 4, 3, 6]."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-2-time-series-data",
    "href": "posts/2023-06-16/index.html#example-2-time-series-data",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 2: Time Series Data",
    "text": "Example 2: Time Series Data\nThe diff() function is particularly handy when working with time series data. Let’s consider a time series dataset representing monthly sales:\n\n# Create a time series\nmonthly_sales &lt;- c(150, 200, 180, 250, 300, 270, 350)\n\n# Compute month-to-month differences\nmonthly_diff &lt;- diff(monthly_sales)\n\n# Display the result\nmonthly_diff\n\n[1]  50 -20  70  50 -30  80\n\n\nHere, the diff() function calculates the changes in sales between consecutive months. The resulting vector, monthly_diff, displays the differences [50, -20, 70, 50, -30, 80]."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-3-advanced-applications",
    "href": "posts/2023-06-16/index.html#example-3-advanced-applications",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 3: Advanced Applications",
    "text": "Example 3: Advanced Applications\nBeyond simple differences, the diff() function can be combined with other R functions to solve more complex problems. Let’s say we have a vector representing the daily closing prices of a stock:\n\n# Create a vector of stock prices\nstock_prices &lt;- c(105.2, 103.9, 105.8, 107.5, 109.1)\n\n# Compute daily price changes as percentages\ndaily_returns &lt;- diff(stock_prices) / stock_prices[-length(stock_prices)] * 100\n\n# Display the result\ndaily_returns\n\n[1] -1.235741  1.828681  1.606805  1.488372\n\n\nIn this example, we calculate the daily returns as a percentage by taking the differences between consecutive closing prices and dividing them by the previous day’s closing price. The resulting vector, daily_returns, represents the daily percentage changes."
  },
  {
    "objectID": "posts/2023-06-16/index.html#example-4-miscellaneous-examples",
    "href": "posts/2023-06-16/index.html#example-4-miscellaneous-examples",
    "title": "Mastering the Power of R’s diff() Function: A Programmer’s Guide",
    "section": "Example 4: Miscellaneous Examples",
    "text": "Example 4: Miscellaneous Examples\n\nx &lt;- rnorm(10)\n\n# Calculate the first-order difference of a vector\ndiff(x)\n\n[1] -0.5814577  0.5824454  1.0677214 -0.7505515  0.9924554 -2.0034078  0.5492343\n[8] -1.8906742  1.1942760\n\n# Calculate the second-order difference of a vector\ndiff(x, differences=2)\n\n[1]  1.163903  0.485276 -1.818273  1.743007 -2.995863  2.552642 -2.439908\n[8]  3.084950\n\n# Calculate the first-order difference of a matrix\ndiff(x, lag=1, differences=1)\n\n[1] -0.5814577  0.5824454  1.0677214 -0.7505515  0.9924554 -2.0034078  0.5492343\n[8] -1.8906742  1.1942760\n\n# Calculate the second-order difference of a matrix\ndiff(x, lag=1, differences=2)\n\n[1]  1.163903  0.485276 -1.818273  1.743007 -2.995863  2.552642 -2.439908\n[8]  3.084950"
  },
  {
    "objectID": "posts/2023-06-20/index.html",
    "href": "posts/2023-06-20/index.html",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "",
    "text": "As a programmer, you’re constantly faced with the task of organizing and analyzing data. One powerful tool in your R arsenal is the xtabs() function. In this blog post, we’ll explore the versatility and simplicity of xtabs() for aggregating data. We’ll use the mtcars dataset and the healthyR.data::healthyR_data dataset to illustrate its functionality. Get ready to dive into the world of data aggregation with xtabs()!"
  },
  {
    "objectID": "posts/2023-06-20/index.html#example-1-analyzing-car-performance-with-mtcars-dataset",
    "href": "posts/2023-06-20/index.html#example-1-analyzing-car-performance-with-mtcars-dataset",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "Example 1: Analyzing Car Performance with mtcars Dataset",
    "text": "Example 1: Analyzing Car Performance with mtcars Dataset\nLet’s start with the mtcars dataset, which contains information about various car models. Suppose we want to understand the distribution of cars based on the number of cylinders and the transmission type. We can use xtabs() to accomplish this:\n\n# Create a contingency table using xtabs()\ntable_cars &lt;- xtabs(~ cyl + am, data = mtcars)\n\n# View the resulting table\ntable_cars\n\n   am\ncyl  0  1\n  4  3  8\n  6  4  3\n  8 12  2\n\n\nIn this example, the formula ~ cyl + am specifies that we want to cross-tabulate the “cyl” (number of cylinders) variable with the “am” (transmission type) variable. The resulting table provides a clear breakdown of car counts based on these two factors.\nThe xtabs() function also allows you to specify the order of the variables in the formula. For example, the following formula would create the same contingency table as the previous formula, but the rows of the table would be ordered by the number of cylinders in the car:\n\nxtabs(~am + cyl, data = mtcars)\n\n   cyl\nam   4  6  8\n  0  3  4 12\n  1  8  3  2"
  },
  {
    "objectID": "posts/2023-06-20/index.html#example-2-analyzing-health-data-with-healthyr.data",
    "href": "posts/2023-06-20/index.html#example-2-analyzing-health-data-with-healthyr.data",
    "title": "Mastering Data Aggregation with xtabs() in R",
    "section": "Example 2: Analyzing Health Data with healthyR.data",
    "text": "Example 2: Analyzing Health Data with healthyR.data\nLet’s now explore the healthyR.data::healthyR_data dataset, which is a simulated administrative dataset. Suppose we’re interested in analyzing the distribution of patients’ insurance type based on their type of stay. Here’s how we can use xtabs() for this analysis:\n\n# Load the dataset\nlibrary(healthyR.data)\n\n# Create a contingency table using xtabs()\ntable_health &lt;- xtabs(~ payer_grouping + ip_op_flag, data = healthyR_data)\n\n# View the resulting table\ntable_health\n\n                ip_op_flag\npayer_grouping       I     O\n  ?                  1     0\n  Blue Cross     10797 13560\n  Commercial      3328  3239\n  Compensation     787  1715\n  Exchange Plans  1206  1194\n  HMO             8113  9331\n  Medicaid        7131  1646\n  Medicaid HMO   15466 10018\n  Medicare A     52621     1\n  Medicare B       293 22270\n  Medicare HMO   13572  5425\n  No Fault        1713   645\n  Self Pay        2089  1560\n\n\nIn this example, the formula ~ payer_grouping + ip_op_flag specifies that we want to cross-tabulate the “payer_grouping” variable with the “ip_op_flag” variable. By using xtabs(), we obtain a comprehensive summary of patients’ insurance type and their stay type."
  },
  {
    "objectID": "posts/2023-06-21/index.html",
    "href": "posts/2023-06-21/index.html",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "",
    "text": "Sampling is a fundamental technique in data analysis and statistical modeling. It allows us to draw meaningful insights and make inferences about a larger population based on a representative subset. In the world of R programming, the sample() function stands as a versatile tool that enables us to create random samples efficiently. In this post, we will explore the sample() function and its various applications through a series of plain English examples.\nFirst, let’s take a look at the syntax:\nsample(x, size, replace = FALSE, prob = NULL)\nwhere:\n\nx is the dataset or vector from which to take the sample\nsize is the number of elements to include in the sample\nreplace is a logical value that indicates whether or not to allow sampling with replacement (the default is FALSE)\nprob is a vector of probabilities that can be used to weight the sample (the default is NULL)"
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-1-simple-random-sampling",
    "href": "posts/2023-06-21/index.html#example-1-simple-random-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 1: Simple Random Sampling",
    "text": "Example 1: Simple Random Sampling\nLet’s say we have a dataset containing the ages of 100 people. To create a random sample of 10 individuals, we can use the sample() function as follows:\n\nages &lt;- 1:100\nrandom_sample &lt;- sample(ages, size = 10)\nrandom_sample\n\n [1] 53 13 84 50 55  9 12 38 79 15\n\n\nThe sample() function randomly selects 10 values from the ages vector, without replacement, resulting in a new vector named random_sample. This technique represents simple random sampling, where each individual in the population has an equal chance of being included in the sample."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-2-sampling-with-replacement",
    "href": "posts/2023-06-21/index.html#example-2-sampling-with-replacement",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 2: Sampling with Replacement",
    "text": "Example 2: Sampling with Replacement\nIn some scenarios, we might want to allow repeated selections from the population. Let’s say we have a bag with colored balls, and we want to simulate drawing 5 balls with replacement. Here’s how we can achieve it:\n\ncolors &lt;- c(\"red\", \"blue\", \"green\", \"yellow\")\nsample_with_replacement &lt;- sample(colors, size = 5, replace = TRUE)\nsample_with_replacement\n\n[1] \"yellow\" \"yellow\" \"green\"  \"green\"  \"red\"   \n\n\nThe sample() function, with the replace = TRUE argument, enables us to randomly select 5 colors from the colors vector, allowing duplicates. This approach represents sampling with replacement, where each selection is independent of the previous ones."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-3-weighted-sampling",
    "href": "posts/2023-06-21/index.html#example-3-weighted-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 3: Weighted Sampling",
    "text": "Example 3: Weighted Sampling\nIn certain situations, we may want to assign different probabilities to elements in the population. Let’s assume we have a list of items and corresponding weights denoting their probabilities of being selected. We can use the sample() function with the prob parameter to achieve weighted sampling. Consider the following example:\n\nlibrary(dplyr)\n\nitems &lt;- c(\"apple\", \"banana\", \"orange\")\nweights &lt;- c(0.4, 0.2, 0.4)\nweighted_sample &lt;- sample(items, size = 1, prob = weights)\nweighted_sample\n\n[1] \"apple\"\n\ntibble(x = 1:10) |&gt; \n  group_by(x) |&gt; \n  mutate(rs = sample(items, size = 1, prob = weights)) |&gt;\n  ungroup()\n\n# A tibble: 10 × 2\n       x rs    \n   &lt;int&gt; &lt;chr&gt; \n 1     1 orange\n 2     2 apple \n 3     3 apple \n 4     4 apple \n 5     5 apple \n 6     6 orange\n 7     7 orange\n 8     8 orange\n 9     9 apple \n10    10 orange\n\n\nBy specifying the prob argument with the corresponding weights, the sample() function randomly selects a single item from the items vector. The probability of each item being chosen is proportional to its weight. In this case, “apple” and “orange” have a higher chance (40% each) of being selected compared to “banana” (20%)."
  },
  {
    "objectID": "posts/2023-06-21/index.html#example-4-stratified-sampling",
    "href": "posts/2023-06-21/index.html#example-4-stratified-sampling",
    "title": "Unleashing the Power of Sampling in R: Exploring the Versatile sample() Function",
    "section": "Example 4: Stratified Sampling",
    "text": "Example 4: Stratified Sampling\nStratified sampling involves dividing the population into subgroups or strata and then sampling from each stratum proportionally. Let’s assume we have a dataset of students’ grades in different subjects, and we want to select a sample that maintains the proportion of students from each subject. We can achieve this using the sample() function along with additional parameters. Consider the following example:\n\nsubjects &lt;- c(\"Math\", \"Science\", \"English\", \"History\")\ngrades &lt;- c(80, 90, 85, 70, 75, 95, 60, 92, 88, 83, 78, 91)\nstrata &lt;- factor(subjects)\nstratified_sample &lt;- unlist(\n  by(\n    grades, \n    rep(strata, 3), \n    FUN = function(x) sample(x, size = 2)\n    )\n  )\nstratified_sample\n\nEnglish1 English2 History1 History2    Math1    Math2 Science1 Science2 \n      78       60       92       91       80       75       90       95 \n\n\nIn this example, we use the by() function to group the grades by subject (strata). Then, we apply the sample() function to each subgroup (subject) using the FUN argument. The result is a stratified sample of two grades from each subject, maintaining the relative proportions of students in the final sample."
  },
  {
    "objectID": "posts/2023-06-22/index.html",
    "href": "posts/2023-06-22/index.html",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, you’re constantly faced with the need to repeat tasks efficiently. Repetition is a fundamental concept in programming, and R provides a powerful tool to accomplish this: the rep() function. In this blog post, we will explore the syntax of the rep() function and delve into several examples to showcase its versatility and practical applications. Whether you’re working with data manipulation, generating sequences, or creating repeated patterns, rep() will become your go-to function for mastering repetition in R."
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-1-repeating-a-single-value",
    "href": "posts/2023-06-22/index.html#example-1-repeating-a-single-value",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 1: Repeating a Single Value",
    "text": "Example 1: Repeating a Single Value\nLet’s start with a simple example. Suppose we want to repeat the value 5 three times. We can achieve this using the following code:\n\nresult &lt;- rep(5, times = 3)\nprint(result)\n\n[1] 5 5 5"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-2-replicating-a-vector",
    "href": "posts/2023-06-22/index.html#example-2-replicating-a-vector",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 2: Replicating a Vector",
    "text": "Example 2: Replicating a Vector\nThe rep() function can also replicate entire vectors. Consider the following example where we replicate the vector c(1, 2, 3) four times:\n\nvector &lt;- c(1, 2, 3)\nresult &lt;- rep(vector, times = 4)\nprint(result)\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-3-repeating-elements-using-each",
    "href": "posts/2023-06-22/index.html#example-3-repeating-elements-using-each",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 3: Repeating Elements Using ‘each’",
    "text": "Example 3: Repeating Elements Using ‘each’\nThe each argument allows us to repeat each element of a vector a specific number of times. Let’s illustrate this with the following example:\n\nvector &lt;- c(1, 2, 3)\nresult &lt;- rep(vector, times = 2, each = 2)\nprint(result)\n\n [1] 1 1 2 2 3 3 1 1 2 2 3 3"
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-4-creating-repeated-patterns",
    "href": "posts/2023-06-22/index.html#example-4-creating-repeated-patterns",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 4: Creating Repeated Patterns",
    "text": "Example 4: Creating Repeated Patterns\nOne interesting use case of the rep() function is to create repeated patterns. Consider this example, where we want to generate a pattern of “ABABAB” ten times:\n\npattern &lt;- rep(c(\"A\", \"B\"), times = 10)\nresult &lt;- paste(pattern, collapse = \"\")\nprint(result)\n\n[1] \"ABABABABABABABABABAB\""
  },
  {
    "objectID": "posts/2023-06-22/index.html#example-5-expanding-factors-or-categories",
    "href": "posts/2023-06-22/index.html#example-5-expanding-factors-or-categories",
    "title": "Mastering Repetition with R’s rep() Function: A Programmer’s Guide",
    "section": "Example 5: Expanding Factors or Categories",
    "text": "Example 5: Expanding Factors or Categories\nThe rep() function is useful for expanding factors or categories. Let’s say we have a factor with three levels, and we want to replicate each level four times:\n\nfactor &lt;- factor(c(\"low\", \"medium\", \"high\"))\nresult &lt;- rep(factor, times = 4)\nprint(result)\n\n [1] low    medium high   low    medium high   low    medium high   low   \n[11] medium high  \nLevels: high low medium"
  },
  {
    "objectID": "posts/2023-06-23/index.html",
    "href": "posts/2023-06-23/index.html",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "",
    "text": "Bootstrap resampling is a powerful technique used in statistics and data analysis to estimate the uncertainty of a statistic by repeatedly sampling from the original data. In R, we can easily implement a bootstrap function using the lapply, rep, and sample functions. In this blog post, we will explore how to write a bootstrap function in R and provide an example using the “mpg” column from the popular “mtcars” dataset."
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-1-load-the-required-dataset",
    "href": "posts/2023-06-23/index.html#step-1-load-the-required-dataset",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 1: Load the required dataset",
    "text": "Step 1: Load the required dataset\nLet’s begin by loading the “mtcars” dataset, which is included in the base R package:\n\ndata(mtcars)"
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-2-define-the-bootstrap-function",
    "href": "posts/2023-06-23/index.html#step-2-define-the-bootstrap-function",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 2: Define the bootstrap function",
    "text": "Step 2: Define the bootstrap function\nWe’ll define a function called bootstrap() that takes two arguments: data (the input data vector) and n (the number of bootstrap iterations).\n\nbootstrap &lt;- function(data, n) {\n  resampled_data &lt;- lapply(1:n, function(i) {\n    resample &lt;- sample(data, replace = TRUE)\n    # Perform desired operations on the resampled data, e.g., compute a statistic\n    # and return the result\n  })\n  return(resampled_data)\n}\n\nbootstrapped_samples &lt;- bootstrap(mtcars$mpg, 5)\nbootstrapped_samples\n\n[[1]]\n [1] 21.0 18.1 33.9 21.4 17.3 19.2 19.2 15.8 16.4 30.4 18.1 14.3 32.4 10.4 15.0\n[16] 16.4 30.4 17.8 21.4 19.2 17.3 22.8 14.3 22.8 30.4 18.7 13.3 13.3 15.2 10.4\n[31] 15.0 13.3\n\n[[2]]\n [1] 18.7 32.4 21.0 10.4 15.0 14.7 24.4 10.4 32.4 10.4 21.0 19.7 21.4 10.4 30.4\n[16] 17.3 10.4 22.8 15.2 15.2 21.4 15.8 21.4 33.9 24.4 15.2 18.1 19.2 21.0 24.4\n[31] 15.5 21.0\n\n[[3]]\n [1] 15.5 30.4 21.0 22.8 27.3 18.1 21.0 13.3 15.2 17.3 15.8 21.0 18.1 14.3 17.8\n[16] 15.8 21.0 18.1 19.2 24.4 19.2 22.8 18.7 14.3 26.0 21.4 22.8 32.4 14.7 15.2\n[31] 15.2 14.3\n\n[[4]]\n [1] 13.3 21.0 13.3 15.0 19.2 18.1 18.1 19.2 22.8 18.7 26.0 21.4 14.7 14.3 17.8\n[16] 22.8 19.7 21.4 30.4 30.4 18.7 17.3 16.4 21.5 18.1 21.0 17.8 21.4 14.3 19.7\n[31] 32.4 18.7\n\n[[5]]\n [1] 15.0 21.4 21.5 26.0 17.3 30.4 18.1 17.8 17.3 30.4 24.4 32.4 21.0 17.8 33.9\n[16] 32.4 19.2 22.8 19.7 16.4 17.8 22.8 14.3 33.9 21.5 10.4 21.4 26.0 33.9 14.7\n[31] 21.5 18.1\n\n\nIn the above code, we use lapply to generate a list of n resampled datasets. Inside the lapply function, we use the sample function to randomly sample from the original data with replacement (replace = TRUE). This ensures that each resampled dataset has the same length as the original dataset."
  },
  {
    "objectID": "posts/2023-06-23/index.html#step-3-perform-desired-operations-on-resampled-data",
    "href": "posts/2023-06-23/index.html#step-3-perform-desired-operations-on-resampled-data",
    "title": "Bootstrap Function in R: Resampling with the lapply and sample Functions",
    "section": "Step 3: Perform desired operations on resampled data",
    "text": "Step 3: Perform desired operations on resampled data\nWithin the lapply function, you can perform any desired operations on the resampled data. This could involve calculating statistics, fitting models, or conducting hypothesis tests. Customize the code within the lapply function to suit your specific needs.\nExample: Bootstrapping the “mpg” column in mtcars: Let’s illustrate the usage of our bootstrap function by resampling the “mpg” column from the “mtcars” dataset. We will calculate the mean of the resampled datasets.\n\n# Step 1: Load the dataset\ndata(mtcars)\n\n# Step 2: Define the bootstrap function\nbootstrap &lt;- function(data, n) {\n  resampled_data &lt;- lapply(1:n, function(i) {\n    resample &lt;- sample(data, replace = TRUE)\n    mean(resample)  # Calculate the mean of each resampled dataset\n  })\n  return(resampled_data)\n}\n\n# Step 3: Perform the bootstrap resampling\nbootstrapped_means &lt;- bootstrap(mtcars$mpg, n = 1000)\n\n# Display the first few resampled means\nhead(bootstrapped_means)\n\n[[1]]\n[1] 20.21562\n\n[[2]]\n[1] 20.09375\n\n[[3]]\n[1] 19.59375\n\n[[4]]\n[1] 20.13437\n\n[[5]]\n[1] 21.17813\n\n[[6]]\n[1] 21.5375\n\n\nIn the above example, we resample the “mpg” column of the “mtcars” dataset 1000 times. The bootstrap() function calculates the mean of each resampled dataset and returns a list of resampled means. The head() function is then used to display the first few resampled means.\nOf course we do not have to specify a statistic function in the bootstrap, we can choose to just return bootstrap samples and then perform some sort of statistic on it. Look at the following example using the above bootstrapped_samples data.\n\nquantile(unlist(bootstrapped_samples), \n         probs = c(0.025, 0.25, 0.5, 0.75, 0.975))\n\n  2.5%    25%    50%    75%  97.5% \n10.400 15.725 19.200 22.800 33.900 \n\nmean(unlist(bootstrapped_samples))\n\n[1] 20.06625\n\nsd(unlist(bootstrapped_samples))\n\n[1] 5.827239"
  },
  {
    "objectID": "posts/2023-06-26/index.html",
    "href": "posts/2023-06-26/index.html",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "",
    "text": "Welcome to the world of data visualization in R! In this blog post, we will explore the abline() function, a versatile tool that allows you to add straight lines to your plots effortlessly. Whether you’re a beginner or an experienced R programmer, mastering abline() will empower you to create more informative and visually appealing graphs. So, let’s dive in!"
  },
  {
    "objectID": "posts/2023-06-26/index.html#example-1.-simple-linear-regression-line",
    "href": "posts/2023-06-26/index.html#example-1.-simple-linear-regression-line",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Example 1. Simple Linear Regression Line:",
    "text": "Example 1. Simple Linear Regression Line:\nLet’s start with a classic example of drawing a linear regression line on a scatter plot. Consider the following data:\n\nx &lt;- 1:10\ny &lt;- c(2, 3, 5, 7, 9, 10, 13, 15, 17, 19)\n\nTo visualize the relationship between x and y, we can plot the points and add a regression line using abline():\n\nplot(x, y, main = \"Linear Regression Example\", xlab = \"x\", ylab = \"y\")\nabline(lm(y ~ x), col = \"red\")"
  },
  {
    "objectID": "posts/2023-06-26/index.html#examle-2.-custom-slope-and-intercept",
    "href": "posts/2023-06-26/index.html#examle-2.-custom-slope-and-intercept",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Examle 2. Custom Slope and Intercept",
    "text": "Examle 2. Custom Slope and Intercept\nThe abline() function allows you to specify custom slope and intercept values. Suppose you have a dataset where y increases by 3 for every unit increase in x. We can draw a line with a slope of 3 and an intercept of 0 using the following code:\n\nplot(x, y, main = \"Custom Slope and Intercept\", xlab = \"x\", ylab = \"y\")\nabline(a = 0, b = 3, col = \"blue\")"
  },
  {
    "objectID": "posts/2023-06-26/index.html#example-3.-vertical-and-horizontal-lines",
    "href": "posts/2023-06-26/index.html#example-3.-vertical-and-horizontal-lines",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Example 3. Vertical and Horizontal Lines:",
    "text": "Example 3. Vertical and Horizontal Lines:\nabline() isn’t limited to just diagonal lines; you can also draw vertical and horizontal lines. For instance, let’s draw a vertical line at x = 5 and a horizontal line at y = 12:\n\nplot(x, y, main = \"Vertical and Horizontal Lines\", xlab = \"x\", ylab = \"y\")\nabline(v = 5, col = \"green\") # Vertical line\nabline(h = 12, col = \"orange\") # Horizontal line"
  },
  {
    "objectID": "posts/2023-06-26/index.html#encouragement-to-try-it-yourself",
    "href": "posts/2023-06-26/index.html#encouragement-to-try-it-yourself",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Encouragement to Try It Yourself",
    "text": "Encouragement to Try It Yourself\nNow that you’ve seen a few examples of what the abline() function can do, I encourage you to unleash your creativity and explore its full potential. Experiment with different datasets, slopes, intercepts, and line styles. The more you practice, the more comfortable you will become with this powerful visualization tool."
  },
  {
    "objectID": "posts/2023-06-26/index.html#conclusion",
    "href": "posts/2023-06-26/index.html#conclusion",
    "title": "Visualization in R: Unleashing the Power of the abline() Function",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we delved into the abline() function in R, exploring its capabilities for adding straight lines to plots. We covered simple linear regression lines, custom slopes and intercepts, as well as vertical and horizontal lines. Armed with this knowledge, you can enhance your data visualizations, making them more informative and engaging. So, go ahead, give abline() a try, and unlock a whole new world of possibilities in R programming! Happy coding!"
  },
  {
    "objectID": "posts/2023-06-27/index.html",
    "href": "posts/2023-06-27/index.html",
    "title": "The ave() Function in R",
    "section": "",
    "text": "In the world of data analysis and statistics, grouping data based on certain criteria is a common task. Whether you’re working with large datasets or analyzing trends within smaller subsets, having a reliable and efficient tool for data grouping can make your life as a programmer much easier. In this blog post, we’ll dive into the R function ave() and explore how it can help you achieve seamless data grouping and computation."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-1-computing-average-sales-by-region",
    "href": "posts/2023-06-27/index.html#example-1-computing-average-sales-by-region",
    "title": "The ave() Function in R",
    "section": "Example 1: Computing Average Sales by Region",
    "text": "Example 1: Computing Average Sales by Region\nLet’s consider a dataset containing sales data for different regions. We’ll use ave() to calculate the average sales for each region.\n\nsales &lt;- data.frame(\n  region = c(\"North\", \"South\", \"North\", \"East\", \"South\", \"East\"),\n  sales = c(500, 700, 600, 450, 800, 550)\n)\n\nsales$avg_sales &lt;- ave(sales$sales, sales$region)\nsales[order(sales$region),]\n\n  region sales avg_sales\n4   East   450       500\n6   East   550       500\n1  North   500       550\n3  North   600       550\n2  South   700       750\n5  South   800       750\n\n\nIn this example, we create a new column called avg_sales and assign the output of ave() to it. The resulting dataset will include the average sales for each region, as computed by ave()."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-2-calculating-median-age-by-gender",
    "href": "posts/2023-06-27/index.html#example-2-calculating-median-age-by-gender",
    "title": "The ave() Function in R",
    "section": "Example 2: Calculating Median Age by Gender",
    "text": "Example 2: Calculating Median Age by Gender\nLet’s explore another scenario where we have a dataset containing information about individuals’ ages and genders. We’ll use ave() to calculate the median age for each gender category.\n\npeople &lt;- data.frame(\n  age = c(32, 28, 35, 40, 26, 30),\n  gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\")\n)\n\npeople$median_age &lt;- ave(people$age, people$gender, FUN = median)\npeople[order(people$gender),]\n\n  age gender median_age\n2  28 Female         30\n4  40 Female         30\n6  30 Female         30\n1  32   Male         32\n3  35   Male         32\n5  26   Male         32\n\n\nIn this example, we introduce the FUN argument to specify the median() function. ave() will compute the median age for each gender category and assign the values to the new column median_age."
  },
  {
    "objectID": "posts/2023-06-27/index.html#example-3-finding-maximum-temperature-by-month",
    "href": "posts/2023-06-27/index.html#example-3-finding-maximum-temperature-by-month",
    "title": "The ave() Function in R",
    "section": "Example 3: Finding Maximum Temperature by Month",
    "text": "Example 3: Finding Maximum Temperature by Month\nLet’s say we have a weather dataset containing temperature readings for different months. We can use ave() to calculate the maximum temperature recorded for each month.\n\nweather &lt;- data.frame(\n  month = rep(c(\"Jan\", \"Feb\", \"Mar\"), each = 4),\n  temperature = c(15, 18, 20, 14, 16, 22, 25, 23, 19, 21, 24, 20)\n)\n\nweather$max_temp &lt;- ave(weather$temperature, weather$month, FUN = max)\nweather\n\n   month temperature max_temp\n1    Jan          15       20\n2    Jan          18       20\n3    Jan          20       20\n4    Jan          14       20\n5    Feb          16       25\n6    Feb          22       25\n7    Feb          25       25\n8    Feb          23       25\n9    Mar          19       24\n10   Mar          21       24\n11   Mar          24       24\n12   Mar          20       24\n\n\nIn this example, we use ave() to compute the maximum temperature for each month, and the resulting values are assigned to the new column max_temp."
  },
  {
    "objectID": "posts/2023-06-28/index.html",
    "href": "posts/2023-06-28/index.html",
    "title": "Exploring Rolling Correlation with the rollapply Function: A Powerful Tool for Analyzing Time-Series Data",
    "section": "",
    "text": "Introduction\nIn the world of data analysis, time-series data is a common sight. Whether it’s stock prices, weather patterns, or website traffic, understanding the relationship between variables over time is crucial. One valuable technique in this domain is calculating rolling correlation, which allows us to examine the evolving correlation between two variables as our data moves through time. In this blog post, we will delve into the rollapply function and its capabilities, exploring its applications through a series of practical examples. So, let’s get started!\n\n\nUnderstanding Rolling Correlation\nBefore we jump into the technical details, let’s quickly recap what correlation means. In simple terms, correlation measures the strength and direction of the linear relationship between two variables. It ranges between -1 and 1, where -1 represents a perfect negative correlation, 1 represents a perfect positive correlation, and 0 represents no correlation.\nRolling correlation takes this concept further by calculating correlation values over a moving window of observations. By doing so, we can observe how the correlation between two variables changes over time, gaining insights into trends, seasonality, or other patterns in our data.\n\n\nIntroducing the rollapply Function\nIn R programming, the rollapply function, available in the zoo package, is a powerful tool for calculating rolling correlation. It enables us to apply a function, such as correlation, to a rolling window of our data. The general syntax for using rollapply is as follows:\nrollapply(data, width, FUN, ...)\nHere’s what each parameter represents: - data: The time-series data we want to analyze. - width: The size of the rolling window, indicating how many observations should be included in each correlation calculation. - FUN: The function we want to apply to each rolling window. In this case, we will use the cor function to calculate correlation. - ...: Additional arguments that can be passed to the correlation function or any other function used with rollapply.\nNow, let’s dive into some practical examples to see the rollapply function in action.\n\n\nExample\nImagine we have a dataset containing daily stock prices for two companies, A and B. Our goal is to explore the rolling correlation between the returns of these two stocks over a 30-day window.\n\nlibrary(zoo)\nlibrary(tidyquant)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndf &lt;- FANG |&gt; \n  filter(symbol %in% c(\"FB\", \"AMZN\")) |&gt; \n  select(symbol, adjusted) |&gt; \n  pivot_wider(values_from = adjusted, names_from = symbol) |&gt;\n  unnest()\n\nfb_rets &lt;- diff(log(df$FB))\namzn_rets &lt;- diff(log(df$AMZN))\ndf_rets &lt;- cbind(fb_rets, amzn_rets)\ncorrelation &lt;- rollapply(\n  df_rets, \n  width = 5, \n  function(x) cor(x[,1], x[,2]), \n  by.column = FALSE\n  )\n\nplot(correlation, type=\"l\")\n\n\n\n\nIn this example, we calculate the logarithmic returns of FB and AMZN using the diff function. Then, we apply the cor function to the rolling window of returns, with a width of 5. The by.column = FALSE parameter ensures that the correlation is computed across rows instead of columns, and the fill = NA parameter fills any incomplete windows with NA values.\n\n\nConclusion\nIn this blog post, we explored the concept of rolling correlation and its significance in analyzing time-series data. We learned how to harness the power of the rollapply function from the zoo package to calculate rolling correlation effortlessly. By utilizing rollapply, we can observe the dynamic nature of correlation, uncover trends, and gain valuable insights from our time-dependent datasets.\nRemember, rolling correlation is just one of the many applications of the rollapply function. Its versatility empowers us to explore various other statistics, such as moving averages, standard deviations, and more. So, dive into the world of time-series analysis with rollapply and unlock the hidden patterns in your data!\nHappy coding and happy analyzing!"
  },
  {
    "objectID": "posts/2023-06-29/index.html",
    "href": "posts/2023-06-29/index.html",
    "title": "How to Use a Windows .bat File to Execute an R Script",
    "section": "",
    "text": "Introduction\nUsing a Windows .bat file to execute an R script can be a convenient way to automate tasks and streamline your workflow. In this blog post, we will explain each line of a sample .bat file and its corresponding R script, along with a simple explanation of what each section does.\n\nThe .bat File:\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\nNow, let’s break down each line:\n\n@echo off: This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem Set the path to the Rscript executable: The rem keyword denotes a comment in a batch file. This line sets the path to the Rscript executable, which is the command-line interface for executing R scripts.\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\": This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nrem Set the path to the R script to execute: This line is another comment, specifying that the next line sets the path to the R script that will be executed.\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\": Here, the path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE%: This line executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nrem Pause so the user can see the output: This comment suggests that the script should pause after execution so that the user can view the output before the command prompt window closes.\nexit: This command exits the batch file and closes the command prompt window.\n\n\n\nThe R Script:\nThe R script contains several sections. Here is the full script and then I will give an explanation of each section:\n# Library Load\nlibrary(DBI)\nlibrary(odbc)\nlibrary(dplyr)\nlibrary(writexl)\nlibrary(stringr)\nlibrary(Microsoft365R)\nlibrary(glue)\nlibrary(blastula)\n\n# Source SSMS Connection Functions \nsource(\"C:/Path/to/SQL_Connection_Functions.r\")\n\n# Connect to SSMS\ndbc &lt;- db_connect()\n\n# Query SSMS\nquery &lt;- DBI::dbGetQuery(\n  conn = dbc,\n  statement = paste0(\n    \"\n    select encounter,\n        pt_no \n    from dbo.c_xfer_fac_tbl \n    where encounter in \n        (\n        select distinct encounter\n        from DBO.c_xfer_fac_tbl \n        group by encounter, file_name \n        having Count(Distinct pt_no) &gt; 1\n        ) \n        and INSERT_DATETIME = \n        (\n        select Max(INSERT_DATETIME) \n        from dbo.c_xfer_fac_tbl\n        ) \n    group by encounter, pt_no \n    order by encounter\n    \"\n  )\n)\n\ndb_disconnect(dbc)\n\n# Save file to disk\npath &lt;- \"C:/Path/to/files/encounter_duplicates/\"\nf_name &lt;- \"Encounter_Duplicates_\"\nf_date &lt;- Sys.time() |&gt; \n  str_replace_all(pattern = \"[-|:]\",\"\") |&gt;\n  str_replace(pattern = \"[ ]\", \"_\")\nfull_file_name &lt;- paste0(f_name, f_date, \".xlsx\")\nfpn &lt;- paste0(path, full_file_name)\n\nwrite_xlsx(\n  x = query,\n  path = fpn\n)\n\n# Compose Email ----\n# Open Outlook\nOutlook &lt;- get_business_outlook()\n\nemail_body &lt;- md(glue(\n\"\n  ## Important!\n  \n  Please see attached file {full_file_name}\n  \n  The file attached contains a list of accounts from Hospital B\n  that have two or more Hospital A account numbers associated with them. We therefore\n  cannot process these accounts.\n  \n  Thank you,\n\n  The Team\n  \"\n))\n\nemail_template &lt;- compose_email(\n  body = email_body,\n  footer = md(\"sent via Microsoft365R and The Team\")\n)\n\n# Create Email\nOutlook$create_email(email_template)$\n  #set_body(email_body, content_type=\"html\")$\n  set_recipients(to=c(\"email1@email.com\", \"email2@email.com\"))$\n  set_subject(\"Encounter Duplicates\")$\n  add_attachment(fpn)$\n  send()\n\n# Archive File after it has been sent\narchive_path &lt;- \"C:/Path/to/Encounter_Duplicate_Files/Sent/\"\nmove_to_path &lt;- paste0(archive_path, full_file_name)\nfile.rename(\n  from = fpn,\n  to = move_to_path\n)\n\n# Clear the Session\nrm(list = ls())\n\nLibrary Load: This section loads various R libraries needed for the script’s functionality, such as database connections, data manipulation, and email composition.\nSource SSMS Connection Functions: Here, a separate R script file (SQL_Connection_Functions.r) is sourced. This file likely contains custom functions related to connecting to and querying a SQL Server Management System (SSMS) database.\nConnect to SSMS: This line establishes a connection to the SSMS database using the db_connect() function.\nQuery SSMS: The script executes a SQL query against the SSMS database using the dbGetQuery() function. The result of the query is assigned to the query variable.\nSave file to disk: The script saves the query result (query) to an Excel file on the local disk using the write_xlsx() function.\nCompose Email: This section composes an email using the blastula package, preparing the email body and setting the recipients, subject, and\n\nattachments.\n\nCreate Email: The composed email is created using the create_email() function from the Microsoft365R package. The body, recipients, subject, and attachment are set.\nSend Email: The email is sent using the send() function, which relies on a connection to Microsoft Outlook. The email body, recipients, subject, and attachment are all included in the email.\nArchive File after it has been sent: The script moves the Excel file to an archive folder after sending the email, using the file.rename() function.\nClear the Session: The rm() function is used to clear the current R session, removing any remaining objects from memory.\n\n\n\n\nConclusion\nUsing a Windows .bat file to execute an R script allows for easy automation and integration of R scripts into your workflow. By understanding each line of the .bat file and the corresponding R script sections, you can customize and adapt the process to suit your specific needs."
  },
  {
    "objectID": "posts/2023-06-30/index.html",
    "href": "posts/2023-06-30/index.html",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "",
    "text": "Managing files is an essential task for any programmer, and when working with R, the file.rename() function can become your best friend. In this blog post, we’ll explore the ins and outs of file.rename(), discuss its syntax, provide real-life examples, and share some best practices to empower you in your file management endeavors. So grab a cup of coffee and let’s dive into the world of file.rename()!"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-1-renaming-a-single-fil",
    "href": "posts/2023-06-30/index.html#example-1-renaming-a-single-fil",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 1: Renaming a Single Fil",
    "text": "Example 1: Renaming a Single Fil\nSuppose you have a file named “old_file.txt,” and you want to rename it to “new_file.txt”. Here’s how you can accomplish this with file.rename():\nfile.rename(from = \"old_file.txt\", to = \"new_file.txt\")"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-2-renaming-multiple-file",
    "href": "posts/2023-06-30/index.html#example-2-renaming-multiple-file",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 2: Renaming Multiple File",
    "text": "Example 2: Renaming Multiple File\nImagine you have a folder with several files that need to be renamed simultaneously. Let’s say you want to change the file extensions from “.doc” to “.docx”. Here’s how you can achieve this using file.rename():\nfiles &lt;- list.files(path = \"path/to/folder\", pattern = \"*.doc\", full.names = TRUE)\nnew_names &lt;- sub(pattern = \".doc$\", replacement = \".docx\", x = files)\nfile.rename(from = files, to = new_names)"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-1-renaming-a-single-file",
    "href": "posts/2023-06-30/index.html#example-1-renaming-a-single-file",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 1: Renaming a Single File",
    "text": "Example 1: Renaming a Single File\nSuppose you have a file named “old_file.txt,” and you want to rename it to “new_file.txt”. Here’s how you can accomplish this with file.rename():\nfile.rename(from = \"old_file.txt\", to = \"new_file.txt\")"
  },
  {
    "objectID": "posts/2023-06-30/index.html#example-2-renaming-multiple-files",
    "href": "posts/2023-06-30/index.html#example-2-renaming-multiple-files",
    "title": "Simplifying File Management in R: Introducing file.rename()",
    "section": "Example 2: Renaming Multiple Files",
    "text": "Example 2: Renaming Multiple Files\nImagine you have a folder with several files that need to be renamed simultaneously. Let’s say you want to change the file extensions from “.doc” to “.docx”. Here’s how you can achieve this using file.rename():\nfiles &lt;- list.files(path = \"path/to/folder\", pattern = \"*.doc\", full.names = TRUE)\nnew_names &lt;- sub(pattern = \".doc$\", replacement = \".docx\", x = files)\nfile.rename(from = files, to = new_names)"
  },
  {
    "objectID": "posts/2023-07-11/index.html",
    "href": "posts/2023-07-11/index.html",
    "title": "A Closer Look at the R Function identical()",
    "section": "",
    "text": "Introduction\nIn the realm of programming, R is a widely-used language for statistical computing and data analysis. Within R, there exists a powerful function called identical() that allows programmers to compare objects for exact equality. In this blog post, we will delve into the syntax and usage of the identical() function, providing clear explanations and practical examples along the way.\n\n\nSyntax of identical()\nThe identical() function in R has the following simple syntax:\nidentical(x, y)\nHere, x and y are the objects that we want to compare. The function returns a logical value of either TRUE or FALSE, indicating whether x and y are exactly identical.\n\n\nExamples\n\nComparing Numeric Values: Let’s start with a simple example comparing two numeric values:\n\n\na &lt;- 5\nb &lt;- 5\nidentical(a, b)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE since both a and b have the same numeric value of 5.\n\nComparing Character Strings: Now, let’s consider an example with character strings:\n\n\nname1 &lt;- \"John\"\nname2 &lt;- \"John\"\nidentical(name1, name2)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE as both name1 and name2 contain the same string “John”.\n\nComparing Vectors: The identical() function can also compare vectors. Let’s see an example:\n\n\nvec1 &lt;- c(1, 2, 3)\nvec2 &lt;- c(1, 2, 3)\nidentical(vec1, vec2)\n\n[1] TRUE\n\n\nHere, the identical() function will return TRUE since vec1 and vec2 have the same values in the same order.\n\nComparing Data Frames: Data frames are a fundamental data structure in R. Let’s compare two data frames using identical():\n\n\ndf1 &lt;- data.frame(a = 1:3, b = c(\"A\", \"B\", \"C\"))\ndf2 &lt;- data.frame(a = 1:3, b = c(\"A\", \"B\", \"C\"))\nidentical(df1, df2)\n\n[1] TRUE\n\n\nIn this case, the identical() function will return TRUE as both df1 and df2 have the same column names, column types, and corresponding values.\n\nHandling Inexact Equality: The identical() function is particularly useful when we want to ensure that two objects are precisely the same. However, it does not handle cases where inexact equality is expected. For example:\n\n\nx &lt;- sqrt(2) * sqrt(2)\ny &lt;- 2\nidentical(x, y)\n\n[1] FALSE\n\n\nSurprisingly, the identical() function will return FALSE in this case. This occurs because sqrt(2) introduces a slight rounding error, resulting in x and y being slightly different despite representing the same mathematical value.\n\n\nConclusion\nIn this blog post, we explored the syntax and various use cases of the identical() function in R. By leveraging this function, you can determine whether two objects are exactly identical, whether they are numbers, strings, vectors, or even complex data structures like data frames. Remember that identical() is designed for exact equality, so if you require inexact comparisons, you may need to explore alternative approaches. Happy coding with R!"
  },
  {
    "objectID": "posts/2023-07-12/index.html",
    "href": "posts/2023-07-12/index.html",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "",
    "text": "As a programmer, working with data is a crucial aspect of our work. In R, there are numerous functions available that simplify data analysis tasks. One such function is colMeans(), which allows us to calculate the mean of columns in a matrix or data frame. In this blog post, we will delve into the colMeans() function, understand its usage, and explore various examples to see how it can help us gain valuable insights from our data."
  },
  {
    "objectID": "posts/2023-07-12/index.html#example-1-calculating-column-means-in-a-matri",
    "href": "posts/2023-07-12/index.html#example-1-calculating-column-means-in-a-matri",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "Example 1: Calculating column means in a matri",
    "text": "Example 1: Calculating column means in a matri\n\n# Create a matrix\nmy_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\n\n# Calculate column means\ncol_means &lt;- colMeans(my_matrix)\n\n# Print the result\nprint(col_means)\n\n[1] 1.5 3.5 5.5\n\n\nIn this example, we created a 2x3 matrix called ‘my_matrix’ and used colMeans() to calculate the means for each column. The resulting vector ‘col_means’ contains the mean values of columns [1 3 5], [2 3 6], which are [1.5, 3.5, 5.5] respectively."
  },
  {
    "objectID": "posts/2023-07-12/index.html#example-2-handling-missing-values",
    "href": "posts/2023-07-12/index.html#example-2-handling-missing-values",
    "title": "Exploring Data with colMeans() in R: A Programmer’s Guide",
    "section": "Example 2: Handling missing values",
    "text": "Example 2: Handling missing values\n\n# Create a matrix with missing values\nmy_matrix &lt;- matrix(c(1, 2, NA, 4, 5, 6), nrow = 2, ncol = 3)\n\n# Calculate column means with missing values removed\ncol_means &lt;- colMeans(my_matrix, na.rm = TRUE)\n\n# Print the result\nprint(col_means)\n\n[1] 1.5 4.0 5.5\n\n\nIn this example, our matrix ‘my_matrix’ contains a missing value (NA). By setting the ‘na.rm’ argument to TRUE, colMeans() excludes the missing value while calculating the means. As a result, we obtain the column means [1.5 4.0 5.5]"
  },
  {
    "objectID": "posts/2023-07-13/index.html",
    "href": "posts/2023-07-13/index.html",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "",
    "text": "As a programmer, you’ll often come across situations where you need to check whether a file exists before performing any operations on it. Thankfully, the R programming language provides a handy function called file.exists() that allows you to easily determine the existence of a file. In this blog post, we’ll explore the syntax and usage of file.exists() and provide you with practical examples to encourage you to try it out for yourself."
  },
  {
    "objectID": "posts/2023-07-13/index.html#example-1-checking-the-existence-of-a-file",
    "href": "posts/2023-07-13/index.html#example-1-checking-the-existence-of-a-file",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "Example 1: Checking the Existence of a File",
    "text": "Example 1: Checking the Existence of a File\nSuppose you want to check whether a file named “data.csv” exists in the current working directory. You can use the following code:\n\nfile_path &lt;- \"data.csv\"\nif (file.exists(file_path)) {\n  print(\"The file exists!\")\n} else {\n  print(\"The file does not exist.\")\n}\n\n[1] \"The file does not exist.\"\n\n\nIn this example, we assign the file path to the variable file_path and then use file.exists() to check if the file exists. If the condition is met, it will print “The file exists!” Otherwise, it will print “The file does not exist.”"
  },
  {
    "objectID": "posts/2023-07-13/index.html#example-2-conditional-operations-with-file.exists",
    "href": "posts/2023-07-13/index.html#example-2-conditional-operations-with-file.exists",
    "title": "Simplifying File Existence Checking in R with file.exists()",
    "section": "Example 2: Conditional Operations with file.exists()",
    "text": "Example 2: Conditional Operations with file.exists()\nLet’s imagine you want to perform different actions based on the existence of multiple files. Consider the following code snippet:\n\nfile1 &lt;- \"data1.csv\"\nfile2 &lt;- \"data2.csv\"\n\nif (file.exists(file1)) {\n  # Perform an operation if file1 exists\n  print(\"Performing operation on file1...\")\n} else {\n  # Perform a different operation if file1 doesn't exist\n  print(\"File1 does not exist.\")\n}\n\n[1] \"File1 does not exist.\"\n\nif (file.exists(file2)) {\n  # Perform an operation if file2 exists\n  print(\"Performing operation on file2...\")\n} else {\n  # Perform a different operation if file2 doesn't exist\n  print(\"File2 does not exist.\")\n}\n\n[1] \"File2 does not exist.\"\n\n\nIn this example, we check the existence of two files, data1.csv and data2.csv, and perform different actions based on their availability. You can modify the code according to your specific needs and perform any desired operations."
  },
  {
    "objectID": "posts/2023-07-14/index.html",
    "href": "posts/2023-07-14/index.html",
    "title": "Covariance in R with the cov() Function",
    "section": "",
    "text": "In the world of data analysis, understanding the relationship between variables is crucial. One powerful tool for measuring this relationship is the covariance. Today, we’ll explore the cov() function in R and delve into the fascinating world of covariance. Whether you’re a beginner or an experienced programmer, this blog post will equip you with the knowledge to harness the potential of cov() in your data analysis projects."
  },
  {
    "objectID": "posts/2023-07-14/index.html#example-1-calculating-covariance-between-two-variables",
    "href": "posts/2023-07-14/index.html#example-1-calculating-covariance-between-two-variables",
    "title": "Covariance in R with the cov() Function",
    "section": "Example 1: Calculating Covariance between Two Variables",
    "text": "Example 1: Calculating Covariance between Two Variables\nSuppose we have two vectors, x and y, representing the number of hours studied and the corresponding test scores, respectively, for a group of students. We want to measure the covariance between these two variables.\n\n# Create example vectors\nx &lt;- c(5, 7, 3, 6, 8)\ny &lt;- c(65, 80, 50, 70, 90)\n\n# Calculate covariance\ncovariance &lt;- cov(x, y)\n\ncovariance\n\n[1] 29\n\n\nIn this example, the cov() function takes the vectors x and y as inputs and returns the covariance between the two variables. The resulting covariance value will help us understand the relationship between the hours studied and the corresponding test scores. What this is particular example is saying is that for every unit increase in x there is a 29 unit increase in y."
  },
  {
    "objectID": "posts/2023-07-14/index.html#example-2-calculating-covariance-matrix",
    "href": "posts/2023-07-14/index.html#example-2-calculating-covariance-matrix",
    "title": "Covariance in R with the cov() Function",
    "section": "Example 2: Calculating Covariance Matrix",
    "text": "Example 2: Calculating Covariance Matrix\nNow let’s consider a scenario where we have multiple variables, and we want to calculate the covariance matrix to gain insights into their relationships.\n\n# Create example vectors\nx &lt;- c(5, 7, 3, 6, 8)\ny &lt;- c(65, 80, 50, 70, 90)\nz &lt;- c(150, 200, 100, 180, 220)\n\n# Combine vectors into a matrix\ndata &lt;- cbind(x, y, z)\n\n# Calculate covariance matrix\ncov_matrix &lt;- cov(data)\ncov_matrix\n\n     x   y    z\nx  3.7  29   90\ny 29.0 230  700\nz 90.0 700 2200\n\n\nIn this example, we have three variables, x, y, and z, representing hours studied, test scores, and total marks, respectively. We use the cbind() function to combine the vectors into a matrix called data. By applying the cov() function to this matrix, we obtain a covariance matrix that reveals the relationships between all the variables."
  },
  {
    "objectID": "posts/2023-07-17/index.html",
    "href": "posts/2023-07-17/index.html",
    "title": "Finding Duplicate Values in a Data Frame in R: A Guide Using Base R and dplyr",
    "section": "",
    "text": "Introduction\nIn data analysis and programming, it’s common to encounter situations where you need to identify duplicate values within a dataset. Whether you’re a beginner or an experienced programmer, knowing how to find duplicate values is a fundamental skill. In this blog post, we will explore two different approaches to accomplish this task using base R functions and the dplyr package in R. By the end, you’ll have a clear understanding of how to detect and manage duplicate values in your own datasets.\n\n\nUsing Base R Functions\nR provides a variety of functions for data manipulation and analysis, including those specifically designed for identifying duplicate values. Let’s consider a simple data frame to demonstrate this approach:\n\n# Creating a sample data frame\ndf &lt;- data.frame(\n  ID = c(1, 2, 3, 3, 4, 5),\n  Name = c(\"John\", \"Jane\", \"Mark\", \"Mark\", \"Luke\", \"Kate\"),\n  Age = c(25, 30, 35, 35, 40, 45)\n)\n\nTo find duplicate values in this data frame using base R functions, we can utilize the duplicated() and table() functions:\n\n# Using base R functions to find duplicate values\nduplicates &lt;- df[duplicated(df), ]\nduplicate_counts &lt;- table(df[duplicated(df), ])\n\nduplicates\n\n  ID Name Age\n4  3 Mark  35\n\nduplicate_counts\n\n, , Age = 35\n\n   Name\nID  Mark\n  3    1\n\n\nThe duplicated() function identifies the duplicate rows in the data frame, while the table() function creates a frequency table of the duplicate values. By combining these two functions, we can detect and examine the duplicate entries in the data frame.\n\n\nUsing dplyr\nThe dplyr package provides a powerful set of tools for data manipulation and analysis. Let’s see how we can accomplish the same task of finding duplicate values using dplyr functions:\n\n# loading the dplyr package\nlibrary(dplyr)\n\n# Using dplyr to find duplicate values\nduplicates &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\nduplicate_counts &lt;- df |&gt;\n  add_count(ID, Name, Age) |&gt;\n  filter(n &gt; 1) |&gt;\n  distinct()\n\nduplicates\n\n# A tibble: 2 × 3\n     ID Name    Age\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     3 Mark     35\n2     3 Mark     35\n\nduplicate_counts\n\n  ID Name Age n\n1  3 Mark  35 2\n\n\nLet’s break the first one down step by step:\nduplicates &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\ndf refers to a data frame in R.\ngroup_by_all() groups the data frame by all columns. This means that the subsequent operations will consider duplicate values across all columns.\nfilter(n() &gt; 1) filters the grouped data frame to only keep rows where the count (n()) of observations is greater than 1. In other words, it keeps only the rows that have duplicates.\nungroup() removes the grouping, ensuring that the resulting data frame is not grouped anymore.\nThe resulting data frame with duplicate rows is assigned to the variable duplicates.\n\nNow, let’s move on to the second part:\nduplicate_counts &lt;- df |&gt;\n  add_count(ID, Name, Age) |&gt;\n  filter(n &gt; 1) |&gt;\n  distinct()\n\nadd_count(ID, Name, Age) adds a new column called “n” to the data frame, which represents the count of observations for each combination of ID, Name, and Age.\nfilter(n &gt; 1) keeps only the rows where the count (“n”) is greater than 1. This retains only the rows that have duplicates based on the specified columns.\ndistinct() removes any duplicate rows that may still exist after the previous steps, keeping only unique rows.\nThe resulting data frame with duplicate counts and unique rows is assigned to the variable duplicate_counts.\n\nIn simple terms, the code first identifies and extracts the duplicate rows from the original data frame (df) and assigns them to duplicates. Then, it calculates the counts of duplicates based on specific columns (ID, Name, and Age) and stores the results, along with unique rows, in duplicate_counts.\nThese operations allow you to conveniently find duplicate rows and examine their counts within a data frame using both base R functions and some simple dplyr code.\n\n\nConclusion\nDetecting and managing duplicate values is an essential task in data analysis and programming. In this blog post, we explored two different approaches to find duplicate values in a data frame using base R functions and the dplyr package. By leveraging these techniques, you can efficiently identify and handle duplicate entries in your own datasets.\nI encourage you to practice using these methods on your own datasets. Familiarize yourself with the functions, experiment with different data frames, and explore various scenarios. This hands-on experience will deepen your understanding and improve your data analysis skills.\nRemember, the ability to identify and manage duplicate values is crucial for ensuring data integrity and obtaining accurate results in your data analysis projects. So go ahead, give it a try, and unlock the power of duplicate value detection in R!"
  },
  {
    "objectID": "posts/2023-07-18/index.html",
    "href": "posts/2023-07-18/index.html",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "",
    "text": "In data analysis and manipulation tasks, it’s common to encounter situations where we need to identify and handle duplicate rows in a dataset. In this blog post, we will explore three different approaches to finding duplicate rows in R: the base R method, the dplyr package, and the data.table package. We’ll compare their performance using the benchmark function and provide insights on when to use each approach. So, grab your coding gear, and let’s dive in!"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-1-base-rs-duplicated-function",
    "href": "posts/2023-07-18/index.html#approach-1-base-rs-duplicated-function",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 1: Base R’s duplicated Function",
    "text": "Approach 1: Base R’s duplicated Function\nThe simplest approach to finding duplicate rows is to use the duplicated function from base R. This function returns a logical vector indicating which rows are duplicates. We can apply it directly to our data frame df.\n\nduplicated_rows_base &lt;- duplicated(df)"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-2-dplyrs-concise-data-manipulation",
    "href": "posts/2023-07-18/index.html#approach-2-dplyrs-concise-data-manipulation",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 2: dplyr’s Concise Data Manipulation",
    "text": "Approach 2: dplyr’s Concise Data Manipulation\nThe dplyr package provides an intuitive and concise way to manipulate data frames. We can leverage its chaining syntax to filter the duplicated rows. The group_by_all function groups the data frame by all columns, and filter(n() &gt; 1) keeps only those rows with more than one occurrence within each group. Finally, ungroup removes the grouping information.\n\nduplicated_rows_dplyr &lt;- df |&gt;\n  group_by_all() |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()"
  },
  {
    "objectID": "posts/2023-07-18/index.html#approach-3-efficient-duplicate-detection-with-data.table",
    "href": "posts/2023-07-18/index.html#approach-3-efficient-duplicate-detection-with-data.table",
    "title": "Efficiently Finding Duplicate Rows in R: A Comparative Analysis",
    "section": "Approach 3: Efficient Duplicate Detection with data.table",
    "text": "Approach 3: Efficient Duplicate Detection with data.table\nIf performance is a crucial factor, the data.table package offers highly optimized operations on large datasets. Converting our data frame to a data.table object allows us to utilize the efficient duplicated function from data.table.\n\ndtdf &lt;- data.table(df)\nduplicated_rows_datatable &lt;- duplicated(dtdf)\n\nBenchmarking and Performance Comparison: To evaluate the performance of the three approaches, we will use the benchmark function from the rbenchmark package. We’ll execute each approach ten times and collect information such as execution time (elapsed), relative performance, and CPU times (user.self and sys.self).\n\nbenchmark(\n  duplicated_rows_base = duplicated(df),\n  duplicated_rows_dplyr = df |&gt; \n    group_by_all() |&gt; \n    filter(n() &gt; 1) |&gt;\n    ungroup(),\n  duplicated_rows_datatable = duplicated(dtdf),\n  replications = 10,\n  columns = c(\"test\",\"replications\",\"elapsed\",\n              \"relative\",\"user.self\",\"sys.self\")\n) |&gt;\n  arrange(relative)\n\n                       test replications elapsed relative user.self sys.self\n1 duplicated_rows_datatable           10    0.05      1.0      0.01     0.01\n2     duplicated_rows_dplyr           10    0.29      5.8      0.27     0.02\n3      duplicated_rows_base           10    3.53     70.6      3.45     0.08"
  },
  {
    "objectID": "posts/2023-07-19/index.html",
    "href": "posts/2023-07-19/index.html",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "If you’re an aspiring data scientist or R programmer, you must be familiar with the powerful data structure called “lists.” Lists in R are collections of elements that can contain various data types such as vectors, matrices, data frames, or even other lists. They offer great flexibility and are widely used in many real-world scenarios.\nIn this blog post, we will explore one of the essential skills in working with lists: subsetting. Subsetting allows you to extract specific elements or portions of a list, helping you access and manipulate data efficiently. So, let’s dive into the world of list subsetting and learn some useful techniques along the way!\n\n\nBefore we start subsetting, let’s review how to access elements within a list. In R, you can access elements of a list using square brackets “[]” you can also use double square brackets “[[ ]]” or the dollar sign “$”. The double square brackets are used when you know the exact position of the element you want to extract, while the dollar sign is used when you know the name of the element.\n\n# Create a sample list\nmy_list &lt;- list(name = \"John\", age = 30, scores = c(85, 90, 78))\n\n# Access elements using double square brackets\nname &lt;- my_list[[1]]\nage &lt;- my_list[[2]]\nscores &lt;- my_list[[3]]\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n# Access elements using dollar sign\nname &lt;- my_list$name\nage &lt;- my_list$age\nscores &lt;- my_list$scores\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n\n\n\n\n\n\nSubsetting by position allows you to extract specific elements based on their index within the list. Remember, R uses 1-based indexing, so the first element is at position 1, the second at position 2, and so on.\n\n# Subsetting by position\nelement_1 &lt;- my_list[[1]]      # Extract the first element\nelement_2 &lt;- my_list[[2]]      # Extract the second element\nelement_last &lt;- my_list[[3]]   # Extract the last element\n\nelement_1\n\n[1] \"John\"\n\nelement_2\n\n[1] 30\n\nelement_last\n\n[1] 85 90 78\n\n# You can also use negative values to exclude elements\nexcluding_last &lt;- my_list[-3] # Exclude the last element\nexcluding_last\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\n\n\n\n\nSubsetting by name is particularly useful when you want to access elements using their names. It provides a more intuitive way to extract specific elements from a list.\n\n# Subsetting by name\nname &lt;- my_list[[\"name\"]]      # Extract the element with the name \"name\"\nscores &lt;- my_list[[\"scores\"]]  # Extract the element with the name \"scores\"\n\n# You can also use the dollar sign notation for name-based subsetting\nage &lt;- my_list$age\n\nname\n\n[1] \"John\"\n\nscores\n\n[1] 85 90 78\n\nage\n\n[1] 30\n\n\n\n\n\nYou can subset multiple elements at once using numeric or character vectors for positions or names, respectively.\n\n# Subsetting multiple elements by position\nelements_1_2 &lt;- my_list[c(1, 2)] # Extract the first and second elements\nelements_1_2\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\nelements_first_last &lt;- my_list[c(1, 3)] # Extract the first and last elements\nelements_first_last\n\n$name\n[1] \"John\"\n\n$scores\n[1] 85 90 78\n\n# Subsetting multiple elements by name\nelements_age_scores &lt;- my_list[c(\"age\", \"scores\")] # Extract elements with names \"age\" \n                                                   # and \"scores\"\nelements_age_scores\n\n$age\n[1] 30\n\n$scores\n[1] 85 90 78\n\n\n\n\n\nLists can contain other lists, creating a nested structure. To access elements within nested lists, you can use multiple “[[ ]]” or “$” operators.\n\n# Create a nested list\nnested_list &lt;- list(personal_info = my_list, hobbies = c(\"Reading\", \"Painting\"))\n\nnested_list\n\n$personal_info\n$personal_info$name\n[1] \"John\"\n\n$personal_info$age\n[1] 30\n\n$personal_info$scores\n[1] 85 90 78\n\n\n$hobbies\n[1] \"Reading\"  \"Painting\"\n\n# Access elements within nested lists\nname &lt;- nested_list[[\"personal_info\"]][[\"name\"]] # Extract the name from the nested list\nname\n\n[1] \"John\"\n\nsecond_hobby &lt;- nested_list[[\"hobbies\"]][[2]] # Extract the second \n                                              # hobby from the nested list\nsecond_hobby\n\n[1] \"Painting\"\n\n\n\n\n\n\nSubsetting lists in R is a fundamental skill that will prove invaluable in your data manipulation tasks. I encourage you to practice these techniques with your own data and explore more advanced subsetting methods, such as using logical conditions or applying functions to subsets.\nBy mastering list subsetting, you’ll unlock the true potential of R for data analysis and gain the confidence to handle complex data structures efficiently.\nSo, don’t hesitate! Dive into the world of list subsetting and enhance your R programming skills today. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-19/index.html#accessing-elements-in-a-list",
    "href": "posts/2023-07-19/index.html#accessing-elements-in-a-list",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Before we start subsetting, let’s review how to access elements within a list. In R, you can access elements of a list using square brackets “[]” you can also use double square brackets “[[ ]]” or the dollar sign “$”. The double square brackets are used when you know the exact position of the element you want to extract, while the dollar sign is used when you know the name of the element.\n\n# Create a sample list\nmy_list &lt;- list(name = \"John\", age = 30, scores = c(85, 90, 78))\n\n# Access elements using double square brackets\nname &lt;- my_list[[1]]\nage &lt;- my_list[[2]]\nscores &lt;- my_list[[3]]\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78\n\n# Access elements using dollar sign\nname &lt;- my_list$name\nage &lt;- my_list$age\nscores &lt;- my_list$scores\n\nname\n\n[1] \"John\"\n\nage\n\n[1] 30\n\nscores\n\n[1] 85 90 78"
  },
  {
    "objectID": "posts/2023-07-19/index.html#subsetting-list-elements",
    "href": "posts/2023-07-19/index.html#subsetting-list-elements",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Subsetting by position allows you to extract specific elements based on their index within the list. Remember, R uses 1-based indexing, so the first element is at position 1, the second at position 2, and so on.\n\n# Subsetting by position\nelement_1 &lt;- my_list[[1]]      # Extract the first element\nelement_2 &lt;- my_list[[2]]      # Extract the second element\nelement_last &lt;- my_list[[3]]   # Extract the last element\n\nelement_1\n\n[1] \"John\"\n\nelement_2\n\n[1] 30\n\nelement_last\n\n[1] 85 90 78\n\n# You can also use negative values to exclude elements\nexcluding_last &lt;- my_list[-3] # Exclude the last element\nexcluding_last\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\n\n\n\n\nSubsetting by name is particularly useful when you want to access elements using their names. It provides a more intuitive way to extract specific elements from a list.\n\n# Subsetting by name\nname &lt;- my_list[[\"name\"]]      # Extract the element with the name \"name\"\nscores &lt;- my_list[[\"scores\"]]  # Extract the element with the name \"scores\"\n\n# You can also use the dollar sign notation for name-based subsetting\nage &lt;- my_list$age\n\nname\n\n[1] \"John\"\n\nscores\n\n[1] 85 90 78\n\nage\n\n[1] 30\n\n\n\n\n\nYou can subset multiple elements at once using numeric or character vectors for positions or names, respectively.\n\n# Subsetting multiple elements by position\nelements_1_2 &lt;- my_list[c(1, 2)] # Extract the first and second elements\nelements_1_2\n\n$name\n[1] \"John\"\n\n$age\n[1] 30\n\nelements_first_last &lt;- my_list[c(1, 3)] # Extract the first and last elements\nelements_first_last\n\n$name\n[1] \"John\"\n\n$scores\n[1] 85 90 78\n\n# Subsetting multiple elements by name\nelements_age_scores &lt;- my_list[c(\"age\", \"scores\")] # Extract elements with names \"age\" \n                                                   # and \"scores\"\nelements_age_scores\n\n$age\n[1] 30\n\n$scores\n[1] 85 90 78\n\n\n\n\n\nLists can contain other lists, creating a nested structure. To access elements within nested lists, you can use multiple “[[ ]]” or “$” operators.\n\n# Create a nested list\nnested_list &lt;- list(personal_info = my_list, hobbies = c(\"Reading\", \"Painting\"))\n\nnested_list\n\n$personal_info\n$personal_info$name\n[1] \"John\"\n\n$personal_info$age\n[1] 30\n\n$personal_info$scores\n[1] 85 90 78\n\n\n$hobbies\n[1] \"Reading\"  \"Painting\"\n\n# Access elements within nested lists\nname &lt;- nested_list[[\"personal_info\"]][[\"name\"]] # Extract the name from the nested list\nname\n\n[1] \"John\"\n\nsecond_hobby &lt;- nested_list[[\"hobbies\"]][[2]] # Extract the second \n                                              # hobby from the nested list\nsecond_hobby\n\n[1] \"Painting\""
  },
  {
    "objectID": "posts/2023-07-19/index.html#explore-further",
    "href": "posts/2023-07-19/index.html#explore-further",
    "title": "How to subset list objects in R",
    "section": "",
    "text": "Subsetting lists in R is a fundamental skill that will prove invaluable in your data manipulation tasks. I encourage you to practice these techniques with your own data and explore more advanced subsetting methods, such as using logical conditions or applying functions to subsets.\nBy mastering list subsetting, you’ll unlock the true potential of R for data analysis and gain the confidence to handle complex data structures efficiently.\nSo, don’t hesitate! Dive into the world of list subsetting and enhance your R programming skills today. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-20/index.html",
    "href": "posts/2023-07-20/index.html",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "",
    "text": "If you’ve been working with R for some time, you might have come across situations where your code becomes cumbersome due to repetitive references to data frames or list elements. Luckily, R provides two powerful functions, with() and within(), to help you streamline your code and make it more readable. These functions offer a simple and elegant solution for manipulating data frames and lists. In this blog post, we’ll explore the syntax of these functions and provide several real-world examples to demonstrate their usefulness. So, let’s dive in and discover how with() and within() can become your new best friends in R programming!"
  },
  {
    "objectID": "posts/2023-07-20/index.html#with-syntax",
    "href": "posts/2023-07-20/index.html#with-syntax",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "with() syntax:",
    "text": "with() syntax:\nwith(data, expr)\n\ndata: The data frame or list you want to use as an environment within the expression (expr).\nexpr: The expression where you can refer to data frame/list elements directly, without prefixing them with the data name.\n\n\nwithin(): The within() function is similar to with(), but it modifies the data frame or list in place and returns the modified object."
  },
  {
    "objectID": "posts/2023-07-20/index.html#within-syntax",
    "href": "posts/2023-07-20/index.html#within-syntax",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "within() syntax:",
    "text": "within() syntax:\nwithin(data, expr)\n\ndata: The data frame or list you want to modify within the expression (expr).\nexpr: The expression where you can manipulate data frame/list elements directly, without prefixing them with the data name.\n\nNow that we know the basics, let’s explore some examples to see these functions in action."
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-1-simplifying-data-manipulation-with-with",
    "href": "posts/2023-07-20/index.html#example-1-simplifying-data-manipulation-with-with",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 1: Simplifying Data Manipulation with with()",
    "text": "Example 1: Simplifying Data Manipulation with with()\nSuppose we have a data frame containing information about employees and their salaries:\n\n# Sample data frame\nemployee_data &lt;- data.frame(\n  name = c(\"John\", \"Jane\", \"Michael\", \"Sara\"),\n  age = c(32, 28, 45, 37),\n  salary = c(50000, 60000, 75000, 62000)\n)\n\nemployee_data\n\n     name age salary\n1    John  32  50000\n2    Jane  28  60000\n3 Michael  45  75000\n4    Sara  37  62000\n\n\nWithout with(), calculating the average salary of employees would require repetitive references to the data frame:\n\n# Without with()\navg_salary &lt;- mean(employee_data$salary)\navg_salary\n\n[1] 61750\n\n\nHowever, with the with() function, we can write the same code more concisely:\n\n# With with()\navg_salary &lt;- with(employee_data, mean(salary))\navg_salary\n\n[1] 61750"
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-2-simplifying-data-transformation-with-within",
    "href": "posts/2023-07-20/index.html#example-2-simplifying-data-transformation-with-within",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 2: Simplifying Data Transformation with within()",
    "text": "Example 2: Simplifying Data Transformation with within()\nLet’s consider a scenario where we want to create a new column bonus for employees based on their age:\n\n# Without within()\nemployee_data$bonus &lt;- ifelse(employee_data$age &gt;= 35, 5000, 3000)\nemployee_data\n\n     name age salary bonus\n1    John  32  50000  3000\n2    Jane  28  60000  3000\n3 Michael  45  75000  5000\n4    Sara  37  62000  5000\n\n\nBy using within(), we can modify the data frame directly without repetitive references:\n\n# With within()\nemployee_data &lt;- within(employee_data, bonus &lt;- ifelse(age &gt;= 45, 5000, 3000))\nemployee_data\n\n     name age salary bonus\n1    John  32  50000  3000\n2    Jane  28  60000  3000\n3 Michael  45  75000  5000\n4    Sara  37  62000  3000"
  },
  {
    "objectID": "posts/2023-07-20/index.html#example-3-simplifying-plotting-with-with",
    "href": "posts/2023-07-20/index.html#example-3-simplifying-plotting-with-with",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Example 3: Simplifying Plotting with with()",
    "text": "Example 3: Simplifying Plotting with with()\nWhen creating visualizations, with() can help you avoid prefixing data frame column names repeatedly. Let’s generate a scatter plot of employee age versus salary:\n\n# Without with()\nplot(\n  employee_data$salary, \n  employee_data$age, \n  xlab = \"Salary\", \n  ylab = \"Age\", \n  main = \"Age vs. Salary\"\n  )\n\n\n\n\nUsing with(), we can eliminate the repetition:\n\n# With with()\nwith(\n  employee_data, \n  plot(\n    salary, \n    age, \n    xlab = \"Salary\", \n    ylab = \"Age\", \n    main = \"Age vs. Salary\"\n    )\n  )\n\n\n\n\nHere are some additional examples of how to use the with() and within() functions. To calculate the mean of the values in the x column of the data data frame, you would use the following code:\nwith(data, mean(x))\nTo create a new data frame that contains the mean of the values in each column, you would use the following code:\nnew_data &lt;- within(data, {\n  for (column in names(data)) {\n    column_mean &lt;- mean(data[[column]])\n    data[[column]] &lt;- column_mean\n  }\n})\n\nnew_data\nTo filter the data data frame to only include rows where the value in the x column is greater than 1, you would use the following code:\nnew_data &lt;- within(data, {\n  new_data &lt;- data[data$x &gt; 1, ]\n})\n\nnew_data"
  },
  {
    "objectID": "posts/2023-07-20/index.html#conclusion",
    "href": "posts/2023-07-20/index.html#conclusion",
    "title": "Simplify Your Code with R’s Powerful Functions: with() and within()",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored two powerful R functions: with() and within(). These functions provide an elegant way to manipulate data frames and lists by reducing repetitive references and simplifying your code. By leveraging the capabilities of with() and within(), you can write more readable and efficient code. I encourage you to try out these functions in your R projects and experience the benefits firsthand. Happy coding!"
  },
  {
    "objectID": "posts/2023-07-21/index.html",
    "href": "posts/2023-07-21/index.html",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "",
    "text": "As a programmer in R, you’ll often find yourself working with textual data and need to manipulate or display it in various ways. Two essential functions at your disposal for these tasks are paste() and cat(). These functions are powerful tools that allow you to combine and display text easily and efficiently. In this blog post, we’ll explore the syntax, similarities, and differences between these functions and provide you with practical examples to get you started. Let’s dive in!"
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-1-basic-concatenation",
    "href": "posts/2023-07-21/index.html#example-1-basic-concatenation",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 1: Basic Concatenation",
    "text": "Example 1: Basic Concatenation\n\nfruit1 &lt;- \"apple\"\nfruit2 &lt;- \"orange\"\nresult &lt;- paste(fruit1, fruit2)\nprint(result) # Output: \"apple orange\"\n\n[1] \"apple orange\""
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-2-using-different-separator",
    "href": "posts/2023-07-21/index.html#example-2-using-different-separator",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 2: Using Different Separator",
    "text": "Example 2: Using Different Separator\n\nmonths &lt;- c(\"January\", \"February\", \"March\")\nresult &lt;- paste(months, collapse = \", \")\nprint(result) # Output: \"January, February, March\"\n\n[1] \"January, February, March\""
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-1-basic-concatenation-and-display",
    "href": "posts/2023-07-21/index.html#example-1-basic-concatenation-and-display",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 1: Basic Concatenation and Display",
    "text": "Example 1: Basic Concatenation and Display\n\nfruit1 &lt;- \"apple\"\nfruit2 &lt;- \"orange\"\ncat(\"My favorite fruits are\", fruit1, \"and\", fruit2) # Output: \"My favorite fruits are apple and orange\"\n\nMy favorite fruits are apple and orange"
  },
  {
    "objectID": "posts/2023-07-21/index.html#example-2-output-to-file",
    "href": "posts/2023-07-21/index.html#example-2-output-to-file",
    "title": "Harness the Power of paste() and cat() in R: Combining and Displaying Text Like a Pro",
    "section": "Example 2: Output to File",
    "text": "Example 2: Output to File\nnumbers &lt;- 1:5\nfile_path &lt;- \"numbers.txt\"\ncat(numbers, file = file_path)\n# The content of \"numbers.txt\": 1 2 3 4 5"
  },
  {
    "objectID": "posts/2023-07-24/index.html",
    "href": "posts/2023-07-24/index.html",
    "title": "How to Calculate Percentage by Group in R using Base R, dplyr, and data.table",
    "section": "",
    "text": "Calculating percentages by group is a common task in data analysis. It allows you to understand the distribution of data within different categories. In this blog post, we’ll walk you through the process of calculating percentages by group using three popular R packages: Base R, dplyr, and data.table. To keep things simple, we will use the well-known Iris dataset.\nThe Iris dataset contains information about different species of iris flowers and their measurements, including sepal length, sepal width, petal length, and petal width. We will focus on the ‘Species’ column and calculate the percentage of each species in the dataset."
  },
  {
    "objectID": "posts/2023-07-24/index.html#example-1-using-base-r",
    "href": "posts/2023-07-24/index.html#example-1-using-base-r",
    "title": "How to Calculate Percentage by Group in R using Base R, dplyr, and data.table",
    "section": "Example 1: Using Base R",
    "text": "Example 1: Using Base R\nStep 1: Load the Iris dataset\n\n# Load the Iris dataset\ndata(iris)\n\nStep 2: Calculate the counts by group\n\n# Use the table() function to get the counts of each species\ngroup_counts &lt;- table(iris$Species)\n\nStep 3: Calculate the total count\n\n# Calculate the total count using the sum() function\ntotal_count &lt;- sum(group_counts)\n\nStep 4: Calculate the percentage by group\n\n# Divide each count by the total count and multiply by 100 to get the percentage\npercentage_by_group &lt;- (group_counts / total_count) * 100\n\nStep 5: Combine group names and percentages into a data frame and display the result\n\n# Combine group names and percentages into a data frame\nresult_base_R &lt;- data.frame(\n  Species = names(percentage_by_group), \n  Percentage = percentage_by_group\n  )\n\n# Print the result\nprint(result_base_R)\n\n     Species Percentage.Var1 Percentage.Freq\n1     setosa          setosa        33.33333\n2 versicolor      versicolor        33.33333\n3  virginica       virginica        33.33333"
  },
  {
    "objectID": "posts/2023-07-24/index.html#example-2-using-dplyr",
    "href": "posts/2023-07-24/index.html#example-2-using-dplyr",
    "title": "How to Calculate Percentage by Group in R using Base R, dplyr, and data.table",
    "section": "Example 2: Using dplyr",
    "text": "Example 2: Using dplyr\nStep 1: Load the necessary library and the Iris dataset\n\n# Load the dplyr library\nlibrary(dplyr)\n\n# Load the Iris dataset\ndata(iris)\n\nStep 2: Calculate the percentage by group using dplyr\n\n# Use the group_by() and summarise() functions to calculate percentages\nresult_dplyr &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(Percentage = n() / nrow(iris) * 100)\n\nStep 3: Display the result\n\n# Print the result\nprint(result_dplyr)\n\n# A tibble: 3 × 2\n  Species    Percentage\n  &lt;fct&gt;           &lt;dbl&gt;\n1 setosa           33.3\n2 versicolor       33.3\n3 virginica        33.3"
  },
  {
    "objectID": "posts/2023-07-24/index.html#example-3-using-data.table",
    "href": "posts/2023-07-24/index.html#example-3-using-data.table",
    "title": "How to Calculate Percentage by Group in R using Base R, dplyr, and data.table",
    "section": "Example 3: Using data.table:",
    "text": "Example 3: Using data.table:\nStep 1: Load the necessary library and the Iris dataset\n\n# Load the data.table library\nlibrary(data.table)\n\n# Convert the Iris dataset to a data.table\niris_dt &lt;- as.data.table(iris)\n\nStep 2: Calculate the percentage by group using data.table\n\n# Use the .N special symbol to calculate counts and by-reference to save memory\nresult_data_table &lt;- iris_dt[, .(Percentage = .N / nrow(iris_dt) * 100), by = Species]\n\nStep 3: Display the result\n\n# Print the result\nprint(result_data_table)\n\n      Species Percentage\n1:     setosa   33.33333\n2: versicolor   33.33333\n3:  virginica   33.33333"
  },
  {
    "objectID": "posts/2023-07-25/index.html",
    "href": "posts/2023-07-25/index.html",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "",
    "text": "As a programmer and data enthusiast, you know that summarizing data is essential to gain insights into its distribution and characteristics. R, being a powerful and versatile programming language for data analysis, offers various functions to aid in this process. One such function that stands out is fivenum(), a hidden gem that computes the five-number summary of a dataset. In this blog post, we will explore the fivenum() function and demonstrate how to leverage it for different scenarios, empowering you to unlock valuable insights from your datasets.\nThe five number summary is a concise way to summarize the distribution of a data set. It consists of the following five values:\n\nThe minimum value\nThe first quartile (Q1)\nThe median\nThe third quartile (Q3)\nThe maximum value\n\nThe minimum value is the smallest value in the data set. The first quartile (Q1) is the value below which 25% of the data points lie. The median is the value below which 50% of the data points lie. The third quartile (Q3) is the value below which 75% of the data points lie. The maximum value is the largest value in the data set.\nThe five number summary can be used to get a quick overview of the distribution of a data set. It can tell us how spread out the data is, whether the data is skewed, and whether there are any outliers."
  },
  {
    "objectID": "posts/2023-07-25/index.html#example-1.-a-vector",
    "href": "posts/2023-07-25/index.html#example-1.-a-vector",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "Example 1. A Vector:",
    "text": "Example 1. A Vector:\nLet’s start with the basics. To compute the five-number summary for a vector in R, all you need is the fivenum() function and your data. For example:\n\n# Sample vector\ndata_vector &lt;- c(12, 24, 36, 48, 60, 72, 84, 96, 108, 120)\n\n# Calculate the five-number summary\nsummary_vector &lt;- fivenum(data_vector)\n\n# Output the results\nprint(summary_vector)\n\n[1]  12  36  66  96 120\n\n\nThe fivenum() function will return the minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum values of the vector. Armed with this information, you can easily visualize the dataset’s distribution using box plots, histograms, or other graphical representations."
  },
  {
    "objectID": "posts/2023-07-25/index.html#example-2.-with-boxplot",
    "href": "posts/2023-07-25/index.html#example-2.-with-boxplot",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "Example 2. With boxplot():",
    "text": "Example 2. With boxplot():\nBox plots, also known as box-and-whisker plots, are a fantastic visualization tool to display the distribution and identify outliers in your data. When combined with fivenum(), you can create insightful box plots with minimal effort. Consider this example:\n\n# Sample vector\ndata_vector &lt;- c(12, 24, 36, 48, 60, 72, 84, 96, 108, 120)\n\n# Create a box plot\nboxplot(data_vector)\n\n\n\n# Calculate the five-number summary and print the results\nsummary_vector &lt;- fivenum(data_vector)\nprint(summary_vector)\n\n[1]  12  36  66  96 120\n\n\nBy incorporating the fivenum() function, you can see the minimum, lower hinge (Q1), median (Q2), upper hinge (Q3), and maximum, represented in the box plot. This graphical representation helps in visualizing the spread of the data, presence of outliers, and skewness."
  },
  {
    "objectID": "posts/2023-07-25/index.html#example-3.-on-a-column-in-a-data.frame",
    "href": "posts/2023-07-25/index.html#example-3.-on-a-column-in-a-data.frame",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "Example 3. On a Column in a Data.frame:",
    "text": "Example 3. On a Column in a Data.frame:\nOften, data is stored in data.frames, which are highly efficient for handling and analyzing datasets. To apply fivenum() on a specific column within a data.frame, use the $ operator to access the desired column. Consider the following example:\n\n# Sample data.frame\ndata_df &lt;- data.frame(ID = 1:5,\n                      Age = c(25, 30, 22, 28, 35))\n\n# Calculate the five-number summary for the \"Age\" column\nsummary_age &lt;- fivenum(data_df$Age)\n\n# Output the results\nprint(summary_age)\n\n[1] 22 25 28 30 35\n\n\nBy applying fivenum() on the “Age” column, you obtain the five-number summary, which reveals valuable information about the age distribution of the dataset."
  },
  {
    "objectID": "posts/2023-07-25/index.html#example-4.-across-multiple-columns-of-a-data.frame-using-sapply",
    "href": "posts/2023-07-25/index.html#example-4.-across-multiple-columns-of-a-data.frame-using-sapply",
    "title": "Unraveling Data Insights with R’s fivenum(): A Programmer’s Guide",
    "section": "Example 4. Across Multiple Columns of a Data.frame Using sapply():",
    "text": "Example 4. Across Multiple Columns of a Data.frame Using sapply():\nTo elevate your data analysis game, you’ll often need to summarize multiple columns simultaneously. In this case, sapply() comes in handy, allowing you to apply fivenum() across several columns at once. Let’s take a look at an example:\n\n# Sample data.frame\ndata_df &lt;- data.frame(ID = 1:5,\n                      Age = c(25, 30, 22, 28, 35),\n                      Salary = c(50000, 60000, 45000, 55000, 70000))\n\n# Apply fivenum() on all numeric columns\nsummary_all_columns &lt;- sapply(data_df[, 2:3], fivenum)\n\n# Output the results\nprint(summary_all_columns)\n\n     Age Salary\n[1,]  22  45000\n[2,]  25  50000\n[3,]  28  55000\n[4,]  30  60000\n[5,]  35  70000\n\n\nIn this example, sapply() is used to calculate the five-number summary for the “Age” and “Salary” columns simultaneously. The output provides a comprehensive summary of these columns, enabling you to quickly assess the distribution of each."
  },
  {
    "objectID": "posts/2023-07-26/index.html",
    "href": "posts/2023-07-26/index.html",
    "title": "Summarizing Data in R: tapply() vs. group_by() and summarize()",
    "section": "",
    "text": "Are you tired of manually calculating summary statistics for your data in R? Look no further! In this blog post, we will explore two powerful ways to summarize data: using the tapply() function and the group_by() and summarize() functions from the dplyr package. Both methods are incredibly useful and can save you time and effort in your data analysis projects."
  },
  {
    "objectID": "posts/2023-07-26/index.html#example-1-summarizing-a-numeric-vector-with-tapply",
    "href": "posts/2023-07-26/index.html#example-1-summarizing-a-numeric-vector-with-tapply",
    "title": "Summarizing Data in R: tapply() vs. group_by() and summarize()",
    "section": "Example 1: Summarizing a Numeric Vector with tapply()",
    "text": "Example 1: Summarizing a Numeric Vector with tapply()\nSuppose you have a dataset with students’ exam scores and their corresponding grades. You want to calculate the average score for each grade.\n\n# Sample data\nscores &lt;- c(85, 90, 78, 92, 88, 76, 84, 92, 95, 89)\ngrades &lt;- c(\"A\", \"A\", \"B\", \"A\", \"B\", \"C\", \"B\", \"A\", \"A\", \"B\")\n\n# Using tapply() to calculate the average score for each grade\navg_scores &lt;- tapply(scores, grades, mean)\n\nprint(avg_scores)\n\n    A     B     C \n90.80 84.75 76.00 \n\n\nOr using the built in iris dataset:\n\nmean_width_by_species &lt;- tapply(iris$Sepal.Width, iris$Species, mean)\n\nprint(mean_width_by_species)\n\n    setosa versicolor  virginica \n     3.428      2.770      2.974 \n\n\nIn this example, tapply() splits the scores vector based on the different grades in the grades vector and calculates the average score for each grade. The same type of thing is done with the second example, splitting the data by Species."
  },
  {
    "objectID": "posts/2023-07-26/index.html#example-2-summarizing-a-data-frame-with-group_by-and-summarize",
    "href": "posts/2023-07-26/index.html#example-2-summarizing-a-data-frame-with-group_by-and-summarize",
    "title": "Summarizing Data in R: tapply() vs. group_by() and summarize()",
    "section": "Example 2: Summarizing a Data Frame with group_by() and summarize()",
    "text": "Example 2: Summarizing a Data Frame with group_by() and summarize()\nSuppose you have a dataset with information about employees, including their department, salary, and years of experience. You want to find the average salary and the maximum years of experience for each department.\nThe group_by() and summarize() functions from the dplyr package provide a more concise way to summarize data. The syntax for these functions is as follows:\ndata %&gt;%\n  group_by(INDEX) %&gt;%\n  summarize(FUN(...))\nWhere:\n\ndata is the data frame that you want to summarize.\nINDEX is the vector that you want to group by.\nFUN is the function that you want to apply to data.\n... are additional arguments that you want to pass to FUN.\n\n\n# Assuming you have already installed and loaded the 'dplyr' package\nlibrary(dplyr)\n\n# Sample data frame\nemployees &lt;- data.frame(\n  department = c(\"HR\", \"Engineering\", \"HR\", \"Engineering\", \"Marketing\", \"Marketing\"),\n  salary = c(50000, 65000, 48000, 70000, 55000, 60000),\n  experience = c(3, 5, 2, 7, 4, 6)\n)\n\n# Using group_by() and summarize() to calculate average salary \n# and max experience by department\nsummary_data &lt;- employees %&gt;%\n  group_by(department) %&gt;%\n  summarize(\n    avg_salary = mean(salary), \n    max_experience = max(experience)\n  )\n\nprint(summary_data)\n\n# A tibble: 3 × 3\n  department  avg_salary max_experience\n  &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Engineering      67500              7\n2 HR               49000              3\n3 Marketing        57500              6\n\n\nThe group_by() function groups the data by the department variable, and then summarize() calculates the average salary and maximum years of experience for each group.\nNow let’s also see how the functions can produce the same results and what it looks like side by side:\n\ntapply(iris$Sepal.Width, iris$Species, mean)\n\n    setosa versicolor  virginica \n     3.428      2.770      2.974 \n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(mean_width = mean(Sepal.Width))\n\n# A tibble: 3 × 2\n  Species    mean_width\n  &lt;fct&gt;           &lt;dbl&gt;\n1 setosa           3.43\n2 versicolor       2.77\n3 virginica        2.97"
  },
  {
    "objectID": "posts/2023-07-27/index.html",
    "href": "posts/2023-07-27/index.html",
    "title": "Unleashing the Power of Cumulative Mean in R: A Step-by-Step Guide",
    "section": "",
    "text": "As data-driven decision-making continues to shape our world, the need for insightful statistical analysis becomes ever more apparent. One crucial tool in a programmer’s arsenal is the “cumulative mean,” a statistical measure that allows us to understand the average value of a dataset as it evolves over time. In this blog post, we will delve into what a cumulative mean is, explore its applications, and equip you with the knowledge to unleash its potential using base R."
  },
  {
    "objectID": "posts/2023-07-27/index.html#example-1-finding-the-cumulative-mean-of-a-simple-vector",
    "href": "posts/2023-07-27/index.html#example-1-finding-the-cumulative-mean-of-a-simple-vector",
    "title": "Unleashing the Power of Cumulative Mean in R: A Step-by-Step Guide",
    "section": "Example 1: Finding the Cumulative Mean of a Simple Vector",
    "text": "Example 1: Finding the Cumulative Mean of a Simple Vector\n\n# Step 1: Create the 'data' vector\ndata &lt;- c(2, 4, 6, 8, 10)\n\n# Step 2: Calculate the cumulative sum\ncumulative_sum &lt;- cumsum(data)\n\n# Step 3: Calculate the cumulative mean\ncumulative_mean &lt;- cumulative_sum / seq_along(data)\n\n# Display the result\ncumulative_mean\n\n[1] 2 3 4 5 6"
  },
  {
    "objectID": "posts/2023-07-27/index.html#example-2-applying-cumulative-mean-to-real-world-data",
    "href": "posts/2023-07-27/index.html#example-2-applying-cumulative-mean-to-real-world-data",
    "title": "Unleashing the Power of Cumulative Mean in R: A Step-by-Step Guide",
    "section": "Example 2: Applying Cumulative Mean to Real-World Data",
    "text": "Example 2: Applying Cumulative Mean to Real-World Data\nLet’s use the cumulative mean to analyze monthly website traffic data.\n\n# Step 1: Create the 'monthly_traffic' vector\nmonthly_traffic &lt;- c(100, 200, 300, 400, 500)\n\n# Step 2: Calculate the cumulative sum\ncumulative_sum &lt;- cumsum(monthly_traffic)\n\n# Step 3: Calculate the cumulative mean\ncumulative_mean &lt;- cumulative_sum / seq_along(monthly_traffic)\n\n# Display the result\ncumulative_mean\n\n[1] 100 150 200 250 300\n\n\nHere are some more examples of how you might want to use a cumulative mean in R:\n\nTo track the average stock price over time\nTo track the average temperature over a period of days\nTo track the average number of visitors to a website over a period of weeks"
  },
  {
    "objectID": "posts/2023-07-28/index.html",
    "href": "posts/2023-07-28/index.html",
    "title": "The intersect() function in R",
    "section": "",
    "text": "Welcome to another exciting blog post where we delve into the world of R programming. Today, we’ll be discussing the intersect() function, a handy tool that helps us find the common elements shared between two or more vectors in R. Whether you’re a seasoned R programmer or just starting your journey, this function is sure to become a valuable addition to your toolkit."
  },
  {
    "objectID": "posts/2023-07-28/index.html#example-1-finding-common-elements",
    "href": "posts/2023-07-28/index.html#example-1-finding-common-elements",
    "title": "The intersect() function in R",
    "section": "Example 1: Finding Common Elements",
    "text": "Example 1: Finding Common Elements\nSuppose we have two vectors, vec1 and vec2, with some elements in common. We want to find those common elements:\n\nvec1 &lt;- c(1, 3, 5, 7, 9)\nvec2 &lt;- c(2, 4, 6, 8, 5, 10)\n\ncommon_elements &lt;- intersect(vec1, vec2)\ncommon_elements\n\n[1] 5\n\n\n\nExplanation\nIn this example, we have two vectors, vec1 and vec2. The intersect() function takes these two vectors as input and identifies the common element between them, which is 5. The function returns a new vector with only the common element."
  },
  {
    "objectID": "posts/2023-07-28/index.html#example-2-removing-duplicates",
    "href": "posts/2023-07-28/index.html#example-2-removing-duplicates",
    "title": "The intersect() function in R",
    "section": "Example 2: Removing Duplicates",
    "text": "Example 2: Removing Duplicates\nThe intersect() function can also be used to remove duplicates from a single vector:\n\nrepeated_vec &lt;- c(1, 2, 3, 4, 1, 2, 5, 6)\n\nunique_elements &lt;- intersect(repeated_vec, repeated_vec)\nunique_elements\n\n[1] 1 2 3 4 5 6\n\n\n\nExplanation\nIn this example, we have a vector repeated_vec with some duplicate elements. By using intersect() with the same vector twice, the function effectively removes all duplicates, giving us a new vector with only unique elements."
  },
  {
    "objectID": "posts/2023-07-28/index.html#example-3-empty-intersection",
    "href": "posts/2023-07-28/index.html#example-3-empty-intersection",
    "title": "The intersect() function in R",
    "section": "Example 3: Empty Intersection",
    "text": "Example 3: Empty Intersection\nIf the input vectors have no common elements, the intersect() function will return an empty vector:\n\nvec3 &lt;- c(11, 22, 33)\nvec4 &lt;- c(44, 55, 66)\n\nempty_intersection &lt;- intersect(vec3, vec4)\nempty_intersection\n\nnumeric(0)\n\n\n\nExplanation\nIn this case, vec3 and vec4 have no elements in common. Thus, the intersect() function returns an empty numeric vector (numeric(0))."
  },
  {
    "objectID": "posts/2023-07-28/index.html#example-4-using-strings",
    "href": "posts/2023-07-28/index.html#example-4-using-strings",
    "title": "The intersect() function in R",
    "section": "Example 4: Using Strings",
    "text": "Example 4: Using Strings\nHere is a final example using strings:\n\nintersect(c(\"apple\", \"banana\", \"cherry\"), c(\"banana\", \"cherry\", \"grape\"))\n\n[1] \"banana\" \"cherry\""
  },
  {
    "objectID": "posts/2023-07-31/index.html",
    "href": "posts/2023-07-31/index.html",
    "title": "The replicate() function in R",
    "section": "",
    "text": "As a programmer, you must have encountered situations where you need to repeat a task multiple times. Repetitive tasks are not only tedious but also prone to errors. What if I tell you there’s an elegant solution to this problem in R? Enter the replicate() function, your ultimate ally when it comes to replicating tasks effortlessly and efficiently."
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-1-generating-random-data",
    "href": "posts/2023-07-31/index.html#example-1-generating-random-data",
    "title": "The replicate() function in R",
    "section": "Example 1: Generating Random Data",
    "text": "Example 1: Generating Random Data\nSuppose you need to simulate a dataset for testing purposes or to understand the behavior of a statistical model. You can easily create 10 random samples, each containing 5 values, from a standard normal distribution using replicate():\n\n# Generate 10 random samples with 5 values each\nrandom_samples &lt;- replicate(10, rnorm(5))\nrandom_samples\n\n           [,1]       [,2]        [,3]       [,4]       [,5]       [,6]\n[1,]  0.5268093 -0.5237928  0.85010590  0.7289362  0.8444399  0.4592547\n[2,] -0.6796813  1.3037502 -1.18353409  0.5008129  0.2064732 -1.2990195\n[3,] -2.0398061 -1.2456373 -0.21356106 -0.3625780  0.1002410 -0.2273825\n[4,]  1.1870052  0.7734783 -0.32729379  2.0315941 -1.1789518 -0.2668686\n[5,] -1.1664056 -2.1379542  0.02431003  1.4414115 -0.7040298  0.7619186\n            [,7]       [,8]       [,9]      [,10]\n[1,] -0.89963532  0.4878779  0.4429697 -2.2369269\n[2,]  1.23571839  1.0790711 -1.4933201  1.4367740\n[3,]  0.06225175  0.1443591 -1.1423172 -0.6037171\n[4,] -1.37931063 -2.0674399 -1.8445978 -0.8033205\n[5,]  1.01821474  1.2571034  0.4151621  1.0140082\n\n\nIn this example, rnorm(5) generates five random values from a standard normal distribution, and replicate(10, ...) repeats this process 10 times, resulting in a matrix with 10 columns and 5 rows."
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-2-rolling-dice-with-replicate",
    "href": "posts/2023-07-31/index.html#example-2-rolling-dice-with-replicate",
    "title": "The replicate() function in R",
    "section": "Example 2: Rolling Dice with Replicate",
    "text": "Example 2: Rolling Dice with Replicate\nLet’s say you want to simulate rolling a fair six-sided die 20 times. With replicate(), you can easily simulate the rolls and get the results in a single line of code:\n\n# Simulate rolling a die 20 times\ndie_rolls &lt;- replicate(20, sample(1:6, 1, replace = TRUE))\ndie_rolls\n\n [1] 2 2 3 3 2 2 6 4 1 4 4 1 1 2 6 2 3 5 2 2\n\n\nIn this case, sample(1:6, 1, replace = TRUE) randomly selects one value from the sequence 1 to 6, simulating a single die roll. replicate(20, ...) repeats this simulation 20 times, giving you a vector of 20 die roll results."
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-3-evaluating-an-expression-multiple-times",
    "href": "posts/2023-07-31/index.html#example-3-evaluating-an-expression-multiple-times",
    "title": "The replicate() function in R",
    "section": "Example 3: Evaluating an Expression Multiple Times",
    "text": "Example 3: Evaluating an Expression Multiple Times\nConsider a scenario where you want to calculate the sum of squares for the numbers 1 to 5. Instead of manually typing out the expression five times, you can use replicate() to handle the repetition for you:\n\n# Calculate sum of squares for numbers 1 to 5\nsum_of_squares &lt;- replicate(5, sum((1:5)^2))\nsum_of_squares\n\n[1] 55 55 55 55 55\n\n\nHere, (1:5)^2 squares each number from 1 to 5, and sum(...) calculates the sum of these squared values. replicate(5, ...) repeats this process five times, giving you the sum of squares for each repetition."
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-4-generate-100-samples-of-a-binomial-distribution",
    "href": "posts/2023-07-31/index.html#example-4-generate-100-samples-of-a-binomial-distribution",
    "title": "The replicate() function in R",
    "section": "Example 4: Generate 100 samples of a binomial distribution",
    "text": "Example 4: Generate 100 samples of a binomial distribution\nTo generate 10 samples of size 100 from a binomial distribution with probability 0.5, you could use the following code:\n\nreplicate(10, sample(0:1, 100, replace = TRUE, prob = c(0.5, 0.5))) |&gt;\n  head(10)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    0    0    0    1    1    0    1    1    1     1\n [2,]    1    0    1    1    1    0    1    1    1     1\n [3,]    1    1    1    1    0    0    1    0    1     1\n [4,]    0    1    0    0    1    0    1    1    1     0\n [5,]    1    0    0    1    0    1    1    0    0     0\n [6,]    0    1    0    1    0    0    1    1    0     1\n [7,]    1    1    0    1    1    1    1    1    1     1\n [8,]    1    1    0    1    1    1    0    1    0     0\n [9,]    1    0    0    1    1    1    0    1    1     1\n[10,]    1    0    0    0    0    0    1    1    1     0"
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-5-calcluatel-meand-and-standard-deviation-of-normal-distribution",
    "href": "posts/2023-07-31/index.html#example-5-calcluatel-meand-and-standard-deviation-of-normal-distribution",
    "title": "The replicate() function in R",
    "section": "Example 5: Calcluatel Meand and Standard Deviation of Normal Distribution",
    "text": "Example 5: Calcluatel Meand and Standard Deviation of Normal Distribution\nTo calculate the mean and standard deviation of a normal distribution with mean 10 and standard deviation 5, you could use the following code:\n\nmeans &lt;- replicate(1000, mean(rnorm(100, 10, 5)))\nsds &lt;- replicate(1000, sd(rnorm(100, 10, 5)))\n\nhead(means)\n\n[1] 10.899256 10.613631 10.243307  9.977991  9.255811  9.440273\n\nhead(sds)\n\n[1] 4.908458 5.442502 4.883132 5.057129 4.953904 4.530682\n\nmean(means)\n\n[1] 9.997757\n\nsd(sds)\n\n[1] 0.3503813"
  },
  {
    "objectID": "posts/2023-07-31/index.html#example-5-calcluatel-mean-and-standard-deviation-of-normal-distribution",
    "href": "posts/2023-07-31/index.html#example-5-calcluatel-mean-and-standard-deviation-of-normal-distribution",
    "title": "The replicate() function in R",
    "section": "Example 5: Calcluatel Mean and Standard Deviation of Normal Distribution",
    "text": "Example 5: Calcluatel Mean and Standard Deviation of Normal Distribution\nTo calculate the mean and standard deviation of a normal distribution with mean 10 and standard deviation 5, you could use the following code:\n\nmeans &lt;- replicate(1000, mean(rnorm(100, 10, 5)))\nsds &lt;- replicate(1000, sd(rnorm(100, 10, 5)))\n\nhead(means)\n\n[1] 10.541465  9.613299  9.204776 10.254969  9.695199 10.398412\n\nhead(sds)\n\n[1] 4.693568 5.696749 6.103586 5.760336 5.045217 5.156735\n\nmean(means)\n\n[1] 9.99477\n\nsd(sds)\n\n[1] 0.352739"
  },
  {
    "objectID": "posts/2023-08-01/index.html",
    "href": "posts/2023-08-01/index.html",
    "title": "R Functions for Getting Objects",
    "section": "",
    "text": "Welcome, fellow programmers, to this exciting journey into the world of R functions! Today, we’ll explore four powerful functions: get(), get0(), dynGet(), and mget(). These functions may sound mysterious, but fear not; we’ll demystify them together and see how they can be incredibly handy tools in your R toolkit. So, let’s dive in!"
  },
  {
    "objectID": "posts/2023-08-01/index.html#example-get",
    "href": "posts/2023-08-01/index.html#example-get",
    "title": "R Functions for Getting Objects",
    "section": "Example get()",
    "text": "Example get()\nLet’s say you have a variable named my_variable stored somewhere in your R environment, and you want to access its value using the get() function:\n\n# Sample variable in the environment\nmy_variable &lt;- 42\n\n# Using get() to retrieve the value\nresult &lt;- get(\"my_variable\")\nresult\n\n[1] 42"
  },
  {
    "objectID": "posts/2023-08-01/index.html#explanation-of-get",
    "href": "posts/2023-08-01/index.html#explanation-of-get",
    "title": "R Functions for Getting Objects",
    "section": "Explanation of get()",
    "text": "Explanation of get()\nIn the example above, we used get(\"my_variable\") to access the value of the variable my_variable. The function returned the value 42, which was stored in the variable."
  },
  {
    "objectID": "posts/2023-08-01/index.html#example-get0",
    "href": "posts/2023-08-01/index.html#example-get0",
    "title": "R Functions for Getting Objects",
    "section": "Example get0()",
    "text": "Example get0()\nLet’s use the same variable my_variable as before and see the difference between get() and get0():\n\n# Sample variable in the environment\nmy_variable &lt;- 42\n\n# Using get0() to retrieve the variable itself\nresult &lt;- get0(\"my_var\", ifnotfound = \"Does Not Exist\")\nresult\n\n[1] \"Does Not Exist\""
  },
  {
    "objectID": "posts/2023-08-01/index.html#explanation-of-get0",
    "href": "posts/2023-08-01/index.html#explanation-of-get0",
    "title": "R Functions for Getting Objects",
    "section": "Explanation of get0()",
    "text": "Explanation of get0()\nIn this example, get0(\"my_var\") returned an error message as the variable was not found."
  },
  {
    "objectID": "posts/2023-08-01/index.html#example-dynget",
    "href": "posts/2023-08-01/index.html#example-dynget",
    "title": "R Functions for Getting Objects",
    "section": "Example dynGet()",
    "text": "Example dynGet()\nConsider a scenario where you have a variable named num inside a custom environment, and you want to access it using dynGet():\n\n# Create a new environment\ncustom_env &lt;- new.env()\n\n# Assign a variable inside the custom environment\ncustom_env$num &lt;- 99\n\n# Using dynGet() to retrieve the value\nresult_env &lt;- dynGet(\"num\", custom_env)\nresult_env\n\n&lt;environment: 0x0000019c769ff050&gt;\n\nresult_num &lt;- dynGet(\"num\", custom_env$num)\nresult_num\n\n[1] 99"
  },
  {
    "objectID": "posts/2023-08-01/index.html#explanation-of-dynget",
    "href": "posts/2023-08-01/index.html#explanation-of-dynget",
    "title": "R Functions for Getting Objects",
    "section": "Explanation of dynGet()",
    "text": "Explanation of dynGet()\nIn this example, we used dynGet(\"num\", custom_env$num) to access the value of the variable num from the specified custom_env environment. The function successfully retrieved the value 99."
  },
  {
    "objectID": "posts/2023-08-01/index.html#example-of-mget",
    "href": "posts/2023-08-01/index.html#example-of-mget",
    "title": "R Functions for Getting Objects",
    "section": "Example of mget()",
    "text": "Example of mget()\nLet’s say we have two variables, x and y, and we want to retrieve their values using mget():\n\n# Sample variables in the environment\nx &lt;- 10\ny &lt;- 20\n\n# Using mget() to retrieve the values of multiple variables\nresult &lt;- mget(c(\"x\", \"y\"))\nresult # Output: a named list with values: $x [1] 10, $y [1] 20\n\n$x\n[1] 10\n\n$y\n[1] 20"
  },
  {
    "objectID": "posts/2023-08-01/index.html#explanation-of-mget",
    "href": "posts/2023-08-01/index.html#explanation-of-mget",
    "title": "R Functions for Getting Objects",
    "section": "Explanation of mget()",
    "text": "Explanation of mget()\nIn this example, we provided the vector c(\"x\", \"y\") to mget(), and it returned a named list with the values of both variables x and y."
  },
  {
    "objectID": "posts/2023-08-02/index.html",
    "href": "posts/2023-08-02/index.html",
    "title": "The unlist() Function in R",
    "section": "",
    "text": "Hey fellow R enthusiasts!\nToday, we’re diving deep into the incredible world of R programming to explore the often-overlooked but extremely handy unlist() function. If you’ve ever found yourself dealing with complex nested lists or vectors, this little gem can be a lifesaver. The unlist() function is like a magician that simplifies your data structures, making them more manageable and easier to work with. Let’s unlock its magic together!"
  },
  {
    "objectID": "posts/2023-08-02/index.html#example-1-flattening-a-simple-list",
    "href": "posts/2023-08-02/index.html#example-1-flattening-a-simple-list",
    "title": "The unlist() Function in R",
    "section": "Example 1: Flattening a Simple List",
    "text": "Example 1: Flattening a Simple List\nLet’s start with a straightforward example of a list containing some numeric values:\n\n# Create a simple list\nmy_list &lt;- list(1, 2, 3, 4, 5)\nmy_list\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n# Flatten the list\nflattened_vector &lt;- unlist(my_list)\nflattened_vector\n\n[1] 1 2 3 4 5\n\n\nIn this example, we had a list containing five numeric elements, and unlist() transformed it into a flat atomic vector."
  },
  {
    "objectID": "posts/2023-08-02/index.html#example-2-flattening-a-nested-list",
    "href": "posts/2023-08-02/index.html#example-2-flattening-a-nested-list",
    "title": "The unlist() Function in R",
    "section": "Example 2: Flattening a Nested List",
    "text": "Example 2: Flattening a Nested List\nThe real magic of unlist() shines when dealing with nested lists. Let’s consider a nested list:\n\n# Create a nested list\nnested_list &lt;- list(1, 2, list(3, 4), 5)\nnested_list\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[[3]][[1]]\n[1] 3\n\n[[3]][[2]]\n[1] 4\n\n\n[[4]]\n[1] 5\n\n# Flatten the nested list\nflattened_vector &lt;- unlist(nested_list)\nflattened_vector\n\n[1] 1 2 3 4 5\n\n\nThe unlist() function works recursively by default, so it will dive into the nested list and create a single vector containing all elements."
  },
  {
    "objectID": "posts/2023-08-02/index.html#example-3-removing-names-from-the-result",
    "href": "posts/2023-08-02/index.html#example-3-removing-names-from-the-result",
    "title": "The unlist() Function in R",
    "section": "Example 3: Removing Names from the Result",
    "text": "Example 3: Removing Names from the Result\nSometimes, you might prefer to discard the names of elements in the resulting vector to keep things simple and clean. You can achieve this using the use.names parameter:\n\n# Create a named list\nnamed_list &lt;- list(a = 10, b = 20, c = 30)\nnamed_list\n\n$a\n[1] 10\n\n$b\n[1] 20\n\n$c\n[1] 30\n\n# Flatten the list without preserving names\nflattened_vector &lt;- unlist(named_list, use.names = TRUE)\nflattened_vector\n\n a  b  c \n10 20 30"
  },
  {
    "objectID": "posts/2023-08-03/index.html",
    "href": "posts/2023-08-03/index.html",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "",
    "text": "Welcome, data enthusiasts! If you’re diving into the realm of data analysis with R, one function you’ll undoubtedly encounter is read.delim(). It’s an essential tool that allows you to read tabular data from a delimited text file and load it into R for further analysis. But fret not, dear reader, as I’ll walk you through this function in simple terms, with plenty of examples to guide you along the way.\n\n\nread.delim() is an R function used to read data from a text file where columns are separated by a delimiter. The default delimiter is a tab character (\\t), but you can customize it to match your data’s format.\nHere’s the basic syntax of read.delim():\nread.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\", ...)\n\nfile is the name of the file to be read.\nheader is a logical value that indicates whether the first line of the file contains the column names. The default value is TRUE.\nsep is the character that separates the columns in the file. The default value is a tab (.\nquote is the character that is used to quote strings in the file. The default value is a double quote (“).\n… are additional arguments that can be passed to the function."
  },
  {
    "objectID": "posts/2023-08-03/index.html#what-is-read.delim-and-its-syntax",
    "href": "posts/2023-08-03/index.html#what-is-read.delim-and-its-syntax",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "",
    "text": "read.delim() is an R function used to read data from a text file where columns are separated by a delimiter. The default delimiter is a tab character (\\t), but you can customize it to match your data’s format.\nHere’s the basic syntax of read.delim():\nread.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\", ...)\n\nfile is the name of the file to be read.\nheader is a logical value that indicates whether the first line of the file contains the column names. The default value is TRUE.\nsep is the character that separates the columns in the file. The default value is a tab (.\nquote is the character that is used to quote strings in the file. The default value is a double quote (“).\n… are additional arguments that can be passed to the function."
  },
  {
    "objectID": "posts/2023-08-03/index.html#example-1-basic-usage",
    "href": "posts/2023-08-03/index.html#example-1-basic-usage",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "Example 1: Basic Usage",
    "text": "Example 1: Basic Usage\nImagine we have a file named data.txt that looks like this:\nName,Age,Country\nJohn,25,USA\nJane,30,Canada\nLet’s make the file:\ncat(\"Name,Age,Country\\nJohn,25,USA\\nJane,30,Canada\\n\", \n    file = \"posts/2023-08-03/data.txt\")\nTo load this data into R:\n\n# Assuming the file is in the current working directory\nread.delim(\"data.txt\")\n\n  Name.Age.Country\n1      John,25,USA\n2   Jane,30,Canada\n\n\nIn this case, read.delim() will automatically detect the tab delimiter and consider the first row as column names. You will notice that it did not separate based upon the delimiter, as this file was not actually tab delimited."
  },
  {
    "objectID": "posts/2023-08-03/index.html#example-2-custom-delimiter",
    "href": "posts/2023-08-03/index.html#example-2-custom-delimiter",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "Example 2: Custom Delimiter",
    "text": "Example 2: Custom Delimiter\nNow, let’s read in that same file but change the sep argument to ',':\n\nread.delim(\"data.txt\", sep = \",\")\n\n  Name Age Country\n1 John  25     USA\n2 Jane  30  Canada"
  },
  {
    "objectID": "posts/2023-08-03/index.html#example-3-file-without-header",
    "href": "posts/2023-08-03/index.html#example-3-file-without-header",
    "title": "A Handy Guide to read.delim() in R - Unraveling the Magic of Reading Tabular Data",
    "section": "Example 3: File Without Header",
    "text": "Example 3: File Without Header\nIn some cases, your file might not have a header row. Let’s consider data_no_header.txt:\nJohn,25,USA\nJane,30,Canada\nYou can handle this by setting header = FALSE:\n\nread.delim(\"data_no_header.txt\",sep = \",\",header = FALSE)\n\n    V1 V2     V3\n1 John 25    USA\n2 Jane 30 Canada"
  },
  {
    "objectID": "posts/2023-08-04/index.html",
    "href": "posts/2023-08-04/index.html",
    "title": "Exploring R’s Versatile str() Function: Unraveling Your Data with Ease!",
    "section": "",
    "text": "Welcome, fellow data enthusiasts, to another exciting blog post! Today, we’re diving deep into R’s invaluable str() function – a powerful tool for gaining insight into your datasets. Whether you’re a seasoned data scientist or just starting with R, str() will undoubtedly become your go-to function for data exploration. Let’s embark on this journey together and unleash the full potential of str()!"
  },
  {
    "objectID": "posts/2023-08-04/index.html#example-1-basic-usage-with-a-vector",
    "href": "posts/2023-08-04/index.html#example-1-basic-usage-with-a-vector",
    "title": "Exploring R’s Versatile str() Function: Unraveling Your Data with Ease!",
    "section": "Example 1: Basic Usage with a Vector",
    "text": "Example 1: Basic Usage with a Vector\nLet’s begin with a simple example. Suppose we have a numeric vector named “ages,” representing the ages of individuals in a survey:\n\nages &lt;- c(25, 30, 22, 40, 35)\nstr(ages)\n\n num [1:5] 25 30 22 40 35\n\n\nHere, the output reveals that “ages” is a numeric vector with five elements, ranging from 25 to 35. It helps us quickly confirm the data type and size."
  },
  {
    "objectID": "posts/2023-08-04/index.html#example-2-investigating-a-data-frame",
    "href": "posts/2023-08-04/index.html#example-2-investigating-a-data-frame",
    "title": "Exploring R’s Versatile str() Function: Unraveling Your Data with Ease!",
    "section": "Example 2: Investigating a Data Frame",
    "text": "Example 2: Investigating a Data Frame\nNow, let’s explore a more complex scenario. We have a data frame named “students,” containing information about students’ names, ages, and grades:\n\nstudents &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(22, 23, 21),\n  grade = c(\"A\", \"B\", \"A-\")\n)\nstr(students)\n\n'data.frame':   3 obs. of  3 variables:\n $ name : chr  \"Alice\" \"Bob\" \"Charlie\"\n $ age  : num  22 23 21\n $ grade: chr  \"A\" \"B\" \"A-\"\n\n\nThe output informs us that “students” is a data frame with three observations (rows) and three variables (columns). It also lists the data types for each column, with “chr” representing character and “num” representing numeric."
  },
  {
    "objectID": "posts/2023-08-04/index.html#example-3-checking-nested-data-structures",
    "href": "posts/2023-08-04/index.html#example-3-checking-nested-data-structures",
    "title": "Exploring R’s Versatile str() Function: Unraveling Your Data with Ease!",
    "section": "Example 3: Checking Nested Data Structures",
    "text": "Example 3: Checking Nested Data Structures\nstr() handles nested data structures effortlessly. Let’s consider a list called “nested_data” containing a data frame and a character vector:\n\nnested_data &lt;- list(\n  data_frame = data.frame(x = 1:3, y = 4:6),\n  character_vector = c(\"hello\", \"world\", \"R\")\n)\nstr(nested_data)\n\nList of 2\n $ data_frame      :'data.frame':   3 obs. of  2 variables:\n  ..$ x: int [1:3] 1 2 3\n  ..$ y: int [1:3] 4 5 6\n $ character_vector: chr [1:3] \"hello\" \"world\" \"R\"\n\n\nThe output provides a comprehensive breakdown of the nested_data list. It consists of two components: a data frame with two variables, “x” and “y,” and a character vector.\nHere are some additional examples of how to use the str() function:\nTo display the structure of a list, you would use the following code:\n\nstr(list(a = 1, b = \"hello\", c = list(1, 2, 3)))\n\nList of 3\n $ a: num 1\n $ b: chr \"hello\"\n $ c:List of 3\n  ..$ : num 1\n  ..$ : num 2\n  ..$ : num 3\n\n\nTo display the structure of a function, you would use the following code:\n\nstr(function(x) x^2)\n\nfunction (x)  \n - attr(*, \"srcref\")= 'srcref' int [1:8] 1 5 1 19 5 19 1 1\n  ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' &lt;environment: 0x000002c3e62abd70&gt; \n\n\nIf you want to see the options that are available to be set to the str() function, then just run the below code:\n\noptions()$str\n\n$strict.width\n[1] \"no\"\n\n$digits.d\n[1] 3\n\n$vec.len\n[1] 4\n\n$list.len\n[1] 99\n\n$deparse.lines\nNULL\n\n$drop.deparse.attr\n[1] TRUE\n\n$formatNum\nfunction (x, ...) \nformat(x, trim = TRUE, drop0trailing = TRUE, ...)\n&lt;environment: 0x000002c3e214ad28&gt;"
  },
  {
    "objectID": "posts/2023-08-07/index.html",
    "href": "posts/2023-08-07/index.html",
    "title": "Enhance Your Plots with the text() Function in R",
    "section": "",
    "text": "As a programmer, you’re well aware of the importance of data visualization. A well-crafted plot can convey complex information with clarity and impact. In R, creating stunning plots is a breeze, especially when you’re armed with the versatile text() function. This little gem allows you to add custom text to your plots, enabling you to annotate and highlight essential details. Let’s dive into the world of text() and uncover its syntax and potential through some hands-on examples."
  },
  {
    "objectID": "posts/2023-08-07/index.html#example-1-simple-annotation",
    "href": "posts/2023-08-07/index.html#example-1-simple-annotation",
    "title": "Enhance Your Plots with the text() Function in R",
    "section": "Example 1: Simple Annotation",
    "text": "Example 1: Simple Annotation\nLet’s start with a basic scatter plot representing the relationship between two variables.\n\n# Create sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(5, 9, 6, 8, 12)\nplot(x, y, main = \"Scatter Plot Example\")\n\n# Add text annotation\ntext(3, 8, \"Key Point\", col = \"blue\", cex = 1.2)\n\n\n\n\nIn this example, we’ve created a scatter plot and used text() to add an annotation (“Key Point”) at the coordinates (3, 8). We’ve also adjusted the text color and size for emphasis."
  },
  {
    "objectID": "posts/2023-08-07/index.html#example-2-annotating-multiple-points",
    "href": "posts/2023-08-07/index.html#example-2-annotating-multiple-points",
    "title": "Enhance Your Plots with the text() Function in R",
    "section": "Example 2: Annotating Multiple Points",
    "text": "Example 2: Annotating Multiple Points\nWhat if you want to annotate multiple points on your plot? No worries, the text() function can handle that too!\n\n# Continue from previous example\npoints &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\")\nplot(x, y)\ntext(x, y, labels = points, pos = 3, col = \"green\")\n\n\n\n\nHere, we’ve added labels “A” through “E” to their respective data points. The pos parameter ensures that the text appears above the points, making the plot more informative."
  },
  {
    "objectID": "posts/2023-08-07/index.html#example-3-mathematical-expressions",
    "href": "posts/2023-08-07/index.html#example-3-mathematical-expressions",
    "title": "Enhance Your Plots with the text() Function in R",
    "section": "Example 3: Mathematical Expressions",
    "text": "Example 3: Mathematical Expressions\nMathematical annotations can elevate your plots, making them more informative.\n\n# Create a sine wave plot\nx &lt;- seq(0, 2 * pi, length.out = 100)\ny &lt;- sin(x)\nplot(x, y, type = \"l\", col = \"red\")\n\n# Add equation using mathematical notation\ntext(pi/2, 1, expression(sin(theta)), col = \"purple\", cex = 1.2)\n\n\n\n\nIn this case, we’ve drawn a sine wave and used an expression to annotate the maximum point with the equation “sin(θ)”."
  },
  {
    "objectID": "posts/2023-08-08/index.html",
    "href": "posts/2023-08-08/index.html",
    "title": "Mastering Data Transformation with the scale() Function in R",
    "section": "",
    "text": "Data analysis often requires preprocessing and transforming data to make it more suitable for analysis. In R, the scale() function is a powerful tool that allows you to standardize or normalize your data, helping you unlock deeper insights. In this blog post, we’ll dive into the syntax of the scale() function, provide real-world examples, and encourage you to explore this function on your own. The scale() function can be used to center and scale the columns of a numeric matrix, or to scale a vector. This can be useful for a variety of tasks, such as:\n\nComparing data that is measured in different units\nImproving the performance of machine learning algorithms\nMaking data more interpretable"
  },
  {
    "objectID": "posts/2023-08-08/index.html#example-1-centering-and-scaling",
    "href": "posts/2023-08-08/index.html#example-1-centering-and-scaling",
    "title": "Mastering Data Transformation with the scale() Function in R",
    "section": "Example 1: Centering and Scaling",
    "text": "Example 1: Centering and Scaling\nLet’s say you have a dataset height_weight with columns ‘Height’ and ‘Weight’, and you want to center and scale the data:\n\n# Sample data\nheight_weight &lt;- data.frame(Height = c(160, 175, 150, 180),\n                             Weight = c(60, 70, 55, 75))\n\n# Centering and scaling\nscaled_data &lt;- scale(height_weight, center = TRUE, scale = TRUE)\nscaled_data\n\n         Height     Weight\n[1,] -0.4539206 -0.5477226\n[2,]  0.6354889  0.5477226\n[3,] -1.1801937 -1.0954451\n[4,]  0.9986254  1.0954451\nattr(,\"scaled:center\")\nHeight Weight \n166.25  65.00 \nattr(,\"scaled:scale\")\n   Height    Weight \n13.768926  9.128709 \n\n\nIn this example, the scale() function calculates the mean and standard deviation for each column. It then subtracts the mean and divides by the standard deviation, giving you centered and scaled data."
  },
  {
    "objectID": "posts/2023-08-08/index.html#example-2-centering-only",
    "href": "posts/2023-08-08/index.html#example-2-centering-only",
    "title": "Mastering Data Transformation with the scale() Function in R",
    "section": "Example 2: Centering Only",
    "text": "Example 2: Centering Only\nLet’s consider a scenario where you want to center the data but not scale it:\n\n# Sample data\ntemperatures &lt;- c(25, 30, 28, 33, 22)\n\n# Centering without scaling\nscaled_temps &lt;- scale(temperatures, center = TRUE, scale = FALSE)\nscaled_temps\n\n     [,1]\n[1,] -2.6\n[2,]  2.4\n[3,]  0.4\n[4,]  5.4\n[5,] -5.6\nattr(,\"scaled:center\")\n[1] 27.6\n\n\nIn this case, the scale() function only centers the data by subtracting the mean, maintaining the original range of values."
  },
  {
    "objectID": "posts/2023-08-08/index.html#example-3-scaling-a-matrix",
    "href": "posts/2023-08-08/index.html#example-3-scaling-a-matrix",
    "title": "Mastering Data Transformation with the scale() Function in R",
    "section": "Example 3: Scaling a Matrix",
    "text": "Example 3: Scaling a Matrix\nHere is an example of how to use the scale() function to scale the columns of a matrix:\n\nm &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3)\nscaled_m &lt;- scale(m)\n\nscaled_m\n\n     [,1] [,2] [,3]\n[1,]   -1   -1   -1\n[2,]    0    0    0\n[3,]    1    1    1\nattr(,\"scaled:center\")\n[1] 2 5 8\nattr(,\"scaled:scale\")\n[1] 1 1 1"
  },
  {
    "objectID": "posts/2023-08-09/index.html",
    "href": "posts/2023-08-09/index.html",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "",
    "text": "When it comes to data visualization in R, the par() function is an indispensable tool that often goes overlooked. This function allows you to control various graphical parameters, unleashing a world of customization possibilities for your plots. In this blog post, we’ll demystify the par() function, break down its syntax, and provide you with hands-on examples to help you create stunning visualizations."
  },
  {
    "objectID": "posts/2023-08-09/index.html#example-1-adjusting-plot-margins",
    "href": "posts/2023-08-09/index.html#example-1-adjusting-plot-margins",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "Example 1: Adjusting Plot Margins",
    "text": "Example 1: Adjusting Plot Margins\n\npar(mar = c(5, 4, 4, 2) + 0.1)\nplot(1:10)\n\n\n\n\nIn this example, we’re using the mar parameter to control the margins of the plot. The vector c(5, 4, 4, 2) + 0.1 specifies the bottom, left, top, and right margins, respectively. Increasing the margins gives more space for titles, labels, and annotations."
  },
  {
    "objectID": "posts/2023-08-09/index.html#example-2-changing-plot-colors",
    "href": "posts/2023-08-09/index.html#example-2-changing-plot-colors",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "Example 2: Changing Plot Colors",
    "text": "Example 2: Changing Plot Colors\n\npar(col.main = \"blue\", col.axis = \"red\")\nplot(1:10, main = \"Custom Colors\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n\n\n\nHere, we’re utilizing col.main and col.axis to change the color of the main title and axis labels. This adds a touch of vibrancy to your plots and enhances readability."
  },
  {
    "objectID": "posts/2023-08-09/index.html#example-3-adjusting-font-size",
    "href": "posts/2023-08-09/index.html#example-3-adjusting-font-size",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "Example 3: Adjusting Font Size:",
    "text": "Example 3: Adjusting Font Size:\n\npar(cex.main = 1.5, cex.axis = 0.8)\nplot(1:10, main = \"Bigger Title, Smaller Labels\")\n\n\n\n\nWith cex.main and cex.axis, you can control the size of the main title and axis labels, respectively. This allows you to emphasize important information and fine-tune the presentation."
  },
  {
    "objectID": "posts/2023-08-09/index.html#example-4-controlling-axis-type",
    "href": "posts/2023-08-09/index.html#example-4-controlling-axis-type",
    "title": "Mastering Data Visualization: A Guide to Harnessing the Power of R’s par() Function",
    "section": "Example 4: Controlling Axis Type",
    "text": "Example 4: Controlling Axis Type\n\npar(log = \"y\")\n\nWarning in par(log = \"y\"): \"log\" is not a graphical parameter\n\nplot(1:10, log = \"y\", main = \"Logarithmic Y-axis\")\n\n\n\n\nBy setting log = \"y\", you’re instructing R to use a logarithmic scale for the y-axis. This is particularly useful when dealing with data that spans several orders of magnitude."
  },
  {
    "objectID": "posts/2023-08-10/index.html",
    "href": "posts/2023-08-10/index.html",
    "title": "Mastering Grouped Counting in R: A Comprehensive Guide",
    "section": "",
    "text": "Introduction\nAs data-driven decision-making becomes more critical in various fields, the ability to extract valuable insights from datasets has never been more important. One common task is to calculate counts by group, which can shed light on trends and patterns within your data. In this guide, we’ll explore three different approaches to achieve this using the powerful R programming language. So, let’s dive into the world of grouped counting with the help of the classic mtcars dataset!\n\n\nThe aggregate() Function: A Solid Foundation\nTo kick things off, let’s start with the aggregate() function available in base R. This function is a versatile tool for aggregating data based on grouping variables. Here’s how you can use it to calculate counts by group using the mtcars dataset:\n\n# Load the dataset\ndata(\"mtcars\")\n\n# Calculate counts by group using aggregate()\ngroup_counts &lt;- aggregate(\n  data = mtcars, \n  carb ~ cyl, \n  FUN = function(x) length(unique(x))\n  )\ngroup_counts\n\n  cyl carb\n1   4    2\n2   6    3\n3   8    4\n\n\nIn this example, we’re counting the number of cars in each cylinder group. The aggregate() function groups the data by the ‘cyl’ variable and applies the length() and unique() functions to count the number of distinct carb per cyl group.\n\n\nHarnessing the Power of dplyr Library\nMoving on, the dplyr package is a staple in data manipulation and offers an elegant way to work with grouped data. The group_by() and summarise() functions are your go-to tools for such tasks. Let’s see how they can be used with the mtcars dataset:\n\n# Load the required library\nlibrary(dplyr)\n\n# Calculate counts by group using dplyr\ngroup_counts_dplyr &lt;- mtcars |&gt;\n  group_by(cyl) |&gt;\n  summarise(count = n_distinct(carb))\ngroup_counts_dplyr\n\n# A tibble: 3 × 2\n    cyl count\n  &lt;dbl&gt; &lt;int&gt;\n1     4     2\n2     6     3\n3     8     4\n\n\nIn this example, we use the group_by() function to group the data by cylinder count and then use summarise() with n_distinct() to create a ‘count’ column containing the number of distinct carb per cyl group.\n\n\nEfficiency and Speed with data.table\nFor those dealing with larger datasets, the data.table package offers lightning-fast performance. It’s especially handy for tasks involving grouping and aggregation. Here’s how you can use it with the mtcars dataset:\n\n# Load the required library\nlibrary(data.table)\n\n# Convert mtcars to data.table\ndt_mtcars &lt;- as.data.table(mtcars)\n\n# Calculate counts by group using data.table\ngroup_counts_dt &lt;- dt_mtcars[, .(count = length(unique(carb))), by = cyl]\nsetorder(group_counts_dt, cols = \"cyl\")\ngroup_counts_dt\n\n   cyl count\n1:   4     2\n2:   6     3\n3:   8     4\n\n\nIn this example, we convert the mtcars dataset to a data.table using as.data.table(). Then, we use the length(unique(carb)) special symbol to count the number of distinct carb in each cyl group.\n\n\nTry It Yourself!\nNow that you’ve seen three powerful ways to calculate counts by group in R, it’s time to roll up your sleeves and give them a try. Experiment with these methods using your own datasets, and witness how easy it is to uncover valuable insights from your data.\nWhether you opt for the solid foundation of aggregate(), the elegance of dplyr, or the efficiency of data.table, each approach has its unique strengths. As you become more comfortable with these techniques, you’ll be better equipped to tackle complex data analysis tasks and make informed decisions.\nSo, don’t hesitate to put your newfound knowledge into action. Happy coding and happy exploring your data!"
  },
  {
    "objectID": "posts/2023-08-11/index.html",
    "href": "posts/2023-08-11/index.html",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "",
    "text": "Title: Unleashing the Power of pmax() and pmin() Functions in R\nIntroduction: In the realm of data manipulation and analysis, R stands tall as a versatile programming language. Among its plethora of functions, pmax() and pmin() shine as unsung heroes that can greatly simplify your coding experience. These functions allow you to effortlessly find the element-wise maximum and minimum values across vectors in R, providing an elegant solution to a common programming challenge. In this blog post, we’ll dive into the syntax and explore real-world examples that showcase the true potential of pmax() and pmin()."
  },
  {
    "objectID": "posts/2023-08-11/index.html#pmax-function",
    "href": "posts/2023-08-11/index.html#pmax-function",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "pmax() Function:",
    "text": "pmax() Function:\npmax(..., na.rm = FALSE)\n\nThe ellipsis (...) signifies the input vectors. You can pass two or more vectors to compare element-wise.\nThe na.rm parameter (defaulting to FALSE) determines whether to remove NAs before computation."
  },
  {
    "objectID": "posts/2023-08-11/index.html#pmin-function",
    "href": "posts/2023-08-11/index.html#pmin-function",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "pmin() Function:",
    "text": "pmin() Function:\npmin(..., na.rm = FALSE)\n\nSimilar to pmax(), the ellipsis (...) denotes the input vectors for element-wise comparison.\nThe na.rm parameter (defaulting to FALSE) decides whether to exclude NAs before calculation."
  },
  {
    "objectID": "posts/2023-08-11/index.html#example-1-using-pmax-to-find-element-wise-maximum",
    "href": "posts/2023-08-11/index.html#example-1-using-pmax-to-find-element-wise-maximum",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "Example 1: Using pmax() to Find Element-wise Maximum",
    "text": "Example 1: Using pmax() to Find Element-wise Maximum\n\nvec1 &lt;- c(3, 9, 2, 6)\nvec2 &lt;- c(7, 1, 8, 4)\nresult &lt;- pmax(vec1, vec2)\nresult\n\n[1] 7 9 8 6"
  },
  {
    "objectID": "posts/2023-08-11/index.html#example-2-using-pmin-to-find-element-wise-minimum",
    "href": "posts/2023-08-11/index.html#example-2-using-pmin-to-find-element-wise-minimum",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "Example 2: Using pmin() to Find Element-wise Minimum",
    "text": "Example 2: Using pmin() to Find Element-wise Minimum\n\ndata1 &lt;- c(12, 5, 9, 16)\ndata2 &lt;- c(6, 14, 8, 11)\nresult &lt;- pmin(data1, data2)\nresult\n\n[1]  6  5  8 11"
  },
  {
    "objectID": "posts/2023-08-11/index.html#example-3-handling-na-values",
    "href": "posts/2023-08-11/index.html#example-3-handling-na-values",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "Example 3: Handling NA Values",
    "text": "Example 3: Handling NA Values\n\ndata1 &lt;- c(7, 3, NA, 12)\ndata2 &lt;- c(9, NA, 5, 8)\nresult &lt;- pmax(data1, data2, na.rm = TRUE)\nresult\n\n[1]  9  3  5 12"
  },
  {
    "objectID": "posts/2023-08-11/index.html#example-4-scaling-with-multiple-vectors",
    "href": "posts/2023-08-11/index.html#example-4-scaling-with-multiple-vectors",
    "title": "pmax() and pmin(): Finding the Parallel Maximum and Minimum in R",
    "section": "Example 4: Scaling with Multiple Vectors",
    "text": "Example 4: Scaling with Multiple Vectors\n\nheights &lt;- c(165, 178, 155, 189)\nweights &lt;- c(68, 82, 61, 76)\nbmi &lt;- pmax(weights / ((heights / 100) ^ 2))\nbmi\n\n[1] 24.97704 25.88057 25.39022 21.27600"
  },
  {
    "objectID": "posts/2023-08-14/index.html",
    "href": "posts/2023-08-14/index.html",
    "title": "The substring() function in R",
    "section": "",
    "text": "The substring() function in R is used to extract a substring from a character vector. The syntax of the function is:\nsubstring(x, start, stop)\nwhere:\n\nx is the character vector from which to extract the substring\nstart is the starting position of the substring\nstop is the ending position of the substring\n\nThe start and stop arguments can be either integers or character strings. If they are integers, they specify the positions of the characters in the string. If they are character strings, they specify the characters that should be used as the starting and ending positions of the substring."
  },
  {
    "objectID": "posts/2023-08-14/index.html#example-1",
    "href": "posts/2023-08-14/index.html#example-1",
    "title": "The substring() function in R",
    "section": "Example 1",
    "text": "Example 1\nFor example, the following code will extract the substring from the string “Hello, world!” that starts at the 5th character and ends at the 8th character:\n\nsubstring(\"Hello, world!\", 8, 12)\n\n[1] \"world\"\n\n\nAs we see this will return the string “world”."
  },
  {
    "objectID": "posts/2023-08-14/index.html#example-2",
    "href": "posts/2023-08-14/index.html#example-2",
    "title": "The substring() function in R",
    "section": "Example 2",
    "text": "Example 2\nThe substring() function can also be used to extract the first N characters of a string, the last N characters of a string, or to replace a substring in a string.\nTo extract the first N characters of a string, you can use the following syntax:\nsubstring(x, 1, N)\nFor example, the following code will extract the first 5 characters of the string “Hello, world!”:\n\nsubstring(\"Hello, world!\", 1, 5)\n\n[1] \"Hello\"\n\n\nAs seen this will return the string “Hello”."
  },
  {
    "objectID": "posts/2023-08-14/index.html#example-3",
    "href": "posts/2023-08-14/index.html#example-3",
    "title": "The substring() function in R",
    "section": "Example 3",
    "text": "Example 3\nTo extract the last N characters of a string, you can use the following syntax:\nsubstring(x, nchar(x) - N + 1, nchar(x))\nwhere nchar(x) is the function that returns the length of the string x.\nFor example, the following code will extract the last 5 characters of the string “Hello, world!”:\n\ns &lt;- \"Hello, world!\"\n\nsubstring(s, nchar(s) - 6 + 1, nchar(s))\n\n[1] \"world!\"\n\n\nThis will return the string “world!”."
  },
  {
    "objectID": "posts/2023-08-14/index.html#example-4",
    "href": "posts/2023-08-14/index.html#example-4",
    "title": "The substring() function in R",
    "section": "Example 4",
    "text": "Example 4\nTo replace a substring in a string, you can use the following syntax:\nsubstring(x, start, stop) &lt;- value\nwhere value is the string that you want to replace the substring with.\nFor example, the following code will replace the substring “world” in the string “Hello, world!” with the string “universe”:\n\ns &lt;- \"Hello, world!\"\nsubstring(s, first = 8) &lt;- \"universe\"\ns\n\n[1] \"Hello, univer\"\n\n\nThis will change the string to “Hello, univer”. You notice that it will not expand the original length of the string.\nIn addition to the substring() function, there are also a few other functions that can be used to extract substrings from strings in R. These functions are:\n\nstr_sub() from the stringr package\nsql_left(), sql_right() and sql_mid() from the healthyR library\n\nThe str_sub() function from the stringr package is a more powerful and flexible function than the substring() function. It supports a wider range of arguments and it can be used to perform more complex string manipulations.\nThe sql_left(), sql_right() and sql_mid() functions from the `{healthyR} library are designed to be similar to the corresponding functions in SQL. They are easy to use and they can be a good choice for users who are familiar with SQL.\nI encourage readers to try things on their own with the substring() function and the other functions mentioned in this blog post. There are many different ways to use these functions to extract substrings from strings in R. Experimenting with different functions and different arguments is a great way to learn how to use them effectively.\nHere is a link to a blog post that shows some examples of how to use the sql_left(), sql_right() and sql_mid() functions: https://www.spsanderson.com/steveondata/posts/rtip-2023-03-01/index.html"
  },
  {
    "objectID": "posts/2023-08-15/index.html",
    "href": "posts/2023-08-15/index.html",
    "title": "Solving Systems of Equations in R using the solve() Function",
    "section": "",
    "text": "In mathematical modeling and data analysis, it is often necessary to solve systems of equations to find the values of unknown variables. R provides the solve() function, which is a powerful tool for solving systems of linear equations. In this blog post, we will explore the purpose of solving systems of equations, explain the syntax of the solve() function, and provide three examples of increasing complexity to demonstrate its usage."
  },
  {
    "objectID": "posts/2023-08-15/index.html#example-1-solving-a-system-of-two-equations",
    "href": "posts/2023-08-15/index.html#example-1-solving-a-system-of-two-equations",
    "title": "Solving Systems of Equations in R using the solve() Function",
    "section": "Example 1 Solving a System of Two Equations",
    "text": "Example 1 Solving a System of Two Equations\nLet’s start with a simple example of solving a system of two equations with two variables. Suppose we have the following system of equations:\n2x + 3y = 10\n4x - 2y = 6\nTo solve this system using the solve() function, we define the coefficient matrix “a” and the constant matrix “b” as follows:\n\na &lt;- matrix(c(2, 3, 4, -2), nrow = 2, byrow = TRUE)\nb &lt;- c(10, 6)\n\nThen, we can use the solve() function to find the values of “x” and “y”:\n\nsolution &lt;- solve(a, b)\nsolution\n\n[1] 2.375 1.750\n\n\nThe solution will be stored in the “solution” variable, which can be accessed to obtain the values of “x” and “y”."
  },
  {
    "objectID": "posts/2023-08-15/index.html#example-2-solving-a-system-of-three-equations",
    "href": "posts/2023-08-15/index.html#example-2-solving-a-system-of-three-equations",
    "title": "Solving Systems of Equations in R using the solve() Function",
    "section": "Example 2 Solving a System of Three Equations",
    "text": "Example 2 Solving a System of Three Equations\nLet’s consider a slightly more complex system of three equations with three variables:\n3x + 2y - z = 7\nx - y + 2z = -1\n2x + 3y + 4z = 12\nTo solve this system, we define the coefficient matrix “a” and the constant matrix “b”:\n\na &lt;- matrix(c(3, 2, -1, 1, -1, 2, 2, 3, 4), nrow = 3, byrow = TRUE)\nb &lt;- c(7, -1, 12)\n\nWe can then use the solve() function to find the values of “x”, “y”, and “z”:\n\nsolution &lt;- solve(a, b)\nsolution\n\n[1] 0.6571429 2.8000000 0.5714286\n\n\nThe solution will be stored in the “solution” variable, and we can access the values of “x”, “y”, and “z” from it."
  },
  {
    "objectID": "posts/2023-08-15/index.html#example-3-solving-a-system-of-equations-with-matrix-coefficients",
    "href": "posts/2023-08-15/index.html#example-3-solving-a-system-of-equations-with-matrix-coefficients",
    "title": "Solving Systems of Equations in R using the solve() Function",
    "section": "Example 3 Solving a System of Equations with Matrix Coefficients",
    "text": "Example 3 Solving a System of Equations with Matrix Coefficients\nIn some cases, the coefficient matrix “a” can be a matrix instead of a vector. For example, consider the following system of equations:\n2x + 3y = 10\n4x - 2y = 6\nWe can represent the coefficient matrix “a” as follows:\n\na &lt;- matrix(c(2, 3, 4, -2), nrow = 2, byrow = TRUE)\n\nThe constant vector “b” remains the same:\n\nb &lt;- c(10, 6)\n\nWe can then use the solve() function to find the values of “x” and “y”:\n\nsolution &lt;- solve(a, b)\nsolution\n\n[1] 2.375 1.750\n\n\nThe solution will be stored in the “solution” variable, and we can access the values of “x” and “y” from it."
  },
  {
    "objectID": "posts/2023-08-16/index.html",
    "href": "posts/2023-08-16/index.html",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "",
    "text": "In the vast world of R programming, there are numerous functions that provide powerful capabilities for data visualization and analysis. One such function that often goes under appreciated is the curve() function. This neat little function allows us to plot mathematical functions and explore their behavior. In this blog post, we will dive into the syntax of the curve() function, provide a couple of examples to demonstrate its usage, and encourage readers to try it on their own."
  },
  {
    "objectID": "posts/2023-08-16/index.html#example-1-plotting-a-simple-line",
    "href": "posts/2023-08-16/index.html#example-1-plotting-a-simple-line",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "Example 1: Plotting a Simple Line",
    "text": "Example 1: Plotting a Simple Line\nLet’s start with a simple example to plot a line. Suppose we want to plot the line y = x. We can achieve this using the curve() function as follows:\n\ncurve((x))\n\n\n\n\nIn this example, we provide the expression (x) to the curve() function. The expression (x) represents the line y = x. By default, the curve() function will plot the curve between 0 and 1. The resulting plot will show a straight line passing through the origin with a slope of 1."
  },
  {
    "objectID": "posts/2023-08-16/index.html#example-2-overlaying-multiple-curves",
    "href": "posts/2023-08-16/index.html#example-2-overlaying-multiple-curves",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "Example 2: Overlaying Multiple Curves",
    "text": "Example 2: Overlaying Multiple Curves\nThe curve() function allows us to overlay multiple curves on the same plot. Let’s consider an example where we plot several curves together:\n\ncurve((x), from = -2, to = 2, lwd = 2)\ncurve(0 * x, add = TRUE, col = \"blue\")\ncurve(0 * x + 1.5, add = TRUE, col = \"green\")\ncurve(x^3, add = TRUE, col = \"red\")\ncurve(-3 * (x + 2), add = TRUE, col = \"orange\")\n\n\n\n\nIn this example, we first plot the line y = x with a thicker line width (lwd = 2). Then, we overlay four additional curves on the same plot: a horizontal line at y = 0 (colored blue), a horizontal line at y = 1.5 (colored green), a cubic curve y = x^3 (colored red), and a linear curve y = -3(x + 2) (colored orange). This example showcases the versatility of the curve() function in visualizing multiple functions simultaneously."
  },
  {
    "objectID": "posts/2023-08-16/index.html#example-3-plotting-a-simple-function",
    "href": "posts/2023-08-16/index.html#example-3-plotting-a-simple-function",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "Example 3: Plotting a Simple Function",
    "text": "Example 3: Plotting a Simple Function\nLet’s start with a simple example. Suppose we want to visualize the curve of the quadratic function f(x) = x^2. Here’s how we can achieve this using the curve() function:\n\ncurve(x^2, from = -5, to = 5, type = \"l\", col = \"blue\",\n      xlab = \"x-axis\", ylab = \"y = x^2\", \n      main = \"Quadratic Curve: y = x^2\")\n\n\n\n\nIn this example, we’ve provided the expression x^2 to the curve() function. We’ve also specified the range of x-values from -5 to 5. The curve type is set to “l” for lines, and we’ve customized the colors, labels, and title of the plot."
  },
  {
    "objectID": "posts/2023-08-16/index.html#example-4-plotting-multiple-functions",
    "href": "posts/2023-08-16/index.html#example-4-plotting-multiple-functions",
    "title": "Exploring the Power of the curve() Function in R",
    "section": "Example 4: Plotting Multiple Functions",
    "text": "Example 4: Plotting Multiple Functions\nNow, let’s take it up a notch and visualize two functions on the same plot. Imagine we want to compare the curves of the sine and cosine functions. Here’s how we can do it:\n\ncurve(sin(x), from = -2 * pi, to = 2 * pi, type = \"l\", col = \"red\",\n      xlab = \"x-axis\", ylab = \"y-axis\", \n      main = \"Sine and Cosine Curves\")\ncurve(cos(x), from = -2 * pi, to = 2 * pi, type = \"l\", col = \"blue\",\n      add = TRUE)\nlegend(\"topright\", legend = c(\"sin(x)\", \"cos(x)\"), \n       col = c(\"red\", \"blue\"), lty = 1)\n\n\n\n\nIn this example, we’ve plotted both the sine and cosine functions on the same plot. By setting add = TRUE in the second curve() call, we overlay the cosine curve on the existing plot. We’ve also added a legend to differentiate between the two curves."
  },
  {
    "objectID": "posts/2023-08-17/index.html",
    "href": "posts/2023-08-17/index.html",
    "title": "Mastering Data Approximation with R’s approx() Function",
    "section": "",
    "text": "Are you tired of dealing with irregularly spaced data points that just don’t seem to fit together? Do you find yourself struggling to interpolate or smooth your data for better analysis? Look no further! In this blog post, we’ll dive deep into the powerful world of data approximation using R’s approx() function. Buckle up, because by the end of this journey, you’ll have a new tool in your R toolkit that can help you tame even the wildest datasets."
  },
  {
    "objectID": "posts/2023-08-17/index.html#examples",
    "href": "posts/2023-08-17/index.html#examples",
    "title": "Mastering Data Approximation with R’s approx() Function",
    "section": "Examples",
    "text": "Examples\n\nExample 1: Basic Linear Interpolation\nSuppose you have a dataset of temperature measurements at irregular intervals and you want to estimate the temperature at a specific time. Here’s how approx() can help:\n\n# Sample data\ntime &lt;- c(0, 2, 5, 8, 10)\ntemperature &lt;- c(20, 25, 30, 28, 22)\n\n# Time point to estimate temperature\ntime_estimate &lt;- 6\n\n# Using approx() for linear interpolation\napproximated_temp &lt;- approx(time, temperature, xout = time_estimate)$y\n\ncat(\"Estimated temperature at time\", time_estimate, \"is\", approximated_temp, \"°C\\n\")\n\nEstimated temperature at time 6 is 29.33333 °C\n\n\n\n\nExample 2: Smoothing Out Noisy Data\nNoisy data can be a nightmare for analysis. Let’s say you have a dataset with some irregularly spaced noisy sine wave points, and you want to create a smoother curve:\n\n# Generating noisy sine wave data\nset.seed(123)\nx &lt;- seq(0, 10, length.out = 20)\ny &lt;- sin(x) + rnorm(length(x), mean = 0, sd = 0.2)\n\n# Smoothing out the curve\nsmoothed &lt;- approx(x, y, xout = seq(0, 10, length.out = 100), f = 0.5)$y\n\n# Plotting the original and smoothed data\nplot(x, y, main = \"Noisy Sine Wave vs. Smoothed\", type = \"p\", col = \"blue\", pch = 16)\nlines(seq(0, 10, length.out = 100), smoothed, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Noisy Data\", \"Smoothed\"), col = c(\"blue\", \"red\"), lwd = 2)"
  },
  {
    "objectID": "posts/2023-08-18/index.html",
    "href": "posts/2023-08-18/index.html",
    "title": "Exploring Data Distribution with Box Plots in R",
    "section": "",
    "text": "Are you ready to dive into the world of data visualization in R? One powerful tool at your disposal is the box plot, also known as a box-and-whisker plot. This versatile chart can help you understand the distribution of your data and identify potential outliers. In this blog post, we’ll walk you through the process of creating box plots using R’s ggplot2 package, using the airquality dataset as an example. Whether you’re a beginner or an experienced R programmer, you’ll find something valuable here."
  },
  {
    "objectID": "posts/2023-08-18/index.html#examples-with-ggplot2",
    "href": "posts/2023-08-18/index.html#examples-with-ggplot2",
    "title": "Exploring Data Distribution with Box Plots in R",
    "section": "Examples with ggplot2",
    "text": "Examples with ggplot2\nBefore we jump into code, let’s get the ggplot2 package loaded and our dataset ready:\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\n\nCreating a Basic Box Plot\nLet’s start with a basic example. Suppose we want to visualize the distribution of ozone levels in the airquality dataset. Here’s how you can create a plain box plot:\n\n# Basic box plot for ozone levels\nbasic_box_plot &lt;- ggplot(airquality, aes(x = factor(1), y = Ozone)) +\n  geom_boxplot() +\n  labs(title = \"Basic Box Plot of Ozone Levels\",\n       x = \"\", y = \"Ozone Levels\") +\n  theme_minimal()\n\nbasic_box_plot\n\nWarning: Removed 37 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nIn this example, we use ggplot() to initiate the plot and specify the x aesthetic as a factor to create a single box plot. The y aesthetic is set to the Ozone variable, and we add the geom_boxplot() layer to create the box plot itself. The labs() function helps us set the title and axis labels.\n\n\nAdding Fill to Box Plots\nIf you want to add more visual depth to your box plots, you can use color to differentiate categories. Let’s create a box plot of ozone levels, grouped by the months:\n\n# Box plot with fill for different months\nfilled_box_plot &lt;- ggplot(\n  airquality, \n  aes(\n    x = factor(Month), \n    y = Ozone, \n    fill = factor(Month)\n    )\n  ) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of Ozone Levels by Month\",\n       x = \"Month\", y = \"Ozone Levels\") +\n  scale_fill_discrete(name = \"Month\") +\n  theme_minimal()\n\nfilled_box_plot\n\nWarning: Removed 37 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nIn this code, we add the fill aesthetic to the aes() function, which creates separate box plots for each month and fills them with different colors based on the Month variable.\n\n\nNotching for Comparing Medians\nA notched box plot can help you compare the medians of different groups. Let’s create a notched box plot to visualize the distribution of ozone levels for different temperatures:\n\n# Notched box plot for ozone levels by temperature\nnotched_box_plot &lt;- ggplot(\n  airquality, \n  aes(\n    x = factor(Temp), \n    y = Ozone, \n    fill = factor(Temp)\n    )\n  ) +\n  geom_boxplot(notch = TRUE) +\n  labs(title = \"Notched Box Plot of Ozone Levels by Temperature\",\n       x = \"Temperature\", y = \"Ozone Levels\") +\n  scale_fill_discrete(name = \"Temperature\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nnotched_box_plot\n\n\n\n\nBy setting notch = TRUE within geom_boxplot(), you add notches to the boxes that provide a rough comparison of medians."
  },
  {
    "objectID": "posts/2023-08-18/index.html#base-r-examples",
    "href": "posts/2023-08-18/index.html#base-r-examples",
    "title": "Exploring Data Distribution with Box Plots in R",
    "section": "Base R Examples",
    "text": "Base R Examples\n\nBase boxplot()\n\n# Create a filled box plot of ozone by month\nboxplot(\n  airquality$Ozone ~ airquality$Month, \n  main = \"Distribution of Ozone by Month\", \n  xlab = \"Month\", \n  ylab = \"Ozone\", \n  col = \"lightblue\"\n  )\n\n\n\n\nExplanation:\n\nIn this example, we use the formula notation (~) to create a filled box plot of the ozone variable (airquality$Ozone) grouped by the month variable (airquality$Month).\nWe provide the same title, x-axis label, and y-axis label as in the previous example.\nAdditionally, we specify the col argument to set the color of the boxes to “lightblue”.\n\n\n\nConclusion\nBox plots are a fantastic tool for quickly understanding the distribution of your data. With the ggplot2 package in R, creating informative and visually appealing box plots is both accessible and customizable. I encourage you to experiment with different aesthetics, variations, and datasets to explore the insights these plots can reveal. So why not grab your R console and embark on your data visualization journey today? Happy plotting!\nRemember, the best way to truly master box plots is by trying them yourself. Copy and paste the code snippets provided here into your R environment, modify them, and observe how the plots change. As you become more comfortable, you can start applying box plots to your own datasets and discover new patterns and trends. Happy coding!"
  },
  {
    "objectID": "posts/2023-08-21/index.html",
    "href": "posts/2023-08-21/index.html",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "",
    "text": "Data visualization is a powerful tool for understanding and interpreting data. In this blog post, we will explore how to create box plots with mean values using both base R and ggplot2. We will use the famous iris dataset as an example. So, grab your coding tools and let’s dive into the world of box plots!"
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-1-box-plots-with-mean-value-in-base-r",
    "href": "posts/2023-08-21/index.html#example-1-box-plots-with-mean-value-in-base-r",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 1: Box Plots with Mean Value in Base R",
    "text": "Example 1: Box Plots with Mean Value in Base R\nTo start, let’s use base R to create box plots with mean values. Here’s the code:\n\n# Calculate the mean for each species\nmean_values &lt;- aggregate(iris$Sepal.Length, by = list(iris$Species), FUN = mean)\n\n# Create a box plot with mean value\nboxplot(iris$Sepal.Length ~ iris$Species, \n        main = \"Box Plot with Mean Value\",\n        xlab = \"Species\", ylab = \"Sepal Length\", \n        col = \"lightblue\")\npoints(mean_values$x ~ mean_values$Group.1, col = \"red\", pch = 19)\n\n\n\n\nIn this code, we first load the iris dataset using the data() function. Then, we calculate the mean value for each species using the aggregate() function. Finally, we create a box plot using boxplot() and add the mean values as red points using points()."
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-2",
    "href": "posts/2023-08-21/index.html#example-2",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 2:",
    "text": "Example 2:\nSection 2: Box Plots with Mean Value in ggplot2 Next, let’s explore how to create box plots with mean values using ggplot2. Here’s the code:\n\n# Create a basic box plot with mean using Base R\nboxplot(iris$Sepal.Length, main=\"Box Plot with Mean (Sepal.Length)\", \n        ylab=\"Sepal Length\", col=\"lightblue\")\nabline(h=mean(iris$Sepal.Length), col=\"red\", lwd=2)\n\n\n\n\nIn this code snippet, we load the Iris dataset and generate a box plot for the Sepal.Length attribute. The abline() function adds a horizontal line at the mean value, highlighted in red. Don’t hesitate to modify attributes like color, line width, or title to customize your plot to your heart’s content!\nConclusion: In this blog post, we explored how to create box plots with mean values using both base R and ggplot2. We used the iris dataset as an example and provided code snippets for each approach. Box plots are a great way to visualize the distribution of data and the addition of mean values provides further insights. We encourage you to try these examples with the iris dataset or apply them to your own data. Happy coding and happy visualizing!\nRemember, data visualization is an art form, so feel free to experiment with different customizations and explore other types of plots. The more you practice, the better you’ll become at creating informative and visually appealing visualizations. So, keep coding and keep exploring the world of data visualization!\nCitations: [1] https://stackoverflow.com/questions/64732557/add-mean-to-grouped-box-plot-in-r-with-ggplot2 [2] https://gexijin.github.io/learnR/visualizing-the-iris-flower-data-set.html [3] https://rgraphs.com/make-a-boxplot-in-r-using-already-computed-statistics/ [4] https://www.kaggle.com/code/susree64/ggplot-basic-data-visualization-on-iris-data [5] https://www.sarfarazalam.com/post/r_ggplot_tutorial_barplot_boxplot/r_tutorial_barplot_boxplot [6] https://rstudio-pubs-static.s3.amazonaws.com/669797_ce311ad305e249c2a7278de2fc1c6aac.html"
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-2-single-boxplot-with-mean-line",
    "href": "posts/2023-08-21/index.html#example-2-single-boxplot-with-mean-line",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 2: Single Boxplot with mean line",
    "text": "Example 2: Single Boxplot with mean line\n\n# Create a basic box plot with mean using Base R\nboxplot(iris$Sepal.Length, main=\"Box Plot with Mean (Sepal.Length)\", \n        ylab=\"Sepal Length\", col=\"lightblue\")\nabline(h=mean(iris$Sepal.Length), col=\"red\", lwd=2)\n\n\n\n\nIn this code snippet, we load the Iris dataset and generate a box plot for the Sepal.Length attribute. The abline() function adds a horizontal line at the mean value, highlighted in red. Don’t hesitate to modify attributes like color, line width, or title to customize your plot to your heart’s content!"
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-3-box-plots-with-mean-value-in-ggplot2",
    "href": "posts/2023-08-21/index.html#example-3-box-plots-with-mean-value-in-ggplot2",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 3: Box Plots with Mean Value in ggplot2",
    "text": "Example 3: Box Plots with Mean Value in ggplot2\nNow let’s use the ggplot2 library.\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create a box plot with mean using ggplot2\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot() +\n  geom_point(data = aggregate(Sepal.Length ~ Species, data = iris, mean),\n             aes(x = Species, y = Sepal.Length), color = \"red\", size = 3) +\n  labs(title = \"Box Plot of Sepal Length by Species\",\n       x = \"Species\",\n       y = \"Sepal Length\") +\n  theme_minimal()\n\n\n\n\n\nWe load the ggplot2 library using library(ggplot2).\nWe use the ggplot() function to create a ggplot object and specify the dataset and aesthetic mappings with the aes() function.\nWe use geom_boxplot() to create the box plot.\nWe use geom_point() to add red points representing the mean values using the aggregate() result.\nlabs() is used to set the plot title and axis labels.\nWe use theme_minimal() to apply a clean and minimal theme to the plot."
  },
  {
    "objectID": "posts/2023-08-21/index.html#example-4-single-boxplot-with-mean-line-ggplot2",
    "href": "posts/2023-08-21/index.html#example-4-single-boxplot-with-mean-line-ggplot2",
    "title": "Exploring Box Plots with Mean Values using Base R and ggplot2",
    "section": "Example 4: Single Boxplot with mean line ggplot2",
    "text": "Example 4: Single Boxplot with mean line ggplot2\n\n# Create a box plot with mean using ggplot2\nggplot(iris, aes(x=\"\", y=Sepal.Length)) +\n  geom_boxplot(fill=\"lightblue\", color=\"black\") +\n  geom_hline(yintercept = mean(iris$Sepal.Length), color=\"red\", linetype=\"dashed\") +\n  labs(title=\"Box Plot with Mean using ggplot2\",\n       y=\"Sepal Length\") +\n  theme_minimal()\n\n\n\n\nHere, we use the ggplot() function to set up the plot structure and aesthetics. The geom_boxplot() function generates the box plot, and the geom_hline() function adds the mean line. Customize the color palette, line types, titles, and themes to make your visualization shine!"
  },
  {
    "objectID": "posts/2023-08-22/index.html",
    "href": "posts/2023-08-22/index.html",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "",
    "text": "Data visualization is a powerful tool that allows us to uncover patterns and insights within datasets. One such tool in the R programming arsenal is the stripchart() function. If you’re looking to reveal distribution patterns in your data with style and simplicity, then this function might just become your new best friend. In this blog post, we’ll dive into the world of stripchart(), exploring its syntax, uses, and providing you with hands-on examples to master its application."
  },
  {
    "objectID": "posts/2023-08-22/index.html#example-1.-comparing-distributions",
    "href": "posts/2023-08-22/index.html#example-1.-comparing-distributions",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "Example 1. Comparing Distributions",
    "text": "Example 1. Comparing Distributions\nLet’s say you have two datasets containing exam scores of students from different schools. You can use stripchart() to visually compare their distributions:\n\n# Sample data\nschool_A &lt;- c(70, 72, 75, 78, 80, 85, 88)\nschool_B &lt;- c(65, 68, 70, 73, 75, 80, 85, 90)\n\n# Create a stripchart\nstripchart(list(School_A = school_A, School_B = school_B),\n           vertical = FALSE, method = \"jitter\",\n           main = \"Exam Score Distributions\",\n           xlab = \"Score\", ylab = \"School\")\n\n\n\n\nIn this example, we’re using the \"jitter\" method to spread out the points along the y-axis, making it easier to see the density of scores."
  },
  {
    "objectID": "posts/2023-08-22/index.html#example-2.-visualizing-data-points",
    "href": "posts/2023-08-22/index.html#example-2.-visualizing-data-points",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "Example 2. Visualizing Data Points",
    "text": "Example 2. Visualizing Data Points\nImagine you have a dataset with the heights of individuals. You can use stripchart() to visualize each individual’s height as a data point:\n\n# Sample data\nheights &lt;- c(160, 170, 175, 155, 180, 165, 172, 158, 185)\n\n# Create a stripchart\nstripchart(heights, method = \"overplot\",\n           main = \"Individual Heights\",\n           xlab = \"Height (cm)\")\n\n\n\n\nIn this case, the \"overplot\" method allows us to see individual data points that might overlap."
  },
  {
    "objectID": "posts/2023-08-22/index.html#example-3.-categorical-data-comparison",
    "href": "posts/2023-08-22/index.html#example-3.-categorical-data-comparison",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "Example 3. Categorical Data Comparison",
    "text": "Example 3. Categorical Data Comparison\nSuppose you have a dataset of employees’ years of service. You can use stripchart() to compare the years of service among different departments:\n\n# Sample data\nhr_dept &lt;- c(2, 3, 4, 2, 5, 3)\ntech_dept &lt;- c(1, 2, 1, 3, 2, 4, 2)\n\n# Create a stripchart\nstripchart(list(HR = hr_dept, Tech = tech_dept),\n           vertical = FALSE, method = \"stack\",\n           main = \"Years of Service by Department\",\n           xlab = \"Years\", ylab = \"Department\")\n\n\n\n\nThe \"divide\" method segments data points based on the provided categories."
  },
  {
    "objectID": "posts/2023-08-22/index.html#example-4.-all-three-methods-in-one",
    "href": "posts/2023-08-22/index.html#example-4.-all-three-methods-in-one",
    "title": "Unveiling Data Distribution Patterns with stripchart() in R",
    "section": "Example 4. All three methods in one",
    "text": "Example 4. All three methods in one\nNow let’s see what all three methods show for the same data set. We will place them all on one plot.\n\nx &lt;- rnorm(100)\n\npar(mfrow = c(2, 2))\n# Create a stripchart of the heights of 100 randomly generated people\nstripchart(x, method = \"overplot\", main = \"Overplot\")\n\n# Create a stripchart of the heights of 100 people, jittering the points to prevent overlapping\nstripchart(x, method = \"jitter\", main = \"Jitter\")\n\n# Create a stripchart of the heights of 100 people, stacking the points on top of each other\nstripchart(x, method = \"stack\", main = \"Stack\")\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "posts/2023-08-23/index.html",
    "href": "posts/2023-08-23/index.html",
    "title": "Exploring Data Distribution in R: A Comprehensive Guide",
    "section": "",
    "text": "Understanding the distribution of your data is a fundamental step in any data analysis process. It gives you insights into the spread, central tendency, and overall shape of your data. In this blog post, we’ll explore two popular functions in R for visualizing data distribution: density() and hist(). We’ll use the classic Iris dataset for our examples. Additionally, we will introduce the {TidyDensity} library and show how it can be used to create distribution plots."
  },
  {
    "objectID": "posts/2023-08-23/index.html#example-1.-visualizing-data-distribution-using-density",
    "href": "posts/2023-08-23/index.html#example-1.-visualizing-data-distribution-using-density",
    "title": "Exploring Data Distribution in R: A Comprehensive Guide",
    "section": "Example 1. Visualizing Data Distribution using density()",
    "text": "Example 1. Visualizing Data Distribution using density()\nThe density() function in R is used to estimate the probability density function of a continuous random variable. This function calculates density curve, allowing us to see the underlying distribution of the data with the plot() function.\n\nSyntax:\ndensity(x, ...)\nWhere x is the numeric vector for which the density will be estimated.\n\n\nExample:\n\n# Plot the density distribution of Sepal Length\nplot(\n  density(iris$Sepal.Length), \n  main=\"Density Plot of Sepal Length\",\n  xlab=\"Sepal Length\", ylab=\"Density\"\n  )\n\n\n\n\nIn this example, we load the Iris dataset and plot the density distribution of Sepal Length. The main, xlab, and ylab arguments are used to provide titles and labels to the plot."
  },
  {
    "objectID": "posts/2023-08-23/index.html#example-2.-visualizing-data-distribution-using-hist",
    "href": "posts/2023-08-23/index.html#example-2.-visualizing-data-distribution-using-hist",
    "title": "Exploring Data Distribution in R: A Comprehensive Guide",
    "section": "Example 2. Visualizing Data Distribution using hist()",
    "text": "Example 2. Visualizing Data Distribution using hist()\nThe hist() function is another powerful tool for visualizing the distribution of data. It creates a histogram, which is a graphical representation of the frequency distribution of a dataset.\n\nSyntax:\nhist(x, ...)\nWhere x is the numeric vector for which the histogram will be created.\n\n\nExample:\n\n# Create a histogram of Petal Width\nhist(iris$Petal.Width, main=\"Histogram of Petal Width\",\n     xlab=\"Petal Width\", ylab=\"Frequency\", col=\"skyblue\")\n\n\n\n\nHere, we create a histogram of the Petal Width from the Iris dataset. The main, xlab, ylab, and col arguments allow customization of the plot’s appearance.\n\n\nAdd lines to a histogram\nHere we will combine the density plot and the histogram together. Sometimes this helps.\n\nx &lt;- iris$Sepal.Length\n\nhist(x, prob = TRUE)\nlines(density(x))"
  },
  {
    "objectID": "posts/2023-08-23/index.html#using-tidydensity-for-data-distribution-visualization",
    "href": "posts/2023-08-23/index.html#using-tidydensity-for-data-distribution-visualization",
    "title": "Exploring Data Distribution in R: A Comprehensive Guide",
    "section": "Using TidyDensity for Data Distribution Visualization",
    "text": "Using TidyDensity for Data Distribution Visualization\nThe TidyDensity library is a convenient way to visualize data distributions with a modern and tidy approach. Let’s take a look at how it works.\n\nExample:\n\n# Load the required library\nlibrary(TidyDensity)\n\n# Extract the 'mpg' column\nx &lt;- mtcars$mpg\n\n# Use TidyDensity functions to visualize data distribution\ntidy_empirical(x) |&gt; tidy_autoplot()\n\n\n\n\nIn this example, we load the TidyDensity library and the mtcars dataset. We extract the ‘mpg’ column and then utilize the tidy_empirical() function to compute the empirical density. The tidy_autoplot() function creates a visually appealing distribution plot."
  },
  {
    "objectID": "posts/2023-08-24/index.html",
    "href": "posts/2023-08-24/index.html",
    "title": "Plotting Multiple Lines on a Graph in R: A Step-by-Step Guide",
    "section": "",
    "text": "Graphs are powerful visual tools for analyzing and presenting data. In this blog post, we will explore how to plot multiple lines on a graph using base R. We will cover two methods: matplot() and lines(). These functions provide flexibility and control over the appearance of the lines, allowing you to create informative and visually appealing plots. So, let’s dive in and learn how to plot multiple lines on a graph in R!"
  },
  {
    "objectID": "posts/2023-08-24/index.html#example-1-using-matplot",
    "href": "posts/2023-08-24/index.html#example-1-using-matplot",
    "title": "Plotting Multiple Lines on a Graph in R: A Step-by-Step Guide",
    "section": "Example 1 Using matplot():",
    "text": "Example 1 Using matplot():\nThe matplot() function is a convenient way to plot multiple lines in one chart when you have a dataset in a wide format. Here’s an example:\n\n# Create sample data\nx &lt;- 1:10\ny1 &lt;- c(1, 4, 3, 6, 5, 8, 7, 9, 10, 2)\ny2 &lt;- c(2, 5, 4, 7, 6, 9, 8, 10, 3, 1)\ny3 &lt;- c(3, 6, 5, 8, 7, 10, 9, 2, 4, 1)\n\n# Plot multiple lines using matplot\nmatplot(x, cbind(y1, y2, y3), type = \"l\", lty = 1, \n        col = c(\"red\", \"blue\", \"green\"), xlab = \"X\", \n        ylab = \"Y\", main = \"Multiple Lines Plot\")\nlegend(\"topright\", legend = c(\"Line 1\", \"Line 2\", \"Line 3\"), \n       col = c(\"red\", \"blue\", \"green\"), \n       lty = 1)\n\n\n\n\n\nExplanation:\n\nWe first create sample data for the x-axis (x) and three lines (y1, y2, y3).\nThe matplot() function is then used to plot the lines. We pass the x-axis values (x) and a matrix of y-axis values (cbind(y1, y2, y3)) as input.\nThe type = \"l\" argument specifies that we want to plot lines.\nThe lty = 1 argument sets the line type to solid.\nThe col argument specifies the colors of the lines.\nThe xlab, ylab, and main arguments set the labels for the x-axis, y-axis, and the main title of the plot, respectively.\nFinally, the legend() function is used to add a legend to the plot, indicating the colors and labels of the lines."
  },
  {
    "objectID": "posts/2023-08-24/index.html#example-2-using-lines",
    "href": "posts/2023-08-24/index.html#example-2-using-lines",
    "title": "Plotting Multiple Lines on a Graph in R: A Step-by-Step Guide",
    "section": "Example 2 Using lines():",
    "text": "Example 2 Using lines():\nAnother way to plot multiple lines is to plot them one by one using the points() and lines() functions. Here’s an example:\n\n# Create sample data\nx &lt;- 1:10\ny1 &lt;- c(1, 4, 3, 6, 5, 8, 7, 9, 10, 2)\ny2 &lt;- c(2, 5, 4, 7, 6, 9, 8, 10, 3, 1)\ny3 &lt;- c(3, 6, 5, 8, 7, 10, 9, 2, 4, 1)\n\n# Create an empty plot\nplot(x, y1, type = \"n\", xlim = c(1, 10), ylim = c(0, 10), \n     xlab = \"X\", ylab = \"Y\", main = \"Multiple Lines Plot\")\n\n# Plot each line one by one\nlines(x, y1, type = \"l\", col = \"red\")\nlines(x, y2, type = \"l\", col = \"blue\")\nlines(x, y3, type = \"l\", col = \"green\")\n\n# Add a legend\nlegend(\"topright\", legend = c(\"Line 1\", \"Line 2\", \"Line 3\"), \n       col = c(\"red\", \"blue\", \"green\"), lty = 1)\n\n\n\n\n\nExplanation\n\nWe create the same sample data as in the previous example.\nThe plot() function is used to create an empty plot with appropriate labels and limits.\nWe then use the lines() function to plot each line one by one. The type = \"l\" argument specifies that we want to plot lines, and the col argument sets the color of each line.\nFinally, the legend() function is used to add a legend to the plot."
  },
  {
    "objectID": "posts/2023-08-25/index.html",
    "href": "posts/2023-08-25/index.html",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "",
    "text": "Histograms are a powerful tool for visualizing the distribution of numerical data. They allow us to quickly understand the frequency distribution of values within a dataset. In this tutorial, we’ll explore how to create multiple histograms using two popular R packages: base R and ggplot2. By the end of this guide, you’ll be able to confidently display multiple histograms on a single graph using both methods."
  },
  {
    "objectID": "posts/2023-08-25/index.html#example-1-creating-side-by-side-histograms",
    "href": "posts/2023-08-25/index.html#example-1-creating-side-by-side-histograms",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Example 1: Creating Side-by-Side Histograms",
    "text": "Example 1: Creating Side-by-Side Histograms\nTo plot multiple histograms side by side using base R, you can make use of the par(mfrow) function. This function allows you to specify the number of rows and columns for your layout. Here’s an example:\n\n# Create two example datasets\ndata1 &lt;- rnorm(100, mean = 0, sd = 1)\ndata2 &lt;- rnorm(100, mean = 2, sd = 1)\n\n# Set up a side-by-side layout\npar(mfrow = c(1, 2))\n\n# Create the first histogram\nhist(data1, main = \"Histogram 1\", xlab = \"Value\", ylab = \"Frequency\")\n\n# Create the second histogram\nhist(data2, main = \"Histogram 2\", xlab = \"Value\", ylab = \"Frequency\")\n\n\n\npar(mfrow = c(1, 1))\n\nIn this example, we first generate two example datasets (data1 and data2). Then, we use par(mfrow = c(1, 2)) to set up a side-by-side layout. Finally, we create the histograms for each dataset using the hist() function.\nNow, let’s plot them on the same graph."
  },
  {
    "objectID": "posts/2023-08-25/index.html#example-2-creating-histograms-on-the-same-graph",
    "href": "posts/2023-08-25/index.html#example-2-creating-histograms-on-the-same-graph",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Example 2: Creating Histograms on the same graph",
    "text": "Example 2: Creating Histograms on the same graph\n\n# Create two example datasets\ndata1 &lt;- rnorm(100, mean = 0, sd = 1)\ndata2 &lt;- rnorm(100, mean = 2, sd = 1)\n\nxmin &lt;- min(data1, data2)\nxmax &lt;- max(data1, data2)\n\n# Create the first histogram\nhist(data1, main = \"Histogram 1\", xlab = \"Value\", ylab = \"Frequency\",\n     col = \"powderblue\", xlim = c(xmin, xmax))\n\n# Create the second histogram\nhist(data2, main = \"Histogram 2\", xlab = \"Value\", ylab = \"Frequency\",\n     col = \"pink\", add = TRUE, xlim = c(xmin, xmax))"
  },
  {
    "objectID": "posts/2023-08-25/index.html#using-ggplot2-to-plot-multiple-histograms",
    "href": "posts/2023-08-25/index.html#using-ggplot2-to-plot-multiple-histograms",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Using ggplot2 to Plot Multiple Histograms",
    "text": "Using ggplot2 to Plot Multiple Histograms\nggplot2 is a highly customizable and versatile package for creating complex visualizations. Let’s see how to use ggplot2 to create multiple histograms.\n\nSyntax for Creating a Histogram in ggplot2\nTo create a histogram using ggplot2, you use the ggplot() function and the geom_histogram() layer. The basic syntax is as follows:\nlibrary(ggplot2)\n\nggplot(data, aes(x = variable)) +\n  geom_histogram(binwidth = width, fill = \"color\") +\n  labs(title = \"Histogram Title\", x = \"X-axis Label\", y = \"Frequency\")\n\ndata: The dataset containing the variable you want to plot.\nvariable: The variable for which you want to create a histogram.\nbinwidth: The width of the histogram bins.\ncolor: The fill color of the bars.\n\n\n\nCreating Multiple Histograms\nTo create multiple histograms using ggplot2, you can utilize facets. Facets allow you to split your data into subsets and create separate histograms for each subset. Here’s an example:\nlibrary(ggplot2)\n\n# Create an example dataset\ndata &lt;- data.frame(\n  group = rep(c(\"Group A\", \"Group B\"), each = 100),\n  value = c(rnorm(100, mean = 0, sd = 1), rnorm(100, mean = 2, sd = 1))\n)\n\n# Create multiple histograms using facets\nggplot(data, aes(x = value)) +\n  geom_histogram(binwidth = 0.5, fill = \"steelblue\") +\n  labs(title = \"Multiple Histograms\", x = \"Value\", y = \"Frequency\") +\n  facet_wrap(~ group, nrow = 1)\nIn this example, we first create an example dataset with two groups (Group A and Group B). Then, we use the facet_wrap() function to create separate histograms for each group."
  },
  {
    "objectID": "posts/2023-08-25/index.html#get-hands-on",
    "href": "posts/2023-08-25/index.html#get-hands-on",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Get Hands-On!",
    "text": "Get Hands-On!\nNow that you have a grasp of how to create multiple histograms using both base R and ggplot2, it’s time to put your skills to the test. Pick a dataset you’re interested in, import it into R, and start creating engaging histograms. Experiment with different bin widths, colors, and layouts to find the visualizations that best convey your data’s story.\nRemember, practice makes perfect! The more you experiment and create histograms, the more comfortable you’ll become with the syntax and options offered by both base R and ggplot2. Happy plotting!"
  },
  {
    "objectID": "posts/2023-08-25/index.html#example-3-using-ggplot2-to-plot-multiple-histograms",
    "href": "posts/2023-08-25/index.html#example-3-using-ggplot2-to-plot-multiple-histograms",
    "title": "How to Plot Multiple Histograms with Base R and ggplot2",
    "section": "Example 3: Using ggplot2 to Plot Multiple Histograms",
    "text": "Example 3: Using ggplot2 to Plot Multiple Histograms\nggplot2 is a highly customizable and versatile package for creating complex visualizations. Let’s see how to use ggplot2 to create multiple histograms."
  },
  {
    "objectID": "posts/2023-08-28/index.html",
    "href": "posts/2023-08-28/index.html",
    "title": "Enhancing Your Histograms in R: Adding Vertical Lines for Better Insights",
    "section": "",
    "text": "Are you tired of looking at plain, vanilla histograms that just show the distribution of your data without any additional context? If so, you’re in for a treat! In this blog post, we’ll explore a simple yet powerful technique to take your histograms to the next level by adding vertical lines that provide valuable insights into your data. We’ll use R, a popular programming language for data analysis and visualization, to demonstrate how to achieve this step by step. Don’t worry if you’re new to R or programming – we’ll break down each code block in easy-to-understand terms."
  },
  {
    "objectID": "posts/2023-08-28/index.html#using-base-r",
    "href": "posts/2023-08-28/index.html#using-base-r",
    "title": "Enhancing Your Histograms in R: Adding Vertical Lines for Better Insights",
    "section": "Using Base R",
    "text": "Using Base R\n\nExample 1: Adding a Solid Vertical Line at a Specific Location\nTo add a solid vertical line at a specific location in a histogram, we can use the abline() function in R. Here’s an example:\n\n# Create a vector of data\ndata &lt;- c(5, 7, 3, 9, 2, 6, 4, 8)\n\n# Create a histogram to visualize the distribution of data\nhist(data)\n\n# Add a vertical line at x = 6\nabline(v = 6)\n\n\n\n\nExplanation:\n\nWe first create a vector of data with some values.\nNext, we create a histogram using the hist() function to visualize the distribution of the data.\nFinally, we use the abline() function with the argument v = 6 to add a vertical line at x = 6 to the histogram.\n\n\n\nExample 2: Adding a Customized Vertical Line at a Specific Location\nIf you want to add a customized vertical line with different colors, line widths, or line types, you can modify the abline() function. Here’s an example:\n\n# Create a vector of data\ndata &lt;- c(5, 7, 3, 9, 2, 6, 4, 8)\n\n# Create a histogram to visualize the distribution of data\nhist(data)\n\n# Add a vertical line at the mean value of the data with a red dashed line\nabline(v = mean(data), col = 'red', lwd = 2, lty = 'dashed')\n\n\n\n\nExplanation:\n\nWe start by creating a vector of data.\nThen, we create a histogram to visualize the distribution of the data.\nFinally, we use the abline() function with the argument v = mean(data) to add a vertical line at the mean value of the data. We also customize the line color to red, line width to 2, and line type to dashed.\n\n\n\nExample 3: Adding Multiple Customized Vertical Lines\nIn some cases, you may want to add multiple customized vertical lines to a histogram. Here’s an example:\n\n# Create a vector of data\ndata &lt;- c(5, 7, 3, 9, 2, 6, 4, 8)\n\n# Create a histogram to visualize the distribution of data\nhist(data)\n\n# Add multiple vertical lines at specific locations with different colors\nabline(v = c(4, 6, 8), col = c('red', 'blue', 'green'), lwd = 2, lty = 'dashed')\n\n\n\n\nExplanation:\n\nWe create a vector of data.\nThen, we create a histogram to visualize the distribution of the data.\nFinally, we use the abline() function with the argument v = c(4, 6, 8) to add multiple vertical lines at specific locations. We customize each line with different colors (red, blue, green), line width (2), and line type (dashed)."
  },
  {
    "objectID": "posts/2023-08-28/index.html#using-ggplot2",
    "href": "posts/2023-08-28/index.html#using-ggplot2",
    "title": "Enhancing Your Histograms in R: Adding Vertical Lines for Better Insights",
    "section": "Using ggplot2",
    "text": "Using ggplot2\n\nExample 1: Marking the Mean\nLet’s start with a simple scenario: you have a dataset of exam scores and you want to visualize the distribution while highlighting the mean score. Here’s how you can do it:\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a sample dataset\ndata &lt;- data.frame(x = c(65, 72, 78, 85, 90, 92, 95, 98, 100))\n\n# Create a histogram with a vertical line for the mean\nggplot(data=data, aes(x=x)) +\n  geom_histogram(binwidth=5, fill=\"blue\", color=\"black\") +\n  geom_vline(aes(xintercept=mean(data)), color=\"red\", linetype=\"dashed\") +\n  labs(title=\"Exam Scores Distribution with Mean Highlighted\", x=\"Scores\", y=\"Frequency\") +\n  theme_minimal()\n\nWarning in mean.default(data): argument is not numeric or logical: returning NA\n\n\nWarning: Removed 9 rows containing missing values (`geom_vline()`).\n\n\n\n\n\nIn this example, we used the ggplot2 library to create a histogram. The geom_vline function adds a vertical line at the position of the mean score. The xintercept argument specifies the position of the line, and we used the color and linetype arguments to style the line.\n\n\nExample 2: Threshold Highlighting\nNow, let’s say you’re analyzing customer purchase data and you want to see how many customers made purchases above a certain threshold. You can add a vertical line to indicate this threshold:\n\n# Create a sample dataset\npurchase_amounts &lt;- data.frame(x= c(20, 30, 45, 50, 55, 60, 70, 80, 90, 100, 110, 130, 150))\n\n# Create a histogram with a vertical line for the threshold\nthreshold &lt;- 70\nggplot(data=data.frame(amount=purchase_amounts), aes(x=x)) +\n  geom_histogram(binwidth=20, fill=\"green\", color=\"black\") +\n  geom_vline(xintercept=threshold, color=\"orange\", linetype=\"dashed\") +\n  labs(title=\"Purchase Amount Distribution with Threshold Highlighted\", x=\"Purchase Amount\", y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\nIn this example, we directly specified the threshold value using the threshold variable. The vertical line is added to the histogram at that threshold value."
  },
  {
    "objectID": "posts/2023-08-28/index.html#i-encourage-you-to-try",
    "href": "posts/2023-08-28/index.html#i-encourage-you-to-try",
    "title": "Enhancing Your Histograms in R: Adding Vertical Lines for Better Insights",
    "section": "I Encourage you to try!",
    "text": "I Encourage you to try!\nAdding vertical lines to histograms in R is a straightforward way to enhance your data visualization. By highlighting specific values or thresholds, you can convey more information to your audience and make your insights clearer. Don’t hesitate to experiment with different datasets, color schemes, and line styles to match your needs and preferences.\nSo, what are you waiting for? Open up R, load your data, and start creating histograms with vertical lines to uncover hidden patterns and insights that may have gone unnoticed. Happy coding and visualizing!\nRemember, practice makes perfect. The more you experiment with these concepts, the more proficient you’ll become at crafting compelling visualizations. Have fun exploring your data in a new light!"
  },
  {
    "objectID": "posts/2023-08-29/index.html",
    "href": "posts/2023-08-29/index.html",
    "title": "Visualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset",
    "section": "",
    "text": "Categorical data is a type of data that represents distinct groups or categories. Visualizing categorical data can provide valuable insights and help in understanding patterns and relationships within the data. In this blog post, we will explore three popular charts for visualizing categorical data in R using the iris dataset: geom_bar() from ggplot2, a grouped boxplot with base R and ggplot2, and a mosaic plot. We will explain each section of code in simple terms and encourage readers to try these charts on their own."
  },
  {
    "objectID": "posts/2023-08-29/index.html#example-1-barplots-with-geom_bar-from-ggplot2",
    "href": "posts/2023-08-29/index.html#example-1-barplots-with-geom_bar-from-ggplot2",
    "title": "Visualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset",
    "section": "Example 1 Barplots with geom_bar() from ggplot2:",
    "text": "Example 1 Barplots with geom_bar() from ggplot2:\nBarplots are a common and effective way to visualize categorical data. We can use the geom_bar() function from the ggplot2 package to create barplots in R. The geom_bar() function accepts a variable for the x-axis and plots the number of times each value of the variable appears in the dataset[3].\n\nlibrary(ggplot2)\n\n# Create a barplot using geom_bar()\nggplot(data = iris, aes(x = Species, fill = factor(Species))) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    title = \"Bar Chart of Species Count\",\n    ylab = \"Count\",\n    fill = \"Species\"\n  )\n\n\n\n\nExplanation: - Load the ggplot2 package using library(ggplot2). - The iris dataset is already available in R, so we can directly use it. - The aes() function specifies the aesthetic mappings, where x represents the variable on the x-axis. - The geom_bar() function creates the barplot.\nTry creating a barplot with the Species variable from the iris dataset using the provided code. Experiment with different variables and datasets to explore the patterns and distributions within your data."
  },
  {
    "objectID": "posts/2023-08-29/index.html#example-2-grouped-boxplot-with-base-r-and-ggplot2",
    "href": "posts/2023-08-29/index.html#example-2-grouped-boxplot-with-base-r-and-ggplot2",
    "title": "Visualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset",
    "section": "Example 2 Grouped Boxplot with base R and ggplot2",
    "text": "Example 2 Grouped Boxplot with base R and ggplot2\nA grouped boxplot is a useful chart for comparing the distribution of a continuous variable across different categories of a categorical variable. We can create a grouped boxplot using both base R and ggplot2.\n\nBase R\n\n# Create a grouped boxplot using  base R\nboxplot(Sepal.Length ~ Species, data = iris)\n\n\n\n\n\n\nggplot2\n\n# Create a grouped boxplot using ggplot2\nggplot(data = iris, aes(x = Species, y = Sepal.Length,\n                        fill = factor(Species))) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(fill = \"Species\")\n\n\n\n\nExplanation: - In base R, we use the boxplot() function to create a grouped boxplot. The formula Sepal.Length ~ Species specifies that the Sepal.Length variable should be plotted against the Species variable[2]. - In ggplot2, we use the geom_boxplot() function to create a grouped boxplot. The aes() function specifies the aesthetic mappings, where x represents the categorical variable and y represents the numeric variable.\nCreate a grouped boxplot with the Sepal.Length variable across different species in the iris dataset using either base R or ggplot2. Compare the distributions of Sepal.Length for each species and observe any differences."
  },
  {
    "objectID": "posts/2023-08-29/index.html#example-3-mosaic-plot",
    "href": "posts/2023-08-29/index.html#example-3-mosaic-plot",
    "title": "Visualizing Categorical Data in R: A Guide with Engaging Charts Using the Iris Dataset",
    "section": "Example 3 Mosaic Plot",
    "text": "Example 3 Mosaic Plot\nA mosaic plot is a graphical representation of the relationship between two or more categorical variables. It displays the proportions of each category within the variables and allows for visual comparison.\n\nmosaicplot(table(iris$Species, iris$Petal.Width))\n\n\n\n\nExplanation: - The table() function creates a contingency table of the two variables, Species and Petal.Width, from the iris dataset. - The mosaicplot() function creates the mosaic plot.\nCreate a mosaic plot with the Species and Petal.Width variables from the iris dataset using the provided code. Explore the relationships and proportions between the variables. Experiment with different combinations of variables to gain insights from the mosaic plot."
  },
  {
    "objectID": "posts/2023-08-30/index.html",
    "href": "posts/2023-08-30/index.html",
    "title": "Exploring Relationships with Correlation Heatmaps in R",
    "section": "",
    "text": "Data visualization is a powerful tool for understanding the relationships between variables in a dataset. One of the most common and insightful ways to visualize correlations is through heatmaps. In this blog post, we’ll dive into the world of correlation heatmaps using R, using the mtcars and iris datasets as examples. By the end of this post, you’ll be equipped to create informative correlation heatmaps on your own."
  },
  {
    "objectID": "posts/2023-08-30/index.html#creating-correlation-heatmaps",
    "href": "posts/2023-08-30/index.html#creating-correlation-heatmaps",
    "title": "Exploring Relationships with Correlation Heatmaps in R",
    "section": "Creating Correlation Heatmaps",
    "text": "Creating Correlation Heatmaps\n\nExample 1: mtcars Dataset\nLet’s start by exploring the relationships within the mtcars dataset, which contains information about various car models and their characteristics.\n\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(mtcars)\n\n# Create a basic correlation heatmap using corrplot\ncorrplot(cor_matrix, method = \"color\")\n\n\n\n\nIn this example, we use the cor() function to compute the correlation matrix for the mtcars dataset. The corrplot() function is then used to create the heatmap. The argument method = \"color\" specifies that we want to represent the correlation values using colors.\n\n\nExample 2: iris Dataset\nNow, let’s explore the relationships within the iris dataset, which contains measurements of various iris flowers.\n\n# Calculate the correlation matrix\ncor_matrix_iris &lt;- cor(iris[, 1:4])  # Consider only numeric columns\n\n# Create a more visually appealing heatmap\nggcorrplot(cor_matrix_iris, type = \"lower\", colors = c(\"#6D9EC1\", \"white\", \"#E46726\"))\n\n\n\n\nIn this example, we calculate the correlation matrix for the first four numeric columns of the iris dataset using cor(). We then use the corrplot() function from the ggcorrplot package to create a more aesthetically pleasing heatmap. The type = \"lower\" argument indicates that we want to display only the lower triangle of the correlation matrix. We also customize the color scheme using the colors argument.\nIf you want to check out how to get a correlation heatmap for a time series lagged against itself you can see this article here."
  },
  {
    "objectID": "posts/2023-08-30/index.html#interpreting-the-heatmap",
    "href": "posts/2023-08-30/index.html#interpreting-the-heatmap",
    "title": "Exploring Relationships with Correlation Heatmaps in R",
    "section": "Interpreting the Heatmap",
    "text": "Interpreting the Heatmap\nIn both examples, the heatmap provides a visual representation of the relationships between variables. Darker colors indicate stronger correlations, while lighter colors suggest weaker or no correlations. By analyzing the heatmap, you can quickly identify which variables are positively, negatively, or not correlated with each other."
  },
  {
    "objectID": "posts/2023-08-31/index.html",
    "href": "posts/2023-08-31/index.html",
    "title": "Creating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2",
    "section": "",
    "text": "When it comes to conveying information effectively, data visualization is a powerful tool that can make complex data more accessible and understandable. One captivating type of data visualization is the lollipop chart. Lollipop charts are a great way to showcase and compare data points while adding a touch of elegance to your presentations. In this blog post, we will dive into what lollipop charts are, why they are useful, and how you can create your own stunning lollipop charts using the ggplot2 package in R."
  },
  {
    "objectID": "posts/2023-08-31/index.html#when-to-use-lollipop-charts",
    "href": "posts/2023-08-31/index.html#when-to-use-lollipop-charts",
    "title": "Creating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2",
    "section": "When to Use Lollipop Charts",
    "text": "When to Use Lollipop Charts\nLollipop charts are particularly effective in the following scenarios:\n\n1. Comparing Data Points:\nLollipop charts excel at highlighting individual data points and comparing their values. When you want to showcase the differences between distinct values, a lollipop chart can provide a clear visual representation.\n\n\n2. Showing Distribution:\nLollipop charts can also be used to display the distribution of data points. By placing lollipops along an axis, you can provide insights into the range and distribution of your data.\n\n\n3. Emphasizing Outliers:\nIf your data contains outliers that you want to draw attention to, lollipop charts can be a fantastic choice. Outliers can be visually distinguished from the rest of the data, aiding in spotting anomalies.\n\n\n4. Limited Data Points:\nWhen you’re working with a small dataset, a lollipop chart can be more effective than a bar chart, which might appear overly crowded for a few data points."
  },
  {
    "objectID": "posts/2023-08-31/index.html#examples-of-lollipop-charts",
    "href": "posts/2023-08-31/index.html#examples-of-lollipop-charts",
    "title": "Creating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2",
    "section": "Examples of Lollipop Charts",
    "text": "Examples of Lollipop Charts\n\nExample 1: Top Movies’ IMDb Ratings\nSuppose we have a dataset containing the top-rated movies and their IMDb ratings. We can use a lollipop chart to visualize these ratings:\n\nmovies &lt;- tibble(\n  Movie = c(\"The Shawshank Redemption\", \"The Godfather\", \"The Dark Knight\", \"Pulp Fiction\") |&gt; factor(),\n  Rating = c(9.3, 9.2, 9.0, 8.9)\n)\n\nlollipop_chart(movies, Movie, Rating, \"Top Movies' IMDb Ratings\")\n\n\n\n\n\n\nExample 2: Exam Scores Comparison\nConsider a scenario where we want to compare the scores of students from two different classes. A lollipop chart can effectively illustrate the differences:\n\nexam_scores &lt;- data.frame(\n  Class = rep(c(\"Class A\", \"Class B\"), each = 5),\n  Student = c(\"Alice\", \"Bob\", \"Carol\", \"David\", \"Emma\", \"Frank\", \"Grace\", \"Hannah\", \"Ivan\", \"Jack\") |&gt; factor(),\n  Score = c(85, 78, 92, 67, 75, 88, 82, 95, 70, 79)\n)\n\nlollipop_chart(exam_scores, Student, Score, \"Exam Scores Comparison\")"
  },
  {
    "objectID": "posts/2023-08-31/index.html#try-lollipop-charts-yourself",
    "href": "posts/2023-08-31/index.html#try-lollipop-charts-yourself",
    "title": "Creating Eye-Catching Data Visualizations with Lollipop Charts in R using ggplot2",
    "section": "Try Lollipop Charts Yourself!",
    "text": "Try Lollipop Charts Yourself!\nLollipop charts provide an engaging way to display and compare data points while highlighting key insights. With the ggplot2 package in R, you have the tools to create stunning lollipop charts for your own datasets. Experiment with different datasets and customize the appearance of your charts to suit your needs. Happy charting!\nIn this blog post, we explored what lollipop charts are, when to use them, and how to create them using the ggplot2 package in R. We provided examples of real-world scenarios where lollipop charts can be valuable and even shared a custom lollipop_chart() function to streamline the chart creation process. Now it’s your turn to apply this knowledge and create captivating lollipop charts with your own data!"
  },
  {
    "objectID": "posts/2023-09-01/index.html",
    "href": "posts/2023-09-01/index.html",
    "title": "Kernel Density Plots in R",
    "section": "",
    "text": "Kernel Density Plots are a type of plot that displays the distribution of values in a dataset using one continuous curve. They are similar to histograms, but they are even better at displaying the shape of a distribution since they aren’t affected by the number of bins used in the histogram. In this blog post, we will discuss what Kernel Density Plots are in simple terms, what they are useful for, and show several examples using both base R and ggplot2."
  },
  {
    "objectID": "posts/2023-09-01/index.html#examples-using-base-r",
    "href": "posts/2023-09-01/index.html#examples-using-base-r",
    "title": "Kernel Density Plots in R",
    "section": "Examples using base R",
    "text": "Examples using base R\nTo create a Kernel Density Plot in base R, we can use the density() function to estimate the density and the plot() function to plot it. Here’s an example:\n\n# Generate data\nset.seed(1234)\nx &lt;- rnorm(500)\n\n# Estimate density\ndens &lt;- density(x)\n\n# Plot density\nplot(dens)\n\n\n\n\nThis will generate a Kernel Density Plot of the x dataset.\nWe can also overlay the density curve over a histogram using the lines() function. Here’s an example:\n\n# Generate data\nset.seed(1234)\nx &lt;- rnorm(500)\n\n# Plot histogram\nhist(x, freq = FALSE)\n\n# Estimate density\ndens &lt;- density(x)\n\n# Overlay density curve\nlines(dens, col = \"red\")\n\n\n\n\nThis will generate a histogram with a Kernel Density Plot overlaid on top."
  },
  {
    "objectID": "posts/2023-09-01/index.html#examples-using-ggplot2",
    "href": "posts/2023-09-01/index.html#examples-using-ggplot2",
    "title": "Kernel Density Plots in R",
    "section": "Examples using ggplot2",
    "text": "Examples using ggplot2\nTo create a Kernel Density Plot in ggplot2, we can use the geom_density() function. Here’s an example:\n\n# Load ggplot2 package\nlibrary(ggplot2)\n\n# Generate data\nset.seed(1234)\nx &lt;- rnorm(500)\n\n# Create data frame\ndf &lt;- data.frame(x = x)\n\n# Plot density\nggplot(df, aes(x = x)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\nThis will generate a Kernel Density Plot of the x dataset using ggplot2.\nWe can also customize the plot by changing the color and fill of the density curve. Here’s an example:\n\n# Generate data\nset.seed(1234)\nx &lt;- rnorm(500)\n\n# Create data frame\ndf &lt;- data.frame(x = x)\n\n# Plot density\nggplot(df, aes(x = x)) +\n  geom_density(color = \"red\", fill = \"blue\", alpha = 0.328) +\n  theme_minimal()\n\n\n\n\nThis will generate a Kernel Density Plot of the x dataset using ggplot2 with a red line, blue fill, and 33% transparency."
  },
  {
    "objectID": "posts/2023-09-01/index.html#conclusion",
    "href": "posts/2023-09-01/index.html#conclusion",
    "title": "Kernel Density Plots in R",
    "section": "Conclusion",
    "text": "Conclusion\nKernel Density Plots are a useful tool for visualizing the distribution of a dataset. They are easy to create in both base R and ggplot2, and can be customized to fit your needs. We encourage readers to try creating their own Kernel Density Plots using the examples provided in this blog post."
  },
  {
    "objectID": "posts/2023-09-01/index.html#example-using-tidydensity",
    "href": "posts/2023-09-01/index.html#example-using-tidydensity",
    "title": "Kernel Density Plots in R",
    "section": "Example using TidyDensity",
    "text": "Example using TidyDensity\nI have posted on it before but TidyDensity can also help in creating density plots for data that use the tidy_ distribution functions with its own autoplot function. Let’s take a look at an example using the same data as above.\n\nlibrary(TidyDensity)\n\nset.seed(1234)\n\ntn &lt;- tidy_normal(.n = 500)\n\ntn |&gt; tidy_autoplot()\n\n\n\n\nNow let’s see it with different means on the same chart.\n\nset.seed(1234)\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 500,\n    .mean = c(-2, 0, 2),\n    .sd = 1,\n    .num_sims = 1\n  )\n) |&gt;\n  tidy_multi_dist_autoplot()\n\n\n\n\nAnd one final one with multiple simulations of each distribution.\n\nset.seed(1234)\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 500,\n    .mean = c(-2,0,2),\n    .sd = 1,\n    .num_sims = 5\n  )\n) |&gt;\n  tidy_multi_dist_autoplot()"
  },
  {
    "objectID": "posts/2023-09-05/index.html",
    "href": "posts/2023-09-05/index.html",
    "title": "When to use Jitter",
    "section": "",
    "text": "As an R programmer, one of the most useful functions to know is the jitter function. The jitter function is used to add random noise to a numeric vector, which can be helpful when visualizing data in a scatterplot. By using the jitter function, we can get a better picture of the true underlying relationship between two variables in a dataset.\n\nWhen to Use Jitter\nScatterplots are excellent for visualizing the relationship between two continuous variables. For example, let’s say we have a dataset of 100 points on the x and y coordinate plane and we want to visualize the relationship between their x and y. We can create a scatterplot using the plot function in R:\n\nx = runif(100, 150, 250)\ny = (x/3) + rnorm(100)\ndata &lt;- data.frame(x, y)\nplot(data$x, data$y, pch = 16, col = 'steelblue')\n\n\n\n\nHowever, if we have a lot of data points that are clustered together, it can be difficult to see the true density of the data. This is where the jitter function comes in. We can add some random noise to the data using the jitter function:\n\nx &lt;- sample(1:10, 200, TRUE)\ny &lt;- 3*x + rnorm(200)\ndata &lt;- data.frame(x, y)\nplot(jitter(data$x, 0.1), jitter(data$y, 0.1), pch = 16, col = 'steelblue')\n\n\n\n\nWe can optionally add a numeric argument to jitter to add even more noise to the data:\n\nplot(jitter(data$x, 0.2), jitter(data$y, 0.2), pch = 16, col = 'steelblue')\n\n\n\n\nWe should be careful not to add too much jitter, though, as this can distort the original data too much:\n\nplot(jitter(data$x, 1), jitter(data$y, 1), pch = 16, col = 'steelblue')\n\n\n\n\n\n\nJittering Provides a Better View of the Data\nAs mentioned before, jittering adds some random noise to data, which can be beneficial when we want to visualize data in a scatterplot. By using the jitter function, we can get a better picture of the true underlying relationship between two variables in a dataset.\nLet’s look at some example data (where the predictor variable is discrete and the outcome is continuous), look at the problems with plotting these kinds of data using R’s defaults, and then look at the jitter function to draw a better scatterplot.\n\nset.seed(1)\nx &lt;- sample(1:10, 200, TRUE)\ny &lt;- 3 * x + rnorm(200, 0, 5)\n\nHere’s what a standard scatterplot of these data looks like:\n\nplot(y ~ x, pch = 15)\n\n\n\n\nscatterplot without jitter\nAs you can see, the data points are stacked on top of each other, making it difficult to see the true density of the data. This is where the jitter function comes in. Let’s add some jitter to the x variable:\n\nplot(y ~ jitter(x), pch = 15)\n\n\n\n\nscatterplot with jitter on x variable\nThis is better, but we can still see some stacking of the data points. Let’s try adding jitter to the y variable:\n\nplot(jitter(y) ~ jitter(x), pch = 15)\n\n\n\n\nscatterplot with jitter on both variables\nThis is much better! We can now see the true density of the data and the underlying relationship between the predictor and outcome variables.\n\n\nConclusion\nThe jitter function is a useful tool for visualizing data in a scatterplot. By adding some random noise to the data, we can get a better picture of the true underlying relationship between two variables in a dataset. However, we should be careful not to add too much jitter, as this can distort the original data too much. I encourage readers to try using the jitter function in their own scatterplots to see how it can improve their visualizations.\n\n\nResources:\n\n[1] https://www.statology.org/jitter-function-r/\n[2] https://www.geeksforgeeks.org/how-to-use-the-jitter-function-in-r-for-scatterplots/\n[3] https://thomasleeper.com/Rcourse/Tutorials/jitter.html\n[4] https://statisticsglobe.com/jitter-r-function-example/\n[5] https://biostats.w.uib.no/creating-a-jitter-plot/\n[6] https://blog.enterprisedna.co/creating-a-jitter-plot-using-ggplot2-in-rstudio/"
  },
  {
    "objectID": "posts/2023-09-06/index.html",
    "href": "posts/2023-09-06/index.html",
    "title": "Exploring Multivariate Data with Principal Component Analysis (PCA) Biplot in R",
    "section": "",
    "text": "Introduction\nWhen it comes to analyzing multivariate data, Principal Component Analysis (PCA) is a powerful technique that can help us uncover hidden patterns, reduce dimensionality, and gain valuable insights. One of the most informative ways to visualize the results of a PCA is by creating a biplot, and in this blog post, we’ll dive into how to do this using the biplot() function in R. To make it more practical, we’ll use the USArrests dataset to demonstrate the process step by step.\n\n\nWhat is a Biplot?\nBefore we get into the details, let’s briefly discuss what a biplot is. A biplot is a graphical representation of a PCA that combines both the scores and loadings into a single plot. The scores represent the data points projected onto the principal components, while the loadings indicate the contribution of each original variable to the principal components. By plotting both, we can see how variables and data points relate to each other in a single chart, making it easier to interpret and analyze the PCA results.\n\n\nGetting Started\nFirst, if you haven’t already, load the necessary R packages. You’ll need the stats package for PCA and the biplot visualization.\n\n# Load required packages\nlibrary(stats)\n\n\n\nPerforming PCA\nNext, let’s perform PCA on the USArrests dataset using the prcomp() function, which is an R function for PCA. We’ll store the PCA results in a variable called pca_result.\n\n# Perform PCA\npca_result &lt;- prcomp(USArrests, scale = TRUE)\n\nIn the code above, we’ve scaled the data (scale = TRUE) to ensure that variables with different scales don’t dominate the PCA.\n\n\nCreating the Biplot\nNow comes the exciting part—creating the biplot! We’ll use the biplot() function to achieve this.\n\n# Create a biplot\nbiplot(pca_result)\n\n\n\n\nWhen you run the biplot() function with your PCA results, R will generate a biplot that combines both the scores and loadings. You’ll see arrows representing the original variables’ contributions to each principal component, and you’ll also see how the data points project onto the components.\n\n\nInterpreting the Biplot\nLet’s break down what you’ll see in the biplot:\n\nData Points: Each point represents a US state in our case, and its position in the biplot indicates how it relates to the principal components.\nArrows: The arrows represent the original variables (in this case, the crime statistics) and show how they contribute to the principal components. Longer arrows indicate stronger contributions.\nPrincipal Components: The biplot will typically show the first two principal components. These components capture the most variation in the data.\n\n\n\nWhat Insights Can You Gain?\nBy examining the biplot, you can draw several conclusions:\n\nClustering: States close to each other on the plot share similar crime profiles.\nVariable Relationships: Variables close to each other on the plot are positively correlated, while those far apart are negatively correlated.\nOutliers: States far from the center may be outliers in terms of their crime statistics.\n\n\n\nTry It Yourself!\nNow that you’ve seen how to create a biplot for PCA using the USArrests dataset, I encourage you to try it with your own data. PCA and biplots are powerful tools for dimensionality reduction and data exploration. They can help you uncover patterns, relationships, and outliers in your data, making it easier to make informed decisions in various fields, from biology to finance.\nIn this tutorial, we’ve barely scratched the surface of what you can do with PCA and biplots. Dive deeper, explore different datasets, and use this knowledge to gain valuable insights into your own multivariate data. Happy analyzing!"
  },
  {
    "objectID": "posts/2023-09-06/index.html#what-insights-can-you-gain",
    "href": "posts/2023-09-06/index.html#what-insights-can-you-gain",
    "title": "Exploring Multivariate Data with Principal Component Analysis (PCA) Biplot in R",
    "section": "What Insights Can You Gain?",
    "text": "What Insights Can You Gain?\nBy examining the biplot, you can draw several conclusions:\n\nClustering: States close to each other on the plot share similar crime profiles.\nVariable Relationships: Variables close to each other on the plot are positively correlated, while those far apart are negatively correlated.\nOutliers: States far from the center may be outliers in terms of their crime statistics."
  },
  {
    "objectID": "posts/2023-09-06/index.html#try-it-yourself",
    "href": "posts/2023-09-06/index.html#try-it-yourself",
    "title": "Exploring Multivariate Data with Principal Component Analysis (PCA) Biplot in R",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nNow that you’ve seen how to create a biplot for PCA using the USArrests dataset, I encourage you to try it with your own data. PCA and biplots are powerful tools for dimensionality reduction and data exploration. They can help you uncover patterns, relationships, and outliers in your data, making it easier to make informed decisions in various fields, from biology to finance.\nIn this tutorial, we’ve barely scratched the surface of what you can do with PCA and biplots. Dive deeper, explore different datasets, and use this knowledge to gain valuable insights into your own multivariate data. Happy analyzing!"
  },
  {
    "objectID": "posts/2023-09-07/index.html",
    "href": "posts/2023-09-07/index.html",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "",
    "text": "Data visualization is a powerful tool for gaining insights from your data. In R, you have a plethora of libraries and functions at your disposal to create stunning and informative plots. One common task is to plot a subset of your data, which allows you to focus on specific aspects or trends within your dataset. In this blog post, we’ll explore various techniques to plot subsets of data in R, and I’ll explain each step in simple terms. Don’t worry if you’re new to R – by the end of this post, you’ll be equipped to create customized plots with ease!\nBefore we start, make sure you have R and RStudio installed on your computer. If not, you can download them from R’s official website and RStudio’s website."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-1-plotting-a-subset-based-on-a-condition",
    "href": "posts/2023-09-07/index.html#example-1-plotting-a-subset-based-on-a-condition",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 1: Plotting a Subset Based on a Condition",
    "text": "Example 1: Plotting a Subset Based on a Condition\nSuppose you have a dataset of monthly sales, and you want to plot only the data points where sales exceeded $10,000. Here’s how you can do it:\n# Load your data (replace 'your_data.csv' with your actual file)\ndata &lt;- read.csv(\"your_data.csv\")\n\n# Create a subset based on the condition\nsubset_data &lt;- data[data$Sales &gt; 10000, ]\n\n# Create a scatter plot\nplot(subset_data$Month, subset_data$Sales, \n     main=\"Monthly Sales &gt; $10,000\", \n     xlab=\"Month\", ylab=\"Sales\")\nExplanation: - We load the data from a CSV file into the ‘data’ variable. - Next, we create a subset of the data using a condition (in this case, sales &gt; $10,000) and store it in ‘subset_data.’ - Finally, we create a scatter plot using the ‘plot’ function, specifying the x-axis (‘Month’) and y-axis (‘Sales’), and adding labels to the plot."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-2-plotting-a-random-subset",
    "href": "posts/2023-09-07/index.html#example-2-plotting-a-random-subset",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 2: Plotting a Random Subset",
    "text": "Example 2: Plotting a Random Subset\nSometimes you might want to plot a random subset of your data. Let’s say you have a large dataset of customer reviews, and you want to visualize a random sample of 100 reviews:\n# Load your data (replace 'your_data.csv' with your actual file)\ndata &lt;- read.csv(\"your_data.csv\")\n\n# Create a random subset\nset.seed(123)  # For reproducibility\nsample_data &lt;- data[sample(nrow(data), 100), ]\n\n# Create a bar plot of review ratings\nbarplot(table(sample_data$Rating), \n        main=\"Random Sample of Customer Reviews\",\n        xlab=\"Rating\", ylab=\"Count\")\nExplanation: - We load the data as before. - Using the sample function, we select 100 random rows from the dataset while setting the seed for reproducibility. - Then, we create a bar plot to visualize the distribution of review ratings."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-3-plotting-data-by-category",
    "href": "posts/2023-09-07/index.html#example-3-plotting-data-by-category",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 3: Plotting Data by Category",
    "text": "Example 3: Plotting Data by Category\nSuppose you have a dataset containing information about various products and you want to plot the sales for each product category. Here’s how you can do it:\n# Load your data (replace 'your_data.csv' with your actual file)\ndata &lt;- read.csv(\"your_data.csv\")\n\n# Create a bar plot of sales by category\nbarplot(tapply(data$Sales, data$Category, sum),\n        main=\"Sales by Product Category\",\n        xlab=\"Category\", ylab=\"Total Sales\")\nExplanation: - We load the data. - Using the tapply function, we group the data by ‘Category’ and calculate the sum of ‘Sales’ for each category. - Finally, we create a bar plot to visualize the total sales for each product category.\nNow for some worked out examples."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-4-using-subset-function",
    "href": "posts/2023-09-07/index.html#example-4-using-subset-function",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 4: Using subset() function",
    "text": "Example 4: Using subset() function\nIn this method, first, a subset of the data is created based on some condition, and then it is plotted using the plot function. Let us first create the subset of the data.\n\ndata_subset &lt;- subset(USArrests, UrbanPop &gt; 70)\nplot(data_subset$Murder, data_subset$Assault)\n\n\n\n\nIn the above code, we have created a subset of the USArrests dataset where UrbanPop is greater than 70. Then we have plotted the Murder and Assault columns of the subset using the plot function."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-5-using-operator",
    "href": "posts/2023-09-07/index.html#example-5-using-operator",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 5: Using [ ] operator",
    "text": "Example 5: Using [ ] operator\nUsing the ‘[ ]’ operator, elements of vectors and observations from data frames can be accessed and subsetted based on some condition.\n\nplot(USArrests$Murder[USArrests$UrbanPop &gt; 70], USArrests$Assault[USArrests$UrbanPop &gt; 70])\n\n\n\n\nIn the above code, we have used the [ ] operator to subset the USArrests dataset where UrbanPop is greater than 70. Then we have plotted the Murder and Assault columns of the subset using the plot function."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-6-using-attributes-for-rows-and-columns",
    "href": "posts/2023-09-07/index.html#example-6-using-attributes-for-rows-and-columns",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 6: Using attributes for rows and columns",
    "text": "Example 6: Using attributes for rows and columns\nIn this method, we pass the row and column attributes to the plot function to plot a subset of the data.\n\nplot(USArrests[USArrests$UrbanPop &gt; 70, c(\"Murder\", \"Assault\")])\n\n\n\n\nIn the above code, we have used the row and column attributes to subset the USArrests dataset where UrbanPop is greater than 70. Then we have plotted the Murder and Assault columns of the subset using the plot function."
  },
  {
    "objectID": "posts/2023-09-07/index.html#example-7-using-dplyr-package",
    "href": "posts/2023-09-07/index.html#example-7-using-dplyr-package",
    "title": "Mastering Data Visualization in R: How to Plot a Subset of Data",
    "section": "Example 7: Using dplyr package",
    "text": "Example 7: Using dplyr package\nThe dplyr package provides a simple and efficient way to subset data.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_subset &lt;- USArrests %&gt;% filter(UrbanPop &gt; 70)\nplot(data_subset$Murder, data_subset$Assault)\n\n\n\n\nIn the above code, we have used the filter function from the dplyr package to subset the USArrests dataset where UrbanPop is greater than 70. Then we have plotted the Murder and Assault columns of the subset using the plot function.\nIn conclusion, there are several ways to plot a subset of data in R. We have explored four methods in this blog post. I encourage readers to try these methods on their own and explore other ways to subset and plot data in R."
  },
  {
    "objectID": "posts/2023-09-08/index.html",
    "href": "posts/2023-09-08/index.html",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "",
    "text": "Are you interested in visualizing demographic data in a unique and insightful way? Population pyramids are a fantastic tool for this purpose! They allow you to compare the distribution of populations across age groups for different genders or time periods. In this blog post, we’ll explore how to create population pyramid plots in R using the powerful ggplot2 library. Don’t worry if you’re new to R or ggplot2; we’ll walk you through the process step by step."
  },
  {
    "objectID": "posts/2023-09-08/index.html#step-1-create-a-basic-bar-chart",
    "href": "posts/2023-09-08/index.html#step-1-create-a-basic-bar-chart",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Step 1: Create a Basic Bar Chart",
    "text": "Step 1: Create a Basic Bar Chart\nStart by creating a basic bar chart representing the population distribution for one gender. We’ll use the geom_bar function to do this.\n\n# Create a basic bar chart for one gender\nbasic_plot &lt;-  ggplot(\n    data, \n    aes(\n        x = Age, \n        fill = Gender, \n        y = ifelse(\n            test = Gender == \"Male\", \n            yes = -Population, \n            no = Population\n            )\n        )\n    ) + \ngeom_bar(stat = \"identity\") \n\nIn this code:\n\nWe filter the data to include only one gender (Male) using subset.\nWe use aes to specify the aesthetic mappings. We map Age to the x-axis, -Population to the y-axis (note the negative sign to flip the bars), and Age to the fill color.\ngeom_bar is used to create the bar chart, and stat = \"identity\" ensures that the heights of the bars are determined by the Population variable.\nFinally, coord_flip() is applied to flip the chart horizontally, making it look like a pyramid."
  },
  {
    "objectID": "posts/2023-09-08/index.html#step-2-combine-both-genders",
    "href": "posts/2023-09-08/index.html#step-2-combine-both-genders",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Step 2: Combine Both Genders",
    "text": "Step 2: Combine Both Genders\nTo create a population pyramid, we need to combine both male and female data. We’ll create two separate plots for each gender and then combine them using the + operator.\n\n# Create population pyramids for both genders and combine them\npopulation_pyramid &lt;- basic_plot +\n  scale_y_continuous(\n    labels = abs, \n    limits = max(data$Population) * c(-1,1)\n  ) + \n  coord_flip() + \n  theme_minimal() +\n  labs(\n    x = \"Age\", \n    y = \"Population\", \n    fill = \"Age\", \n    title = \"Population Pyramid\"\n  )\n\nIn this step:\n\nscale_y_continuous(labels = abs, limits = max(data$Population) * c(-1,1)):\nThis part adjusts the y-axis (vertical axis) of the plot.\nlabels = abs means that the labels on the y-axis will show the absolute values (positive numbers) rather than negative values.\nlimits = max(data$Population) * c(-1,1) sets the limits of the y-axis. It ensures that the y-axis extends from the maximum population value (positive) to the minimum (negative) value, creating a symmetrical pyramid shape.\ncoord_flip(): This function flips the coordinate system of the plot. By default, the x-axis (horizontal) represents age, and the y-axis (vertical) represents population. coord_flip() swaps them so that the x-axis represents population and the y-axis represents age, creating the pyramid effect.\ntheme_minimal(): This sets the overall visual theme of the plot to a minimalistic style. It adjusts the background, gridlines, and other visual elements to a simple and clean appearance.\nlabs(x = “Age”, y = “Population”, fill = “Age”, title = “Population Pyramid”): This part labels various elements of the plot:\n\nx = “Age” labels the x-axis as “Age.”\ny = “Population” labels the y-axis as “Population.”\nfill = “Age” specifies that the “Age” variable will be used to fill the bars in the plot.\ntitle = “Population Pyramid” sets the title of the plot as “Population Pyramid.”"
  },
  {
    "objectID": "posts/2023-09-08/index.html#step-3-customize-your-plot",
    "href": "posts/2023-09-08/index.html#step-3-customize-your-plot",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Step 3: Customize Your Plot",
    "text": "Step 3: Customize Your Plot\nFeel free to customize your plot further by adding labels, adjusting colors, or modifying other aesthetics to match your preferences. The ggplot2 library provides extensive customization options."
  },
  {
    "objectID": "posts/2023-09-08/index.html#step-4-visualize-your-population-pyramid",
    "href": "posts/2023-09-08/index.html#step-4-visualize-your-population-pyramid",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Step 4: Visualize Your Population Pyramid",
    "text": "Step 4: Visualize Your Population Pyramid\nTo visualize your population pyramid, simply print the population_pyramid object:\n\npopulation_pyramid\n\n\n\n\nThis will display the population pyramid plot in your R graphics window."
  },
  {
    "objectID": "posts/2023-09-08/index.html#conclusion",
    "href": "posts/2023-09-08/index.html#conclusion",
    "title": "Creating Population Pyramid Plots in R with ggplot2",
    "section": "Conclusion",
    "text": "Conclusion\nCreating population pyramid plots in R using ggplot2 can be a powerful way to visualize demographic data. In this blog post, we walked through the process step by step, from loading libraries and preparing data to constructing and customizing the pyramid plot. Now it’s your turn to give it a try with your own data or explore additional features and customization options in ggplot2. Happy plotting!"
  },
  {
    "objectID": "posts/2023-09-11/index.html",
    "href": "posts/2023-09-11/index.html",
    "title": "Plotting SVM Decision Boundaries with e1071 in R",
    "section": "",
    "text": "Support Vector Machines (SVM) are a powerful tool in the world of machine learning and classification. They excel in finding the optimal decision boundary between different classes of data. However, understanding and visualizing these decision boundaries can be a bit tricky. In this blog post, we’ll explore how to plot an SVM object using the e1071 library in R, making it easier to grasp the magic happening under the hood."
  },
  {
    "objectID": "posts/2023-09-11/index.html#interpreting-the-plot",
    "href": "posts/2023-09-11/index.html#interpreting-the-plot",
    "title": "Plotting SVM Decision Boundaries with e1071 in R",
    "section": "Interpreting the Plot",
    "text": "Interpreting the Plot\nThe resulting plot will display your data points with red dots and blue squares, representing the true class labels. The decision boundary will be shown as a mix of red and blue points, indicating where the SVM has classified the data. The legend on the top-right helps you distinguish between the two classes.\nWe can also more simply plot out the model, see below:\n\nplot(svm_model, data = data)\n\n\n\n# Change the colors\nplot(svm_model, data = data, color.palette = heat.colors)"
  },
  {
    "objectID": "posts/2023-09-11/index.html#try-it-yourself",
    "href": "posts/2023-09-11/index.html#try-it-yourself",
    "title": "Plotting SVM Decision Boundaries with e1071 in R",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nNow that you’ve seen how to plot an SVM decision boundary using the e1071 package, I encourage you to try it with your own datasets and experiment with different kernels (e.g., radial or polynomial) to see how the decision boundary changes.\nSVMs are a versatile tool for classification tasks, and visualizing their decision boundaries can provide valuable insights into your data and model. Happy plotting!"
  },
  {
    "objectID": "posts/2023-09-12/index.html",
    "href": "posts/2023-09-12/index.html",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "",
    "text": "If you’re an R enthusiast looking to take your data visualization to the next level, you’re in for a treat. In this blog post, we’re going to dive into the world of 3D plotting using R’s powerful persp() function. Whether you’re visualizing surfaces, mathematical functions, or complex data, persp() is a versatile tool that can help you create stunning three-dimensional plots."
  },
  {
    "objectID": "posts/2023-09-12/index.html#the-syntax-of-persp",
    "href": "posts/2023-09-12/index.html#the-syntax-of-persp",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "The Syntax of persp()",
    "text": "The Syntax of persp()\nBefore we dive into examples, let’s take a look at the basic syntax of the persp() function:\npersp(x, y, z, theta = 30, phi = 30, col = \"lightblue\",\n      border = \"black\", scale = TRUE, ... )\n\nx, y, and z are the vectors or matrices representing the x, y, and z coordinates of the data points.\ntheta and phi control the orientation of the plot. theta sets the azimuthal angle (rotation around the z-axis), and phi sets the polar angle (rotation from the xy-plane). These angles are in degrees.\ncol and border control the color of the surface and its border, respectively.\nscale is a logical value that determines whether the axes should be scaled to match the data range.\nAdditional parameters can be passed as ... to customize the plot further.\n\nNow, let’s jump into some examples to see how persp() works in action!"
  },
  {
    "objectID": "posts/2023-09-12/index.html#example-1-creating-a-simple-surface-plot",
    "href": "posts/2023-09-12/index.html#example-1-creating-a-simple-surface-plot",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "Example 1: Creating a Simple Surface Plot",
    "text": "Example 1: Creating a Simple Surface Plot\n\n# Create data for a simple surface plot\nx &lt;- seq(-5, 5, length.out = 50)\ny &lt;- seq(-5, 5, length.out = 50)\nz &lt;- outer(x, y, function(x, y) cos(sqrt(x^2 + y^2)))\n\n# Create a 3D surface plot\npersp(x, y, z, col = \"lightblue\", border = \"black\")\n\n\n\n\nIn this example, we generate a grid of x and y values and calculate the corresponding z values based on a mathematical function. The persp() function then creates a 3D surface plot, using the provided x, y, and z data."
  },
  {
    "objectID": "posts/2023-09-12/index.html#example-2-customizing-the-perspective",
    "href": "posts/2023-09-12/index.html#example-2-customizing-the-perspective",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "Example 2: Customizing the Perspective",
    "text": "Example 2: Customizing the Perspective\n\n# Create data for a surface plot\nx &lt;- seq(-10, 10, length.out = 100)\ny &lt;- seq(-10, 10, length.out = 100)\nz &lt;- outer(x, y, function(x, y) 2 * sin(sqrt(x^2 + y^2)) / sqrt(x^2 + y^2))\n\n# Create a customized 3D surface plot\npersp(x, y, z, col = \"lightblue\", border = \"black\", theta = 60, phi = 20)\n\n\n\n\nIn this example, we create a similar surface plot but customize the perspective by changing the theta and phi angles. This gives the plot a different orientation, providing a unique view of the data."
  },
  {
    "objectID": "posts/2023-09-12/index.html#example-3-scaling-the-axes",
    "href": "posts/2023-09-12/index.html#example-3-scaling-the-axes",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "Example 3: Scaling the Axes",
    "text": "Example 3: Scaling the Axes\n\n# Create data for a surface plot\nx &lt;- seq(-2, 2, length.out = 50)\ny &lt;- seq(-2, 2, length.out = 50)\nz &lt;- outer(x, y, function(x, y) exp(-x^2 - y^2))\n\n# Create a 3D surface plot with scaled axes\npersp(x, y, z, col = \"lightblue\", border = \"black\", scale = TRUE)\n\n\n\n\nHere, we enable axis scaling with the scale parameter, which ensures that the x, y, and z axes are scaled to match the data range."
  },
  {
    "objectID": "posts/2023-09-12/index.html#example-4-multiple-plots",
    "href": "posts/2023-09-12/index.html#example-4-multiple-plots",
    "title": "Exploring the Third Dimension with R: A Guide to the persp() Function",
    "section": "Example 4: Multiple Plots",
    "text": "Example 4: Multiple Plots\n\n# Create data\nx &lt;- seq(-10, 10, length.out = 50)\ny &lt;- seq(-10, 10, length.out = 50)\nz1 &lt;- outer(x, y, function(x, y) dnorm(sqrt(x^2 + y^2)))\nz2 &lt;- outer(x, y, function(x, y) dnorm(sqrt((x-2)^2 + (y-2)^2)))\nz3 &lt;- outer(x, y, function(x, y) dnorm(sqrt((x+2)^2 + (y+2)^2)))\n\n# Plot data\npar(mfrow = c(1, 3))\n\npersp(x, y, z1, theta = 30, phi = 30, col = \"lightblue\", border = NA, shade = 0.5, ticktype = \"detailed\", nticks = 5, xlab = \"X\", ylab = \"Y\", zlab = \"Z1\")\npersp(x, y, z2, theta = 30, phi = 30, col = \"lightblue\", border = NA, shade = 0.5, ticktype = \"detailed\", nticks = 5, xlab = \"X\", ylab = \"Y\", zlab = \"Z2\")\npersp(x, y, z3, theta = 30, phi = 30, col = \"lightblue\", border = NA, shade = 0.5, ticktype = \"detailed\", nticks = 5, xlab = \"X\", ylab = \"Y\", zlab = \"Z3\")\n\n\n\npar(mfrow = c(1, 1))\n\nIn this example, we create data for three different Gaussian distributions. We define the x- and y-axes and use the outer() function to calculate the z-values based on the normal distribution. We then use the persp() function to plot the data. We set the color to light blue, the border to NA, and the shading to 0.5. We also set the tick type to detailed and the number of ticks to 5. Finally, we label the x-, y-, and z-axes. We use the par() function to create multiple 3D plots in one figure."
  },
  {
    "objectID": "posts/2023-09-13/index.html",
    "href": "posts/2023-09-13/index.html",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "",
    "text": "Data visualization is a crucial aspect of data analysis. In R, the flexibility and power of its plotting capabilities allow you to create compelling visualizations. One common scenario is the need to display multiple plots on the same graph. In this blog post, we’ll explore three different approaches to achieve this using the same dataset. We’ll use the set.seed(123) and generate data with x and y equal to cumsum(rnorm(25)) for consistency across examples."
  },
  {
    "objectID": "posts/2023-09-13/index.html#example-1-overlaying-multiple-lines-on-the-same-graph",
    "href": "posts/2023-09-13/index.html#example-1-overlaying-multiple-lines-on-the-same-graph",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "Example 1: Overlaying Multiple Lines on the Same Graph",
    "text": "Example 1: Overlaying Multiple Lines on the Same Graph\nIn this example, we will overlay two lines on the same graph. This is a great way to compare trends between two variables in a single plot.\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Generate the data\nx &lt;- 1:25\ny1 &lt;- cumsum(rnorm(25))\ny2 &lt;- cumsum(rnorm(25))\n\n# Create the plot\nplot(x, y1, type = 'l', col = 'blue', ylim = c(min(y1, y2), max(y1, y2)), \n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Overlaying Multiple Lines')\nlines(x, y2, col = 'red')\nlegend('topleft', legend = c('Line 1', 'Line 2'), col = c('blue', 'red'), lty = 1)\n\n\n\n\nIn this code, we first generate the data for y1 and y2. Then, we use the plot() function to create a plot of y1. We specify type = 'l' to create a line plot and set the color to blue. Next, we use the lines() function to overlay y2 on the same plot with a red line. Finally, we add a legend to distinguish the two lines."
  },
  {
    "objectID": "posts/2023-09-13/index.html#example-2-side-by-side-plots",
    "href": "posts/2023-09-13/index.html#example-2-side-by-side-plots",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "Example 2: Side-by-Side Plots",
    "text": "Example 2: Side-by-Side Plots\nSometimes, you might want to display multiple plots side by side to compare different variables. We can achieve this using the par() function and layout options.\n\n# Create a side-by-side layout\npar(mfrow = c(1, 2))\n\n# Create the first plot\nplot(x, y1, type = 'l', col = 'blue', \n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Side-by-Side Plots (1)')\n\n# Create the second plot\nplot(x, y2, type = 'l', col = 'red',\n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Side-by-Side Plots (2)')\n\n\n\n# Reset Par\npar(mfrow = c(1, 1))\n\nIn this example, we use par(mfrow = c(1, 2)) to set up a side-by-side layout. Then, we create two separate plots for y1 and y2."
  },
  {
    "objectID": "posts/2023-09-13/index.html#example-3-stacked-plots",
    "href": "posts/2023-09-13/index.html#example-3-stacked-plots",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "Example 3: Stacked Plots",
    "text": "Example 3: Stacked Plots\nStacked plots are useful when you want to compare the overall trend while preserving the individual patterns of different variables. Here, we stack two line plots on top of each other.\n\npar(mfrow = c(2, 1), mar = c(2, 4, 4, 2))\n\n# Create the first plot\nplot(x, y1, type = 'l', col = 'blue', \n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Stacked Plots')\n\n# Create the second plot\nplot(x, y2, type = 'l', col = 'red',\n     xlab = 'X-axis', ylab = 'Y-axis', main = 'Side-by-Side Plots (2)')\n\n\n\npar(mfrow = c(1, 1))\n\nThe first line of code, par(mfrow = c(2, 1), mar = c(2, 4, 4, 2)), tells R to create a 2x1 (two rows, one column) plot with margins of 2, 4, 4, and 2. This means that the two plots will be stacked on top of each other.\nThe next line of code, plot(x, y1, type = 'l', col = 'blue', xlab = 'X-axis', ylab = 'Y-axis', main = 'Stacked Plots'), create the first plot. The plot() function creates a plot of the data in the vectors x and y1. The type = 'l' argument tells R to create a line plot, the col = ‘blue’ argument tells R to use blue color for the line, and the other arguments set the labels for the axes and the title of the plot.\nThe fourth line of code, plot(x, y2, type = 'l', col = 'red', xlab = 'X-axis', ylab = 'Y-axis', main = 'Side-by-Side Plots (2)'), create the second plot. This plot is similar to the first plot, except that the line is red.\nThe last line of code, par(mfrow = c(1, 1)), resets the plot to a single plot.\nIn summary, this code creates two line plots, one stacked on top of the other. The first plot uses blue lines and the second plot uses red lines. The plots are labeled and titled appropriately."
  },
  {
    "objectID": "posts/2023-09-13/index.html#conclusion",
    "href": "posts/2023-09-13/index.html#conclusion",
    "title": "How to Plot Multiple Plots on the Same Graph in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored three different techniques for plotting multiple plots on the same graph in R. Whether you need to overlay lines, display plots side by side, or stack them, R offers powerful tools to visualize your data effectively. Try these examples with your own data to harness the full potential of R’s plotting capabilities and create informative visualizations for your analyses. Happy plotting!"
  },
  {
    "objectID": "posts/2023-09-14/index.html",
    "href": "posts/2023-09-14/index.html",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "",
    "text": "Histograms are a fantastic way to visualize the distribution of data. They provide insights into the underlying patterns and help us understand our data better. But what if you want to add some color to your histograms to make them more visually appealing or to highlight specific data points? In this blog post, we’ll explore how to create histograms with different colors in R, and we’ll provide several examples to guide you through the process."
  },
  {
    "objectID": "posts/2023-09-14/index.html#example-1-basic-histogram-with-a-single-color",
    "href": "posts/2023-09-14/index.html#example-1-basic-histogram-with-a-single-color",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "Example 1: Basic Histogram with a Single Color",
    "text": "Example 1: Basic Histogram with a Single Color\nLet’s start with the basics. To create a simple histogram with a single color, we’ll use the built-in hist() function and then customize it with the col parameter:\n\n# Generate some example data\ndata &lt;- rnorm(1000)\n\n# Create a basic histogram with a single color (e.g., blue)\nhist(data, col = \"blue\", main = \"Basic Histogram\")\n\n\n\n\nIn this example, we generated 1000 random data points and created a histogram with blue bars. You can replace \"blue\" with any valid color name or code you prefer."
  },
  {
    "objectID": "posts/2023-09-14/index.html#example-2-customizing-bin-colors",
    "href": "posts/2023-09-14/index.html#example-2-customizing-bin-colors",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "Example 2: Customizing Bin Colors",
    "text": "Example 2: Customizing Bin Colors\nSometimes, you might want to use different colors for individual bins in your histogram. Here’s how you can achieve that:\n\n# Generate example data\ndata &lt;- rnorm(100)\n\n# Define custom colors for each bin\nbin_colors &lt;- c(\"red\", \"green\", \"blue\", \"yellow\", \"purple\")\n\n# Create a histogram with custom bin colors\nhist(data, breaks = 5, col = bin_colors, main = \"Custom Bin Colors\")\n\n\n\n\nIn this example, we’ve specified five custom colors for our histogram’s bins, creating a colorful representation of the data distribution."
  },
  {
    "objectID": "posts/2023-09-14/index.html#example-3-overlaying-multiple-histograms",
    "href": "posts/2023-09-14/index.html#example-3-overlaying-multiple-histograms",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "Example 3: Overlaying Multiple Histograms",
    "text": "Example 3: Overlaying Multiple Histograms\nYou may also want to compare multiple data distributions in a single histogram. To do this, you can overlay histograms with different colors. Here’s an example:\n\n# Generate two sets of example data\ndata1 &lt;- rnorm(1000, mean = 0, sd = 1)\ndata2 &lt;- rnorm(1000, mean = 2, sd = 1)\n\n# Create histograms for each dataset and overlay them\nhist(data1, col = \"blue\", main = \"Overlayed Histograms\")\nhist(data2, col = \"red\", add = TRUE)\nlegend(\"topright\", legend = c(\"Data 1\", \"Data 2\"), fill = c(\"blue\", \"red\"))\n\n\n\n\nIn this example, we generated two datasets and overlaid their histograms with different colors. The alpha parameter controls the transparency of the bars, making it easier to see overlapping areas."
  },
  {
    "objectID": "posts/2023-09-14/index.html#experiment-and-explore",
    "href": "posts/2023-09-14/index.html#experiment-and-explore",
    "title": "How to Create a Histogram with Different Colors in R",
    "section": "Experiment and Explore",
    "text": "Experiment and Explore\nNow that you’ve seen how to create histograms with different colors in R, I encourage you to experiment with your own datasets and colors. R provides numerous options for customizing your histograms, so you can tailor them to your specific needs. Play around with colors, transparency, and other graphical parameters to create engaging and informative visualizations.\nRemember, the best way to learn is by doing, so fire up your R environment and start creating colorful histograms today!"
  },
  {
    "objectID": "posts/2023-09-15/index.html",
    "href": "posts/2023-09-15/index.html",
    "title": "Histograms with Two or More Variables in R",
    "section": "",
    "text": "Histograms are powerful tools for visualizing the distribution of a single variable, but what if you want to compare the distributions of two variables side by side? In this blog post, we’ll explore how to create a histogram of two variables in R, a popular programming language for data analysis and visualization.\nWe’ll cover various scenarios, from basic histograms to more advanced techniques, and explain the code step by step in simple terms. So, grab your favorite dataset or generate some random data, and let’s dive into the world of dual-variable histograms!"
  },
  {
    "objectID": "posts/2023-09-15/index.html#basic-dual-variable-histogram",
    "href": "posts/2023-09-15/index.html#basic-dual-variable-histogram",
    "title": "Histograms with Two or More Variables in R",
    "section": "Basic Dual-Variable Histogram",
    "text": "Basic Dual-Variable Histogram\nLet’s begin with the most straightforward scenario: creating a histogram of two variables using the hist() function. We’ll use the built-in mtcars dataset, which contains information about various car models.\n\nx1 &lt;- rnorm(1000)\nx2 &lt;- rnorm(1000, mean = 2)\nminx &lt;- min(x1, x2)\nmaxx &lt;- max(x1, x2)\n\n# Create a basic dual-variable histogram\nhist(x1, main=\"Histogram of rnorm with mean 0 and 2\", xlab=\"\", \n     ylab=\"\", col=\"lightblue\", xlim = c(minx, maxx))\nhist(x2, xlab=\"\", \n     ylab=\"\", col=\"lightgreen\", add=TRUE)\nlegend(\"topright\", legend=c(\"Mean: 0\", \"Mean: 2\"), fill=c(\"lightblue\", \"lightgreen\"))\n\n\n\n\nThe given R code generates a dual-variable histogram in R using the hist() function. The first two lines of code generate two vectors x1 and x2 of 1000 random normal numbers each, with x1 having a mean of 0 and x2 having a mean of 2. The min() and max() functions are then used to find the minimum and maximum values between x1 and x2. These values are used to set the limits of the x-axis of the histogram.\nThe hist() function is then called twice to create two histograms, one for x1 and one for x2. The col argument is used to set the color of each histogram. The add argument is set to TRUE for the second histogram so that it is overlaid on top of the first histogram. Finally, the legend() function is used to add a legend to the plot indicating which histogram corresponds to which variable.\nIn summary, the code generates a dual-variable histogram of two vectors of random normal numbers with different means. The histogram shows the distribution of values for each variable and allows for easy comparison between the two variables."
  },
  {
    "objectID": "posts/2023-09-15/index.html#dual-variable-histogram-with-transparency",
    "href": "posts/2023-09-15/index.html#dual-variable-histogram-with-transparency",
    "title": "Histograms with Two or More Variables in R",
    "section": "Dual-Variable Histogram with Transparency",
    "text": "Dual-Variable Histogram with Transparency\nAdding transparency to the histograms can make the visualization more informative when the bars overlap. We can achieve this by setting the alpha parameter in the col argument. Let’s use the same dataset and create a dual-variable histogram with transparency:\n\n# Create a dual-variable histogram with transparency\nminx &lt;- min(mtcars$mpg, mtcars$hp)\nmaxx &lt;- max(mtcars$mpg, mtcars$hp)\nhist(\n  mtcars$mpg, \n  main=\"Histogram of MPG and Horsepower\", \n  xlab=\"Value\",\n  ylab=\"Frequency\", \n  col=rgb(0, 0, 1, alpha=0.5), \n  xlim=c(minx, maxx))\nhist(\n  mtcars$hp, \n  col=rgb(1, 0, 0, alpha=0.5), \n  add=TRUE\n  )\nlegend(\"topright\", legend=c(\"MPG\", \"Horsepower\"), fill=c(rgb(0, 0, 1, alpha=0.5), rgb(1, 0, 0, alpha=0.5)))\n\n\n\n\nHere, we use the rgb() function to set the color with transparency. The alpha parameter controls the transparency level, with values between 0 (completely transparent) and 1 (completely opaque)."
  },
  {
    "objectID": "posts/2023-09-15/index.html#side-by-side-histograms",
    "href": "posts/2023-09-15/index.html#side-by-side-histograms",
    "title": "Histograms with Two or More Variables in R",
    "section": "Side-by-Side Histograms",
    "text": "Side-by-Side Histograms\nIf you prefer to display the histograms side by side, you can use the par() function to adjust the layout. Here’s an example:\n\n# Set up a side-by-side layout\npar(mfrow=c(1, 2))\n\n# Create side-by-side histograms\nhist(mtcars$mpg, main=\"Histogram of MPG\", xlab=\"Miles Per Gallon\", \n     ylab=\"Frequency\", col=\"lightblue\", xlim=c(10, 35))\nhist(mtcars$hp, main=\"Histogram of Horsepower\", xlab=\"Horsepower\", \n     ylab=\"Frequency\", col=\"lightgreen\")\n\n\n\npar(mfrow=c(1,1))\n\nIn this code, we use par(mfrow=c(1, 2)) to set up a 1x2 layout, which means two plots will appear side by side."
  },
  {
    "objectID": "posts/2023-09-15/index.html#customizing-dual-variable-histograms",
    "href": "posts/2023-09-15/index.html#customizing-dual-variable-histograms",
    "title": "Histograms with Two or More Variables in R",
    "section": "Customizing Dual-Variable Histograms",
    "text": "Customizing Dual-Variable Histograms\nYou can customize your dual-variable histograms further by adjusting various parameters, such as bin width, titles, labels, and colors. Experiment with different settings to create visualizations that best convey your data’s story.\nRemember, the key to effective data visualization is experimentation and exploration. Try different datasets, play with colors and styles, and find the representation that best suits your needs."
  },
  {
    "objectID": "posts/2023-09-15/index.html#conclusion",
    "href": "posts/2023-09-15/index.html#conclusion",
    "title": "Histograms with Two or More Variables in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we’ve explored several ways to create histograms of two variables in R. Whether you’re comparing distributions or just visualizing your data, histograms are a valuable tool in your data analysis toolkit. Experiment with the provided examples and take your data visualization skills to the next level!\nSo, fire up your R environment, load your data, and start creating dual-variable histograms today. Happy coding!"
  },
  {
    "objectID": "posts/2023-09-17/index.html",
    "href": "posts/2023-09-17/index.html",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "",
    "text": "Histograms are a fundamental tool in data analysis and visualization, allowing us to explore the distribution of data quickly and effectively. While creating a histogram in R is straightforward, specifying breaks appropriately can make a world of difference in the insights you can draw from your data. In this blog post, we will delve into the art of specifying breaks in a histogram, providing you with multiple examples and encouraging you to experiment on your own.\nBefore we get started, it’s worth mentioning that this topic has been explored in depth by Steve Sanderson in his previous blog post. If you’re interested in diving even deeper, make sure to check out his article here: Steve’s Blog Post on Optimal Binning. Now, let’s embark on our journey into the fascinating world of histogram breaks in R."
  },
  {
    "objectID": "posts/2023-09-17/index.html#example-1-default-breaks",
    "href": "posts/2023-09-17/index.html#example-1-default-breaks",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "Example 1: Default Breaks",
    "text": "Example 1: Default Breaks\nLet’s start with a simple example using R’s built-in mtcars dataset:\n\n# Create a histogram with default breaks\nhist(mtcars$mpg, main = \"Default Breaks\", xlab = \"Miles per Gallon\")\n\n\n\n\nIn this case, R automatically selects the breaks based on the range of the data. The resulting histogram might not reveal finer details, and it’s essential to understand how to customize breaks to suit your analysis."
  },
  {
    "objectID": "posts/2023-09-17/index.html#example-2-specifying-equal-breaks",
    "href": "posts/2023-09-17/index.html#example-2-specifying-equal-breaks",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "Example 2: Specifying Equal Breaks",
    "text": "Example 2: Specifying Equal Breaks\nYou can specify equal-width breaks using the breaks parameter. Here’s an example:\n\n# Create a histogram with equal-width breaks\nhist(mtcars$mpg, main = \"Equal Width Breaks\", xlab = \"Miles per Gallon\", breaks = 10)\n\n\n\n\nIn this example, we divided the data into 10 equal-width bins. This approach can help reveal underlying patterns in the data distribution."
  },
  {
    "objectID": "posts/2023-09-17/index.html#example-3-custom-breaks",
    "href": "posts/2023-09-17/index.html#example-3-custom-breaks",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "Example 3: Custom Breaks",
    "text": "Example 3: Custom Breaks\nSometimes, you may have domain knowledge that suggests specific break points. Let’s explore a case where we set custom breaks:\n\n# Create a histogram with custom breaks\ncustom_breaks &lt;- c(10, 15, 20, 25, 30, 35)\nhist(mtcars$mpg, main = \"Custom Breaks\", xlab = \"Miles per Gallon\", breaks = custom_breaks)\n\n\n\n\nHere, we’ve defined custom break points, which can help emphasize critical thresholds in the data."
  },
  {
    "objectID": "posts/2023-09-17/index.html#example-4-logarithmic-breaks",
    "href": "posts/2023-09-17/index.html#example-4-logarithmic-breaks",
    "title": "Mastering Histogram Breaks in R: Unveiling the Power of Data Visualization",
    "section": "Example 4: Logarithmic Breaks",
    "text": "Example 4: Logarithmic Breaks\nIn some cases, data may follow a logarithmic distribution. You can use logarithmic breaks to visualize such data effectively:\n\n# Create a histogram with logarithmic breaks\nhist(log(mtcars$mpg), main = \"Logarithmic Breaks\", xlab = \"Log(Miles per Gallon)\")\n\n\n\n\nBy taking the logarithm of the data and setting appropriate breaks, you can bring out patterns that might be obscured in a standard histogram."
  },
  {
    "objectID": "posts/2023-09-19/index.html",
    "href": "posts/2023-09-19/index.html",
    "title": "Exploring Data with Scatter Plots by Group in R",
    "section": "",
    "text": "Data visualization is a powerful tool for gaining insights from your data. Scatter plots, in particular, are excellent for visualizing relationships between two continuous variables. But what if you want to compare multiple groups within your data? In this blog post, we’ll explore how to create engaging scatter plots by group in R. We’ll walk through the process step by step, providing several examples and explaining the code blocks in simple terms. So, whether you’re a data scientist, analyst, or just curious about R, let’s dive in and discover how to make your data come to life!"
  },
  {
    "objectID": "posts/2023-09-19/index.html#using-ggplot2",
    "href": "posts/2023-09-19/index.html#using-ggplot2",
    "title": "Exploring Data with Scatter Plots by Group in R",
    "section": "Using ggplot2",
    "text": "Using ggplot2\n\nCreating Scatter Plots by Group:\nTo create scatter plots by group, we’ll use the popular R package, ggplot2. If you haven’t installed it yet, you can do so using the following command:\n\nif(!require(ggplot2)){install.packages(\"ggplot2\")}\n\nNow, let’s load the ggplot2 library:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n\nExample 1: Basic Scatter Plot\nLet’s start with a basic scatter plot that shows the relationship between Sepal.Length and Sepal.Width for all iris species. We’ll color the points by species to distinguish them:\n\n# Create a basic scatter plot\nggplot(\n  data = iris, \n  aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  labs(title = \"Sepal Length vs. Sepal Width by Species\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\") +\n  theme_minimal()\n\n\n\n\nIn this code: - We specify the dataset (iris) and the variables we want to plot. - geom_point() adds the points to the plot. - labs() is used to add a title and label the axes.\n\n\nExample 2: Faceted Scatter Plot\nNow, let’s take it a step further and create separate scatter plots for each iris species using faceting:\n\n# Create faceted scatter plots\nggplot(\n  data = iris, \n  aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_wrap(~Species) +\n  labs(title = \"Sepal Length vs. Sepal Width by Species\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\") +\n  theme_minimal()\n\n\n\n\nIn this example, facet_wrap(~Species) creates three individual scatter plots, one for each iris species. This makes it easier to compare the species’ characteristics.\n\n\nExample 3: Customized Scatter Plot\nLet’s customize our scatter plot further by adding regression lines and adjusting point aesthetics:\n\n# Create a customized scatter plot\nggplot(\n  data = iris, \n  aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3, alpha = 0.7, shape = 19) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Customized Sepal Length vs. Sepal Width by Species\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIn this example: - geom_point() now includes size, alpha (transparency), and shape aesthetics. - geom_smooth() adds linear regression lines to each group."
  },
  {
    "objectID": "posts/2023-09-19/index.html#using-base-r",
    "href": "posts/2023-09-19/index.html#using-base-r",
    "title": "Exploring Data with Scatter Plots by Group in R",
    "section": "Using Base R",
    "text": "Using Base R\n\nExample 1: Basic Scatter Plot in Base R\nTo create a basic scatter plot in base R, we can use the plot() function. Here’s how to create a scatter plot of Sepal.Length vs. Sepal.Width by grouping on the “Species” variable:\n\n# Create a basic scatter plot\nplot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species, \n     pch = 19, main = \"Sepal Length vs. Sepal Width by Species\",\n     xlab = \"Sepal Length\", ylab = \"Sepal Width\")\nlegend(\"topright\", legend = levels(iris$Species), col = 1:3, pch = 19)\n\n\n\n\nIn this code: - plot() is used to create the scatter plot. - We specify the x and y variables, and we use the col argument to color the points by species. - pch specifies the point character (shape). - main, xlab, and ylab are used to add a title and label the axes. - legend() adds a legend to distinguish the species colors.\n\n\nExample 2: Faceted Scatter Plot in Base R\nTo create faceted scatter plots in base R, we can use the split() function to split the data by the “Species” variable and then create individual scatter plots for each group:\n\n# Split the data by species\nsplit_data &lt;- split(iris, iris$Species)\n\n# Create faceted scatter plots\npar(mfrow = c(1, 3))  # Arrange plots in one row and three columns\nfor (i in 1:3) {\n  plot(split_data[[i]]$Sepal.Length, split_data[[i]]$Sepal.Width, \n       pch = 19, main = levels(iris$Species)[i], \n       xlab = \"Sepal Length\", ylab = \"Sepal Width\")\n}\n\n\n\npar(mfrow = c(1, 1))\n\nIn this code: - We first use split() to split the data into three groups based on the “Species” variable. - Then, we use a for loop to create individual scatter plots for each group. - par(mfrow = c(1, 3)) arranges the plots in one row and three columns.\n\n\nExample 3: Customized Scatter Plot in Base R\nTo create a customized scatter plot in base R, we can adjust various graphical parameters. Here’s an example with customized aesthetics and regression lines:\n\n# Create a customized scatter plot with regression lines\nplot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species, \n     pch = 19, main = \"Customized Sepal Length vs. Sepal Width by Species\",\n     xlab = \"Sepal Length\", ylab = \"Sepal Width\")\nlegend(\"topright\", legend = levels(iris$Species), col = 1:3, pch = 19)\n\n# Add regression lines\nfor (i in 1:3) {\n  group_data &lt;- split_data[[i]]\n  lm_fit &lt;- lm(Sepal.Width ~ Sepal.Length, data = group_data)\n  abline(lm_fit, col = i)\n}\n\n\n\n\nIn this code: - We add regression lines to each group using a for loop and the abline() function. - The lm() function is used to fit linear regression models to each group separately.\nNow you have recreated the scatter plots by group using base R. Feel free to explore more customization options and adapt these examples to your specific needs. Happy coding!"
  },
  {
    "objectID": "posts/2023-09-20/index.html",
    "href": "posts/2023-09-20/index.html",
    "title": "Mastering Data Visualization in R: Plotting Predicted Values with the mtcars Dataset",
    "section": "",
    "text": "Introduction\nData visualization is a powerful tool in a data scientist’s toolkit. It not only helps us understand our data but also presents it in a way that is easy to comprehend. In this blog post, we will explore how to plot predicted values in R using the mtcars dataset. We will train a simple regression model to predict the miles per gallon (mpg) of cars based on their attributes and then visualize the predictions. By the end of this tutorial, you’ll have a clear understanding of how to plot predicted values and can apply this knowledge to your own data analysis projects.\nStep 1: Load the Required Libraries\nBefore we dive into the code, let’s make sure we have the necessary libraries installed. We’ll be using ggplot2 for plotting and caret for model training and evaluation. You can install them if you haven’t already using:\ninstall.packages(\"ggplot2\")\ninstall.packages(\"caret\")\nNow, let’s load the libraries:\n\nlibrary(ggplot2)\nlibrary(caret)\n\nStep 2: Load and Explore the Data\nWe’ll use the classic mtcars dataset, which contains various attributes of different car models. Our goal is to predict the fuel efficiency (mpg) of these cars. Let’s load and explore the dataset:\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThis will display the first few rows of the dataset, giving you an idea of what it looks like.\nStep 3: Split the Data into Training and Testing Sets\nBefore we proceed with modeling and prediction, we need to split our data into training and testing sets. We’ll use 80% of the data for training and the remaining 20% for testing:\n\nset.seed(123)  # for reproducibility\nsplitIndex &lt;- createDataPartition(mtcars$mpg, p = 0.8, list = FALSE)\ntraining_data &lt;- mtcars[splitIndex, ]\ntesting_data &lt;- mtcars[-splitIndex, ]\n\nStep 4: Build a Simple Linear Regression Model\nNow, let’s build a simple linear regression model to predict mpg based on other attributes. We’ll use the lm() function:\n\nmodel &lt;- lm(mpg ~ ., data = training_data)\n\nThis line of code fits the linear regression model using the training data.\nStep 5: Make Predictions\nWith our model trained, we can now make predictions on the testing data:\n\npredictions &lt;- predict(model, newdata = testing_data)\n\nStep 6: Create a Scatter Plot of Predicted vs. Actual Values\nThe most exciting part is visualizing the predicted values. We can do this using a scatter plot. Let’s create one:\n\n# Combine actual and predicted values\nplot_data &lt;- data.frame(Actual = testing_data$mpg, Predicted = predictions)\n\n# Create a scatter plot\nggplot(plot_data, aes(x = Actual, y = Predicted)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"red\") +\n  labs(\n    x = \"Actual MPG\", \n    y = \"Predicted MPG\", \n    title = \"Actual vs. Predicted MPG\"\n    ) +\n  theme_minimal()\n\n\n\n\nThis code generates a scatter plot with the actual MPG values on the x-axis and predicted MPG values on the y-axis. The red line represents a linear regression line that helps us see how well our predictions align with the actual data.\nHere is how we also plot the data in base R.\n\n# Combine actual and predicted values\nplot_data &lt;- data.frame(Actual = testing_data$mpg, Predicted = predictions)\n\n# Create a scatter plot\nplot(plot_data$Actual, plot_data$Predicted,\n     xlab = \"Actual MPG\", ylab = \"Predicted MPG\",\n     main = \"Actual vs. Predicted MPG\",\n     pch = 19, col = \"blue\")\n\n# Add a regression line\nabline(lm(Predicted ~ Actual, data = plot_data), col = \"red\")\n\n\n\n\n\n\nConclusion\nCongratulations! You’ve successfully learned how to plot predicted values in R using the mtcars dataset. Visualization is a vital part of data analysis, and it can provide valuable insights into the performance of your predictive models.\nI encourage you to try this on your own datasets and explore more advanced visualization techniques. Experiment with different models and datasets to gain a deeper understanding of data visualization in R. Happy coding!"
  },
  {
    "objectID": "posts/2023-09-22/index.html",
    "href": "posts/2023-09-22/index.html",
    "title": "Creating Confidence Intervals for a Linear Model in R Using Base R and the Iris Dataset",
    "section": "",
    "text": "Introduction\nLinear regression is a fundamental statistical technique used to model the relationship between a dependent variable and one or more independent variables. While fitting a linear model is relatively straightforward in R, it’s also essential to understand the uncertainty associated with our model’s predictions. One way to visualize this uncertainty is by creating confidence intervals around the regression line. In this blog post, we’ll walk through how to perform linear regression and plot confidence intervals using base R with the popular Iris dataset.\n\n\nAbout the Iris Dataset\nThe Iris dataset is a well-known dataset in the field of statistics and machine learning. It contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers: setosa, versicolor, and virginica. For our purposes, we’ll focus on predicting petal length based on petal width for one of the iris species.\n\n\nLoading the Data\nFirst, let’s load the Iris dataset and take a quick look at its structure:\n# Load the Iris dataset\ndata(iris)\nNow view it\n\n# View the first few rows of the dataset\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\nFitting a Linear Model\nWe want to predict petal length (dependent variable) based on petal width (independent variable). To do this, we’ll fit a linear regression model using the lm() function in R:\n\n# Fit a linear regression model\nmodel &lt;- lm(Petal.Length ~ Petal.Width, data = iris)\n\nNow that we have our model, let’s move on to creating confidence intervals for the regression line.\n\n\nCalculating Confidence Intervals\nTo calculate confidence intervals for the regression line, we’ll use the predict() function with the interval argument set to “confidence”:\n\n# Calculate confidence intervals\nconfidence_intervals &lt;- predict(\n  model, \n  interval = \"confidence\", \n  level = 0.95\n)\n\n# View the first few rows of the confidence intervals\nhead(confidence_intervals)\n\n       fit      lwr      upr\n1 1.529546 1.402050 1.657042\n2 1.529546 1.402050 1.657042\n3 1.529546 1.402050 1.657042\n4 1.529546 1.402050 1.657042\n5 1.529546 1.402050 1.657042\n6 1.975534 1.863533 2.087536\n\n\nThe confidence_intervals object now contains the lower and upper bounds of the confidence intervals for our predictions.\n\n\nCreating the Plot\nWith the confidence intervals calculated, we can create a visually appealing plot to display our linear regression model and the associated confidence intervals:\n\n# Create a scatterplot of the data\nplot(\n  iris$Petal.Width, \n  iris$Petal.Length, \n  main = \"Linear Regression with Confidence Intervals\", \n  xlab = \"Petal Width\", ylab = \"Petal Length\"\n)\n\n# Add the regression line\nabline(model, col = \"blue\")\n\n# Add confidence intervals as shaded areas\npolygon(\n  c(iris$Petal.Width, rev(iris$Petal.Width)),\n  c(\n    confidence_intervals[, \"lwr\"], \n    rev(confidence_intervals[, \"upr\"])\n    ), \n  col = rgb(0, 0, 1, 0.2), border = NA)\n\n# Add a legend\nlegend(\n  \"topright\", \n  legend = c(\"Regression Line\", \"95% Confidence Interval\"), \n  col = c(\"blue\", rgb(0, 0, 1, 0.2)), \n  fill = c(NA, rgb(0, 0, 1, 0.2))\n)\n\n\n\n\nIn this plot, we start by creating a scatterplot of the data points, then overlay the regression line in blue. The shaded area represents the 95% confidence interval around the regression line, giving us an idea of the uncertainty in our predictions.\nHere is a slightly different method, the confidence intervals:\n\n# Calculate confidence intervals\nconf_intervals &lt;- predict(model, interval = \"confidence\")\n\nNow the plot:\n\n# Create a scatterplot\nplot(\n  iris$Petal.Width, \n  iris$Petal.Length, \n  main = \"Linear Model with Confidence Intervals\",\n  xlab = \"Petal Width\", \n  ylab = \"Petal Length\", \n  pch = 19, \n  col = \"blue\"\n)\n\n# Add the regression line\nabline(model, col = \"red\")\n\n# Add confidence intervals\nlines(\n  iris$Petal.Width, \n  conf_intervals[, \"lwr\"], \n  col = \"green\", \n  lty = 2\n)\nlines(\n  iris$Petal.Width, \n  conf_intervals[, \"upr\"], \n  col = \"green\", \n  lty = 2\n)\n\n\n\n\n\n\nConclusion\nIn this blog post, we’ve demonstrated how to perform linear regression and plot confidence intervals using base R with the Iris dataset. Understanding and visualizing the uncertainty associated with our regression model is crucial for making informed decisions based on the model’s predictions. You can apply these techniques to other datasets and regression problems to gain deeper insights into your data.\nLinear regression is just one of the many statistical techniques that R offers. As you continue your data analysis journey, you’ll find R to be a powerful tool for exploring, modeling, and visualizing data."
  },
  {
    "objectID": "posts/2023-09-25/index.html",
    "href": "posts/2023-09-25/index.html",
    "title": "Mastering Data Visualization with Pairs Plots in Base R",
    "section": "",
    "text": "Data visualization is a crucial tool in data analysis, allowing us to gain insights from our data quickly. One of the fundamental techniques for exploring relationships between variables is the pairs plot. In this blog post, we’ll dive into the world of pairs plots in base R. We’ll explore what they are, why they are useful, and how to create and interpret them."
  },
  {
    "objectID": "posts/2023-09-25/index.html#customizing-your-pairs-plot",
    "href": "posts/2023-09-25/index.html#customizing-your-pairs-plot",
    "title": "Mastering Data Visualization with Pairs Plots in Base R",
    "section": "Customizing Your Pairs Plot",
    "text": "Customizing Your Pairs Plot\nYou can customize your pairs plot in various ways to make it more informative and visually appealing. Here are some customization options:\n\nColoring by Groups: If your dataset has categorical variables that define groups, you can use colors to distinguish between them. For example, you can color data points by species in the “iris” dataset.\n\n# Color points by species\npairs(iris[, 1:4], main = \"Pairs Plot of Iris Data\", col = iris$Species)\n\n\n\n\nAdding Regression Lines: To visualize linear relationships more clearly, you can add regression lines to the scatterplots.\n\n\n# Add regression lines\npairs(iris[, 1:4], panel=function(x,y){\n  points(x,y)\n  abline(lm(y~x), col='red')})\n\n\n\n\nNow let’s add color back into the plot:\n\npairs(iris[, 1:4], panel=function(x,y){\n  # Get a vector of colors for each point in the plot\n  colors &lt;- ifelse(iris$Species == \"setosa\", \"red\",\n                   ifelse(iris$Species == \"versicolor\", \"green\", \"blue\"))\n\n  # Plot the points with the corresponding colors\n  points(x, y, col = colors)\n\n  # Add a regression line\n  abline(lm(y~x), col='red')\n})"
  },
  {
    "objectID": "posts/2023-09-26/index.html",
    "href": "posts/2023-09-26/index.html",
    "title": "Mastering Data Visualization with ggplot2: A Guide to Using facet_grid()",
    "section": "",
    "text": "Introduction\nData visualization is a crucial tool in the data scientist’s toolkit. It allows us to explore and communicate complex patterns and insights effectively. In the world of R programming, one of the most powerful and versatile packages for data visualization is ggplot2. Among its many features, ggplot2 offers the facet_grid() function, which enables you to create multiple plots arranged in a grid, making it easier to visualize different groups of data simultaneously.\nIn this blog post, we’ll dive into the fascinating world of facet_grid() using a practical example. We’ll generate some synthetic data, split it into multiple groups, and then use facet_grid() to create a visually appealing grid of plots.\n\n\nGenerating Synthetic Data\nLet’s start by generating some synthetic data using the TidyDensity package in R. We’ll create three groups of data, each with 100 observations, and a mean of -2, 0, and 2, respectively, all with a standard deviation of 1. We’ll also perform this simulation five times to create a diverse dataset.\n\nlibrary(TidyDensity)\n\ndata &lt;- tidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\", \n  .param_list = list(\n    .n = 100, \n    .mean = c(-2, 0, 2), \n    .sd = 1, \n    .num_sims = 5\n    )\n  )\n\nNow that we have our data, it’s time to visualize it using facet_grid().\n\n\nUsing facet_grid() to Visualize Multiple Groups\nThe facet_grid() function in ggplot2 is a versatile tool for creating a grid of plots based on one or more categorical variables. It allows you to create small multiples, which are a series of similar plots, each showing a subset of your data.\nIn our synthetic data, we have three groups (mean of -2, 0, and 2), and we want to visualize each group’s distribution. Here’s how you can do it:\n\n# Create a ggplot object\n# Load ggplot2\nlibrary(ggplot2)\n\n# Create a ggplot object\np &lt;- ggplot(data, aes(x = y, color = sim_number, group = sim_number)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(. ~ dist_name)\n\n# Customize the plot\np + labs(title = \"Density Plots of Three Different Means\",\n         x = \"Value\",\n         y = \"Density\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nIn this code:\n\nWe load the ggplot2 package, which is essential for creating our plots.\nWe create a ggplot object p where we specify the aesthetics (x-axis, fill color) and the geometry (density plot). We use facet_grid(. ~ simulation) to split the data into separate facets based on the simulation variable. This means that each facet will represent one of the five simulations.\nWe add labels and customize the plot’s appearance using the labs() and theme_minimal() functions.\nFinally, we display the plot by evaluating p.\n\nThe resulting plot will show a grid of density plots, with each facet representing one simulation. Within each facet, you’ll see the density distribution of the data, colored by the group mean.\n\n\nConclusion\nIn this blog post, we explored the power of ggplot2’s facet_grid() function for visualizing multiple groups of data. By generating synthetic data and using ggplot2, we created an informative grid of density plots, allowing us to compare and contrast the distributions of different groups.\nThe ability to create small multiples with facet_grid() is invaluable for gaining insights from complex datasets. Whether you’re working with synthetic data or real-world data, mastering ggplot2’s facet_grid() function will enhance your data visualization skills and help you communicate your findings more effectively.\nSo, go ahead and experiment with your data. Create your own grid of plots using facet_grid() and unlock new ways to visualize and understand your data. Happy plotting!"
  },
  {
    "objectID": "posts/2023-09-27/index.html",
    "href": "posts/2023-09-27/index.html",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "",
    "text": "Data visualization is a crucial aspect of data analysis and exploration. It allows us to gain insights, spot trends, and communicate our findings effectively. In R, there are numerous packages and libraries available for creating sophisticated plots, but understanding the basics of base R plotting is essential for any data analyst or scientist.\nIn this blog post, we’ll explore how to overlay points or lines on a plot using Base R. We’ll use the plot() function to create the initial plot and then show how to overlay points with points() and lines with lines(). We’ll provide several examples, explaining each code block in simple terms, and encourage you to try them out on your own datasets."
  },
  {
    "objectID": "posts/2023-09-27/index.html#example-1-overlaying-points-on-a-scatter-plot",
    "href": "posts/2023-09-27/index.html#example-1-overlaying-points-on-a-scatter-plot",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "Example 1: Overlaying Points on a Scatter Plot",
    "text": "Example 1: Overlaying Points on a Scatter Plot\nLet’s begin with a simple scatter plot. Suppose we have two vectors, x and y, representing data points.\n\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 1, 5, 7)\n\n# Create the initial scatter plot\nplot(x, y, main = \"Scatter Plot with Overlay Points\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n# Overlay additional points (red circles) on the plot\npoints(x, y, col = \"red\", pch = 16)\n\n\n\n\nIn this example, we use the plot() function to create a scatter plot of x and y. Then, we overlay red circles on the existing plot using points(). The col argument specifies the color, and pch determines the point shape (16 represents circles)."
  },
  {
    "objectID": "posts/2023-09-27/index.html#example-2-overlaying-lines-on-a-line-plot",
    "href": "posts/2023-09-27/index.html#example-2-overlaying-lines-on-a-line-plot",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "Example 2: Overlaying Lines on a Line Plot",
    "text": "Example 2: Overlaying Lines on a Line Plot\nNow, let’s work with line plots. Suppose we have two vectors, x and y, representing data points for a line graph.\n\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 1, 5, 7)\n\n# Create the initial line plot\nplot(x, y, type = \"l\", main = \"Line Plot with Overlay Lines\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n# Overlay a new line (dashed, blue) on the plot\nlines(x, y + 1, col = \"blue\", lty = 2)\n\n\n\n\nIn this example, we use the plot() function with type = \"l\" to create a line plot. Then, we overlay a dashed blue line on the plot using lines(). The col argument sets the line color, and lty specifies the line type (2 stands for dashed)."
  },
  {
    "objectID": "posts/2023-09-27/index.html#example-3-combining-points-and-lines",
    "href": "posts/2023-09-27/index.html#example-3-combining-points-and-lines",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "Example 3: Combining Points and Lines",
    "text": "Example 3: Combining Points and Lines\nIn some cases, you might want to overlay both points and lines on the same plot to illustrate relationships more clearly. Let’s see how to do that:\n\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 1, 5, 7)\n\n# Create the initial scatter plot\nplot(x, y, main = \"Scatter Plot with Overlay Points and Lines\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n# Overlay points (green triangles)\npoints(x, y, col = \"green\", pch = 17)\n\n# Overlay a line (purple) connecting the points\nlines(x, y, col = \"purple\")\n\n\n\n\nIn this example, we start with a scatter plot and overlay green triangles using points() and a purple line using lines(). The combination of points and lines can help emphasize patterns and relationships in your data."
  },
  {
    "objectID": "posts/2023-09-27/index.html#conclusion",
    "href": "posts/2023-09-27/index.html#conclusion",
    "title": "Enhancing Your Data Visualizations with Base R: Overlaying Points and Lines",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we’ve explored how to overlay points and lines on a plot in Base R. We’ve covered scatter plots, line plots, and combinations of both. Overlaying points and lines can be a powerful way to enhance your data visualizations and convey insights effectively.\nNow it’s your turn! Experiment with different datasets and customize your plots by adjusting colors, shapes, and line styles. Base R provides a solid foundation for data visualization, and mastering it will enable you to create informative plots for your data analysis projects."
  },
  {
    "objectID": "posts/2023-09-28/index.html",
    "href": "posts/2023-09-28/index.html",
    "title": "How to Reorder Boxplots in R: A Comprehensive Guide",
    "section": "",
    "text": "Boxplots are a great way to visualize the distribution of a dataset. However, sometimes the default ordering of boxplots may not be ideal for the data being presented. In this blog post, we will explore how to reorder boxplots in R using base R. We will provide at least three examples and explain them in simple terms. We encourage readers to try things on their own."
  },
  {
    "objectID": "posts/2023-09-28/index.html#example-1-reorder-based-on-specific-order",
    "href": "posts/2023-09-28/index.html#example-1-reorder-based-on-specific-order",
    "title": "How to Reorder Boxplots in R: A Comprehensive Guide",
    "section": "Example 1: Reorder Based on Specific Order",
    "text": "Example 1: Reorder Based on Specific Order\nThe first example shows how to order the boxplots based on a specific order for the variable being plotted. We will use the built-in airquality dataset in R. The following code shows how to order the boxplots based on the following order for the Month variable: 5, 8, 6, 9, 7.\n\n# Load the airquality dataset\ndata(airquality)\n\n# Reorder Month values\nairquality$Month &lt;- factor(airquality$Month, levels=c(5, 8, 6, 9, 7))\n\n# Create boxplot of temperatures by month using the order we specified\nboxplot(Temp ~ Month, data=airquality, col=\"lightblue\", border=\"black\")\n\n\n\n\nNotice that the boxplots now appear in the order that we specified using the levels argument."
  },
  {
    "objectID": "posts/2023-09-28/index.html#example-2-reorder-based-on-median-value",
    "href": "posts/2023-09-28/index.html#example-2-reorder-based-on-median-value",
    "title": "How to Reorder Boxplots in R: A Comprehensive Guide",
    "section": "Example 2: Reorder Based on Median Value",
    "text": "Example 2: Reorder Based on Median Value\nThe second example shows how to order the boxplots in ascending order based on the median value for each group. We will use the built-in PlantGrowth dataset in R.\n\n# Load the PlantGrowth dataset\ndata(PlantGrowth)\n\n# Create boxplot of weight by group\nboxplot(weight ~ group, data=PlantGrowth, col=\"lightblue\", border=\"black\")\n\n\n\n# Reorder the groups based on median weight\ngroup_order &lt;- names(sort(tapply(PlantGrowth$weight, PlantGrowth$group, median)))\nPlantGrowth$group &lt;- factor(PlantGrowth$group, levels=group_order)\n\n# Create boxplot of weight by group using the new order\nboxplot(weight ~ group, data=PlantGrowth, col=\"lightblue\", border=\"black\")\n\n\n\n\nNotice that the boxplots now appear in ascending order based on the median weight for each group."
  },
  {
    "objectID": "posts/2023-09-28/index.html#example-3-reorder-based-on-custom-function",
    "href": "posts/2023-09-28/index.html#example-3-reorder-based-on-custom-function",
    "title": "How to Reorder Boxplots in R: A Comprehensive Guide",
    "section": "Example 3: Reorder Based on Custom Function",
    "text": "Example 3: Reorder Based on Custom Function\nThe third example shows how to order the boxplots based on a custom function. We will use the built-in ToothGrowth dataset in R.\n\n# Load the ToothGrowth dataset\ndata(ToothGrowth)\n\n# Create boxplot of length by dose\nboxplot(len ~ dose, data=ToothGrowth, col=\"lightblue\", border=\"black\")\n\n# Reorder the groups based on the mean length multiplied by the dose\ngroup_order &lt;- names(sort(tapply(ToothGrowth$len * ToothGrowth$dose, ToothGrowth$dose, mean)))\nToothGrowth$dose &lt;- factor(ToothGrowth$dose, levels=group_order)\n\n# Create boxplot of length by dose using the new order\nboxplot(len ~ dose, data=ToothGrowth, col=\"lightblue\", border=\"black\")\n\n\n\n\nNotice that the boxplots now appear in order based on the mean length multiplied by the dose for each group."
  },
  {
    "objectID": "posts/2023-09-29/index.html",
    "href": "posts/2023-09-29/index.html",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "",
    "text": "Decision trees are a powerful machine learning algorithm that can be used for both classification and regression tasks. They are easy to understand and interpret, and they can be used to build complex models without the need for feature engineering.\nOnce you have trained a decision tree model, you can use it to make predictions on new data. However, it can also be helpful to plot the decision tree to better understand how it works and to identify any potential problems.\nIn this blog post, we will show you how to plot decision trees in R using the rpart and rpart.plot packages. We will also provide an extensive example using the iris data set and explain the code blocks in simple to use terms."
  },
  {
    "objectID": "posts/2023-09-29/index.html#load-the-libraries",
    "href": "posts/2023-09-29/index.html#load-the-libraries",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nlibrary(rpart)\nlibrary(rpart.plot)"
  },
  {
    "objectID": "posts/2023-09-29/index.html#split-the-data-into-training-and-test-sets",
    "href": "posts/2023-09-29/index.html#split-the-data-into-training-and-test-sets",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "Split the data into training and test sets",
    "text": "Split the data into training and test sets\n\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(iris), size = 0.7 * nrow(iris))\ntrain &lt;- iris[train_index, ]\ntest &lt;- iris[-train_index, ]"
  },
  {
    "objectID": "posts/2023-09-29/index.html#train-a-decision-tree-model",
    "href": "posts/2023-09-29/index.html#train-a-decision-tree-model",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "Train a decision tree model",
    "text": "Train a decision tree model\n\ntree &lt;- rpart(Species ~ ., data = train, method = \"class\")"
  },
  {
    "objectID": "posts/2023-09-29/index.html#plot-the-decision-tree",
    "href": "posts/2023-09-29/index.html#plot-the-decision-tree",
    "title": "Plotting Decision Trees in R with rpart and rpart.plot",
    "section": "Plot the decision tree",
    "text": "Plot the decision tree\n\nrpart.plot(tree, main = \"Decision Tree for the Iris Dataset\")"
  },
  {
    "objectID": "posts/2023-10-02/index.html",
    "href": "posts/2023-10-02/index.html",
    "title": "Horizontal Boxplots in R using the Palmer Penguins Data Set",
    "section": "",
    "text": "Introduction\nBoxplots are a great way to visualize the distribution of a numerical variable. They show the median, quartiles, and outliers of the data, and can be used to compare the distributions of multiple groups.\nHorizontal boxplots are a variant of the traditional boxplot, where the x-axis is horizontal and the y-axis is vertical. This can be useful for visualizing data where the x-axis variable is categorical, such as species or treatment group.\n\n\nCreating horizontal boxplots in base R\nTo create a horizontal boxplot in base R, we can use the boxplot() function with the horizontal argument set to TRUE.\n\nlibrary(palmerpenguins)\n\n\n# Create a horizontal boxplot of bill length by species\nboxplot(\n  bill_length_mm ~ species,\n  data = penguins,\n  horizontal = TRUE,\n  main = \"Bill length by species in Palmer penguins\",\n  xlab = \"Bill length (mm)\",\n  ylab = \"Species\"\n)\n\n\n\n\nThis code will produce a horizontal boxplot with one box for each species of penguin. The boxes show the median, quartiles, and outliers of the bill length data for each species.\n\n\nCreating horizontal boxplots in ggplot2\nTo create a horizontal boxplot in ggplot2, we can use the geom_boxplot() function with the coord_flip() function.\n\nlibrary(ggplot2)\n\n# Create a horizontal boxplot of bill length by species using ggplot2\nggplot(penguins, aes(x = bill_length_mm, y = species)) +\n  geom_boxplot() +\n  labs(\n    title = \"Bill length by species in Palmer penguins\",\n    x = \"Bill length (mm)\",\n    y = \"Species\"\n  )\n\n\n\n\nThis code will produce a horizontal boxplot that is similar to the one produced by the base R code above. However, the ggplot2 code is more flexible and allows us to customize the appearance of the plot more easily.\n\n\nEncouragement\nI encourage you to try creating horizontal boxplots for your own data. You can use the Palmer penguins data set as a starting point, or you can use your own data. Experiment with different options to customize the appearance of your plots.\nHere are some ideas for things to try:\n\nCompare the distribution of different numerical variables across different groups. For example, you could compare the distribution of bill length across the three species of penguins, or you could compare the distribution of body mass across male and female penguins.\nUse different colors and fill patterns to distinguish between groups.\nAdd jitter to the data points to avoid overplotting.\nAdd a legend to identify the different groups.\nSave your plots to files or export them to other applications.\n\nI hope this blog post has been helpful. If you have any questions, please leave a comment below.\n\n\nConclusion\nHorizontal boxplots can be a useful way to visualize the distribution of data when the x-axis variable is categorical. They are easy to create in both base R and ggplot2."
  },
  {
    "objectID": "posts/2023-10-03/index.html",
    "href": "posts/2023-10-03/index.html",
    "title": "Creating Interactive Radar Charts in R with the ‘fmsb’ Library",
    "section": "",
    "text": "Radar charts, also known as spider, web, polar, or star plots, are a useful way to visualize multivariate data. In R, we can create radar charts using the fmsb library. Here are several examples of how to create radar charts in R using the fmsb library:"
  },
  {
    "objectID": "posts/2023-10-03/index.html#example-1-basic-radar-chart",
    "href": "posts/2023-10-03/index.html#example-1-basic-radar-chart",
    "title": "Creating Interactive Radar Charts in R with the ‘fmsb’ Library",
    "section": "Example 1: Basic radar chart",
    "text": "Example 1: Basic radar chart\nThe following code creates a basic radar chart using the radarchart() function from the fmsb package. The input data format is specific, where each row represents an entity and each column is a variable. The first row should be the maximum values of the data, and the second row should be the minimum values. The default radar chart can be customized using various options, such as line color, fill color, line width, and axis label color.\n\n# Load the fmsb package\nlibrary(fmsb)\n\n# Create sample data\ndata &lt;- as.data.frame(matrix(sample(2:20, 10, replace = T), \n                             ncol = 10))\ncolnames(data) &lt;- c(\"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \n                    \"Var6\", \"Var7\", \"Var8\", \"Var9\", \"Var10\")\ndata &lt;- rbind(rep(20, 10), rep(0, 10), data)\n\n# Create a basic radar chart\nradarchart(data)"
  },
  {
    "objectID": "posts/2023-10-03/index.html#example-2-customized-radar-chart",
    "href": "posts/2023-10-03/index.html#example-2-customized-radar-chart",
    "title": "Creating Interactive Radar Charts in R with the ‘fmsb’ Library",
    "section": "Example 2: Customized radar chart",
    "text": "Example 2: Customized radar chart\nThe following code creates a customized radar chart using the radarchart() function. The axistype argument is set to 1 to customize the polygon, and the pcol, pfcol, and plwd arguments are used to customize the grid and line properties.\n\n# Create sample data\ndata &lt;- as.data.frame(matrix(sample(2:20, 10, replace = T), \n                             ncol = 10))\ncolnames(data) &lt;- c(\"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \n                    \"Var6\", \"Var7\", \"Var8\", \"Var9\", \"Var10\")\ndata &lt;- rbind(rep(20, 10), rep(0, 10), data)\n\n# Customize the radar chart\nradarchart(data, axistype = 1, pcol = rgb(0.2, 0.5, 0.5, 0.9), \n           pfcol = rgb(0.2, 0.5, 0.5, 0.5), plwd = 4, cglcol =\n          \"grey\", cglty = 1)"
  },
  {
    "objectID": "posts/2023-10-03/index.html#example-3-radar-chart-with-multiple-groups",
    "href": "posts/2023-10-03/index.html#example-3-radar-chart-with-multiple-groups",
    "title": "Creating Interactive Radar Charts in R with the ‘fmsb’ Library",
    "section": "Example 3: Radar chart with multiple groups",
    "text": "Example 3: Radar chart with multiple groups\nThe following code creates a radar chart with multiple groups using the radarchart() function. The input data frame should have more than three variables as axes, and the rows indicate cases as series. The first row should show the maximum values, the second row should show the minimum values, and the actual data should be given as row 3 and lower rows.\n\n# Create sample data\nset.seed(1)\ndata &lt;- data.frame(rbind(rep(10, 8), rep(0, 8), \n                         matrix(sample(0:10, 24, replace = TRUE),\n                                nrow = 3)))\ncolnames(data) &lt;- paste(\"Var\", 1:8)\n\n# Create a radar chart with multiple groups\nradarchart(data, axistype = 1, pcol = 1, plwd = 2, \n           pdensity = 10, pangle = 40, cglty = 1, \n           cglcol = \"gray\")"
  },
  {
    "objectID": "posts/2023-10-04/index.html",
    "href": "posts/2023-10-04/index.html",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "",
    "text": "Stacked dot plots are a type of plot that displays frequencies using dots, piled one over the other. In R, there are several ways to create stacked dot plots, including using base R and ggplot2. In this blog post, we will explore how to create stacked dot plots in both Base R and ggplot2, and provide several examples of each."
  },
  {
    "objectID": "posts/2023-10-04/index.html#method-1-the-stripchart-function",
    "href": "posts/2023-10-04/index.html#method-1-the-stripchart-function",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "Method 1: The stripchart() function",
    "text": "Method 1: The stripchart() function\nThe stripchart() function in base R can be used to create a basic stacked dot plot. Here is an example of how to use it:\n\n# create some fake data\nset.seed(0)\ndata &lt;- sample(0:20, 100, replace = TRUE)\n\n# create stacked dot plot\nstripchart(data, method = \"stack\")\n\n\n\n\nThis will create a basic stacked dot plot. However, we can customize it to make it more aesthetically pleasing. Here is an example of how to do that:\n\n# create some fake data\nset.seed(0)\ndata &lt;- sample(0:20, 100, replace = TRUE)\n\n# create stacked dot plot\nstripchart(data, method = \"stack\", offset = .5, at = 0,\n           pch = 19, col = \"steelblue\", \n           main = \"Stacked Dot Plot\", xlab = \"Data Values\")\n\n\n\n\nThis will create a stacked dot plot with a blue color scheme and a more aesthetically pleasing layout."
  },
  {
    "objectID": "posts/2023-10-04/index.html#method-1-the-geom_dotplot-function",
    "href": "posts/2023-10-04/index.html#method-1-the-geom_dotplot-function",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "Method 1: The geom_dotplot() function",
    "text": "Method 1: The geom_dotplot() function\nThe geom_dotplot() function in ggplot2 can be used to create a basic stacked dot plot. Here is an example of how to use it:\n\n# load ggplot2\nlibrary(ggplot2)\n\n# create some fake data\nset.seed(0)\ndata &lt;- data.frame(x = sample(0:20, 100, replace = TRUE))\n\n# create stacked dot plot\nggplot(data, aes(x = x)) + geom_dotplot() + theme_minimal()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nThis will create a basic stacked dot plot. However, we can customize it to make it more aesthetically pleasing. Here is an example of how to do that:\n\n# create some fake data\nset.seed(0)\ndata &lt;- data.frame(x = sample(0:20, 100, replace = TRUE))\n\n# create customized stacked dot plot\nggplot(data, aes(x = x)) + \n  geom_dotplot(dotsize = .75, stackratio = 1.2, \n               fill = \"steelblue\") + \n  scale_y_continuous(NULL, breaks = NULL) + \n  labs(title = \"Stacked Dot Plot\", x = \"Data Values\") +\n  theme_minimal()\n\n\n\n\nThis will create a stacked dot plot with a blue color scheme and a more aesthetically pleasing layout."
  },
  {
    "objectID": "posts/2023-10-04/index.html#method-2-the-geom_jitter-function",
    "href": "posts/2023-10-04/index.html#method-2-the-geom_jitter-function",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "Method 2: The geom_jitter() function",
    "text": "Method 2: The geom_jitter() function\nAnother way to create a stacked dot plot in ggplot2 is to use the geom_jitter() function. Here is an example of how to use it:\n\n# create some fake data\nset.seed(0)\ndata &lt;- data.frame(x = sample(0:20, 100, replace = TRUE))\n\n# create stacked dot plot\nggplot(data, aes(x = x, y = 0)) + \n  geom_jitter(height = .1, width = 0, alpha = .5, \n              color = \"steelblue\") + \n  labs(title = \"Stacked Dot Plot\", x = \"Data Values\") +\n  theme_minimal()\n\n\n\n\nThis will create a stacked dot plot with a blue color scheme and a more aesthetically pleasing layout.\nIn conclusion, creating stacked dot plots in R is a simple and effective way to visualize frequency data. By using either base R or ggplot2, you can create aesthetically pleasing plots that are easy to interpret. We encourage readers to try creating their own stacked dot plots using the examples provided in this blog post."
  },
  {
    "objectID": "posts/2023-10-04/index.html#method-2-the-dotchart-function",
    "href": "posts/2023-10-04/index.html#method-2-the-dotchart-function",
    "title": "Creating Stacked Dot Plots in R: A Guide with Base R and ggplot2",
    "section": "Method 2: The dotchart() function",
    "text": "Method 2: The dotchart() function\nAnother way to create a stacked dot plot in base R is to use the dotchart() function. Here is an example of how to use it:\n\n# create some fake data\nset.seed(0)\ndata &lt;- sample(0:20, 100, replace = TRUE)\n\n# create stacked dot plot\ndotchart(data, cex = .7, col = \"steelblue\", \n         main = \"Stacked Dot Plot\", xlab = \"Data Values\")\n\n\n\n\nThis will create a stacked dot plot with a blue color scheme and a more aesthetically pleasing layout."
  },
  {
    "objectID": "posts/2023-10-05/index.html",
    "href": "posts/2023-10-05/index.html",
    "title": "Introduction",
    "section": "",
    "text": "As an R programmer, you may want to create added variable plots to visualize the relationship between a predictor variable and the response variable while controlling for the effects of other predictor variables. In this blog post, we will use the car library and the avPlots() function to create added variable plots in R."
  },
  {
    "objectID": "posts/2023-10-05/index.html#interpretation-of-added-variable-plots",
    "href": "posts/2023-10-05/index.html#interpretation-of-added-variable-plots",
    "title": "Introduction",
    "section": "Interpretation of Added Variable Plots",
    "text": "Interpretation of Added Variable Plots\nThe added variable plots show the relationship between each predictor variable and the response variable while controlling for the effects of the other predictor variables. The x-axis represents the partial residuals of the predictor variable, and the y-axis represents the partial residuals of the response variable. The line in the plot represents the fitted values from a linear regression model of the partial residuals of the response variable on the partial residuals of the predictor variable.\nIf the relationship between the predictor variable and the response variable is linear, the line in the plot should be approximately horizontal. If the relationship is non-linear, the line may be curved. If there is an outlier, it may be visible as a point that is far away from the other points in the plot."
  },
  {
    "objectID": "posts/2023-10-05/index.html#conclusion",
    "href": "posts/2023-10-05/index.html#conclusion",
    "title": "Introduction",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have learned how to create added variable plots in R using the car library and the avPlots() function. We have also discussed the interpretation of added variable plots and their usefulness in identifying non-linear relationships and outliers. I encourage you to try creating added variable plots on your own and explore the relationships between predictor variables and response variables in your own datasets."
  },
  {
    "objectID": "posts/2023-10-06/index.html",
    "href": "posts/2023-10-06/index.html",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "",
    "text": "Legends are an essential part of data visualization. They help us understand the meaning behind the colors and shapes in our plots. But what if your legend is too big or clutters your plot? Fear not, fellow R enthusiast! In this blog post, we’ll explore how to draw a legend outside of a plot using base R, with a step-by-step example that’s easy to follow."
  },
  {
    "objectID": "posts/2023-10-06/index.html#step-1-create-your-data",
    "href": "posts/2023-10-06/index.html#step-1-create-your-data",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "Step 1: Create Your Data",
    "text": "Step 1: Create Your Data\nLet’s start by creating some sample data. We’ll use a simple scatterplot to demonstrate how to draw a legend outside of the plot. Imagine we have data on two different species of flowers, and we want to distinguish them with different colors.\n\n# Sample data\nset.seed(123)\ndata &lt;- data.frame(\n  x = rnorm(20),\n  y = rnorm(20),\n  species = rep(c(\"A\", \"B\"), each = 10)\n)"
  },
  {
    "objectID": "posts/2023-10-06/index.html#step-2-create-the-plot",
    "href": "posts/2023-10-06/index.html#step-2-create-the-plot",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "Step 2: Create the Plot",
    "text": "Step 2: Create the Plot\nNext, let’s create a scatterplot of our data using the plot() function.\n\n# Create the scatterplot\nplot(\n  data$x, data$y,\n  pch = ifelse(data$species == \"A\", 16, 17),\n  col = ifelse(data$species == \"A\", \"red\", \"blue\"),\n  main = \"Scatterplot with Legend Outside\"\n)\n\n# create margin around plot\npar(mar = c(3, 3, 3, 8), xpd = TRUE)\n\n# Draw the legend outside the plot with inset\nlegend(\n  \"topright\",                           # Position of the legend\n  legend = c(\"Species A\", \"Species B\"), # Legend labels\n  pch = c(16, 17),                      # Point shapes\n  col = c(\"red\", \"blue\"),               # Colors\n  bty = \"n\",                            # No box around the legend\n  inset = c(-0.1, 0)                   # Adjust the inset (move it to the left\n)\n\n\n\n\nIn this code, we’re using the pch argument to specify different point shapes based on the “species” variable and the col argument to set different colors. This creates a scatterplot with points that represent two species, A and B."
  },
  {
    "objectID": "posts/2023-10-06/index.html#step-3-draw-the-legend-outside",
    "href": "posts/2023-10-06/index.html#step-3-draw-the-legend-outside",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "Step 3: Draw the Legend Outside",
    "text": "Step 3: Draw the Legend Outside\nWe already drew the legend but let’s now understand what we did. We’ll use the legend() function for this. Here’s how you can do it:\n# Draw the legend outside the plot with inset\nlegend(\n  \"topright\",                           # Position of the legend\n  legend = c(\"Species A\", \"Species B\"), # Legend labels\n  pch = c(16, 17),                      # Point shapes\n  col = c(\"red\", \"blue\"),               # Colors\n  bty = \"n\",                            # No box around the legend\n  inset = c(-0.16, 0)                   # Adjust the inset (move it to the left)\n)\nIn this code, we specify the position of the legend using the \"topright\" argument. We also provide labels, point shapes, and colors for our legend. The bty = \"n\" argument removes the box around the legend for a cleaner look."
  },
  {
    "objectID": "posts/2023-10-06/index.html#step-4-enjoy-your-plot",
    "href": "posts/2023-10-06/index.html#step-4-enjoy-your-plot",
    "title": "Mastering Legends in R: Drawing Them Outside the Plot",
    "section": "Step 4: Enjoy Your Plot",
    "text": "Step 4: Enjoy Your Plot\nThat’s it! You’ve successfully drawn a legend outside of your plot. Your scatterplot now looks clean, and the legend is clearly separated."
  },
  {
    "objectID": "posts/2023-10-10/index.html",
    "href": "posts/2023-10-10/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Changing the size of the legend on a plot in R can be a handy skill, especially when you want to enhance the readability and aesthetics of your visualizations. In this blog post, we’ll explore different methods to resize legends on R plots with practical examples. Whether you’re a beginner or an experienced R user, this guide should help you master this essential aspect of data visualization."
  },
  {
    "objectID": "posts/2023-10-10/index.html#try-it-yourself",
    "href": "posts/2023-10-10/index.html#try-it-yourself",
    "title": "Introduction",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nNow that you’ve seen how to change the size of legends in R, I encourage you to experiment with your own plots. Adjust the cex parameter, use guides() or customize the theme in ggplot2 to suit your specific needs. Practicing these techniques will enhance your data visualization skills and help you create compelling graphics.\nRemember, effective legends are crucial for conveying the meaning of your plots, so don’t hesitate to tweak their size until your visualizations look just right. Happy coding, and happy plotting!"
  },
  {
    "objectID": "posts/2023-10-11/index.html",
    "href": "posts/2023-10-11/index.html",
    "title": "Horizontal Legends in Base R",
    "section": "",
    "text": "Introduction\nCreating a horizontal legend in base R can be a useful skill when you want to label multiple categories in a plot without taking up too much vertical space. In this blog post, we’ll explore various methods to create horizontal legends in R and provide examples with clear explanations.\n\n\nWhy Do We Need Horizontal Legends?\nVertical legends are great for smaller plots, but in larger visualizations, they can become a space-consuming eyesore. Horizontal legends, on the other hand, allow you to neatly label categories without cluttering the plot area. They are especially useful when you have many categories to label.\n\n\nUsing the legend Function\nThe most straightforward way to create a horizontal legend in base R is by using the legend function. Here’s a simple example:\n\n# Create a sample plot\nplot(1:5, col = 1:5, pch = 19)\n\n# Add a horizontal legend\nlegend(\"top\", legend = c(\"A\", \"B\", \"C\", \"D\", \"E\"), \n       fill = 1:5, horiz = TRUE, x.intersp = 0.2)\n\n\n\n\nIn this code, we first create a basic scatter plot and then use the legend function to add a legend at the top of the plot (\"top\"). The horiz = TRUE argument specifies a horizontal legend.\n\n\nCustomizing the Horizontal Legend\nYou can further customize the horizontal legend to match your preferences. Here are some common parameters:\n\nx.intersp controls the horizontal spacing between legend elements.\ninset adjusts the distance of the legend from the plot.\ntitle adds a title to the legend.\n\n# Customize the horizontal legend\nlegend(\"top\", legend = c(\"A\", \"B\", \"C\", \"D\", \"E\"), \n  fill = 1:5, horiz = TRUE, x.intersp = 0.2, inset = 0.02, \n  title = \"Categories\")\n\n\nAdding Multiple Horizontal Legends\nIn some cases, you might need multiple horizontal legends in a single plot. You can achieve this by specifying different locations for each legend.\n\n# Create a sample plot\nplot(1:5, col = 1:5, pch = 19)\n\n# Add two horizontal legends\nlegend(\"top\", legend = c(\"A\", \"B\", \"C\"), \n       fill = 1:3, horiz = TRUE, x.intersp = 0.2, inset = 0.02,\n       title = \"Top Legend\")\nlegend(\"bottom\", legend = c(\"D\", \"E\"), \n       fill = 4:5, horiz = TRUE, x.intersp = 0.2, inset = 0.02,\n       title = \"Bottom Legend\")\n\n\n\n\nIn this example, we add two horizontal legends at the top and bottom of the plot, each with its set of labels and colors.\n\n\nExperiment\nCreating horizontal legends in base R is a versatile skill that you can use in various data visualization projects. I encourage you to experiment with different plot types, colors, and parameters to create the perfect horizontal legend for your specific needs. Don’t be afraid to get creative and tailor your legends to make your plots more informative and visually appealing.\nBy following these simple steps and experimenting with your own plots, you’ll be able to master the art of horizontal legends in R. So go ahead and give it a try! Your future visualizations will thank you for the extra clarity and elegance that horizontal legends provide. Happy coding!"
  },
  {
    "objectID": "posts/2023-10-12/index.html",
    "href": "posts/2023-10-12/index.html",
    "title": "How to Use cex to Change the Size of Plot Elements in base R",
    "section": "",
    "text": "Introduction\nLet’s dive into the world of R and explore how to use cex to change the size of plot elements in base R. Whether you’re a seasoned R user or just starting out, understanding how to control the size of text and symbols in your plots can greatly enhance the clarity and aesthetics of your data visualizations. In this blog post, we’ll break it down into simple terms and provide several examples to get you started.\n\n\nWhat is cex?\ncex stands for “character expansion.” It’s a parameter in R that allows you to adjust the size of text, symbols, and other graphical elements within your plots. You can think of it as a scaling factor that determines the size of these elements relative to the default size.\n\n\nThe Basics\nLet’s start with the basics. The cex parameter is typically used within functions that create plots, like plot() or text(). It takes a numeric value, where 1.0 represents the default size, and values greater than 1 make elements larger, while values between 0 and 1 make elements smaller.\n\n\nExamples\nHere’s a simple example of changing the size of text in a scatter plot:\n\n# Create a scatter plot\nplot(1:5, 1:5, main=\"Default cex Size\")\n\n\n\n# Change the text size with cex\nplot(1:5, 1:5, main=\"Larger cex\", cex=1.5)\n\n\n\n\nIn the second plot() call, we set cex to 1.5, making the text 1.5 times larger than the default size. Play around with different cex values to see the effect on your plots.\n\n\nText and Labels\ncex is particularly handy when you want to adjust the size of text labels in your plots. For example, when creating a bar plot, you might want to make the bar labels more legible:\n\n# Create a bar plot\nbarplot(1:5, names.arg=c(\"A\", \"B\", \"C\", \"D\", \"E\"), main=\"Default Label Size\")\n\n\n\n# Change the label size with cex\nbarplot(1:5, names.arg=c(\"A\", \"B\", \"C\", \"D\", \"E\"), main=\"Larger Labels\", cex.names=1.5)\n\n\n\n\nIn the second barplot() call, we use cex.names to specifically adjust the size of the labels. This keeps the rest of the plot elements at their default sizes.\n\n\nExperiment!\nThe best way to master the use of cex is to experiment. Try different values, and see how they impact your plots. Whether you’re adjusting text size, label size, or symbol size, cex offers a flexible way to customize your visualizations.\nDon’t hesitate to explore more advanced uses of cex when working on complex plots. With practice, you’ll develop an intuitive sense of how to use this parameter effectively.\nSo, go ahead and give it a try! Experiment with cex in your R plots and discover how it can help you create more engaging and informative data visualizations.\nIn the world of data analysis and visualization, understanding these nuances can be a game-changer, and cex is a valuable tool in your R arsenal. Happy coding!"
  },
  {
    "objectID": "posts/2023-10-13/index.html",
    "href": "posts/2023-10-13/index.html",
    "title": "Mastering the Art of Drawing Circles in Plots with R",
    "section": "",
    "text": "Introduction\nAs an R programmer, you may want to draw circles in plots to highlight certain data points or to create visualizations. Here are some simple steps to draw circles in plots using R:\n\n\nExamples\n\nFirst, create a scatter plot using the plot() function in R. For example, you can create a scatter plot of x and y values using the following code:\n\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 6, 8, 10)\nplot(x, y)\n\n\n\n\n\nTo draw a circle on the plot, you can use the symbols() function in R. The symbols() function allows you to draw various shapes, including circles, squares, triangles, and more. To draw a circle, set the circles argument to TRUE. For example, to draw a circle with a radius of 0.5 at the point (3, 6), use the following code:\n\n\nplot(x, y)\nsymbols(3, 6, circles = 1, add = TRUE)\n\n\n\n\n\nYou can also customize the color and border of the circle using the bg and fg arguments. For example, to draw a red circle with a blue border, use the following code:\n\n\nplot(x, y)\nsymbols(3, 6, circles = 1, add = TRUE, bg = \"red\", fg = \"blue\")\n\n\n\n\n\nTo draw multiple circles on the plot, you can use a loop to iterate over a list of coordinates and radii. For example, to draw three circles with different radii at different points, use the following code:\n\n\nplot(x, y)\n\ncoords &lt;- list(c(2, 4), c(3, 6), c(4, 8))\nradii &lt;- c(0.1, 0.2, 0.3)\n\nfor (i in 1:length(coords)) {\n  symbols(\n    coords[[i]][1], coords[[i]][2], circles = radii[[i]], \n    add = TRUE, bg = \"red\", fg = \"blue\", inches = FALSE\n  )\n}\n\n\n\n\n\nFinally, you can add a title and axis labels to the plot using the title(), xlab(), and ylab() functions. For example, to add a title and axis labels to the plot, use the following code:\n\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 6, 8, 10)\nplot(\n  x, y, main = \"Scatter Plot with Circles\", \n  xlab = \"X Values\", ylab = \"Y Values\"\n)\n\ncoords &lt;- list(c(2, 4), c(3, 6), c(4, 8))\nradii &lt;- c(0.1, 0.2, 0.3)\n\nfor (i in 1:length(coords)) {\n  symbols(\n    coords[[i]][1], coords[[i]][2], circles = radii[[i]], \n    add = TRUE, bg = \"red\", fg = \"blue\", inches = FALSE\n  )\n}\n\n\n\n\nHere is one last exmple:\n\n# Create a scatter plot with multiple circles\nn &lt;- 10\nx &lt;- runif(n, -2, 2)\ny &lt;- runif(n, -2, 2)\nsize &lt;- runif(n, 0.1, 1)\nfill &lt;- sample(colors(), n)\nborder &lt;- sample(colors(), n)\n\nsymbols(x, y, circles = size, inches = FALSE, add = F, bg = fill, fg = border)\n\n\n\n\n\n\nConclusion\nOverall, drawing circles in plots is a simple and effective way to highlight certain data points or to create visualizations. Try experimenting with different coordinates, radii, colors, and borders to create your own custom plots."
  },
  {
    "objectID": "posts/2023-10-16/index.html",
    "href": "posts/2023-10-16/index.html",
    "title": "Analyzing Time Series Growth with ts_growth_rate_vec() in healthyR.ts",
    "section": "",
    "text": "Introduction\nTime series data is essential for understanding trends and making forecasts in various fields, from finance to healthcare. Analyzing the growth rate of time series data is a crucial step in uncovering valuable insights. In the world of R programming, the healthyR.ts library introduces a powerful tool to calculate growth rates and log-differenced growth rates with the ts_growth_rate_vec() function. In this blog post, we’ll explore how this function works and how it can be used for effective time series analysis.\n\n\nUnderstanding ts_growth_rate_vec():\nThe ts_growth_rate_vec() function is part of the healthyR.ts library, designed to work with numeric vectors or time series data. It calculates the growth rate or log-differenced growth rate of the provided data, offering valuable insights into the underlying trends and patterns.\n\n\nSyntax\nHere is the function syntax:\nts_growth_rate_vec(\n  .x, \n  .scale = 100, \n  .power = 1, \n  .log_diff = FALSE, \n  .lags = 1\n)\n\n.x - A numeric vector\n.scale - A numeric value that is used to scale the output\n.power - A numeric value that is used to raise the output to a power\n.log_diff - A logical value that determines whether the output is a log difference\n.lags - An integer that determines the number of lags to use\n\nYou can find the documentation here\n\n\nExamples\nLet’s first take a look at the data we are going to be working with in this post, AirPassengers.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nplot(AirPassengers)\n\n\n\n\nLet’s load in the {healthyR.ts} library and see some examples to illustrate its functionality:\n\nlibrary(healthyR.ts)\n\n\nCalculating Basic Growth Rate:\n\n\nts_growth_rate_vec(AirPassengers) |&gt; head(12)\n\n [1]         NA   5.357143  11.864407  -2.272727  -6.201550  11.570248\n [7]   9.629630   0.000000  -8.108108 -12.500000 -12.605042  13.461538\n\nplot(ts(ts_growth_rate_vec(AirPassengers)))\n\n\n\n\nThe output provides growth rates for the AirPassengers dataset. This basic calculation can help you understand how the data is evolving over time. The growth rates are calculated from one point to the next, giving you an idea of the speed at which the values are changing.\n\nApplying Scaling and Power Transformation:\n\n\nts_growth_rate_vec(AirPassengers, .log_diff = TRUE) |&gt; head(12)\n\n [1]         NA   5.218575  11.211730  -2.298952  -6.402186  10.948423\n [7]   9.193750   0.000000  -8.455739 -13.353139 -13.473259  12.629373\n\nplot(ts(ts_growth_rate_vec(AirPassengers, .log_diff = TRUE)))\n\n\n\n\nThis example introduces the option to apply scaling and a power transformation. The resulting growth rates can help uncover trends that might not be apparent in the original data. Using a log-differenced growth rate is particularly useful for capturing the percentage change, making it easier to interpret the data.\n\nHandling Lagged Data:\n\n\nts_growth_rate_vec(AirPassengers, .lags = -1) |&gt; head(12)\n\n [1]  -5.084746 -10.606061   2.325581   6.611570 -10.370370  -8.783784\n [7]   0.000000   8.823529  14.285714  14.423077 -11.864407   2.608696\n\nplot.ts(ts_growth_rate_vec(AirPassengers, .lags = -1))\n\n\n\n\nIn this case, the function calculates the log differences of the time series with lags. This is helpful when you want to observe the changes between data points at different time intervals. It can reveal patterns that might not be apparent in the basic growth rate calculation.\n\nCombining Scaling, Transformation, and Lags:\n\n\nts_growth_rate_vec(AirPassengers, .log_diff = TRUE, .lags = -1) |&gt; head(12)\n\n [1]  -5.218575 -11.211730   2.298952   6.402186 -10.948423  -9.193750\n [7]   0.000000   8.455739  13.353139  13.473259 -12.629373   2.575250\n\nplot.ts(ts_growth_rate_vec(AirPassengers, .log_diff = TRUE, .lags = -1))\n\n\n\n\nThis example combines all the mentioned features to provide a comprehensive analysis of the data. It’s a powerful way to understand how the growth rate is affected by various factors, such as scaling and time lags.\n\n\nConclusion:\nThe ts_growth_rate_vec() function in the healthyR.ts library is a versatile tool for time series analysis. Whether you need a basic growth rate, want to apply scaling and transformation, or work with lagged data, this function has you covered. It’s a valuable asset for R programmers, helping them uncover hidden insights within time series data.\nIncorporating this function into your data analysis workflow can provide you with a deeper understanding of how values change over time. Whether you’re working with financial data, healthcare data, or any other time series dataset, ts_growth_rate_vec() is a powerful addition to your R programming toolkit. Start exploring your time series data today and discover the trends and patterns that lie within."
  },
  {
    "objectID": "posts/2023-10-17/index.html",
    "href": "posts/2023-10-17/index.html",
    "title": "Testing stationarity with the ts_adf_test() function in R",
    "section": "",
    "text": "Introduction\nHey there, R enthusiasts! Today, we’re going to dive into the fascinating world of time series analysis using the ts_adf_test() function from the healthyR.ts R library. If you’re into data, statistics, and R coding, this is a must-know tool for your arsenal.\n\n\nWhat’s the Deal with Augmented Dickey-Fuller?\nBefore we delve into the ts_adf_test() function, let’s understand the concept behind it. The Augmented Dickey-Fuller (ADF) test is a crucial tool in time series analysis. It’s like the Sherlock Holmes of time series data, helping us detect whether a series is stationary or not. Stationarity is a fundamental assumption in time series modeling because many models work best when applied to stationary data.\nSo, why “Augmented”? Well, it’s an extension of the original Dickey-Fuller test that accounts for more complex relationships within the time series data.\n\n\nThe ts_adf_test() Function\nNow, let’s get to the star of the show, the ts_adf_test() function. This function is part of the healthyR.ts library, and its primary job is to perform the ADF test on a given time series. In R, a time series can be represented as a numeric vector. Here’s the basic syntax:\nts_adf_test(.x, .k = NULL)\n\n.x is your time series data, the numeric vector you want to analyze.\n.k is an optional parameter that allows you to specify the lag order. If you leave it empty (like .k = NULL), don’t worry; the function will calculate it for you based on the number of observations using a clever formula.\n\n\n\nShow Me the Stats!\nSo, what does ts_adf_test() return? It gives you a list object containing two vital pieces of information:\n\nTest Statistic: This is the heart of the ADF test. It tells us how strongly our data deviates from being stationary. A more negative value indicates stronger evidence for stationarity.\nP-Value: This is another critical number. It represents the probability that you’d observe a test statistic as extreme as the one you obtained if the data were not stationary. In simpler terms, a low p-value suggests that your data is likely stationary, while a high p-value implies non-stationarity.\n\n\n\nLet’s Get Practical\nEnough theory! Let’s see some action with a couple of examples. Say we have the AirPassengers and BJsales datasets, and we want to check their stationarity:\n\nlibrary(healthyR.ts)\n\n# ADF test for AirPassengers\nresult_air &lt;- ts_adf_test(AirPassengers)\ncat(\"AirPassengers ADF Test Result:\\n\")\n\nAirPassengers ADF Test Result:\n\nprint(result_air)\n\n$test_stat\n[1] -7.318571\n\n$p_value\n[1] 0.01\n\n# ADF test for BJsales\nresult_bj &lt;- ts_adf_test(BJsales)\ncat(\"\\nBJsales ADF Test Result:\\n\")\n\n\nBJsales ADF Test Result:\n\nprint(result_bj)\n\n$test_stat\n[1] -2.110919\n\n$p_value\n[1] 0.5301832\n\n\nIn the AirPassengers example, we get a test statistic of -7.318571 and a p-value of 0.01. This suggests strong evidence for stationarity in this dataset.\nHowever, for BJsales, we get a test statistic of -2.110919 and a p-value of 0.5301832. The higher p-value here indicates that the data is less likely to be stationary.\nNow let’s see what happens when we change the lags of the series by one period.\n\nts_adf_test(AirPassengers, 1)\n\n$test_stat\n[1] -7.652287\n\n$p_value\n[1] 0.01\n\nts_adf_test(BJsales, 1)\n\n$test_stat\n[1] -1.316414\n\n$p_value\n[1] 0.8611925\n\n\n\n\nConclusion\nThe ts_adf_test() function in the healthyR.ts library is a valuable tool for any data scientist or R coder working with time series data. It helps you determine whether your data is stationary, a crucial step in building reliable time series models.\nSo, the next time you’re faced with a time series dataset, remember to call on your trusty companion, ts_adf_test(), to solve the mystery of stationarity. Happy coding, R enthusiasts!"
  },
  {
    "objectID": "posts/2023-10-18/index.html",
    "href": "posts/2023-10-18/index.html",
    "title": "Making Time Series Stationary Made Easy with auto_stationarize()",
    "section": "",
    "text": "Introduction\nWhen working with time series data, one common challenge is dealing with non-stationary data. Non-stationary time series can be a headache for analysts, but fear not, because we have a handy tool to make your life easier. Say hello to the auto_stationarize() function from the {healthyR.ts} package.\n\n\nWhat’s in the Box?\nBefore we get into the nitty-gritty of how this function works, let’s take a look at its syntax:\nauto_stationarize(.time_series)\nThe .time_series parameter should be a vector or a time series object. This function’s primary goal is to attempt to stationarize your time series data. But what does that mean, and why is it important?\n\n\nStationarity: The Why and the How\nStationarity is a crucial concept in time series analysis. A stationary time series is one whose statistical properties, like mean, variance, and autocorrelation, don’t change over time. Dealing with stationary data is much simpler because many time series models assume stationarity.\nNow, here’s the magic of auto_stationarize(): it automatically handles stationarity for you.\n\n\nThe Swiss Army Knife of Time Series\nThis function is like a Swiss Army knife for your time series data. It checks if your data is already stationary using the Augmented Dickey-Fuller test. If it is, great, you get your original time series back.\nBut what if it’s not? Well, that’s where the real fun begins.\n\n\nTransformations Galore\nIf your time series isn’t stationary, auto_stationarize() goes the extra mile to make it so. It attempts a series of transformations until it succeeds. Here’s the process:\n\nAugmented Dickey-Fuller Test: First, the function runs the Augmented Dickey-Fuller test to determine if your time series is stationary. If it’s already stationary, you’re done.\nLogarithmic Transformation: If the test suggests your data isn’t stationary, the function tries a logarithmic transformation. This transformation can be helpful when dealing with data that grows exponentially over time.\nDifferencing: If logarithmic transformation doesn’t do the trick, the function resorts to differencing. Differencing involves subtracting each value from its previous value, effectively converting your data into the change between time periods.\n\n\n\nWhat You Get\nIf auto_stationarize() succeeds in making your data stationary, it returns a list with two valuable elements:\n\nstationary_ts: This is your shiny, new stationary time series, ready for analysis.\nndiffs: This little number tells you the order of differencing applied to make your data stationary. It’s a useful piece of information if you need to understand how your data was transformed.\n\n\n\nExamples\nLet’s see some examples.\n\nlibrary(healthyR.ts)\n\nauto_stationarize(AirPassengers)\n\nThe time series is already stationary via ts_adf_test().\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nauto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\nDifferencing of order 1 made the time series stationary.\n\n\n$stationary_ts\nTime Series:\nStart = 2 \nEnd = 150 \nFrequency = 1 \n  [1] -0.6 -0.1 -0.5  0.1  1.2 -1.6  1.4  0.3  0.9  0.4 -0.1  0.0  2.0  1.4  2.2\n [16]  3.4  0.0 -0.7 -1.0  0.7  3.7  0.5  1.4  3.6  1.1  0.7  3.3 -1.0  1.0 -2.1\n [31]  0.6 -1.5 -1.4  0.7  0.5 -1.7 -1.1 -0.1 -2.7  0.3  0.6  0.8  0.0  1.0  1.0\n [46]  4.2  2.0 -2.7 -1.5 -0.7 -1.3 -1.7 -1.1 -0.1 -1.7 -1.8  1.6  0.7 -1.0 -1.5\n [61] -0.7  1.7 -0.2  0.4 -1.8  0.8  0.7 -2.0 -0.3 -0.6  1.3 -1.4 -0.3 -0.9  0.0\n [76]  0.0  1.8  1.3  0.9 -0.3  2.3  0.5  2.2  1.3  1.9  1.5  4.5  1.7  4.8  2.5\n [91]  1.4  3.5  3.2  1.5  0.7  0.3  1.4 -0.1  0.2  1.6 -0.4  0.9  0.6  1.0 -2.5\n[106] -1.4  1.2  1.6  0.3  2.3  0.7  1.3  1.2 -0.2  1.4  3.0 -0.4  1.3 -0.9  1.2\n[121] -0.8 -1.0 -0.8 -0.1 -1.5  0.3  0.2 -0.5 -0.1  0.3  1.3 -1.1 -0.1 -0.5  0.3\n[136] -0.7  0.7 -0.5  0.6 -0.3  0.2  2.1  1.5  1.8  0.4 -0.5 -1.0  0.4  0.5\n\n$ndiffs\n[1] 1\n\n\nThe function attempted to stationarize the BJsales data set, let’s take a visuali look at it before and after, we will also use the adf_test() function on it before and after.\n\nplot(BJsales)\n\n\n\nts_adf_test(BJsales)\n\n$test_stat\n[1] -2.110919\n\n$p_value\n[1] 0.5301832\n\nstationary_time_series &lt;- auto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\nDifferencing of order 1 made the time series stationary.\n\nplot(stationary_time_series$stationary_ts)\n\n\n\n\n\n\nTry It Yourself\nThe best way to grasp the power of auto_stationarize() is by trying it yourself. Install the {healthyR.ts} package, load your time series data, and give it a whirl. The ease and simplicity of making your time series stationary with just one function call will leave you impressed.\n\n\nConclusion\nIn the world of time series analysis, making your data stationary is a crucial step. The auto_stationarize() function from the {healthyR.ts} package takes the headache out of this process. Whether you’re dealing with financial data, weather patterns, or any other time series, this function is your trusty companion.\nSo, what are you waiting for? Transform your non-stationary time series into a stationary one with ease, thanks to auto_stationarize(). Your future self will thank you for it.\nHappy coding and data analysis!"
  },
  {
    "objectID": "posts/2023-10-19/index.html",
    "href": "posts/2023-10-19/index.html",
    "title": "Mastering Interaction Plots in R: Unveiling Hidden Relationships",
    "section": "",
    "text": "Introduction\nIn the world of data analysis, uncovering hidden relationships between variables is often the key to making informed decisions. Interaction plots in R can be your secret weapon, revealing how two or more variables interact to affect an outcome. In this blog post, we’ll dive into the world of interaction plots, demystifying the process and showing you how to create these insightful visuals using base R.\n\n\nWhat Are Interaction Plots?\nInteraction plots display how the relationship between two variables changes depending on the value of a third variable. They are particularly useful when dealing with categorical variables, allowing you to see how the effect of one variable on the outcome depends on the levels of another variable. In simpler terms, interaction plots help us understand how the relationship between two variables is influenced by a third variable, making them a valuable tool for data exploration.\n\n\nGetting Started: Preparing Your Data\nBefore we create interaction plots, we need some data. For this example, we’ll use a hypothetical dataset about customer satisfaction, where we want to explore how the relationship between “Product Type” and “Price” is influenced by “Customer Segment.”\n\nset.seed(123)\n# Create a sample dataset\ndata &lt;- data.frame(\n  ProductType = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 10),\n  Price = trunc(runif(40, 15, 35)),\n  CustomerSegment = rep(c(\"Seg. 1\", \"Seg. 2\"), times = 20),\n  Satisfaction = trunc(runif(40, 2, 5))\n)\n\nNow that we have our data, let’s create an interaction plot.\n\n\nCreating the Interaction Plot\nWe’ll use the base R package to create our interaction plot. Here’s how you can do it:\n\n# Create the interaction plot\ninteraction.plot(\n  x.factor = data$ProductType,\n  trace.factor = data$CustomerSegment,\n  response = data$Satisfaction,\n  fun = median,\n  ylab = \"Satisfaction\",\n  xlab = \"Customer Segment\",\n  lty = 1,\n  lwd = 2, \n  col = c(\"steelblue\",\"lightgreen\"),\n  fixed = TRUE,\n  legend = TRUE,\n  trace.label = \"Segment\"\n)\n\n# Adding labels and a title\ntitle(\"Interaction Plot: Product Type vs. Satisfaction by Customer Segment\")\n\n\n\n\nIn the code above: - x.factor represents the variable on the x-axis. - trace.factor represents the variable that distinguishes different lines on the plot. - response is the variable we’re interested in. - type = \"b\" specifies that we want to connect points with lines and plot points. - fixed = TRUE ensures that the x-axis is evenly spaced. - legend = TRUE adds a legend to the plot.\n\n\nInterpreting the Plot\nIn our plot, you’ll see lines for each customer segment (Segment 1 and Segment 2). The lines show how satisfaction levels change with different product types (A, B, C and D). If the lines are parallel, it indicates that there’s no interaction between “Product Type” and “Customer Segment.” However, if the lines cross or diverge, it suggests an interaction, meaning that the effect of the product type on satisfaction differs across customer segments.\n\n\nConclusion: Your Turn to Explore!\nCreating interaction plots in R can be a valuable skill for anyone working with data. They provide deep insights into how variables influence each other. Don’t hesitate to apply this technique to your own datasets and discover the hidden relationships within your data.\nSo, what are you waiting for? Give it a try and start visualizing the interactions in your data. Happy coding!"
  },
  {
    "objectID": "posts/2023-10-20/index.html",
    "href": "posts/2023-10-20/index.html",
    "title": "Creating Pareto Charts in R with the qcc Package",
    "section": "",
    "text": "A Pareto chart is a type of bar chart that shows the frequency of different categories in a dataset, ordered by frequency from highest to lowest. It is often used to identify the most common problems or causes of a problem, so that resources can be focused on addressing them.\nTo create a Pareto chart in R, we can use the qcc package. The qcc package provides a number of functions for quality control, including the pareto.chart() function for creating Pareto charts."
  },
  {
    "objectID": "posts/2023-10-20/index.html#example-1-creating-a-pareto-chart-from-a-data-frame",
    "href": "posts/2023-10-20/index.html#example-1-creating-a-pareto-chart-from-a-data-frame",
    "title": "Creating Pareto Charts in R with the qcc Package",
    "section": "Example 1: Creating a Pareto chart from a data frame",
    "text": "Example 1: Creating a Pareto chart from a data frame\nThe following code shows how to create a Pareto chart from a data frame:\n\nlibrary(qcc)\n\n# Create a data frame with the product and its count\ndf &lt;- data.frame(\n  product = c(\"Office desks\", \"Chairs\", \"Filing cabinets\", \"Bookcases\"),\n  count = c(100, 80, 70, 60)\n)\n\n# Create the Pareto chart\npareto.chart(df$count, main = \"Pareto Chart of Product Sales\")\n\n\n\n\n   \nPareto chart analysis for df$count\n    Frequency Cum.Freq. Percentage Cum.Percent.\n  A 100.00000 100.00000   32.25806     32.25806\n  B  80.00000 180.00000   25.80645     58.06452\n  C  70.00000 250.00000   22.58065     80.64516\n  D  60.00000 310.00000   19.35484    100.00000\n\n\nThis code will create a Pareto chart of the product sales, with the office desks bar at the top and the bookcases bar at the bottom. The cumulative percentage line is also plotted, which shows the percentage of total sales that each product accounts for."
  },
  {
    "objectID": "posts/2023-10-20/index.html#example-2-creating-a-pareto-chart-from-a-vector",
    "href": "posts/2023-10-20/index.html#example-2-creating-a-pareto-chart-from-a-vector",
    "title": "Creating Pareto Charts in R with the qcc Package",
    "section": "Example 2: Creating a Pareto chart from a vector",
    "text": "Example 2: Creating a Pareto chart from a vector\nWe can also create a Pareto chart from a vector. The following code shows how to create a Pareto chart of the number of defects found in a manufacturing process:\n\n# Create a vector with the number of defects found in each category\ndefects &lt;- c(10, 8, 7, 6, 5)\n\n# Create the Pareto chart\npareto.chart(defects, main = \"Pareto Chart of Defects\")\n\n\n\n\n   \nPareto chart analysis for defects\n    Frequency Cum.Freq. Percentage Cum.Percent.\n  A  10.00000  10.00000   27.77778     27.77778\n  B   8.00000  18.00000   22.22222     50.00000\n  C   7.00000  25.00000   19.44444     69.44444\n  D   6.00000  31.00000   16.66667     86.11111\n  E   5.00000  36.00000   13.88889    100.00000\n\n\nThis code will create a Pareto chart of the number of defects found, with the most common defect category at the top and the least common defect category at the bottom. The cumulative percentage line is also plotted, which shows the percentage of total defects that each category accounts for."
  },
  {
    "objectID": "posts/2023-10-20/index.html#customizing-the-pareto-chart",
    "href": "posts/2023-10-20/index.html#customizing-the-pareto-chart",
    "title": "Creating Pareto Charts in R with the qcc Package",
    "section": "Customizing the Pareto chart",
    "text": "Customizing the Pareto chart\nWe can customize the appearance of the Pareto chart using a number of arguments to the pareto.chart() function. For example, we can change the title of the chart, the labels of the x- and y-axes, the colors of the bars, and the line type of the cumulative percentage line.\nThe following code shows how to customize the Pareto chart from the first example:\n\n# Create a data frame with the product and its count\ndf &lt;- data.frame(\n  product = c(\"Office desks\", \"Chairs\", \"Filing cabinets\", \"Bookcases\"),\n  count = c(100, 80, 70, 60)\n)\n\n# Create the Pareto chart\npareto.chart(\n  df$count,\n  main = \"Pareto Chart of Product Sales\",\n  xlab = \"Product\",\n  ylab = \"Count\",\n  col = heat.colors(length(df$count)),\n  lwd = 2\n)\n\n\n\n\n   \nPareto chart analysis for df$count\n    Frequency Cum.Freq. Percentage Cum.Percent.\n  A 100.00000 100.00000   32.25806     32.25806\n  B  80.00000 180.00000   25.80645     58.06452\n  C  70.00000 250.00000   22.58065     80.64516\n  D  60.00000 310.00000   19.35484    100.00000\n\n\nThis code will create a Pareto chart with a title of “Pareto Chart of Product Sales”, x-axis label of “Product”, y-axis label of “Count”, bar colors in a heatmap palette, and a cumulative percentage line width of 2."
  },
  {
    "objectID": "posts/2023-10-23/index.html",
    "href": "posts/2023-10-23/index.html",
    "title": "How to Create a Bubble Chart in R using ggplot2",
    "section": "",
    "text": "Bubble charts are a great way to visualize data with three dimensions. The size of the bubbles represents a third variable, which can be used to show the importance of that variable or to identify relationships between the three variables.\nTo create a bubble chart in R using ggplot2, you will need to use the geom_point() function. This function will plot points on your chart, and you can use the size aesthetic to control the size of the points."
  },
  {
    "objectID": "posts/2023-10-23/index.html#example-1-basic-bubble-chart",
    "href": "posts/2023-10-23/index.html#example-1-basic-bubble-chart",
    "title": "How to Create a Bubble Chart in R using ggplot2",
    "section": "Example 1: Basic Bubble Chart",
    "text": "Example 1: Basic Bubble Chart\nLet’s start with a simple example using randomly generated data. We’ll create a bubble chart that shows the relationship between two variables and represents a third variable using bubble sizes.\n\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Generate random data\nset.seed(123)\ndata &lt;- data.frame(\n  x = rnorm(10),\n  y = rnorm(10),\n  size = runif(10, min = 5, max = 20)\n)\n\n# Create a bubble chart\nggplot(data, aes(x, y, size = size)) +\n  geom_point() +\n  scale_size_continuous(range = c(3, 10)) +\n  labs(\n    title = \"Basic Bubble Chart\", \n    x = \"X-Axis\", \n    y = \"Y-Axis\",\n    size = \"Y\") +\n  theme_minimal()\n\n\n\n\nIn this example, we create a bubble chart with random data points, where x and y are the coordinates, and size represents the bubble size. The geom_point() function is used to add the points, and we adjust the size range using scale_size_continuous()."
  },
  {
    "objectID": "posts/2023-10-23/index.html#example-2-customizing-bubble-chart",
    "href": "posts/2023-10-23/index.html#example-2-customizing-bubble-chart",
    "title": "How to Create a Bubble Chart in R using ggplot2",
    "section": "Example 2: Customizing Bubble Chart",
    "text": "Example 2: Customizing Bubble Chart\nNow, let’s customize our bubble chart further. We’ll use a sample dataset to visualize car data, with car names on the bubbles.\n\n# Sample data\ncars &lt;- mtcars\ncars$name &lt;- rownames(cars)\n\n# Create a bubble chart\nggplot(cars, aes(x = mpg, y = disp, size = hp, label = name)) +\n  geom_point() +\n  geom_text(vjust = 1, hjust = 1, size = 3) +\n  scale_size_continuous(range = c(3, 20)) +\n  labs(\n    title = \"Customized Bubble Chart\", \n    x = \"Miles per Gallon\", \n    y = \"Displacement\",\n    size = \"HP\") +\n  theme_minimal()\n\n\n\n\nIn this example, we’re using the mtcars dataset to create a bubble chart that displays car names using geom_text(). The vjust and hjust parameters control the text placement."
  },
  {
    "objectID": "posts/2023-10-24/index.html",
    "href": "posts/2023-10-24/index.html",
    "title": "Creating a Scree Plot in Base R",
    "section": "",
    "text": "Introduction\nA scree plot is a line plot that shows the eigenvalues or variance explained by each principal component (PC) in a Principal Component Analysis (PCA). It is a useful tool for determining the number of PCs to retain in a PCA model.\nIn this blog post, we will show you how to create a scree plot in base R. We will use the iris dataset as an example.\n\n\nStep 1: Load the dataset and prepare the data\n\n# Drop the non-numerical column\ndf &lt;- iris[, -5]\n\nE Step 2: Perform Principal Component Analysis\n\n# Perform PCA on the iris dataset\npca &lt;- prcomp(df, scale = TRUE)\n\nE Step 3: Create the scree plot\n\n# Extract the eigenvalues from the PCA object\neigenvalues &lt;- pca$sdev^2\n\n# Create a scree plot\nplot(eigenvalues, type = \"b\",\n     xlab = \"Principal Component\",\n     ylab = \"Eigenvalue\")\n\n# Add a line at y = 1 to indicate the elbow\nabline(v = 2, col = \"red\")\n\n\n\n# Percentage of variance explained\nplot(eigenvalues/sum(eigenvalues), type = \"b\",\n     xlab = \"Principal Component\",\n     ylab = \"Percentage of Variance Explained\")\nabline(v = 2, col = \"red\")\n\n\n\n\n\n\nInterpretation\nThe scree plot shows that the first two principal components explain the most variance in the data. The third and fourth principal components explain much less variance.\nBased on the scree plot, we can conclude that the first two principal components are sufficient for capturing the most important information in the data.\nHere are the eigenvalues and the percentage explained\n\neigenvalues\n\n[1] 2.91849782 0.91403047 0.14675688 0.02071484\n\neigenvalues/sum(eigenvalues)\n\n[1] 0.729624454 0.228507618 0.036689219 0.005178709\n\n\n\n\nTry it yourself\nTry creating a scree plot for another dataset of your choice. You can use the same steps outlined above.\nHere are some additional tips for creating scree plots:\n\nIf you are using a dataset with a large number of variables, you may want to consider scaling the data before performing PCA. This will ensure that all of the variables are on the same scale and that no one variable has undue influence on the results.\nYou can also add a line to the scree plot at y = 1 to indicate the elbow. The elbow is the point where the scree plot begins to level off. This is often used as a heuristic for determining the number of PCs to retain.\nFinally, keep in mind that the interpretation of a scree plot is subjective. There is no single rule for determining the number of PCs to retain. The best approach is to consider the scree plot in conjunction with other factors, such as your research goals and the specific dataset you are using."
  },
  {
    "objectID": "posts/2023-10-25/index.html",
    "href": "posts/2023-10-25/index.html",
    "title": "What’s a Bland-Altman Plot? In Base R",
    "section": "",
    "text": "Introduction\nBefore we dive into the code, let’s briefly understand what a Bland-Altman plot is. It’s a graphical method to visualize the agreement between two measurement techniques, often used in fields like medicine or any domain with comparative measurements. The plot displays the differences between two measurements (Y-axis) against their means (X-axis).\n\n\nStep 1: Data Preparation\nStart by loading your data into R. In our example, we’ll create some synthetic data for illustration purposes. You’d replace this with your real data.\n\n# Creating example data\nmethod_A &lt;- c(10, 12, 15, 18, 22, 25)\nmethod_B &lt;- c(9.5, 11, 14, 18, 22, 24.5)\n\n# Calculate the differences and means\ndiff_values &lt;- method_A - method_B\nmean_values &lt;- (method_A + method_B) / 2\n\ndf &lt;- data.frame(method_A, method_B, mean_values, diff_values)\n\n\n\nStep 2: Calculate Average Difference and CI\nNow that we have our data prepared, let’s create the Bland-Altman plot.\n\nmean_diff &lt;- mean(df$diff_values)\nmean_diff\n\n[1] 0.5\n\nlower &lt;- mean_diff - 1.96 * sd(df$diff_values)\nupper &lt;- mean_diff + 1.96 * sd(df$diff_values)\n\nlower\n\n[1] -0.3765386\n\nupper\n\n[1] 1.376539\n\n\n\n\nStep 3: Creating the Bland-Altman Plot\nWe are going to do this in base R.\n\n# Create a scatter plot\nplot(df$mean_values, df$diff_values, \n     xlab = \"Mean of Methods A and B\",\n     ylab = \"Difference (Method A - Method B)\",\n     main = \"Bland-Altman Plot\",\n     ylim = c(lower + (lower * .1), upper * 1.1))\n\n# Add a horizontal line at the mean difference\nabline(h = mean(diff_values), col = \"red\", lty = 2)\n\n# Add Confidence Intervals\nabline(h = upper, col = \"blue\", lty = 2)\nabline(h = lower, col = \"blue\", lty = 2)\n\n\n\n\nThis code will generate a simple Bland-Altman plot, and here’s what each part does:\n\nplot(): Creates the scatter plot with means on the X-axis and differences on the Y-axis.\nabline(h = mean(diff_values), col = \"red\", lty = 2): Adds a red dashed line at the mean difference.\nabline(h = upper, col = \"green\", lty = 2): Adds blue dashed lines representing the 95% limits of agreement.\n\n\n\nStep 4: Interpretation\nNow that you’ve generated your Bland-Altman plot, let’s interpret it:\n\nThe red line represents the mean difference between the two methods.\nThe blue dashed lines show the 95% limits of agreement, which help you assess the spread of the differences.\n\nIf most data points fall within the blue lines, it indicates good agreement between the two methods. If data points are scattered widely outside the lines, there may be a systematic bias or inconsistency between the methods.\n\n\nStep 5: Exploration\nI encourage you to try this out with your own data. Replace the example data with your measurements and see what insights your Bland-Altman plot reveals.\nIn conclusion, creating a Bland-Altman plot in R is a valuable technique to visualize agreement or bias between two measurement methods. It’s an essential tool for quality control and validation in various fields. I hope this step-by-step guide helps you get started. Happy plotting!"
  },
  {
    "objectID": "posts/2023-10-26/index.html",
    "href": "posts/2023-10-26/index.html",
    "title": "Plotting a Logistic Regression In Base R",
    "section": "",
    "text": "Introduction\nLogistic regression is a statistical method used for predicting the probability of a binary outcome. It’s a fundamental tool in machine learning and statistics, often employed in various fields such as healthcare, finance, and marketing. We use logistic regression when we want to understand the relationship between one or more independent variables and a binary outcome, which can be “yes/no,” “1/0,” or any two-class distinction.\n\n\nGetting Started\nBefore we dive into plotting the logistic regression curve, let’s start with the basics. First, you’ll need some data. For this blog post, I’ll assume you have your dataset ready. If you don’t, you can easily find sample datasets online to practice with.\n\n\nLoad the Data\nIn R, we use the read.csv function to load a CSV file into a data frame. For example, if you have a dataset called “mydata.csv,” you can load it like this:\n# Load the data into a data frame\ndata &lt;- read.csv(\"mydata.csv\")\nWe will instead use the following data set:\n\nlibrary(dplyr)\n\nset.seed(123)\ndf &lt;- tibble(\n    x = runif(100, 0, 10),\n    y = rbinom(100, 1, 1 / (1 + exp(-1 * (0.5 * x - 2.5))))\n)\n\nhead(df)\n\n# A tibble: 6 × 2\n      x     y\n  &lt;dbl&gt; &lt;int&gt;\n1 2.88      0\n2 7.88      1\n3 4.09      0\n4 8.83      0\n5 9.40      1\n6 0.456     0\n\n\n\n\nFit a Logistic Regression Model\nNext, we need to fit a logistic regression model to our data. We’ll use the glm (Generalized Linear Model) function to do this. Suppose we want to predict the probability of a “success” (1) based on a single predictor variable “x.”\n\n# Fit a logistic regression model\nmodel &lt;- glm(y ~ x, data = df, family = binomial)\n\nbroom::glance(model)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1          138.      99  -51.5  107.  112.     103.          98   100\n\nbroom::tidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)   -2.63      0.571     -4.60 0.00000422 \n2 x              0.505     0.102      4.96 0.000000699\n\nhead(broom::augment(model), 1) |&gt; \n  dplyr::glimpse()\n\nRows: 1\nColumns: 8\n$ y          &lt;int&gt; 0\n$ x          &lt;dbl&gt; 2.875775\n$ .fitted    &lt;dbl&gt; -1.175925\n$ .resid     &lt;dbl&gt; -0.7333581\n$ .hat       &lt;dbl&gt; 0.01969748\n$ .sigma     &lt;dbl&gt; 1.028093\n$ .cooksd    &lt;dbl&gt; 0.003162007\n$ .std.resid &lt;dbl&gt; -0.7406892\n\n\n\n\nPredict Probabilities\nNow that we have our model, we can use it to predict probabilities. We’ll create a sequence of values for our predictor variable, and for each value, we’ll predict the probability of success, in this case y.\n\n# Create a sequence of predictor values\nx_seq &lt;- seq(0, 10, 0.01)\n\n# Predict probabilities\nprobabilities &lt;- predict(\n  model, \n  newdata = data.frame(x = x_seq), \n  type = \"response\"\n  )\n\nhead(x_seq)\n\n[1] 0.00 0.01 0.02 0.03 0.04 0.05\n\nhead(probabilities)\n\n         1          2          3          4          5          6 \n0.06732923 0.06764710 0.06796636 0.06828702 0.06860908 0.06893255 \n\n\nThe predict function here calculates the probabilities using our logistic regression model.\n\n\nPlot the Logistic Regression Curve\nFinally, let’s plot the logistic regression curve. We’ll use the plot function to create a scatter plot of the data points, and then we’ll overlay the logistic curve using the lines function.\n\n# Plot the data points\nplot(\n  df$x, df$y, \n  pch = 16, \n  col = \"blue\", \n  xlab = \"Predictor Variable\", \n  ylab = \"Probability of Success\"\n  )\n\n# Add the logistic regression curve\nlines(x_seq, probabilities, col = \"red\", lwd = 2)\n\n\n\n\nAnd there you have it! You’ve successfully plotted a logistic regression curve in base R. The blue dots represent your data points, and the red curve is the logistic regression curve, showing how the probability of success changes with the predictor variable.\n\n\nConclusion\nI encourage you to try this out with your own dataset. Logistic regression is a powerful tool for modeling binary outcomes, and visualizing the curve helps you understand the relationship between your predictor variable and the probability of success. Experiment with different datasets and predictor variables to gain a deeper understanding of this essential statistical technique.\nRemember, practice makes perfect, and the more you work with logistic regression in R, the more proficient you’ll become. Happy coding!"
  },
  {
    "objectID": "posts/2023-10-27/index.html",
    "href": "posts/2023-10-27/index.html",
    "title": "Plotting Log Log Plots In Base R",
    "section": "",
    "text": "A log-log plot is a type of graph where both the x-axis and y-axis are in logarithmic scales. This is particularly useful when dealing with data that spans several orders of magnitude. By taking the logarithm of the data, we can compress large values and reveal patterns that might be hidden on a linear scale.\nLet’s start with a simple example using base R."
  },
  {
    "objectID": "posts/2023-10-27/index.html#example-1-scatter-plot-with-log-log-scales",
    "href": "posts/2023-10-27/index.html#example-1-scatter-plot-with-log-log-scales",
    "title": "Plotting Log Log Plots In Base R",
    "section": "Example 1: Scatter Plot with Log-Log Scales",
    "text": "Example 1: Scatter Plot with Log-Log Scales\n\n# Sample data\nx &lt;- c(1, 10, 100, 1000)\ny &lt;- c(0.1, 1, 10, 100)\n\n# Create a log-log plot\nplot(x, y, log = \"xy\", main = \"Log-Log Plot Example\", \n     xlab = \"X (log scale)\", ylab = \"Y (log scale)\")\n\n\n\n\nIn this code, we create a scatter plot with log scales for both the x and y-axes using the plot function. The log = \"xy\" argument specifies that both axes should be in logarithmic scale. This makes it easier to visualize the relationship between x and y."
  },
  {
    "objectID": "posts/2023-10-27/index.html#example-2-line-plot-with-log-log-scales",
    "href": "posts/2023-10-27/index.html#example-2-line-plot-with-log-log-scales",
    "title": "Plotting Log Log Plots In Base R",
    "section": "Example 2: Line Plot with Log-Log Scales",
    "text": "Example 2: Line Plot with Log-Log Scales\nLet’s say you have data for a power law relationship, where y is proportional to x raised to a power. A log-log plot can help you confirm this relationship.\n\n# Generate data for a power law relationship\nx &lt;- 1:10\ny &lt;- 2 * x^2\n\n# Create a log-log plot\nplot(x, y, log = \"xy\", type = \"b\", pch = 19, col = \"blue\", \n     main = \"Log-Log Plot for Power Law\", xlab = \"X (log scale)\", ylab = \"Y (log scale)\")\n\n\n\n\nHere, we generate data for a power law relationship (y = 2 * x^2) and create a log-log plot. The type = \"b\" argument adds both points and lines, making the plot easier to interpret. You can see that on a log-log scale, this power law relationship appears as a straight line."
  },
  {
    "objectID": "posts/2023-10-27/index.html#example-3-customizing-log-log-plots",
    "href": "posts/2023-10-27/index.html#example-3-customizing-log-log-plots",
    "title": "Plotting Log Log Plots In Base R",
    "section": "Example 3: Customizing Log-Log Plots",
    "text": "Example 3: Customizing Log-Log Plots\nYou can further customize your log-log plots with various options.\n\n# Customizing a log-log plot\nx &lt;- c(1, 10, 100, 1000)\ny &lt;- c(0.1, 1, 10, 100)\n\nplot(x, y, log = \"xy\", main = \"Custom Log-Log Plot\",\n     xlab = \"X (log scale)\", ylab = \"Y (log scale)\",\n     xlim = c(0.1, 1000), ylim = c(0.1, 100), col = \"red\", pch = 15)\n\n# Adding grid lines\ngrid()\n\n# Adding a trendline (linear regression)\nabline(lm(log10(y) ~ log10(x)), col = \"blue\")\n\n\n\n\nIn this example, we customize the log-log plot by setting axis limits, changing the point color and type, adding grid lines, and even fitting a trendline using linear regression."
  },
  {
    "objectID": "posts/2023-10-30/index.html",
    "href": "posts/2023-10-30/index.html",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "Randomness is an essential part of many statistical and machine learning tasks. In R, there are a number of functions that can be used to generate random numbers, but the runif() function is the most commonly used.\n\n\nThe runif() function generates random numbers from a uniform distribution. A uniform distribution is a distribution in which all values are equally likely. The runif() function takes three arguments:\n\nn: the number of random numbers to generate\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nThe default values for min and max are 0 and 1, respectively.\nHere is an example of how to use the runif() function to generate 10 random numbers from a uniform distribution between 0 and 1:\n\nset.seed(123)\nr &lt;- runif(10)\n\nOutput:\n\nprint(r)\n\n [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n [8] 0.8924190 0.5514350 0.4566147\n\n\nThe runif() function can also be used to generate random numbers from other distributions, such as the normal distribution, the Poisson distribution, and the binomial distribution.\n\n\n\nThe punif() function calculates the cumulative probability density function (CDF) of the uniform distribution. The CDF is the probability that a random variable will be less than or equal to a certain value.\nThe punif() function takes three arguments:\n\nx: the value at which to calculate the CDF\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the punif() function to calculate the CDF of a uniform distribution between 0 and 1 at the value 0.5:\n\nset.seed(123)\np &lt;- punif(0.5, min = 0, max = 1)\n\nOutput:\n\nprint(p)\n\n[1] 0.5\n\n\nThis means that there is a 50% chance that a random variable from this distribution will be less than or equal to 0.5.\n\n\n\nThe dunif() function calculates the probability density function (PDF) of the uniform distribution. The PDF is the probability that a random variable will be equal to a certain value.\nThe dunif() function takes three arguments:\n\nx: the value at which to calculate the PDF\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the dunif() function to calculate the PDF of a uniform distribution between 0 and 1 at the value 0.5:\n\nset.seed(123)\nd &lt;- dunif(0.5, min = 0, max = 1)\n\nOutput:\nprint(d)\nThis means that the probability of a random variable from this distribution being equal to 0.5 is 1.\n\n\n\nThe quinf() function calculates the quantile function of the uniform distribution. The quantile function is the inverse of the CDF. It takes a probability as an input and returns the value that has that probability.\nThe quinf() function takes two arguments:\n\np: the probability\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the quinf() function to calculate the quantile of a uniform distribution between 0 and 1 at the probability 0.5:\n\nset.seed(123)\nq &lt;- qunif(0.5, min = 0, max = 1)\n\nOutput:\n\nprint(q)\n\n[1] 0.5\n\n\nThis means that there is a 50% chance that a random variable\nIf you want to easily see different versions of the uniform distribution then you can either code them out or use the TidyDensity package. Let’s take a quick look.\n\npacman::p_load(TidyDensity)\n\nn &lt;- 5000\n\ntidy_uniform(.n = n) |&gt;\n  tidy_autoplot()\n\n\n\n\n\n\n\nNow different variations can be visualized with the following workflow:\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_uniform\",\n  .param_list = list(\n    .n = n,\n    .min = 0,\n    .max = c(1,5,10),\n    .num_sims = 5\n  )\n) |&gt;\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/2023-10-30/index.html#the-runif-function",
    "href": "posts/2023-10-30/index.html#the-runif-function",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "The runif() function generates random numbers from a uniform distribution. A uniform distribution is a distribution in which all values are equally likely. The runif() function takes three arguments:\n\nn: the number of random numbers to generate\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nThe default values for min and max are 0 and 1, respectively.\nHere is an example of how to use the runif() function to generate 10 random numbers from a uniform distribution between 0 and 1:\n\nset.seed(123)\nr &lt;- runif(10)\n\nOutput:\n\nprint(r)\n\n [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n [8] 0.8924190 0.5514350 0.4566147\n\n\nThe runif() function can also be used to generate random numbers from other distributions, such as the normal distribution, the Poisson distribution, and the binomial distribution."
  },
  {
    "objectID": "posts/2023-10-30/index.html#the-punif-function",
    "href": "posts/2023-10-30/index.html#the-punif-function",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "The punif() function calculates the cumulative probability density function (CDF) of the uniform distribution. The CDF is the probability that a random variable will be less than or equal to a certain value.\nThe punif() function takes three arguments:\n\nx: the value at which to calculate the CDF\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the punif() function to calculate the CDF of a uniform distribution between 0 and 1 at the value 0.5:\n\nset.seed(123)\np &lt;- punif(0.5, min = 0, max = 1)\n\nOutput:\n\nprint(p)\n\n[1] 0.5\n\n\nThis means that there is a 50% chance that a random variable from this distribution will be less than or equal to 0.5."
  },
  {
    "objectID": "posts/2023-10-30/index.html#the-dunif-function",
    "href": "posts/2023-10-30/index.html#the-dunif-function",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "The dunif() function calculates the probability density function (PDF) of the uniform distribution. The PDF is the probability that a random variable will be equal to a certain value.\nThe dunif() function takes three arguments:\n\nx: the value at which to calculate the PDF\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the dunif() function to calculate the PDF of a uniform distribution between 0 and 1 at the value 0.5:\n\nset.seed(123)\nd &lt;- dunif(0.5, min = 0, max = 1)\n\nOutput:\nprint(d)\nThis means that the probability of a random variable from this distribution being equal to 0.5 is 1."
  },
  {
    "objectID": "posts/2023-10-30/index.html#the-quinf-function",
    "href": "posts/2023-10-30/index.html#the-quinf-function",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "The quinf() function calculates the quantile function of the uniform distribution. The quantile function is the inverse of the CDF. It takes a probability as an input and returns the value that has that probability.\nThe quinf() function takes two arguments:\n\np: the probability\nmin: the lower bound of the distribution\nmax: the upper bound of the distribution\n\nHere is an example of how to use the quinf() function to calculate the quantile of a uniform distribution between 0 and 1 at the probability 0.5:\n\nset.seed(123)\nq &lt;- qunif(0.5, min = 0, max = 1)\n\nOutput:\n\nprint(q)\n\n[1] 0.5\n\n\nThis means that there is a 50% chance that a random variable\nIf you want to easily see different versions of the uniform distribution then you can either code them out or use the TidyDensity package. Let’s take a quick look.\n\npacman::p_load(TidyDensity)\n\nn &lt;- 5000\n\ntidy_uniform(.n = n) |&gt;\n  tidy_autoplot()"
  },
  {
    "objectID": "posts/2023-10-30/index.html#with-tidydensity",
    "href": "posts/2023-10-30/index.html#with-tidydensity",
    "title": "Randomness in R: runif(), punif(), dunif(), and quinf()",
    "section": "",
    "text": "Now different variations can be visualized with the following workflow:\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_uniform\",\n  .param_list = list(\n    .n = n,\n    .min = 0,\n    .max = c(1,5,10),\n    .num_sims = 5\n  )\n) |&gt;\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/2023-10-31/index.html",
    "href": "posts/2023-10-31/index.html",
    "title": "Multinomial Distribution in R",
    "section": "",
    "text": "The multinomial distribution is a probability distribution that describes the probability of obtaining a specific number of counts for k different outcomes, when each outcome has a fixed probability of occurring.\nIn R, we can use the rmultinom() function to simulate random samples from a multinomial distribution, and the dmultinom() function to calculate the probability of a specific outcome."
  },
  {
    "objectID": "posts/2023-10-31/index.html#example-1",
    "href": "posts/2023-10-31/index.html#example-1",
    "title": "Multinomial Distribution in R",
    "section": "Example 1",
    "text": "Example 1\nSuppose we have a fair die, and we want to simulate rolling the die 10 times. We can use the rmultinom() function to do this as follows:\n\n# Simulate rolling a fair die 10 times\ndie_rolls &lt;- rmultinom(\n  n = 10, size = 1, \n  prob = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\n  )\n\n# Print the results\nprint(die_rolls)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    0    0    0    0    1    0    0    0    0     0\n[2,]    0    1    0    0    0    0    0    0    0     0\n[3,]    0    0    0    0    0    0    1    0    0     0\n[4,]    0    0    1    0    0    1    0    0    0     0\n[5,]    0    0    0    1    0    0    0    1    1     0\n[6,]    1    0    0    0    0    0    0    0    0     1"
  },
  {
    "objectID": "posts/2023-10-31/index.html#example-2",
    "href": "posts/2023-10-31/index.html#example-2",
    "title": "Multinomial Distribution in R",
    "section": "Example 2",
    "text": "Example 2\nSuppose we want to calculate the probability of getting exactly two ones, two threes, two fours, two fives, and two sixes when rolling a fair die 10 times. We can use the dmultinom() function to do this as follows:\n\n# Calculate the probability of getting exactly two ones, two threes, two fours, two fives, and two sixes when rolling a fair die 10 times\nprobability &lt;- dmultinom(\n  x = c(2, 0, 2, 2, 2, 2), \n  size = 10, \n  prob = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\n  )\n\n# Print the result\nprint(probability)\n\n[1] 0.001875429"
  },
  {
    "objectID": "posts/2023-10-31/index.html#try-it-on-your-own",
    "href": "posts/2023-10-31/index.html#try-it-on-your-own",
    "title": "Multinomial Distribution in R",
    "section": "Try it on your own!",
    "text": "Try it on your own!\nI encourage readers to try using the rmultinom() and dmultinom() functions on their own data. For example, you could simulate rolling a die 100 times and see how often each outcome occurs. Or, you could calculate the probability of getting a certain number of heads and tails when flipping a coin 10 times.\nHere is an example of how to use the rmultinom() function to simulate flipping a coin 10 times and calculate the probability of getting exactly five heads and five tails:\n\n# Simulate flipping a coin 10 times\ncoin_flips &lt;- rmultinom(n = 10, size = 1, prob = c(0.5, 0.5))\n\n# Calculate the probability of getting exactly five heads and five tails\nprobability &lt;- dmultinom(x = c(5, 5), size = 10, prob = c(0.5, 0.5))\n\n# Print the result\nprint(probability)\n\n[1] 0.2460938\n\n\nI hope this blog post has helped you learn how to use the multinomial distribution in R. Please feel free to leave a comment if you have any questions."
  },
  {
    "objectID": "posts/2023-11-01/index.html",
    "href": "posts/2023-11-01/index.html",
    "title": "Understanding the Triangular Distribution and Its Application in R",
    "section": "",
    "text": "Introduction\nAs an R programmer and enthusiast, I’m excited to delve into the fascinating world of probability distributions. One of the lesser-known but incredibly useful distributions is the Triangular Distribution, and today we’ll explore what it is and how to leverage it in R using the EnvStats library.\n\n\nWhat is the Triangular Distribution?\nThe Triangular Distribution is a continuous probability distribution with a triangular shape, hence the name. It is defined by three parameters: min, max, and mode. These parameters determine the range of values the distribution can take and the most likely value within that range. In mathematical terms, the probability density function (PDF) of the Triangular Distribution is given by:\nf(x) = (2 / (b - a)) * (x - a) / (c - a)      for a ≤ x &lt; c\nf(x) = (2 / (b - a)) * (b - x) / (b - c)      for c ≤ x ≤ b\nWhere: - a is the minimum value (min parameter). - b is the maximum value (max parameter). - c is the mode, which is the peak or most likely value (mode parameter).\n\n\nUsing the EnvStats R Library\nTo work with the Triangular Distribution in R, we can use the functions provided by the EnvStats library. Here are the key functions you need to know:\n\ndtri(x, min = 0, max = 1, mode = 1/2): This function calculates the probability density at a given x. You can specify the min, max, and mode parameters to define the distribution.\nptri(q, min = 0, max = 1, mode = 1/2): Use this function to find the cumulative probability up to a given q. Again, you can customize the min, max, and mode parameters.\nqtri(p, min = 0, max = 1, mode = 1/2): The quantile function, which helps you find the value of x for a given cumulative probability p. As always, you can set min, max, and mode to match your specific distribution.\nrtri(n, min = 0, max = 1, mode = 1/2): This function generates a random set of n numbers following the Triangular Distribution with the specified parameters.\n\n\n\nPractical Example in R\nLet’s see how to use these functions in a practical example. Suppose we want to model the distribution of daily temperatures in a specific region. We have historical data indicating that the minimum temperature is -5°C, the maximum temperature is 30°C, and the most likely temperature (mode) is around 20°C.\nHere’s how you can work with this scenario in R using the EnvStats library:\n\n# Load the EnvStats library\nlibrary(EnvStats)\n\n# Define the parameters\nmin_temp &lt;- -5\nmax_temp &lt;- 30\nmode_temp &lt;- 20\n\n# Calculate the density at x = 15°C\ndensity_at_15 &lt;- dtri(15, min = min_temp, max = max_temp, mode = mode_temp)\ncat(\"Density at 15°C:\", density_at_15, \"\\n\")\n\nDensity at 15°C: 0.04571429 \n\n# Calculate the cumulative probability up to 25°C\ncumulative_prob_up_to_25 &lt;- ptri(25, min = min_temp, max = max_temp, mode = mode_temp)\ncat(\"Cumulative Probability up to 25°C:\", cumulative_prob_up_to_25, \"\\n\")\n\nCumulative Probability up to 25°C: 0.9285714 \n\n# Find the temperature value for a cumulative probability of 0.75\ntemperature_for_prob_0.75 &lt;- qtri(0.75, min = min_temp, max = max_temp, mode = mode_temp)\ncat(\"Temperature for Cumulative Probability 0.75:\", temperature_for_prob_0.75, \"\\n\")\n\nTemperature for Cumulative Probability 0.75: 20.64586 \n\n# Generate a random set of 10 temperatures\nrandom_temperatures &lt;- rtri(10, min = min_temp, max = max_temp, mode = mode_temp)\ncat(\"Random Temperatures:\", random_temperatures, \"\\n\")\n\nRandom Temperatures: 26.21123 6.59049 13.64297 19.18005 9.697022 10.96856 6.626135 18.84034 -3.711269 25.5044 \n\n\nIn this example, we’ve used the Triangular Distribution to model daily temperatures, calculate probabilities, find quantiles, and generate random temperature values.\nThe Triangular Distribution is a versatile tool for modeling scenarios where you have some knowledge about the range and likelihood of an event or outcome. Whether you’re simulating real-world scenarios or conducting risk assessments, the EnvStats library in R makes it easy to work with this distribution.\nSo, the next time you need to model uncertain events with known bounds and modes, remember the Triangular Distribution and its helpful functions in R!"
  },
  {
    "objectID": "posts/2023-11-02/index.html",
    "href": "posts/2023-11-02/index.html",
    "title": "Fitting a Distribution to Data in R",
    "section": "",
    "text": "Introduction\nThe gamma distribution is a continuous probability distribution that is often used to model waiting times or other positively skewed data. It is a two-parameter distribution, where the shape parameter controls the skewness of the distribution and the scale parameter controls the spread of the distribution.\n\n\nFitting a gamma distribution to a dataset in R\nThere are two main ways to fit a gamma distribution to a dataset in R:\n\nMaximum likelihood estimation (MLE): This method estimates the parameters of the gamma distribution that are most likely to have produced the observed data.\nMethod of moments: This method estimates the parameters of the gamma distribution by equating the sample mean and variance to the theoretical mean and variance of the gamma distribution.\n\nMLE is the more common and generally more reliable method of fitting a gamma distribution to a dataset. To fit a gamma distribution to a dataset using MLE, we can use the fitdist() function from the fitdistrplus package.\n\n# Install the fitdistrplus package if necessary\n#install.packages(\"fitdistrplus\")\n\n# Load the fitdistrplus package\nlibrary(fitdistrplus)\nlibrary(TidyDensity)\n\nset.seed(123)\ndata &lt;- tidy_gamma(.n = 500)$y\n\n# Fit a gamma distribution to the data\nfit &lt;- fitdist(data, distr = \"gamma\", method = \"mle\")\n\nThe fit object contains the estimated parameters of the gamma distribution, as well as other information about the fit. We can access the estimated parameters using the coef() function. Now the tidy_gamma() function from the TidyDensity package comes with a default setting of a .scale = 0.3 and shape = 1. The rate is 1/.scale, so by default it is 3.33333\n\n# Get the estimated parameters of the gamma distribution\ncoef(fit)\n\n   shape     rate \n1.031833 3.594773 \n\n\nNow let’s see how that compares to the built in TidyDensity function:\n\nutil_gamma_param_estimate(data)$parameter_tbl[1,c(\"shape\",\"scale\",\"shape_ratio\")]\n\n# A tibble: 1 × 3\n  shape scale shape_ratio\n  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 0.983 0.292        3.36\n\n\nIn the above, the shape_ratio is the rate\n\n\nTry on your own!\nI encourage you to try fitting a gamma distribution to your own data. You can use the fitdistrplus package in R to fit a gamma distribution to any dataset. Once you have fitted a gamma distribution to your data, you can use the estimated parameters to generate random samples from the gamma distribution or to calculate the probability of observing a particular value."
  },
  {
    "objectID": "posts/2023-11-03/index.html",
    "href": "posts/2023-11-03/index.html",
    "title": "Introducing TidyDensity’s New Powerhouse: The convert_to_ts() Function",
    "section": "",
    "text": "Introduction\nIf you’re an R enthusiast like me, you know that data manipulation is at the core of everything we do. The ability to transform your data swiftly and efficiently can make or break your data analysis projects. That’s why I’m thrilled to introduce a game-changing function in TidyDensity, my very own R library. Say hello to convert_to_ts()!\nIn the world of data analysis, time series data is like a treasure chest of insights waiting to be unlocked. Whether you’re tracking stock prices, monitoring patient data, or analyzing the temperature over the years, having your data in a time series format is a crucial step in the process. With convert_to_ts(), that process just got a whole lot easier.\n\n\nThe Basics\nLet’s start with the basics. The syntax of convert_to_ts() is straightforward:\nconvert_to_ts(.data, .return_ts = TRUE, .pivot_longer = FALSE)\n\n.data: This is your data, the data frame or tibble you want to convert into a time series format. It’s the heart of your analysis.\n.return_ts: A logical value that lets you decide whether you want to return the time series data. By default, it’s set to TRUE, which is usually what you’ll want.\n.pivot_longer: Another logical value that determines whether you want to pivot the data into long format. By default, it’s set to FALSE, but you can change that if needed.\n\n\n\nThe Magic of convert_to_ts()\nSo, what exactly does convert_to_ts() do, and why is it a game-changer? Imagine you have a data frame with time-based data in a wide format. You’ve got columns representing different time points, and you want to transform it into a time series format for easier analysis. This is where convert_to_ts() steps in.\nBy simply passing your data frame as the .data argument, convert_to_ts() does the heavy lifting for you. It reshapes your data into a tidy time series format, making it easier to work with and analyze. If you set .return_ts to TRUE, it will return the time series data, ready for your next analysis step.\nBut that’s not all. Sometimes, you might want to pivot your data into long format for specific analyses or visualizations. That’s where the .pivot_longer argument comes into play. If you set it to TRUE, convert_to_ts() will pivot your data into long format, providing you with even more flexibility in your data manipulation.\n\n\nReal-World Applications\nLet’s talk about the real-world applications of convert_to_ts(). Consider you are working with some time series data and it follows some distribution fairly well. You may want to run multiple simulations of that data, which can be done with one of the tidy_ distribution functions, and you can take that output and pipe it right into convert_to_ts() and see different simulations of a time series generated from some distribution. I do this on a regular basis at my day job in healthcare.\nBut it’s not just limited to healthcare. Stock analysts, meteorologists, and anyone dealing with time-based data can benefit from this versatile function. The possibilities are endless, and the power is in your hands.\n\n\nExamples\n\n\nExample 1: Convert data to time series format without returning time series data\n\nlibrary(TidyDensity)\n\nx &lt;- tidy_normal()\nresult &lt;- convert_to_ts(x, FALSE)\nhead(result)\n\n# A tibble: 6 × 1\n        y\n    &lt;dbl&gt;\n1  1.23  \n2  0.715 \n3  0.738 \n4  1.08  \n5 -0.0613\n6  1.19  \n\n\n\n\nExample 2: Convert data to time series format and pivot it into long format\n\nx &lt;- tidy_normal(.num_sims = 4)\nresult &lt;- convert_to_ts(x, FALSE, TRUE)\nhead(result)\n\n# A tibble: 6 × 2\n  sim_number      y\n  &lt;chr&gt;       &lt;dbl&gt;\n1 1           0.881\n2 1          -0.105\n3 1          -0.655\n4 1          -0.564\n5 1           0.600\n6 1          -0.811\n\nunique(result$sim_number)\n\n[1] \"1\" \"2\" \"3\" \"4\"\n\nconvert_to_ts(x, TRUE, TRUE) |&gt; head()\n\n              1           2          3          4\n[1,]  0.8807494  0.04680495 -0.1993147 -1.5549585\n[2,] -0.1045016  0.14023102  0.5433512 -1.9247656\n[3,] -0.6549336 -0.20818900 -0.1689992 -0.2934131\n[4,] -0.5638595  0.87588361  0.4802693 -1.2052377\n[5,]  0.6002684  0.26137176  1.5993445 -0.5379518\n[6,] -0.8111576 -0.60834621 -0.4859808 -0.2178982\n\n\n\n\nExample 3: Convert data to time series format and return the time series data\n\nx &lt;- tidy_normal()\nresult &lt;- convert_to_ts(x)\nhead(result)\n\n               y\n[1,] -0.21509987\n[2,] -0.88989659\n[3,]  0.69464989\n[4,] -0.03296698\n[5,] -0.82499955\n[6,] -0.71676037\n\n\n\n\nConclusion\nIn the ever-evolving world of data analysis, having the right tools at your disposal is crucial. convert_to_ts() in TidyDensity is one such tool that can simplify your data transformation processes and elevate your data analysis game. It’s all about efficiency and flexibility, allowing you to focus on what matters most – deriving valuable insights from your data.\nSo, whether you’re a data enthusiast, a coding wizard, or someone curious about the world of R, convert_to_ts() is here to make your life easier. Give it a try, explore its capabilities, and unlock the potential of your time series data. With TidyDensity, the possibilities are endless, and your data analysis journey just got a whole lot smoother. Happy coding!"
  },
  {
    "objectID": "posts/2023-11-06/index.html",
    "href": "posts/2023-11-06/index.html",
    "title": "Demystifying Data: A Comprehensive Guide to Calculating and Plotting Cumulative Distribution Functions (CDFs) in R",
    "section": "",
    "text": "Introduction\nIn the realm of statistics, a cumulative distribution function (CDF) serves as a crucial tool for understanding the behavior of data. It provides a comprehensive picture of how a variable’s values are distributed across its range. In this blog post, we’ll embark on an exciting journey to unravel the mysteries of CDFs and explore how to effortlessly calculate and visualize them using the powerful R programming language.\n\n\nUnderstanding the Essence of CDFs\nBefore delving into the world of R programming, let’s first grasp the fundamental concept of a CDF. Imagine a group of students eagerly awaiting their exam results. The CDF for their scores would depict the probability of encountering a student with a score less than or equal to a specific value. For instance, if the CDF indicates a value of 0.7 at 80%, it implies that there’s a 70% chance of finding a student with a score of 80 or lower.\n\n\nCalculating CDFs with the ecdf() Function\nR, our trusty programming companion, offers a user-friendly function called ecdf() to calculate CDFs. This function takes a vector of data as input and returns a corresponding CDF object. Let’s put this function into action by generating a sample dataset of exam scores:\n\nexam_scores &lt;- c(75, 82, 94, 68, 88, 90, 72, 85, 91, 79)\n\nNow, we can effortlessly calculate the CDF using the ecdf() function:\n\ncdf_scores &lt;- ecdf(exam_scores)\n\nThe cdf_scores object now holds the calculated CDF values for the exam scores.\n\n\nVisualizing CDFs with the plot() Function\nTo gain a deeper understanding of the CDF, we can visualize it using the plot() function. This function takes the CDF object as input and generates a corresponding plot. Simply type the following command:\n\nplot(cdf_scores)\n\n\n\n\nVoila! You should now see a captivating plot depicting the CDF of the exam scores. The x-axis represents the exam scores, and the y-axis represents the corresponding cumulative probabilities.\n\n\nExplore!\nWe’ve successfully calculated and visualized CDFs in R. Now it’s time for you to explore and experiment with this powerful tool. Gather your own data, calculate the CDF, and interpret its meaning. Remember, data holds valuable insights, and CDFs are the keys to unlocking those insights."
  },
  {
    "objectID": "posts/2023-11-07/index.html",
    "href": "posts/2023-11-07/index.html",
    "title": "How to Simulate & Plot a Bivariate Normal Distribution in R: A Hands-on Guide",
    "section": "",
    "text": "Introduction\nWelcome to the fascinating world of bivariate normal distributions! In this blog post, we’ll embark on a journey to understand, simulate, and visualize these distributions using the powerful R programming language. Whether you’re a seasoned R expert or a curious beginner, this guide will equip you with the necessary tools to explore this intriguing aspect of probability theory.\n\n\nUnderstanding Bivariate Normal Distributions\nImagine two variables, like height and weight, that exhibit a joint distribution. The bivariate normal distribution captures the relationship between these variables, describing how their values tend to cluster around certain means and how they vary together. It’s like a two-dimensional bell curve, where the peak represents the most likely combination of values for both variables.\n\n\nSimulating a Bivariate Normal Distribution\nNow, let’s bring this distribution to life using R. The MASS package provides the mvrnorm() function, which generates random samples from a multivariate normal distribution. We’ll use this function to simulate a bivariate normal distribution with mean vector [10, 20] and covariance matrix [[5, 3], [3, 6]]. These parameters determine the center and shape of the distribution.\n\nlibrary(MASS)\n\n# Simulate 100 observations from a bivariate normal distribution\nset.seed(123) # Set a seed for reproducibility\nbvnData &lt;- mvrnorm(\n  n = 100, \n  mu = c(10, 20), \n  Sigma = matrix(c(5, 3, 3, 6), \n                 ncol = 2)\n  )\n\n\n\nVisualizing the Bivariate Normal Distribution\nTo truly appreciate the beauty of the bivariate normal distribution, let’s visualize it using the plot() and density() functions\n\nlibrary(mnormt)\n\nx &lt;- bvnData[,1] |&gt; sort()\ny &lt;- bvnData[,2] |&gt; sort()\nmu &lt;- c(10, 20)\nsigma &lt;- matrix(c(5, 3, 3, 6), \n                 ncol = 2)\nf &lt;- function(x, y) dmnorm(cbind(x, y), mu, sigma)\nz &lt;- outer(x,y,f)\ncontour(x,y,z)\n\n\n\n# Create a density plot of the simulated data\nplot(density(bvnData))\n\n\n\n\nThis plot should reveal an elliptical shape, with the highest density concentrated around the mean values. The contours represent the regions of equal probability.\n\n\nTry It On Your Own!\nNow, it’s your turn to experiment! Change the mean vector, covariance matrix, and sample size to see how they affect the shape and spread of the distribution. Play with different visualization options to explore different perspectives of the data.\nRemember, R is a vast and ever-evolving language, so there’s always more to learn. Keep exploring, asking questions, and seeking out new challenges to become a master R programmer."
  },
  {
    "objectID": "posts/2023-11-13/index.html",
    "href": "posts/2023-11-13/index.html",
    "title": "Unlocking the Power of Prediction Intervals in R: A Practical Guide",
    "section": "",
    "text": "Introduction\nPrediction intervals are a powerful tool for understanding the uncertainty of your predictions. They allow you to specify a range of values within which you are confident that the true value will fall. This can be useful for many tasks, such as setting realistic goals, making informed decisions, and communicating your findings to others.\nIn this blog post, we will show you how to create a prediction interval in R using the mtcars dataset. The mtcars dataset is a built-in dataset in R that contains information about fuel economy, weight, displacement, and other characteristics of 32 cars.\n\n\nCreating a Prediction Interval\nTo create a prediction interval in R, we can use the predict() function. The predict() function takes a fitted model and a new dataset as input and returns the predicted values for the new dataset.\nWe can also use the predict() function to calculate prediction intervals. To do this, we need to specify the interval argument. The interval argument can take two values: confidence and prediction.\nA confidence interval is the range of values within which we are confident that the true mean of the population will fall. A prediction interval is the range of values within which we are confident that the true value of a new observation will fall.\nTo create a prediction interval for the mpg variable in the mtcars dataset, we can use the following code:\n\n# Fit a linear model\nmodel &lt;- lm(mpg ~ disp, data = mtcars)\n\n# Create a prediction interval\nprediction_intervals &lt;- predict(\n  model, \n  newdata = mtcars, \n  interval = \"prediction\", \n  level = 0.95\n  )\n\n# Print the prediction interval\nhead(prediction_intervals)\n\n                       fit       lwr      upr\nMazda RX4         23.00544 16.227868 29.78300\nMazda RX4 Wag     23.00544 16.227868 29.78300\nDatsun 710        25.14862 18.302683 31.99456\nHornet 4 Drive    18.96635 12.217933 25.71477\nHornet Sportabout 14.76241  7.905308 21.61952\nValiant           20.32645 13.582915 27.06999\n\n\nThe prediction interval shows that we are 95% confident that the true mpg value for a new car with a given displacement will fall within the range specified by the lwr and upr columns.\n\n\nVisualize\nFirst lets bind the data together with cbind()\n\nfull_res &lt;- cbind(mtcars, prediction_intervals)\n\nhead(full_res)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb      fit\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 23.00544\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 23.00544\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 25.14862\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 18.96635\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 14.76241\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 20.32645\n                        lwr      upr\nMazda RX4         16.227868 29.78300\nMazda RX4 Wag     16.227868 29.78300\nDatsun 710        18.302683 31.99456\nHornet 4 Drive    12.217933 25.71477\nHornet Sportabout  7.905308 21.61952\nValiant           13.582915 27.06999\n\n\nNow let’s plot the actual, the fitted and the prediction confidence bands.\n\nlibrary(ggplot2)\n\nfull_res |&gt;\n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point() +\n  geom_point(aes(y = fit), col = \"steelblue\", size = 2.5) +\n  geom_line(aes(y = fit)) +\n  geom_line(aes(y = lwr), linetype = \"dashed\", col = \"red\") +\n  geom_line(aes(y = upr), linetype = \"dashed\", col = \"red\") +\n  theme_minimal() +\n  labs(\n    title = \"mpg ~ disp, data = mtcars\",\n    subtitle = \"With Prediction Intervals\"\n  )\n\n\n\n\nAbove we are capturing the prediction interval which gives us the uncertainty around a single point, whereas the confidence interval gives us the uncertainty around the mean predicted values. This means that the prediction interval will always be wider than the confidence interval for the same value.\n\n\nTrying It Out Yourself\nNow it’s your turn to try out creating a prediction interval in R. Here are some ideas:\n\nTry creating a prediction interval for a different variable in the mtcars dataset, such as wt or hp.\nTry creating a prediction interval for a variable in a different dataset.\nTry creating a prediction interval for a more complex model, such as a multiple linear regression model or a logistic regression model.\n\n\n\nConclusion\nCreating prediction intervals in R is a straightforward process. By using the predict() function, you can easily calculate prediction intervals for any fitted model and any new dataset. This can be a valuable tool for understanding the uncertainty of your predictions and making more informed decisions."
  },
  {
    "objectID": "posts/2023-11-14/index.html",
    "href": "posts/2023-11-14/index.html",
    "title": "How to Predict a Single Value Using a Regression Model in R",
    "section": "",
    "text": "Introduction\nRegression models are a powerful tool for predicting future values based on historical data. They are used in a wide range of industries, including finance, healthcare, and marketing. In this blog post, we will learn how to predict a single value using a regression model in R. We will use the mtcars dataset, which contains information about cars, including their weight, horsepower, and fuel efficiency.\n\n\nBuilding a Linear Regression Model\nThe first step in predicting a single value is to build a regression model. We can do this using the lm() function in R. The lm() function takes two arguments: a formula and a data frame. The formula specifies the relationship between the dependent variable (the variable we want to predict) and the independent variables (the variables we use to predict the dependent variable). The data frame contains the values of the dependent and independent variables.\nTo build a linear regression model to predict the fuel efficiency of a car based on its weight and horsepower, we would use the following code:\n\n# Create a linear regression model\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\nThe model object now contains the fitted regression model. We can inspect the model by using the summary() function.\n\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nThe output of the summary() function shows the estimated coefficients, standard errors, and p-values for the independent variables in the model. The coefficients represent the change in the dependent variable for a one-unit increase in the independent variable, holding all other variables constant.\n\n\nPredicting a Single Value\nOnce we have fitted a regression model, we can use it to predict single values. We can do this using the predict() function. The predict() function takes two arguments: the fitted model and a new data frame containing the values of the independent variables for which we want to make predictions.\nTo predict the fuel efficiency of a car with a weight of 3,000 pounds and a horsepower of 150, we would use the following code:\n\n# Create a new data frame containing the values of the independent \n# variables for which we want to make predictions\nnewdata &lt;- data.frame(wt = 3, hp = 150) # Wt is in 1000 lbs\n\n# Predict the fuel efficiency of the car\nprediction &lt;- predict(model, newdata)\n\n# Print the predicted fuel efficiency\nprint(prediction)\n\n       1 \n20.82784 \n\n\nThe output of the predict() function is a vector containing the predicted values for the dependent variable. In this case, the predicted fuel efficiency is 20.8278358 miles per gallon.\n\n\nConclusion\nIn this blog post, we have learned how to predict a single value using a regression model in R. We used the mtcars dataset to build a linear regression model to predict the fuel efficiency of a car based on its weight and horsepower. We then used the predict() function to predict the fuel efficiency of a car with a specific weight and horsepower.\n\n\nTry It Yourself\nNow that you know how to predict a single value using a regression model in R, try it yourself! Here are some ideas:\n\nBuild a linear regression model to predict the price of a house based on its size and number of bedrooms.\nBuild a linear regression model to predict the salary of a person based on their education level and years of experience.\nBuild a linear regression model to predict the number of customers that will visit a store on a given day based on the day of the week and the weather forecast.\n\nOnce you have built a regression model, you can use it to predict single values for new data. This can be a valuable tool for making decisions about the future."
  },
  {
    "objectID": "posts/2023-11-15/index.html",
    "href": "posts/2023-11-15/index.html",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "",
    "text": "Multiple linear regression is a powerful statistical method that allows us to examine the relationship between a dependent variable and multiple independent variables."
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-1-load-the-dataset",
    "href": "posts/2023-11-15/index.html#step-1-load-the-dataset",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 1: Load the dataset",
    "text": "Step 1: Load the dataset\n# Load the mtcars dataset\ndata(mtcars)"
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-2-build-the-model",
    "href": "posts/2023-11-15/index.html#step-2-build-the-model",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 2: Build the model",
    "text": "Step 2: Build the model\nNow, let’s create the multiple linear regression model using the specified variables: disp, hp, and drat.\n\n# Build the multiple linear regression model\nmodel &lt;- lm(mpg ~ disp + hp + drat, data = mtcars)"
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-3-examine-the-data",
    "href": "posts/2023-11-15/index.html#step-3-examine-the-data",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 3: Examine the data",
    "text": "Step 3: Examine the data\nIt’s always a good idea to take a look at the relationships between variables before diving into the model. The pairs() function helps us with that.\n\n# Examine relationships between variables\npairs(mtcars[,c(\"mpg\",\"disp\",\"hp\",\"drat\")])"
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-4-check-for-multicollinearity",
    "href": "posts/2023-11-15/index.html#step-4-check-for-multicollinearity",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 4: Check for multicollinearity",
    "text": "Step 4: Check for multicollinearity\nMulticollinearity is when independent variables in a regression model are highly correlated. It can affect the stability and reliability of our model. Keep an eye on the scatterplots in the pairs plot to get a sense of this."
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-5-plot-the-residuals",
    "href": "posts/2023-11-15/index.html#step-5-plot-the-residuals",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 5: Plot the residuals",
    "text": "Step 5: Plot the residuals\nNow, let’s check the model’s residuals using a scatterplot. Residuals are the differences between observed and predicted values. They should ideally show no pattern.\n\n# Plot the residuals\nplot(\n  model$residuals, \n  main = \"Residuals vs Fitted Values\", \n  xlab = \"Fitted Values\", \n  ylab = \"Residuals\"\n  )"
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-6-evaluate-the-model",
    "href": "posts/2023-11-15/index.html#step-6-evaluate-the-model",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 6: Evaluate the model",
    "text": "Step 6: Evaluate the model\nBy examining the residuals vs. fitted values plot, we can identify patterns that may suggest non-linearity or heteroscedasticity. Ideally, residuals should be randomly scattered."
  },
  {
    "objectID": "posts/2023-11-15/index.html#step-7-encourage-readers-to-try-it-themselves",
    "href": "posts/2023-11-15/index.html#step-7-encourage-readers-to-try-it-themselves",
    "title": "How to Perform Multiple Linear Regression in R",
    "section": "Step 7: Encourage readers to try it themselves",
    "text": "Step 7: Encourage readers to try it themselves\nI’d encourage readers to take the code snippets, run them in their R environment, and explore. Maybe try different variables, tweak the model, or even use another dataset. Hands-on experience is the best teacher!\nRemember, understanding the data and interpreting the results is as important as running the code. It’s a fascinating journey into uncovering patterns and relationships within your data.\nFeel free to reach out if you have any questions or if there’s anything specific you’d like to explore further. Happy coding!"
  },
  {
    "objectID": "posts/2011-11-16/index.html",
    "href": "posts/2011-11-16/index.html",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "",
    "text": "My R package {healthyR.ts} has been updated to version 0.3.0; you can install it from either CRAN, r-universe or GitHub. Let’s go over some of the changes and improvements."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_log_ts---logging-time-series-data",
    "href": "posts/2011-11-16/index.html#util_log_ts---logging-time-series-data",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "1. util_log_ts() - Logging Time Series Data",
    "text": "1. util_log_ts() - Logging Time Series Data\nOne of the standout additions is the introduction of util_log_ts(). This function seems like a game-changer, providing a streamlined way to log time series data. This is incredibly useful, especially when dealing with extensive datasets, making the whole process more efficient and user-friendly. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_singlediff_ts---single-differences-for-time-series",
    "href": "posts/2011-11-16/index.html#util_singlediff_ts---single-differences-for-time-series",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "2. util_singlediff_ts() - Single Differences for Time Series",
    "text": "2. util_singlediff_ts() - Single Differences for Time Series\nThe addition of util_singlediff_ts() expands the toolkit, offering a function dedicated to handling single differences in time series data. This is valuable for various applications, such as identifying trends or preparing data for further analysis. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_doublediff_ts---double-differences-for-time-series",
    "href": "posts/2011-11-16/index.html#util_doublediff_ts---double-differences-for-time-series",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "3. util_doublediff_ts() - Double Differences for Time Series",
    "text": "3. util_doublediff_ts() - Double Differences for Time Series\nBuilding on the concept of differencing, util_doublediff_ts() seems to provide a higher level of sophistication, allowing users to perform double differences on time series data. This could be pivotal in cases where a more refined analysis is required. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_difflog_ts---combining-differences-and-log-transformation",
    "href": "posts/2011-11-16/index.html#util_difflog_ts---combining-differences-and-log-transformation",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "4. util_difflog_ts() - Combining Differences and Log Transformation",
    "text": "4. util_difflog_ts() - Combining Differences and Log Transformation\nThe fusion of differencing and log transformation in util_difflog_ts() is a remarkable addition. This could be particularly beneficial in scenarios where both operations are needed to unlock deeper insights from the time series data. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#util_doubledifflog_ts---double-differences-with-log-transformation",
    "href": "posts/2011-11-16/index.html#util_doubledifflog_ts---double-differences-with-log-transformation",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "5. util_doubledifflog_ts() - Double Differences with Log Transformation",
    "text": "5. util_doubledifflog_ts() - Double Differences with Log Transformation\nThe introduction of util_doubledifflog_ts() appears to take things a step further by combining double differences and log transformation. This function seems poised to provide a comprehensive solution for users dealing with complex time series data. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2011-11-16/index.html#attributes-enhancement-in-ts_growth_rate_vec",
    "href": "posts/2011-11-16/index.html#attributes-enhancement-in-ts_growth_rate_vec",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "1. Attributes Enhancement in ts_growth_rate_vec()",
    "text": "1. Attributes Enhancement in ts_growth_rate_vec()\nThe attention to detail is evident with the addition of attributes to the output of ts_growth_rate_vec(). This enhancement not only improves the clarity of results but also contributes to a more informative and user-friendly experience."
  },
  {
    "objectID": "posts/2011-11-16/index.html#refinement-of-auto_stationarize-in-response-to-user-feedback",
    "href": "posts/2011-11-16/index.html#refinement-of-auto_stationarize-in-response-to-user-feedback",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "2. Refinement of auto_stationarize() in Response to User Feedback",
    "text": "2. Refinement of auto_stationarize() in Response to User Feedback\nUpdates to auto_stationarize() based on user feedback (Fix #481 #483) demonstrate a commitment to refining existing features. This responsiveness to the community’s needs is commendable and ensures that the package evolves in sync with user expectations. It has taken all of the util_ transforms mentioned above in order to improve it’s functionality."
  },
  {
    "objectID": "posts/2011-11-16/index.html#integration-with-auto_arima-engine-in-ts_auto_arima",
    "href": "posts/2011-11-16/index.html#integration-with-auto_arima-engine-in-ts_auto_arima",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "3. Integration with auto_arima Engine in ts_auto_arima()",
    "text": "3. Integration with auto_arima Engine in ts_auto_arima()\nThe integration of ts_auto_arima() with the parsnip engine of auto_arima is a notable improvement. This update, triggered when .tune is set to FALSE, aligns the package with cutting-edge tools, potentially enhancing the efficiency and accuracy of time series modeling.\nIn conclusion, the release of healthyR.ts version 0.3.0 is an exciting leap forward. The new features introduce powerful capabilities, while the minor fixes and improvements showcase a commitment to providing a robust and user-friendly package. Users can look forward to a more versatile and refined experience in time series analysis. Great job on this release, and I’m sure the community is eager to explore these enhancements!"
  },
  {
    "objectID": "posts/2011-11-16/index.html#auto_stationarize",
    "href": "posts/2011-11-16/index.html#auto_stationarize",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "auto_stationarize()",
    "text": "auto_stationarize()\n\nlibrary(healthyR.ts)\n\nauto_stationarize(AirPassengers)\n\nThe time series is already stationary via ts_adf_test().\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nauto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n$stationary_ts\nTime Series:\nStart = 3 \nEnd = 150 \nFrequency = 1 \n  [1]  0.5 -0.4  0.6  1.1 -2.8  3.0 -1.1  0.6 -0.5 -0.5  0.1  2.0 -0.6  0.8  1.2\n [16] -3.4 -0.7 -0.3  1.7  3.0 -3.2  0.9  2.2 -2.5 -0.4  2.6 -4.3  2.0 -3.1  2.7\n [31] -2.1  0.1  2.1 -0.2 -2.2  0.6  1.0 -2.6  3.0  0.3  0.2 -0.8  1.0  0.0  3.2\n [46] -2.2 -4.7  1.2  0.8 -0.6 -0.4  0.6  1.0 -1.6 -0.1  3.4 -0.9 -1.7 -0.5  0.8\n [61]  2.4 -1.9  0.6 -2.2  2.6 -0.1 -2.7  1.7 -0.3  1.9 -2.7  1.1 -0.6  0.9  0.0\n [76]  1.8 -0.5 -0.4 -1.2  2.6 -1.8  1.7 -0.9  0.6 -0.4  3.0 -2.8  3.1 -2.3 -1.1\n [91]  2.1 -0.3 -1.7 -0.8 -0.4  1.1 -1.5  0.3  1.4 -2.0  1.3 -0.3  0.4 -3.5  1.1\n[106]  2.6  0.4 -1.3  2.0 -1.6  0.6 -0.1 -1.4  1.6  1.6 -3.4  1.7 -2.2  2.1 -2.0\n[121] -0.2  0.2  0.7 -1.4  1.8 -0.1 -0.7  0.4  0.4  1.0 -2.4  1.0 -0.4  0.8 -1.0\n[136]  1.4 -1.2  1.1 -0.9  0.5  1.9 -0.6  0.3 -1.4 -0.9 -0.5  1.4  0.1\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -6.562008\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"double_diff\"\n\n$ret\n[1] TRUE\n\nplot.ts(auto_stationarize(BJsales)$stationary_ts)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n\n\nauto_stationarize(BJsales.lead)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n$stationary_ts\nTime Series:\nStart = 2 \nEnd = 150 \nFrequency = 1 \n  [1]  0.06  0.25 -0.57  0.58 -0.20  0.23 -0.04 -0.19  0.03  0.42  0.04  0.24\n [13]  0.34 -0.46 -0.18 -0.08  0.29  0.56 -0.37  0.20  0.54 -0.31  0.03  0.52\n [25] -0.70  0.35 -0.63  0.44 -0.38 -0.01  0.22  0.10 -0.50  0.01  0.30 -0.76\n [37]  0.52  0.15  0.06 -0.10  0.21 -0.01  0.70 -0.22 -0.76  0.06  0.02 -0.17\n [49] -0.08  0.01  0.11 -0.39  0.01  0.50 -0.02 -0.37 -0.13  0.05  0.54 -0.46\n [61]  0.25 -0.52  0.44  0.02 -0.47  0.11  0.06  0.25 -0.35  0.00 -0.06  0.21\n [73] -0.09  0.36  0.09 -0.04 -0.20  0.44 -0.23  0.40 -0.01  0.17  0.08  0.58\n [85] -0.27  0.79 -0.21  0.02  0.30  0.28 -0.27 -0.01  0.03  0.16 -0.28  0.15\n [97]  0.26 -0.36  0.32 -0.11  0.22 -0.65  0.00  0.47  0.16 -0.19  0.48 -0.26\n[109]  0.21  0.00 -0.20  0.35  0.38 -0.48  0.20 -0.32  0.43 -0.50  0.12 -0.17\n[121]  0.15 -0.36  0.35 -0.03 -0.18  0.16  0.07  0.21 -0.50  0.23 -0.13  0.14\n[133] -0.15  0.19 -0.24  0.26 -0.22  0.17  0.37 -0.06  0.29 -0.34 -0.12 -0.16\n[145]  0.25  0.08 -0.07  0.26 -0.37\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -4.838625\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"diff\"\n\n$ret\n[1] TRUE\n\nplot.ts(auto_stationarize(BJsales.lead)$stationary_ts)\n\nThe time series is not stationary. Attempting to make it stationary..."
  },
  {
    "objectID": "posts/2011-11-16/index.html#ts_auto_arima",
    "href": "posts/2011-11-16/index.html#ts_auto_arima",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "ts_auto_arima()",
    "text": "ts_auto_arima()\nThis use to only use the Arima engine if the .tune parameter was set to FALSE, thus it would many times give a simple straight line forecast. This was changed to make the engine auto_arima if .tune is set to FALSE.\n\nlibrary(timetk)\nlibrary(dplyr)\nlibrary(modeltime)\n\ndata &lt;- AirPassengers |&gt;\n  ts_to_tbl() |&gt;\n  select(-index)\n\nsplits &lt;- time_series_split(\n  data\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\nts_aa &lt;- ts_auto_arima(\n  .data = data,\n  .num_cores = 2,\n  .date_col = date_col,\n  .value_col = value,\n  .rsamp_obj = splits,\n  .formula = value ~ .,\n  .grid_size = 5,\n  .cv_slice_limit = 2,\n  .tune = FALSE\n)\n\nts_aa$recipe_info\n\n$recipe_call\nrecipe(.data = data, .date_col = date_col, .value_col = value, \n    .formula = value ~ ., .rsamp_obj = splits, .tune = FALSE, \n    .grid_size = 5, .num_cores = 2, .cv_slice_limit = 2)\n\n$recipe_syntax\n[1] \"ts_arima_recipe &lt;-\"                                                                                                                                                                           \n[2] \"\\n  recipe(.data = data, .date_col = date_col, .value_col = value, .formula = value ~ \\n    ., .rsamp_obj = splits, .tune = FALSE, .grid_size = 5, .num_cores = 2, \\n    .cv_slice_limit = 2)\"\n\n$rec_obj\n\nts_aa$model_info\n\n$model_spec\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nSeries: outcome \nARIMA(1,1,0)(0,1,0)[12] \n\nCoefficients:\n          ar1\n      -0.2431\ns.e.   0.0894\n\nsigma^2 = 109.8:  log likelihood = -447.95\nAIC=899.9   AICc=900.01   BIC=905.46\n\n$was_tuned\n[1] \"not_tuned\"\n\nts_aa$model_calibration\n\n$plot\n\n$calibration_tbl\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc             .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;workflow&gt; ARIMA(1,1,0)(0,1,0)[12] Test  &lt;tibble [12 × 4]&gt;\n\n$model_accuracy\n# A tibble: 1 × 9\n  .model_id .model_desc             .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 ARIMA(1,1,0)(0,1,0)[12] Test   18.5  4.18 0.384  4.03  23.9 0.955\n\nts_aa$model_calibration$plot\n\n\n\n\n\nFinally enhancement to add attributes to ts_growth_rate_vec()\n\nts_growth_rate_vec(AirPassengers)\n\n  [1]          NA   5.3571429  11.8644068  -2.2727273  -6.2015504  11.5702479\n  [7]   9.6296296   0.0000000  -8.1081081 -12.5000000 -12.6050420  13.4615385\n [13]  -2.5423729   9.5652174  11.9047619  -4.2553191  -7.4074074  19.2000000\n [19]  14.0939597   0.0000000  -7.0588235 -15.8227848 -14.2857143  22.8070175\n [25]   3.5714286   3.4482759  18.6666667  -8.4269663   5.5214724   3.4883721\n [31]  11.7977528   0.0000000  -7.5376884 -11.9565217  -9.8765432  13.6986301\n [37]   3.0120482   5.2631579   7.2222222  -6.2176166   1.1049724  19.1256831\n [43]   5.5045872   5.2173913 -13.6363636  -8.6124402  -9.9476440  12.7906977\n [49]   1.0309278   0.0000000  20.4081633  -0.4237288  -2.5531915   6.1135371\n [55]   8.6419753   3.0303030 -12.8676471 -10.9704641 -14.6919431  11.6666667\n [61]   1.4925373  -7.8431373  25.0000000  -3.4042553   3.0837004  12.8205128\n [67]  14.3939394  -2.9801325 -11.6040956 -11.5830116 -11.3537118  12.8078818\n [73]   5.6768559  -3.7190083  14.5922747   0.7490637   0.3717472  16.6666667\n [79]  15.5555556  -4.6703297 -10.0864553 -12.1794872 -13.5036496  17.2995781\n [85]   2.1582734  -2.4647887  14.4404332  -1.2618297   1.5974441  17.6100629\n [91]  10.4278075  -1.9370460 -12.3456790 -13.8028169 -11.4379085  12.9151292\n [97]   2.9411765  -4.4444444  18.2724252  -2.2471910   2.0114943  18.8732394\n[103]  10.1895735   0.4301075 -13.4903640 -14.1089109 -12.1037464  10.1639344\n[109]   1.1904762  -6.4705882  13.8364780  -3.8674033   4.3103448  19.8347107\n[115]  12.8735632   2.8513238 -20.0000000 -11.1386139 -13.6490251   8.7096774\n[121]   6.8249258  -5.0000000  18.7134503  -2.4630542   6.0606061  12.3809524\n[127]  16.1016949   2.0072993 -17.1735242 -12.0950324 -11.0565111  11.8784530\n[133]   2.9629630  -6.2350120   7.1611253  10.0238663   2.3861171  13.3474576\n[139]  16.2616822  -2.5723473 -16.1716172  -9.2519685 -15.4013015  10.7692308\nattr(,\"vector_attributes\")\nattr(,\"vector_attributes\")$tsp\n[1] 1949.000 1960.917   12.000\n\nattr(,\"vector_attributes\")$class\n[1] \"ts\"\n\nattr(,\"name\")\n[1] \"AirPassengers\""
  },
  {
    "objectID": "posts/2023-11-17/index.html",
    "href": "posts/2023-11-17/index.html",
    "title": "Quadratic Regression in R: Unveiling Non-Linear Relationships",
    "section": "",
    "text": "Introduction\nIn the realm of data analysis, quadratic regression emerges as a powerful tool for uncovering the hidden patterns within datasets that exhibit non-linear relationships. Unlike its linear counterpart, quadratic regression ventures beyond straight lines, gracefully capturing curved relationships between variables. This makes it an essential technique for understanding a wide range of phenomena, from predicting stock prices to modeling population growth.\nEmbark on a journey into the world of quadratic regression using the versatile R programming language. We’ll explore the steps involved in fitting a quadratic model, interpreting its parameters, and visualizing the results. Along the way, you’ll gain hands-on experience with this valuable technique, enabling you to tackle your own data analysis challenges with confidence.\n\n\nSetting the Stage: Data Preparation\nBefore embarking on our quadratic regression adventure, let’s assemble our data. Suppose we’re investigating the relationship between study hours and exam scores. We’ve gathered data from a group of students, recording their study hours and corresponding exam scores.\n\n# Create a data frame to store the data\nstudy_hours &lt;- c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60)\nexam_scores &lt;- c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27)\ndata &lt;- data.frame(study_hours, exam_scores)\ndata\n\n   study_hours exam_scores\n1            6          14\n2            9          28\n3           12          50\n4           14          70\n5           30          89\n6           35          94\n7           40          90\n8           47          75\n9           51          59\n10          55          44\n11          60          27\n\n\n\n\nVisualizing the Relationship: A Scatterplot’s Revelation\nTo gain an initial impression of the relationship between study hours and exam scores, let’s create a scatterplot. This simple yet powerful visualization will reveal the underlying pattern in our data.\n\n# Create a scatterplot of exam scores versus study hours\nplot(\n  data$study_hours, \n  data$exam_scores, \n  main = \"Exam Scores vs. Study Hours\", \n  xlab = \"Study Hours\", \n  ylab = \"Exam Scores\"\n  )\n\n\n\n\nUpon examining the scatterplot, a hint of a non-linear relationship emerges. The data points don’t fall along a straight line, suggesting a more complex association between study hours and exam scores. This is where quadratic regression steps in.\n\n\nFitting the Quadratic Model: Capturing the Curve\nTo capture the curvature evident in our data, we’ll employ the lm() function in R to fit a quadratic regression model. This model incorporates a second-degree term, allowing it to represent curved relationships between variables.\n\n# Fit a quadratic regression model to the data\nquadratic_model &lt;- lm(exam_scores ~ study_hours + I(study_hours^2), data = data)\n\nThe I() function in the model formula ensures that the square of study hours is treated as a separate variable, enabling the model to capture the non-linearity.\n\n\nInterpreting the Model: Unraveling the Parameters\nNow that we’ve fitted the quadratic model, let’s delve into its parameters and understand their significance.\n\n# Summarize the quadratic regression model\nsummary(quadratic_model)\n\n\nCall:\nlm(formula = exam_scores ~ study_hours + I(study_hours^2), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2484 -3.7429 -0.1812  1.1464 13.6678 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -18.25364    6.18507  -2.951   0.0184 *  \nstudy_hours        6.74436    0.48551  13.891 6.98e-07 ***\nI(study_hours^2)  -0.10120    0.00746 -13.565 8.38e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.218 on 8 degrees of freedom\nMultiple R-squared:  0.9602,    Adjusted R-squared:  0.9502 \nF-statistic: 96.49 on 2 and 8 DF,  p-value: 2.51e-06\n\n\nThe output of the summary function provides valuable insights into the model’s performance and the significance of its parameters. It indicates the intercept, representing the predicted exam score when study hours are zero, and the coefficients for the linear and quadratic terms.\n\n\nVisualizing the Model: Bringing the Curve to Life\nTo fully appreciate the quadratic model’s ability to capture the non-linear relationship between study hours and exam scores, let’s visualize the model alongside the data points.\n\n# Calculate the predicted exam scores for a range of study hours\npredicted_scores &lt;- predict(\n  quadratic_model, \n  newdata = data.frame(\n    study_hours = seq(min(study_hours), \n                      max(study_hours), \n                      length.out = 100\n                      )\n    )\n  )\n\n# Plot the data points and the predicted scores\nplot(\n  data$study_hours, \n  data$exam_scores, \n  main = \"Exam Scores vs. Study Hours\", \n  xlab = \"Study Hours\", \n  ylab = \"Exam Scores\"\n  )\nlines(seq(min(study_hours), \n          max(study_hours), \n          length.out = 100), \n      predicted_scores, col = \"red\"\n      )\n\n\n\n\nThe resulting plot reveals the graceful curve of the quadratic model, fitting the data points closely. This visualization reinforces the model’s ability to capture the non-linear relationship between study hours and exam scores.\n\n\nYour Turn: Embarking on Your Own Quadratic Regression Adventure\nArmed with the knowledge and skills gained from this tutorial, you’re now ready to embark on your own quadratic regression adventures. Gather your data, fit the model, interpret the parameters, and visualize the results."
  },
  {
    "objectID": "posts/2023-11-20/index.html",
    "href": "posts/2023-11-20/index.html",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "",
    "text": "Hey folks, welcome back to another exciting R programming journey! Today, we’re diving into the fascinating world of exponential regression using base R. Exponential regression is a powerful tool, especially in the realm of data science, and we’ll walk through the process step by step. So, grab your coding hats, and let’s get started!"
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-1-your-data",
    "href": "posts/2023-11-20/index.html#step-1-your-data",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 1: Your Data",
    "text": "Step 1: Your Data\n\nYear &lt;- c(2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, \n          2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)\nPopulation &lt;- c(500, 550, 610, 680, 760, 850, 950, 1060, 1180, 1320, 1470, \n                1640, 1830, 2040, 2280, 2540, 2830, 3140, 3480, 3850)\n\ndf &lt;- data.frame(Year, Population)\n\nMake sure to replace “your_data.csv” with the actual file name and path of your dataset. This is the foundation of our analysis, so choose a dataset that suits your exponential regression exploration."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-2-explore-your-data",
    "href": "posts/2023-11-20/index.html#step-2-explore-your-data",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 2: Explore Your Data",
    "text": "Step 2: Explore Your Data\n\n# Take a sneak peek at your data\nhead(df)\n\n  Year Population\n1 2001        500\n2 2002        550\n3 2003        610\n4 2004        680\n5 2005        760\n6 2006        850\n\nsummary(df)\n\n      Year        Population    \n Min.   :2001   Min.   : 500.0  \n 1st Qu.:2006   1st Qu.: 827.5  \n Median :2010   Median :1395.0  \n Mean   :2010   Mean   :1678.0  \n 3rd Qu.:2015   3rd Qu.:2345.0  \n Max.   :2020   Max.   :3850.0  \n\n\nUnderstanding your data is crucial. The ‘head()’ function displays the first few rows, and ‘summary()’ gives you a statistical summary. Look for patterns that might indicate exponential growth or decay."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-3-plot-your-data",
    "href": "posts/2023-11-20/index.html#step-3-plot-your-data",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 3: Plot Your Data",
    "text": "Step 3: Plot Your Data\n\n# Create a scatter plot\nplot(\n  Year, \n  Population, \n  main = \"Exponential Regression\", \n  xlab = \"Independent Variable\", \n  ylab = \"Dependent Variable\"\n)\n\n\n\n\nVisualizing your data helps in identifying trends. A scatter plot is an excellent choice to see if there’s a potential exponential relationship."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-4-fit-exponential-model",
    "href": "posts/2023-11-20/index.html#step-4-fit-exponential-model",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 4: Fit Exponential Model",
    "text": "Step 4: Fit Exponential Model\n\n# Fit exponential regression model\nmodel &lt;- lm(log(Population) ~ Year, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = log(Population) ~ Year, data = df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0134745 -0.0032271  0.0008587  0.0037029  0.0108613 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.113e+02  4.637e-01  -455.7   &lt;2e-16 ***\nYear         1.087e-01  2.307e-04   471.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.005948 on 18 degrees of freedom\nMultiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 \nF-statistic: 2.221e+05 on 1 and 18 DF,  p-value: &lt; 2.2e-16\n\n\nHere, we take the logarithm of the dependent variable ‘y’ to linearize the relationship. This facilitates using linear regression to model the data."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-5-make-predictions",
    "href": "posts/2023-11-20/index.html#step-5-make-predictions",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 5: Make Predictions",
    "text": "Step 5: Make Predictions\n\n# Make predictions\nprediction_interval &lt;- exp(predict(\n  model, \n  newdata = df,\n  interval=\"prediction\",\n  level = 0.95\n  ))\n\nReplace ‘new_x’ with the values for which you want to predict ‘y’. The ‘exp()’ function is used to reverse the logarithmic transformation."
  },
  {
    "objectID": "posts/2023-11-20/index.html#step-6-visualize-results",
    "href": "posts/2023-11-20/index.html#step-6-visualize-results",
    "title": "Mastering Exponential Regression in R: A Step-by-Step Guide",
    "section": "Step 6: Visualize Results",
    "text": "Step 6: Visualize Results\n\n# Plot the original data and the regression line\nplot(df$Year, df$Population, main=\"Exponential Regression\", xlab=\"Year\", \n     ylab=\"Population\", pch=19)\nlines(df$Year, prediction_interval[,1], col=\"red\", lty=2)\nlines(df$Year, prediction_interval[,2], col=\"blue\", lty=2)\nlines(df$Year, prediction_interval[,3], col=\"blue\", lty=2)\nlegend(\"topright\", legend=\"Exponential Regression\", col=\"red\", lwd=2)\n\n\n\n\nThis code adds the exponential regression line to your scatter plot. It’s a visual confirmation of how well your model fits the data."
  },
  {
    "objectID": "posts/2023-11-21/index.html",
    "href": "posts/2023-11-21/index.html",
    "title": "Logarithmic Regression in R: A Step-by-Step Guide with Prediction Intervals",
    "section": "",
    "text": "Introduction\nLogarithmic regression is a statistical technique used to model the relationship between a dependent variable and an independent variable when the relationship is logarithmic. In other words, it is used to model situations where the dependent variable changes at a decreasing rate as the independent variable increases.\nIn this blog post, we will guide you through the process of performing logarithmic regression in R, from data preparation to visualizing the results. We will also discuss how to calculate prediction intervals and plot them along with the regression line.\n\n\nStep 1: Data Preparation\nBefore diving into the analysis, it is essential to ensure that your data is properly formatted and ready for analysis. This may involve data cleaning, checking for missing values, and handling outliers.\n\n\nStep 2: Visualizing the Data\nA quick scatterplot of the dependent variable versus the independent variable can provide valuable insights into the relationship between the two variables. This will help you determine if a logarithmic regression model is appropriate for your data.\n\n# Load the data\nx &lt;- seq(from = 1, to = 100, by = 1)\ny &lt;- log(seq(from = 1000, to = 1, by = -10))\ny &lt;- y * exp(-0.05 * x)\ndata &lt;- data.frame(dependent = y, independent = x)\n\n# Create a scatterplot\nplot(data$independent, data$dependent)\n\n\n\n\n\n\nStep 3: Fitting the Logarithmic Regression Model\nThe lm() function in R can be used to fit a logarithmic regression model. The syntax for fitting a logarithmic regression model is as follows:\n\nmodel &lt;- lm(dependent ~ log(independent), data = data)\n\n\n\nStep 4: Evaluating the Model\nOnce the model has been fitted, it is important to evaluate its performance. There are several metrics that can be used to evaluate the performance of a logarithmic regression model, such as the coefficient of determination (R-squared) and the mean squared error (MSE).\n\nsummary(model)\n\n\nCall:\nlm(formula = dependent ~ log(independent), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.12938 -0.24849 -0.03559  0.23825  0.55343 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       7.70024    0.12087   63.71   &lt;2e-16 ***\nlog(independent) -1.76239    0.03221  -54.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2974 on 98 degrees of freedom\nMultiple R-squared:  0.9683,    Adjusted R-squared:  0.968 \nF-statistic:  2994 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nStep 5: Calculating Prediction Intervals\nPrediction intervals provide a range of values within which we expect the true value of the dependent variable to fall for a given value of the independent variable. There are several methods for calculating prediction intervals, but one common method is to use the predict() function in R.\n\nnewdata &lt;- data.frame(independent = seq(from = 1, to = 100, length.out = 1000))\n\npredictions &lt;- predict(model, \n                       newdata = newdata, \n                       interval = \"prediction\",\n                       level = 0.95)\n\n\n\nStep 6: Plotting the Predictions and Intervals\nPlotting the predictions and intervals along with the regression line can help visualize the relationship between the variables and the uncertainty in the predictions.\n\nplot(data$independent, data$dependent)\nlines(predictions[, 1] ~ newdata$independent, lwd = 2)\nmatlines(newdata$independent, predictions[, 2:3], lty = 2, lwd = 2)\n\n\n\n\n\n\nConclusion\nLogarithmic regression is a powerful statistical technique that can be used to model a variety of relationships between variables. By following the steps outlined in this blog post, you can implement logarithmic regression in R to gain valuable insights from your data.\n\n\nYou Try!!\nWe encourage you to try out logarithmic regression on your own data. Start by exploring the relationship between your variables using a scatterplot. Then, fit a logarithmic regression model using the lm() function and evaluate its performance using the summary() function. Finally, calculate prediction intervals and plot them along with the regression line to visualize the relationship between the variables and the uncertainty in the predictions."
  },
  {
    "objectID": "posts/2023-11-22/index.html",
    "href": "posts/2023-11-22/index.html",
    "title": "Unveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars",
    "section": "",
    "text": "If you’ve ever found yourself grappling with noisy data and yearning for a smoother representation, LOESS regression might be the enchanting solution you’re seeking. In this blog post, we’ll unravel the mysteries of LOESS regression using the power of R, and walk through a practical example using the iconic mtcars dataset."
  },
  {
    "objectID": "posts/2023-11-22/index.html#understanding-loess-the-basics",
    "href": "posts/2023-11-22/index.html#understanding-loess-the-basics",
    "title": "Unveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars",
    "section": "Understanding LOESS: The Basics",
    "text": "Understanding LOESS: The Basics\nNow, let’s delve into the heart of LOESS regression. In R, the magic happens with the loess() function. This function fits a smooth curve through your data, adjusting to the local characteristics.\n\n# Fit a LOESS model\nloess_model &lt;- loess(mpg ~ wt, data = mtcars)\n\nCongratulations, you’ve just cast the LOESS spell on the fuel efficiency and weight relationship of these iconic cars!"
  },
  {
    "objectID": "posts/2023-11-22/index.html#visualizing-the-enchantment",
    "href": "posts/2023-11-22/index.html#visualizing-the-enchantment",
    "title": "Unveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars",
    "section": "Visualizing the Enchantment",
    "text": "Visualizing the Enchantment\nWhat good is magic if you can’t see it? Let’s visualize the results with a compelling plot.\n\n# Generate predictions from the LOESS model\npredictions &lt;- predict(loess_model, newdata = mtcars)\npredictions &lt;- cbind(mtcars, predictions)\npredictions &lt;- predictions[order(predictions$wt), ]\n\n# Create a scatter plot of the original data\nplot(\n  predictions$wt,\n  predictions$mpg, \n  col = \"blue\", \n  main = \"LOESS Regression: Unveiling the Magic with mtcars\", \n  xlab = \"Weight (1000 lbs)\", \n  ylab = \"Miles Per Gallon\"\n)\n\n# Add the LOESS curve to the plot\nlines(predictions$predictions, col = \"red\", lwd = 2)\n\n\n\n\nBehold, as the red curve gracefully dances through the blue points, smoothing out the rough edges and revealing the underlying trends in the relationship between weight and fuel efficiency.\nNow, we did not specify any parameters for the loess() function, so it used the default values. Let’s take a look at the default parameters.\nloess(formula, data, weights, subset, na.action, model = FALSE,\n      span = 0.75, enp.target, degree = 2,\n      parametric = FALSE, drop.square = FALSE, normalize = TRUE,\n      family = c(\"gaussian\", \"symmetric\"),\n      method = c(\"loess\", \"model.frame\"),\n      control = loess.control(...), ...)\nIf you want to see the documentation in R you can use ?loess or help(loess). I have it here for you anyways but it is good to know how to check it on the fly:\nArguments formula - a formula specifying the numeric response and one to four numeric predictors (best specified via an interaction, but can also be specified additively). Will be coerced to a formula if necessary.\ndata - an optional data frame, list or environment (or object coercible by as.data.frame to a data frame) containing the variables in the model. If not found in data, the variables are taken from environment(formula), typically the environment from which loess is called.\nweights - optional weights for each case.\nsubset - an optional specification of a subset of the data to be used.\nna.action - the action to be taken with missing values in the response or predictors. The default is given by getOption(“na.action”).\nmodel - should the model frame be returned?\nspan - the parameter α which controls the degree of smoothing.\nenp.target - an alternative way to specify span, as the approximate equivalent number of parameters to be used.\ndegree - the degree of the polynomials to be used, normally 1 or 2. (Degree 0 is also allowed, but see the ‘Note’.)\nparametric - should any terms be fitted globally rather than locally? Terms can be specified by name, number or as a logical vector of the same length as the number of predictors.\ndrop.square - for fits with more than one predictor and degree = 2, should the quadratic term be dropped for particular predictors? Terms are specified in the same way as for parametric.\nnormalize - should the predictors be normalized to a common scale if there is more than one? The normalization used is to set the 10% trimmed standard deviation to one. Set to false for spatial coordinate predictors and others known to be on a common scale.\nfamily - if “gaussian” fitting is by least-squares, and if “symmetric” a re-descending M estimator is used with Tukey’s biweight function. Can be abbreviated.\nmethod - fit the model or just extract the model frame. Can be abbreviated.\ncontrol - control parameters: see loess.control.\n... - control parameters can also be supplied directly (if control is not specified).\nNow that we see we can set things like span and degree let’s try it out.\n\n# Create the data frame\ndf &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14), \n                 y=c(1, 4, 7, 13, 19, 24, 20, 15, 13, 11, 15, 18, 22, 27))\n\n# Fit LOESS regression models\nloess50 &lt;- loess(y ~ x, data=df, span=0.5)\nsmooth50 &lt;- predict(loess50)\nloess75 &lt;- loess(y ~ x, data=df, span=0.75)\nsmooth75 &lt;- predict(loess75)\nloess90 &lt;- loess(y ~ x, data=df, span=0.9)\nsmooth90 &lt;- predict(loess90)\nloess50_degree1 &lt;- loess(y ~ x, data=df, span=0.5, degree=1)\nsmooth50_degree1 &lt;- predict(loess50_degree1)\nloess50_degree2 &lt;- loess(y ~ x, data=df, span=0.5, degree=2)\nsmooth50_degree2 &lt;- predict(loess50_degree2)\n\n# Create scatterplot with each regression line overlaid\nplot(df$x, df$y, pch=19, main='Loess Regression Models')\nlines(smooth50, x=df$x, col='red')\nlines(smooth75, x=df$x, col='purple')\nlines(smooth90, x=df$x, col='blue')\nlines(smooth50_degree1, x=df$x, col='green')\nlines(smooth50_degree2, x=df$x, col='orange')"
  },
  {
    "objectID": "posts/2023-11-22/index.html#empowering-you-try-it-yourself",
    "href": "posts/2023-11-22/index.html#empowering-you-try-it-yourself",
    "title": "Unveiling the Magic of LOESS Regression in R: A Step-by-Step Guide with mtcars",
    "section": "Empowering You: Try It Yourself!",
    "text": "Empowering You: Try It Yourself!\nNow comes the most exciting part – empowering you to wield the magic wand with the mtcars dataset or any other dataset of your choice. Encourage your readers to try the code on their own datasets, and witness the transformative power of LOESS regression.\n# Your readers can replace this with their own dataset\nuser_data &lt;- read.csv(\"user_dataset.csv\")\n\n# Fit a LOESS model on their data\nuser_loess_model &lt;- loess(Y ~ X, data = user_data)\n\n# Visualize the results\nuser_predictions &lt;- predict(user_loess_model, newdata = user_data)\nplot(user_data$X, user_data$Y, col = \"green\", main = \"Your Turn: Unleash LOESS Magic\", xlab = \"X\", ylab = \"Y\")\nlines(user_data$X, user_predictions, col = \"purple\", lwd = 2)"
  },
  {
    "objectID": "posts/2023-11-16/index.html",
    "href": "posts/2023-11-16/index.html",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "",
    "text": "My R package {healthyR.ts} has been updated to version 0.3.0; you can install it from either CRAN, r-universe or GitHub. Let’s go over some of the changes and improvements."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_log_ts---logging-time-series-data",
    "href": "posts/2023-11-16/index.html#util_log_ts---logging-time-series-data",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "1. util_log_ts() - Logging Time Series Data",
    "text": "1. util_log_ts() - Logging Time Series Data\nOne of the standout additions is the introduction of util_log_ts(). This function seems like a game-changer, providing a streamlined way to log time series data. This is incredibly useful, especially when dealing with extensive datasets, making the whole process more efficient and user-friendly. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_singlediff_ts---single-differences-for-time-series",
    "href": "posts/2023-11-16/index.html#util_singlediff_ts---single-differences-for-time-series",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "2. util_singlediff_ts() - Single Differences for Time Series",
    "text": "2. util_singlediff_ts() - Single Differences for Time Series\nThe addition of util_singlediff_ts() expands the toolkit, offering a function dedicated to handling single differences in time series data. This is valuable for various applications, such as identifying trends or preparing data for further analysis. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_doublediff_ts---double-differences-for-time-series",
    "href": "posts/2023-11-16/index.html#util_doublediff_ts---double-differences-for-time-series",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "3. util_doublediff_ts() - Double Differences for Time Series",
    "text": "3. util_doublediff_ts() - Double Differences for Time Series\nBuilding on the concept of differencing, util_doublediff_ts() seems to provide a higher level of sophistication, allowing users to perform double differences on time series data. This could be pivotal in cases where a more refined analysis is required. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_difflog_ts---combining-differences-and-log-transformation",
    "href": "posts/2023-11-16/index.html#util_difflog_ts---combining-differences-and-log-transformation",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "4. util_difflog_ts() - Combining Differences and Log Transformation",
    "text": "4. util_difflog_ts() - Combining Differences and Log Transformation\nThe fusion of differencing and log transformation in util_difflog_ts() is a remarkable addition. This could be particularly beneficial in scenarios where both operations are needed to unlock deeper insights from the time series data. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#util_doubledifflog_ts---double-differences-with-log-transformation",
    "href": "posts/2023-11-16/index.html#util_doubledifflog_ts---double-differences-with-log-transformation",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "5. util_doubledifflog_ts() - Double Differences with Log Transformation",
    "text": "5. util_doubledifflog_ts() - Double Differences with Log Transformation\nThe introduction of util_doubledifflog_ts() appears to take things a step further by combining double differences and log transformation. This function seems poised to provide a comprehensive solution for users dealing with complex time series data. This is a helper function for auto_stationarize()."
  },
  {
    "objectID": "posts/2023-11-16/index.html#attributes-enhancement-in-ts_growth_rate_vec",
    "href": "posts/2023-11-16/index.html#attributes-enhancement-in-ts_growth_rate_vec",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "1. Attributes Enhancement in ts_growth_rate_vec()",
    "text": "1. Attributes Enhancement in ts_growth_rate_vec()\nThe attention to detail is evident with the addition of attributes to the output of ts_growth_rate_vec(). This enhancement not only improves the clarity of results but also contributes to a more informative and user-friendly experience."
  },
  {
    "objectID": "posts/2023-11-16/index.html#refinement-of-auto_stationarize-in-response-to-user-feedback",
    "href": "posts/2023-11-16/index.html#refinement-of-auto_stationarize-in-response-to-user-feedback",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "2. Refinement of auto_stationarize() in Response to User Feedback",
    "text": "2. Refinement of auto_stationarize() in Response to User Feedback\nUpdates to auto_stationarize() based on user feedback (Fix #481 #483) demonstrate a commitment to refining existing features. This responsiveness to the community’s needs is commendable and ensures that the package evolves in sync with user expectations. It has taken all of the util_ transforms mentioned above in order to improve it’s functionality."
  },
  {
    "objectID": "posts/2023-11-16/index.html#integration-with-auto_arima-engine-in-ts_auto_arima",
    "href": "posts/2023-11-16/index.html#integration-with-auto_arima-engine-in-ts_auto_arima",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "3. Integration with auto_arima Engine in ts_auto_arima()",
    "text": "3. Integration with auto_arima Engine in ts_auto_arima()\nThe integration of ts_auto_arima() with the parsnip engine of auto_arima is a notable improvement. This update, triggered when .tune is set to FALSE, aligns the package with cutting-edge tools, potentially enhancing the efficiency and accuracy of time series modeling.\nIn conclusion, the release of healthyR.ts version 0.3.0 is an exciting leap forward. The new features introduce powerful capabilities, while the minor fixes and improvements showcase a commitment to providing a robust and user-friendly package. Users can look forward to a more versatile and refined experience in time series analysis. Great job on this release, and I’m sure the community is eager to explore these enhancements!"
  },
  {
    "objectID": "posts/2023-11-16/index.html#auto_stationarize",
    "href": "posts/2023-11-16/index.html#auto_stationarize",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "auto_stationarize()",
    "text": "auto_stationarize()\n\nlibrary(healthyR.ts)\n\nauto_stationarize(AirPassengers)\n\nThe time series is already stationary via ts_adf_test().\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nauto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n$stationary_ts\nTime Series:\nStart = 3 \nEnd = 150 \nFrequency = 1 \n  [1]  0.5 -0.4  0.6  1.1 -2.8  3.0 -1.1  0.6 -0.5 -0.5  0.1  2.0 -0.6  0.8  1.2\n [16] -3.4 -0.7 -0.3  1.7  3.0 -3.2  0.9  2.2 -2.5 -0.4  2.6 -4.3  2.0 -3.1  2.7\n [31] -2.1  0.1  2.1 -0.2 -2.2  0.6  1.0 -2.6  3.0  0.3  0.2 -0.8  1.0  0.0  3.2\n [46] -2.2 -4.7  1.2  0.8 -0.6 -0.4  0.6  1.0 -1.6 -0.1  3.4 -0.9 -1.7 -0.5  0.8\n [61]  2.4 -1.9  0.6 -2.2  2.6 -0.1 -2.7  1.7 -0.3  1.9 -2.7  1.1 -0.6  0.9  0.0\n [76]  1.8 -0.5 -0.4 -1.2  2.6 -1.8  1.7 -0.9  0.6 -0.4  3.0 -2.8  3.1 -2.3 -1.1\n [91]  2.1 -0.3 -1.7 -0.8 -0.4  1.1 -1.5  0.3  1.4 -2.0  1.3 -0.3  0.4 -3.5  1.1\n[106]  2.6  0.4 -1.3  2.0 -1.6  0.6 -0.1 -1.4  1.6  1.6 -3.4  1.7 -2.2  2.1 -2.0\n[121] -0.2  0.2  0.7 -1.4  1.8 -0.1 -0.7  0.4  0.4  1.0 -2.4  1.0 -0.4  0.8 -1.0\n[136]  1.4 -1.2  1.1 -0.9  0.5  1.9 -0.6  0.3 -1.4 -0.9 -0.5  1.4  0.1\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -6.562008\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"double_diff\"\n\n$ret\n[1] TRUE\n\nplot.ts(auto_stationarize(BJsales)$stationary_ts)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n\n\nauto_stationarize(BJsales.lead)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\n$stationary_ts\nTime Series:\nStart = 2 \nEnd = 150 \nFrequency = 1 \n  [1]  0.06  0.25 -0.57  0.58 -0.20  0.23 -0.04 -0.19  0.03  0.42  0.04  0.24\n [13]  0.34 -0.46 -0.18 -0.08  0.29  0.56 -0.37  0.20  0.54 -0.31  0.03  0.52\n [25] -0.70  0.35 -0.63  0.44 -0.38 -0.01  0.22  0.10 -0.50  0.01  0.30 -0.76\n [37]  0.52  0.15  0.06 -0.10  0.21 -0.01  0.70 -0.22 -0.76  0.06  0.02 -0.17\n [49] -0.08  0.01  0.11 -0.39  0.01  0.50 -0.02 -0.37 -0.13  0.05  0.54 -0.46\n [61]  0.25 -0.52  0.44  0.02 -0.47  0.11  0.06  0.25 -0.35  0.00 -0.06  0.21\n [73] -0.09  0.36  0.09 -0.04 -0.20  0.44 -0.23  0.40 -0.01  0.17  0.08  0.58\n [85] -0.27  0.79 -0.21  0.02  0.30  0.28 -0.27 -0.01  0.03  0.16 -0.28  0.15\n [97]  0.26 -0.36  0.32 -0.11  0.22 -0.65  0.00  0.47  0.16 -0.19  0.48 -0.26\n[109]  0.21  0.00 -0.20  0.35  0.38 -0.48  0.20 -0.32  0.43 -0.50  0.12 -0.17\n[121]  0.15 -0.36  0.35 -0.03 -0.18  0.16  0.07  0.21 -0.50  0.23 -0.13  0.14\n[133] -0.15  0.19 -0.24  0.26 -0.22  0.17  0.37 -0.06  0.29 -0.34 -0.12 -0.16\n[145]  0.25  0.08 -0.07  0.26 -0.37\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -4.838625\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"diff\"\n\n$ret\n[1] TRUE\n\nplot.ts(auto_stationarize(BJsales.lead)$stationary_ts)\n\nThe time series is not stationary. Attempting to make it stationary..."
  },
  {
    "objectID": "posts/2023-11-16/index.html#ts_auto_arima",
    "href": "posts/2023-11-16/index.html#ts_auto_arima",
    "title": "{healthyR.ts} New Features: Unlocking More Power",
    "section": "ts_auto_arima()",
    "text": "ts_auto_arima()\nThis use to only use the Arima engine if the .tune parameter was set to FALSE, thus it would many times give a simple straight line forecast. This was changed to make the engine auto_arima if .tune is set to FALSE.\n\nlibrary(timetk)\nlibrary(dplyr)\nlibrary(modeltime)\n\ndata &lt;- AirPassengers |&gt;\n  ts_to_tbl() |&gt;\n  select(-index)\n\nsplits &lt;- time_series_split(\n  data\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\nts_aa &lt;- ts_auto_arima(\n  .data = data,\n  .num_cores = 2,\n  .date_col = date_col,\n  .value_col = value,\n  .rsamp_obj = splits,\n  .formula = value ~ .,\n  .grid_size = 5,\n  .cv_slice_limit = 2,\n  .tune = FALSE\n)\n\nts_aa$recipe_info\n\n$recipe_call\nrecipe(.data = data, .date_col = date_col, .value_col = value, \n    .formula = value ~ ., .rsamp_obj = splits, .tune = FALSE, \n    .grid_size = 5, .num_cores = 2, .cv_slice_limit = 2)\n\n$recipe_syntax\n[1] \"ts_arima_recipe &lt;-\"                                                                                                                                                                           \n[2] \"\\n  recipe(.data = data, .date_col = date_col, .value_col = value, .formula = value ~ \\n    ., .rsamp_obj = splits, .tune = FALSE, .grid_size = 5, .num_cores = 2, \\n    .cv_slice_limit = 2)\"\n\n$rec_obj\n\nts_aa$model_info\n\n$model_spec\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nSeries: outcome \nARIMA(1,1,0)(0,1,0)[12] \n\nCoefficients:\n          ar1\n      -0.2431\ns.e.   0.0894\n\nsigma^2 = 109.8:  log likelihood = -447.95\nAIC=899.9   AICc=900.01   BIC=905.46\n\n$was_tuned\n[1] \"not_tuned\"\n\nts_aa$model_calibration\n\n$plot\n\n$calibration_tbl\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc             .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;workflow&gt; ARIMA(1,1,0)(0,1,0)[12] Test  &lt;tibble [12 × 4]&gt;\n\n$model_accuracy\n# A tibble: 1 × 9\n  .model_id .model_desc             .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 ARIMA(1,1,0)(0,1,0)[12] Test   18.5  4.18 0.384  4.03  23.9 0.955\n\nts_aa$model_calibration$plot\n\n\n\n\n\nFinally enhancement to add attributes to ts_growth_rate_vec()\n\nts_growth_rate_vec(AirPassengers)\n\n  [1]          NA   5.3571429  11.8644068  -2.2727273  -6.2015504  11.5702479\n  [7]   9.6296296   0.0000000  -8.1081081 -12.5000000 -12.6050420  13.4615385\n [13]  -2.5423729   9.5652174  11.9047619  -4.2553191  -7.4074074  19.2000000\n [19]  14.0939597   0.0000000  -7.0588235 -15.8227848 -14.2857143  22.8070175\n [25]   3.5714286   3.4482759  18.6666667  -8.4269663   5.5214724   3.4883721\n [31]  11.7977528   0.0000000  -7.5376884 -11.9565217  -9.8765432  13.6986301\n [37]   3.0120482   5.2631579   7.2222222  -6.2176166   1.1049724  19.1256831\n [43]   5.5045872   5.2173913 -13.6363636  -8.6124402  -9.9476440  12.7906977\n [49]   1.0309278   0.0000000  20.4081633  -0.4237288  -2.5531915   6.1135371\n [55]   8.6419753   3.0303030 -12.8676471 -10.9704641 -14.6919431  11.6666667\n [61]   1.4925373  -7.8431373  25.0000000  -3.4042553   3.0837004  12.8205128\n [67]  14.3939394  -2.9801325 -11.6040956 -11.5830116 -11.3537118  12.8078818\n [73]   5.6768559  -3.7190083  14.5922747   0.7490637   0.3717472  16.6666667\n [79]  15.5555556  -4.6703297 -10.0864553 -12.1794872 -13.5036496  17.2995781\n [85]   2.1582734  -2.4647887  14.4404332  -1.2618297   1.5974441  17.6100629\n [91]  10.4278075  -1.9370460 -12.3456790 -13.8028169 -11.4379085  12.9151292\n [97]   2.9411765  -4.4444444  18.2724252  -2.2471910   2.0114943  18.8732394\n[103]  10.1895735   0.4301075 -13.4903640 -14.1089109 -12.1037464  10.1639344\n[109]   1.1904762  -6.4705882  13.8364780  -3.8674033   4.3103448  19.8347107\n[115]  12.8735632   2.8513238 -20.0000000 -11.1386139 -13.6490251   8.7096774\n[121]   6.8249258  -5.0000000  18.7134503  -2.4630542   6.0606061  12.3809524\n[127]  16.1016949   2.0072993 -17.1735242 -12.0950324 -11.0565111  11.8784530\n[133]   2.9629630  -6.2350120   7.1611253  10.0238663   2.3861171  13.3474576\n[139]  16.2616822  -2.5723473 -16.1716172  -9.2519685 -15.4013015  10.7692308\nattr(,\"vector_attributes\")\nattr(,\"vector_attributes\")$tsp\n[1] 1949.000 1960.917   12.000\n\nattr(,\"vector_attributes\")$class\n[1] \"ts\"\n\nattr(,\"name\")\n[1] \"AirPassengers\""
  },
  {
    "objectID": "posts/2023-11-27/index.html",
    "href": "posts/2023-11-27/index.html",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "",
    "text": "In the realm of statistics, power regression stands out as a versatile tool for exploring the relationship between two variables, where one variable is the power of the other. This type of regression is particularly useful when there’s an inherent nonlinear relationship between the variables, often characterized by an exponential or inverse relationship.\nPower regression takes the form of y = ax^b, where:\n\ny: The response variable, the quantity we’re trying to predict\nx: The predictor variable, the quantity we’re using to make predictions\na: The intercept, the value of y when x = 1\nb: The power coefficient, which determines the rate at which y changes as x increases or decreases"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-1-gathering-the-data",
    "href": "posts/2023-11-27/index.html#step-1-gathering-the-data",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 1: Gathering the Data",
    "text": "Step 1: Gathering the Data\nTo embark on our power regression journey, we’ll need some data to work with. Let’s simulate a dataset that exhibits an exponential relationship between two variables:\n\n# Simulate data\nx &lt;- seq(1, 100, 1)\ny &lt;- 2 * x^3 + rnorm(100)"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-2-visualizing-the-data",
    "href": "posts/2023-11-27/index.html#step-2-visualizing-the-data",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 2: Visualizing the Data",
    "text": "Step 2: Visualizing the Data\nBefore diving into the regression analysis, it’s crucial to visualize the data to gain a deeper understanding of the underlying relationship between the variables. A scatterplot can effectively reveal any patterns or trends in the data.\n\n# Create scatterplot\nplot(x, y)"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-3-transforming-the-data",
    "href": "posts/2023-11-27/index.html#step-3-transforming-the-data",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 3: Transforming the Data",
    "text": "Step 3: Transforming the Data\nSince power regression assumes a nonlinear relationship between the variables, we need to transform the data to fit the model’s structure. This involves taking the logarithm of both sides of the power regression equation:\n\n# Transform data\nlog_y &lt;- log(y)\nlog_x &lt;- log(x)"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-4-fitting-the-power-regression-model",
    "href": "posts/2023-11-27/index.html#step-4-fitting-the-power-regression-model",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 4: Fitting the Power Regression Model",
    "text": "Step 4: Fitting the Power Regression Model\nNow that the data is suitably transformed, we can proceed with fitting the power regression model using the lm() function in R:\n\n# Fit power regression model\nmodel &lt;- lm(log_y ~ log_x)"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-5-examining-the-model-results",
    "href": "posts/2023-11-27/index.html#step-5-examining-the-model-results",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 5: Examining the Model Results",
    "text": "Step 5: Examining the Model Results\nThe summary() function provides valuable insights into the model’s performance, including the estimated regression coefficients, their standard errors, and the p-values associated with each coefficient.\n\n# Summarize model results\nsummary(model)\n\n\nCall:\nlm(formula = log_y ~ log_x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10472 -0.01800  0.00221  0.01433  0.61505 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.812631   0.027647   29.39   &lt;2e-16 ***\nlog_x       2.969125   0.007367  403.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06803 on 98 degrees of freedom\nMultiple R-squared:  0.9994,    Adjusted R-squared:  0.9994 \nF-statistic: 1.624e+05 on 1 and 98 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-6-visualizing-the-fitted-model",
    "href": "posts/2023-11-27/index.html#step-6-visualizing-the-fitted-model",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 6: Visualizing the Fitted Model",
    "text": "Step 6: Visualizing the Fitted Model\nVisualizing the fitted model allows us to evaluate how well the model captures the underlying relationship between the variables. We can add the fitted model to the scatterplot using the predict() function (don’t forget to exponentiate!):\n\n# Predict fitted values\nfitted_values &lt;- predict(model, newdata = data.frame(x = x),\n                        interval = \"prediction\",\n                        level = 0.95)\n\n# Add fitted model to scatterplot\nplot(x, y)\nlines(x, exp(fitted_values[, 1]), col = \"red\")"
  },
  {
    "objectID": "posts/2023-11-27/index.html#step-7-calculating-prediction-intervals",
    "href": "posts/2023-11-27/index.html#step-7-calculating-prediction-intervals",
    "title": "Unveiling Power Regression: A Step-by-Step Guide in R",
    "section": "Step 7: Calculating Prediction Intervals",
    "text": "Step 7: Calculating Prediction Intervals\nPrediction intervals provide a range of plausible values for the response variable at a given level of confidence. We calculated the prediction intervals using the predict() function above:\n\n# Add fitted model to scatterplot\nplot(x, y)\nlines(x, exp(fitted_values[, 1]), col = \"red\")\n\n# Add prediction intervals to scatterplot\nlines(x, exp(fitted_values[, 2]), col = \"blue\", lty = 2)\nlines(x, exp(fitted_values[, 3]), col = \"blue\", lty = 2)"
  },
  {
    "objectID": "posts/2023-11-28/index.html",
    "href": "posts/2023-11-28/index.html",
    "title": "Understanding and Implementing Robust Regression in R",
    "section": "",
    "text": "If you’re familiar with linear regression in R, you’ve probably encountered the traditional lm() function. While this is a powerful tool, it might not be the best choice when dealing with outliers or influential observations. In such cases, robust regression comes to the rescue, and in R, the rlm() function from the MASS package is a valuable resource. In this blog post, we’ll delve into the step-by-step process of performing robust regression in R, using a dataset to illustrate the differences between the base R lm model and the robust rlm model."
  },
  {
    "objectID": "posts/2023-11-28/index.html#traditional-linear-regression-lm",
    "href": "posts/2023-11-28/index.html#traditional-linear-regression-lm",
    "title": "Understanding and Implementing Robust Regression in R",
    "section": "Traditional Linear Regression (lm)",
    "text": "Traditional Linear Regression (lm)\n\n# Fit the lm model\nlm_model &lt;- lm(y ~ x1 + x2, data = df)\n\n# Print the summary\nsummary(lm_model)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-72.020 -27.290  -0.138   4.487 124.144 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   41.106     29.940   1.373    0.188\nx1            -0.605      2.066  -0.293    0.773\nx2             1.075      1.857   0.579    0.570\n\nResidual standard error: 49.42 on 17 degrees of freedom\nMultiple R-squared:  0.02203,   Adjusted R-squared:  -0.09303 \nF-statistic: 0.1914 on 2 and 17 DF,  p-value: 0.8275\n\n\nThe lm() function provides a standard linear regression model. However, it assumes that the data follows a normal distribution and is sensitive to outliers. This sensitivity can lead to biased coefficient estimates."
  },
  {
    "objectID": "posts/2023-11-28/index.html#robust-linear-regression-rlm",
    "href": "posts/2023-11-28/index.html#robust-linear-regression-rlm",
    "title": "Understanding and Implementing Robust Regression in R",
    "section": "Robust Linear Regression (rlm)",
    "text": "Robust Linear Regression (rlm)\nNow, let’s contrast this with the robust approach using the rlm() function:\n\n# Load the MASS package\nlibrary(MASS)\n\n# Fit the rlm model\nrobust_model &lt;- rlm(y ~ x1 + x2, data = df)\n\n# Print the summary\nsummary(robust_model)\n\n\nCall: rlm(formula = y ~ x1 + x2, data = df)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.9296  -6.1604  -0.5812   6.4648 170.4612 \n\nCoefficients:\n            Value   Std. Error t value\n(Intercept) 21.4109  5.9703     3.5862\nx1           2.3077  0.4121     5.6004\nx2          -0.2449  0.3703    -0.6615\n\nResidual standard error: 9.369 on 17 degrees of freedom\n\n\nThe rlm() function, part of the MASS package, uses a robust M-estimation approach. It downplays the impact of outliers, making it more suitable for datasets with influential observations."
  },
  {
    "objectID": "posts/2023-11-29/index.html",
    "href": "posts/2023-11-29/index.html",
    "title": "Navigating Quantile Regression with R: A Comprehensive Guide",
    "section": "",
    "text": "Introduction\nQuantile regression is a robust statistical method that goes beyond traditional linear regression by allowing us to model the relationship between variables at different quantiles of the response distribution. In this blog post, we’ll explore how to perform quantile regression in R using the quantreg library.\n\n\nSetting the Stage\nFirst things first, let’s create some data to work with. We’ll generate a data frame df with two variables: ‘hours’ and ‘score’. The relationship between ‘hours’ and ‘score’ will have a bit of noise to make things interesting.\n\n# Create data frame\nhours &lt;- runif(100, 1, 10)\nscore &lt;- 60 + 2 * hours + rnorm(100, mean = 0, sd = 0.45 * hours)\ndf &lt;- data.frame(hours, score)\n\n\n\nVisualizing the Data\nBefore we jump into regression, it’s always a good idea to visualize our data. Let’s start with a scatter plot to get a sense of the relationship between hours and scores.\n\n# Scatter plot\nplot(df$hours, df$score, \n     main = \"Scatter Plot of Hours vs. Score\", \n     xlab = \"Hours\", ylab = \"Score\"\n     )\n\n\n\n\nNow that we’ve got a clear picture of our data, it’s time to perform quantile regression.\n\n\nQuantile Regression with quantreg\nWe’ll use the quantreg library to perform quantile regression. The key function here is rq() (Quantile Regression). We’ll run quantile regression for a few quantiles, say 0.25, 0.5, and 0.75.\n\n# Install and load quantreg if not already installed\n# install.packages(\"quantreg\")\nlibrary(quantreg)\n\n# Quantile regression\nquant_reg_25 &lt;- rq(score ~ hours, data = df, tau = 0.25)\nquant_reg_50 &lt;- rq(score ~ hours, data = df, tau = 0.5)\nquant_reg_75 &lt;- rq(score ~ hours, data = df, tau = 0.75)\n\npurrr::map(list(quant_reg_25, quant_reg_50, quant_reg_75), broom::tidy)\n\n[[1]]\n# A tibble: 2 × 5\n  term        estimate conf.low conf.high   tau\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)    60.3     59.0      61.1   0.25\n2 hours           1.56     1.33      1.82  0.25\n\n[[2]]\n# A tibble: 2 × 5\n  term        estimate conf.low conf.high   tau\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)    60.2     59.6      60.5    0.5\n2 hours           1.96     1.86      2.20   0.5\n\n[[3]]\n# A tibble: 2 × 5\n  term        estimate conf.low conf.high   tau\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)    59.9     59.5      60.7   0.75\n2 hours           2.36     2.16      2.53  0.75\n\npurrr::map(list(quant_reg_25, quant_reg_50, quant_reg_75), broom::glance)\n\n[[1]]\n# A tibble: 1 × 5\n    tau logLik      AIC   BIC df.residual\n  &lt;dbl&gt; &lt;logLik&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;\n1  0.25 -259.6364  523.  528.          98\n\n[[2]]\n# A tibble: 1 × 5\n    tau logLik      AIC   BIC df.residual\n  &lt;dbl&gt; &lt;logLik&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;\n1   0.5 -249.6752  503.  509.          98\n\n[[3]]\n# A tibble: 1 × 5\n    tau logLik      AIC   BIC df.residual\n  &lt;dbl&gt; &lt;logLik&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;\n1  0.75 -252.0106  508.  513.          98\n\n\n\n\nVisualizing Model Performance\nNow, let’s visualize how well our quantile regression models perform. We’ll overlay the regression lines on our scatter plot.\n\n# Scatter plot with regression lines\n# Scatter plot with regression lines\nplot(df$hours, df$score, \n     main = \"Quantile Regression: Hours vs. Score\", \n     xlab = \"Hours\", ylab = \"Score\")\nabline(a = coef(quant_reg_25), \n       b = coef(quant_reg_25)[\"hours\"], \n       col = \"red\", lty = 2)\nabline(a = coef(quant_reg_50), \n       b = coef(quant_reg_50)[\"hours\"], \n       col = \"blue\", lty = 2)\nabline(a = coef(quant_reg_75), \n       b = coef(quant_reg_75)[\"hours\"], \n       col = \"green\", lty = 2)\nlegend(\"topleft\", legend = c(\"Quantile 0.25\", \"Quantile 0.5\", \"Quantile 0.75\"),\n       col = c(\"red\", \"blue\", \"green\"), lty = 2)\n\n\n\n\n\n\nConclusion\nIn this blog post, we delved into the fascinating world of quantile regression using R and the quantreg library. We generated some synthetic data, visualized it, and then performed quantile regression at different quantiles. The final touch was overlaying the regression lines on our scatter plot to visualize how well our models fit the data.\nQuantile regression provides a more nuanced view of the relationship between variables, especially when dealing with skewed or non-normally distributed data. It’s a valuable tool in your statistical toolkit. Happy coding, and may your regressions be ever quantile-wise accurate!"
  },
  {
    "objectID": "posts/2023-12-01/index.html",
    "href": "posts/2023-12-01/index.html",
    "title": "tidyAML: Now supporting gee models",
    "section": "",
    "text": "I am happy to announce that a new version of tidyAML is now available on CRAN. This version includes support for gee models. This is a big step forward for tidyAML as it now supports a wide variety of regression and classification models."
  },
  {
    "objectID": "posts/2023-12-01/index.html#load-library",
    "href": "posts/2023-12-01/index.html#load-library",
    "title": "tidyAML: Now supporting gee models",
    "section": "Load Library",
    "text": "Load Library\n\nlibrary(tidyAML)\n\nNow, let’s build a model that will fail, it’s important I think to see the failure message so you can understand what is happening. It’s likely because the library is not loaded, let’s face it, it has happened to all of us.\n\nlibrary(recipes)\nlibrary(dplyr)\n\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\nfrt_tbl &lt;- fast_regression(\n  mtcars, \n  rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(frt_tbl)\n\nRows: 3\nColumns: 8\n$ .model_id       &lt;int&gt; 1, 2, 3\n$ .parsnip_engine &lt;chr&gt; \"lm\", \"gee\", \"glm\"\n$ .parsnip_mode   &lt;chr&gt; \"regression\", \"regression\", \"regression\"\n$ .parsnip_fns    &lt;chr&gt; \"linear_reg\", \"linear_reg\", \"linear_reg\"\n$ model_spec      &lt;list&gt; [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            &lt;list&gt; [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     &lt;list&gt; [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       &lt;list&gt; [&lt;tbl_df[8 x 1]&gt;], &lt;NULL&gt;, [&lt;tbl_df[8 x 1]&gt;]\n\nfrt_tbl |&gt; pull(pred_wflw) \n\n[[1]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1 22.9 \n2 18.0 \n3 21.1 \n4 32.9 \n5 18.5 \n6 10.4 \n7  9.59\n8 24.7 \n\n[[2]]\nNULL\n\n[[3]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1 22.9 \n2 18.0 \n3 21.1 \n4 32.9 \n5 18.5 \n6 10.4 \n7  9.59\n8 24.7 \n\nfrt_tbl |&gt; pull(fitted_wflw) |&gt; purrr::map(broom::tidy)\n\n[[1]]\n# A tibble: 11 × 5\n   term         estimate std.error statistic p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -40.3       30.5       -1.32   0.209 \n 2 cyl           0.907      1.09       0.830  0.421 \n 3 disp          0.0105     0.0189     0.557  0.587 \n 4 hp           -0.00487    0.0248    -0.196  0.847 \n 5 drat          1.73       2.04       0.846  0.413 \n 6 wt           -5.71       2.35      -2.43   0.0302\n 7 qsec          3.39       1.34       2.54   0.0248\n 8 vs           -3.85       2.99      -1.29   0.220 \n 9 am            2.16       2.29       0.942  0.364 \n10 gear          1.40       1.68       0.838  0.417 \n11 carb          0.200      0.835      0.239  0.815 \n\n[[2]]\n# A tibble: 0 × 0\n\n[[3]]\n# A tibble: 11 × 5\n   term         estimate std.error statistic p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -40.3       30.5       -1.32   0.209 \n 2 cyl           0.907      1.09       0.830  0.421 \n 3 disp          0.0105     0.0189     0.557  0.587 \n 4 hp           -0.00487    0.0248    -0.196  0.847 \n 5 drat          1.73       2.04       0.846  0.413 \n 6 wt           -5.71       2.35      -2.43   0.0302\n 7 qsec          3.39       1.34       2.54   0.0248\n 8 vs           -3.85       2.99      -1.29   0.220 \n 9 am            2.16       2.29       0.942  0.364 \n10 gear          1.40       1.68       0.838  0.417 \n11 carb          0.200      0.835      0.239  0.815 \n\n\nWe see that the gee model failed. This is because we did not load the multilevelmod package. Let’s load it and try again.\n\nlibrary(multilevelmod)\n\nfrt_tbl &lt;- fast_regression(\n  mtcars, \n  rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(frt_tbl)\n\nRows: 3\nColumns: 8\n$ .model_id       &lt;int&gt; 1, 2, 3\n$ .parsnip_engine &lt;chr&gt; \"lm\", \"gee\", \"glm\"\n$ .parsnip_mode   &lt;chr&gt; \"regression\", \"regression\", \"regression\"\n$ .parsnip_fns    &lt;chr&gt; \"linear_reg\", \"linear_reg\", \"linear_reg\"\n$ model_spec      &lt;list&gt; [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            &lt;list&gt; [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     &lt;list&gt; [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       &lt;list&gt; [&lt;tbl_df[8 x 1]&gt;], [&lt;tbl_df[8 x 1]&gt;], [&lt;tbl_df[8 x 1]&gt;…\n\nfrt_tbl |&gt; pull(pred_wflw) \n\n[[1]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1  20.6\n2  15.6\n3  15.8\n4  26.1\n5  27.8\n6  16.6\n7  25.4\n8  21.6\n\n[[2]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1  21.3\n2  15.2\n3  15.3\n4  25.7\n5  27.3\n6  16.5\n7  25.7\n8  20.5\n\n[[3]]\n# A tibble: 8 × 1\n  .pred\n  &lt;dbl&gt;\n1  20.6\n2  15.6\n3  15.8\n4  26.1\n5  27.8\n6  16.6\n7  25.4\n8  21.6\n\nfrt_tbl |&gt; pull(fitted_wflw) |&gt; purrr::map(broom::tidy)\n\n[[1]]\n# A tibble: 11 × 5\n   term        estimate std.error statistic p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -3.70      18.9       -0.195  0.848 \n 2 cyl          0.668      1.03       0.647  0.529 \n 3 disp         0.00831    0.0153     0.544  0.596 \n 4 hp          -0.0124     0.0173    -0.717  0.486 \n 5 drat         2.87       1.44       2.00   0.0674\n 6 wt          -2.87       1.40      -2.05   0.0609\n 7 qsec         0.777      0.618      1.26   0.231 \n 8 vs           0.169      1.64       0.103  0.920 \n 9 am           1.90       1.79       1.07   0.306 \n10 gear         1.31       1.44       0.907  0.381 \n11 carb        -0.601      0.730     -0.823  0.425 \n\n[[2]]\n# A tibble: 10 × 6\n   term         estimate std.error statistic p.value        ``\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  6.54       10.2     0.643     4.01    1.63    \n 2 disp         0.0119      0.0140  0.849     0.0134  0.887   \n 3 hp          -0.0149      0.0165 -0.901     0.0104 -1.43    \n 4 drat         2.36        1.18    2.01      0.859   2.75    \n 5 wt          -3.01        1.35   -2.23      1.35   -2.23    \n 6 qsec         0.577       0.524   1.10      0.193   2.99    \n 7 vs           0.000922    1.58    0.000582  1.07    0.000860\n 8 am           1.35        1.54    0.880     0.886   1.53    \n 9 gear         1.00        1.33    0.752     0.527   1.90    \n10 carb        -0.355       0.611  -0.582     0.378  -0.940   \n\n[[3]]\n# A tibble: 11 × 5\n   term        estimate std.error statistic p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -3.70      18.9       -0.195  0.848 \n 2 cyl          0.668      1.03       0.647  0.529 \n 3 disp         0.00831    0.0153     0.544  0.596 \n 4 hp          -0.0124     0.0173    -0.717  0.486 \n 5 drat         2.87       1.44       2.00   0.0674\n 6 wt          -2.87       1.40      -2.05   0.0609\n 7 qsec         0.777      0.618      1.26   0.231 \n 8 vs           0.169      1.64       0.103  0.920 \n 9 am           1.90       1.79       1.07   0.306 \n10 gear         1.31       1.44       0.907  0.381 \n11 carb        -0.601      0.730     -0.823  0.425"
  },
  {
    "objectID": "posts/2023-12-04/index.html",
    "href": "posts/2023-12-04/index.html",
    "title": "Understanding Spline Regression",
    "section": "",
    "text": "Spline regression is particularly useful when the relationship between the independent and dependent variables is not adequately captured by a linear model. It involves fitting a piecewise continuous curve (spline) to the data. Let’s dive into the process using R."
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-1-load-the-necessary-libraries",
    "href": "posts/2023-12-04/index.html#step-1-load-the-necessary-libraries",
    "title": "Understanding Spline Regression",
    "section": "Step 1: Load the Necessary Libraries",
    "text": "Step 1: Load the Necessary Libraries\n\n# Install and load the required libraries\n# install.packages(\"splines\")\nlibrary(splines)"
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-2-generate-sample-data",
    "href": "posts/2023-12-04/index.html#step-2-generate-sample-data",
    "title": "Understanding Spline Regression",
    "section": "Step 2: Generate Sample Data",
    "text": "Step 2: Generate Sample Data\nFor our example, let’s create a hypothetical dataset:\n\n# Generate sample data\nset.seed(123)\nx &lt;- seq(1, 10, length.out = 100)\ny &lt;- 3 * sin(x) + rnorm(100, mean = 0, sd = 0.5)"
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-3-fit-a-spline-regression-model",
    "href": "posts/2023-12-04/index.html#step-3-fit-a-spline-regression-model",
    "title": "Understanding Spline Regression",
    "section": "Step 3: Fit a Spline Regression Model",
    "text": "Step 3: Fit a Spline Regression Model\nNow, let’s fit a spline regression model to our data:\n\n# Fit a spline regression model\nspline_model &lt;- lm(y ~ ns(x, df = 4))\n\nHere, ns from the splines package is used to create a natural spline basis with 4 degrees of freedom."
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-4-visualize-the-results",
    "href": "posts/2023-12-04/index.html#step-4-visualize-the-results",
    "title": "Understanding Spline Regression",
    "section": "Step 4: Visualize the Results",
    "text": "Step 4: Visualize the Results\nVisualizing the data and the fitted spline is crucial for understanding the model’s performance:\n\n# Visualize the data and fitted spline\nplot(x, y, main = \"Spline Regression Example\", xlab = \"X\", ylab = \"Y\")\nlines(x, predict(spline_model), col = \"red\", lwd = 2)\nlegend(\"topright\", legend = \"Fitted Spline\", col = \"red\", lwd = 2)\n\n\n\n\nThis code generates a plot with the original data points and overlays the fitted spline."
  },
  {
    "objectID": "posts/2023-12-04/index.html#step-5-examine-residuals",
    "href": "posts/2023-12-04/index.html#step-5-examine-residuals",
    "title": "Understanding Spline Regression",
    "section": "Step 5: Examine Residuals",
    "text": "Step 5: Examine Residuals\nChecking residuals helps assess the model’s goodness of fit:\n\n# Examine residuals\nresiduals &lt;- residuals(spline_model)\nplot(x, residuals, main = \"Residuals of Spline Regression\", xlab = \"X\", \n     ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\nThis plot shows the residuals (the differences between observed and predicted values) against the independent variable."
  },
  {
    "objectID": "posts/2023-12-05/index.html",
    "href": "posts/2023-12-05/index.html",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "",
    "text": "Hey folks! 👋 Today, let’s embark on a coding adventure and explore the fascinating world of Polynomial Regression in R. Whether you’re new to R or a seasoned coder, we’re going to break down the complexities and make this journey enjoyable and insightful."
  },
  {
    "objectID": "posts/2023-12-05/index.html#step-1-set-the-stage",
    "href": "posts/2023-12-05/index.html#step-1-set-the-stage",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "Step 1: Set the Stage",
    "text": "Step 1: Set the Stage\nFirst things first, fire up your RStudio and load your favorite dataset. For our journey, I’ll use a hypothetical dataset about, say, the growth of healthyR packages over the years.\n\n# Assume 'years' and 'growth' are our dataset columns\ndata &lt;- data.frame(years = c(1, 2, 3, 4, 5),\n                   growth = c(10, 25, 40, 60, 90))\n\n# Visualize the data\nplot(data$years, data$growth, \n     main = \"HealthyR Package Growth Over the Years\",\n     xlab = \"Years\", ylab = \"Growth\", col = \"blue\", pch = 16)"
  },
  {
    "objectID": "posts/2023-12-05/index.html#step-2-lets-fit-a-polynomial",
    "href": "posts/2023-12-05/index.html#step-2-lets-fit-a-polynomial",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "Step 2: Let’s Fit a Polynomial",
    "text": "Step 2: Let’s Fit a Polynomial\nNow, let’s fit a polynomial regression model to our data. We’ll use the lm() function, and don’t worry, it’s simpler than it sounds!\n\n# Fit a polynomial regression model (let's go quadratic)\nmodel &lt;- lm(growth ~ poly(years, 2), data = data)"
  },
  {
    "objectID": "posts/2023-12-05/index.html#step-3-visualize-the-magic",
    "href": "posts/2023-12-05/index.html#step-3-visualize-the-magic",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "Step 3: Visualize the Magic",
    "text": "Step 3: Visualize the Magic\nTime to visualize the results. We’ll create a smooth curve representing our polynomial fit, compare it against the actual data, and also peek at the residuals.\n\n# Generate points for smooth curve\ncurve_data &lt;- data.frame(years = seq(1, 5, length.out = 100))\n\n# Predict growth based on the model\npredictions &lt;- predict(model, newdata = curve_data)\n\n# The data\nplot(data$years, data$growth, \n     main = \"HealthyR Package Growth Over the Years\",\n     xlab = \"Years\", ylab = \"Growth\", col = \"blue\", pch = 16)\n# Visualize the fitted model\nlines(curve_data$years, predictions, col = \"red\", type = \"l\")"
  },
  {
    "objectID": "posts/2023-12-05/index.html#step-4-assess-residuals",
    "href": "posts/2023-12-05/index.html#step-4-assess-residuals",
    "title": "Unveiling the Magic of Polynomial Regression in R: A Step-by-Step Guide",
    "section": "Step 4: Assess Residuals",
    "text": "Step 4: Assess Residuals\nTo ensure our model is doing its job, let’s examine the residuals. These are the differences between our predictions and the actual values.\n\n# Calculate residuals\nresiduals &lt;- residuals(model)\n\n# Visualize the residuals\nplot(data$years, residuals, main = \"Residuals Analysis\",\n     xlab = \"Years\", ylab = \"Residuals\", col = \"green\", pch = 16)\nabline(h = 0, col = \"red\", lty = 2)"
  },
  {
    "objectID": "posts/2023-12-06/index.html",
    "href": "posts/2023-12-06/index.html",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "",
    "text": "Stepwise regression is a powerful technique used to build predictive models by iteratively adding or removing variables based on statistical criteria. In R, this can be achieved using functions like step() or manually with forward and backward selection."
  },
  {
    "objectID": "posts/2023-12-06/index.html#forward-stepwise-regression",
    "href": "posts/2023-12-06/index.html#forward-stepwise-regression",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "Forward Stepwise Regression:",
    "text": "Forward Stepwise Regression:\n\n# Initialize model\nforward_model &lt;- lm(mpg ~ ., data = mtcars)\n\n# Forward stepwise regression\nforward_model &lt;- step(forward_model, direction = \"forward\", scope = formula(~ .))\n\nStart:  AIC=70.9\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n\nIn simple terms, we start with a model containing all of the predictors (mpg ~ .) and iteratively add the most statistically significant variables until no improvement is observed."
  },
  {
    "objectID": "posts/2023-12-06/index.html#backward-stepwise-regression",
    "href": "posts/2023-12-06/index.html#backward-stepwise-regression",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "Backward Stepwise Regression:",
    "text": "Backward Stepwise Regression:\n\n# Initialize a model with all predictors\nbackward_model &lt;- lm(mpg ~ ., data = mtcars)\n\n# Backward stepwise regression\nbackward_model &lt;- step(backward_model, direction = \"backward\", trace = 0)\n\nHere, we begin with a model including all predictors and iteratively remove the least statistically significant variables until the model no longer improves."
  },
  {
    "objectID": "posts/2023-12-06/index.html#both-direction-stepwise-regression",
    "href": "posts/2023-12-06/index.html#both-direction-stepwise-regression",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "Both-Direction Stepwise Regression:",
    "text": "Both-Direction Stepwise Regression:\n\n# Initialize a model with all predictors\nboth_model &lt;- lm(mpg ~ ., data = mtcars)\n\n# Both-direction stepwise regression\nboth_model &lt;- step(both_model, direction = \"both\", trace = 0)\n\nIn both-direction regression, the algorithm combines both forward and backward steps, optimizing the model by adding significant variables and removing insignificant ones.\n\nVisualizing Data and Model Fit:\nNow, let’s visualize the data and model fit using base R plots.\n\n# Scatter plot of mpg vs. hp\nplot(mtcars$hp, mtcars$mpg, \n     main = \"Scatter Plot of mpg vs. hp\", \n     xlab = \"hp\", ylab = \"mpg\", pch = 20\n     )\nabline(lm(mpg ~ hp, data = mtcars), col = \"black\", lwd = 2)\npoints(sort(mtcars$hp), intercept_model$fitted.values, col = \"purple\", pch = 20)\npoints(sort(mtcars$hp), forward_model$fitted.values, col = \"red\", pch = 20)\npoints(sort(mtcars$hp), backward_model$fitted.values, col = \"blue\", pch = 20)\npoints(sort(mtcars$hp), both_model$fitted.values, col = \"green\", pch = 20)\n\nlegend(\n  \"topright\", \n  legend = c(\n    \"Intercept Only\", \n    \"Forward\", \n    \"Backward\", \n    \"Both-Direction\"\n    ),\n  col = c(\"red\", \"blue\", \"green\"), pch = 20\n)\n\n\n\n\nThis plot displays the scatter plot of mpg against hp with fitted lines for each stepwise regression. The colors correspond to the models created earlier.\n\n\nVisualizing Residuals:\n\n# Residual plots for each model\npar(mfrow = c(2, 2))\n\n# Intercept Model\nplot(intercept_model$residuals, main = \"Intercept Residuals\", ylab = \"Residuals\")\n\n# Forward stepwise regression residuals\nplot(forward_model$residuals, main = \"Forward Residuals\", ylab = \"Residuals\")\n\n# Backward stepwise regression residuals\nplot(backward_model$residuals, main = \"Backward Residuals\", ylab = \"Residuals\")\n\n# Both-direction stepwise regression residuals\nplot(both_model$residuals, main = \"Both-Direction Residuals\", ylab = \"Residuals\")\n\n\n\npar(mfrow = c(1, 1))\n\nThese plots help assess how well the models fit the data by examining the residuals."
  },
  {
    "objectID": "posts/2023-12-07/index.html",
    "href": "posts/2023-12-07/index.html",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "",
    "text": "Hey there, fellow R enthusiasts! Today, let’s embark on a fascinating journey into the realm of piecewise regression using R. If you’ve ever wondered how to uncover hidden trends and breakpoints in your data, you’re in for a treat. Buckle up, and let’s dive into the world of piecewise regression!"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-1-load-your-data-and-libraries",
    "href": "posts/2023-12-07/index.html#step-1-load-your-data-and-libraries",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 1: Load Your Data and Libraries",
    "text": "Step 1: Load Your Data and Libraries\n\n# Install and load necessary packages\n# install.packages(\"segmented\")\nlibrary(segmented)\n\n# Sample data\nset.seed(123)\nx &lt;- 1:100\ny &lt;- 2 + 1.5 * pmax(x - 35, 0) - 1.5 * pmax(x - 70, 0) + rnorm(100)\n\n# Combine data\ndata &lt;- data.frame(x, y)"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-2-explore-your-data",
    "href": "posts/2023-12-07/index.html#step-2-explore-your-data",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 2: Explore Your Data",
    "text": "Step 2: Explore Your Data\nBefore diving into the regression, let’s take a peek at our data. Visualizing the data often provides insights into potential breakpoints.\n\n# Scatter plot to visualize the data\nplot(\n  data$x, data$y, \n  main = \"Scatter Plot of Your Data\",\n  xlab = \"Independent Variable (x)\", \n  ylab = \"Dependent Variable (y)\")"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-3-perform-piecewise-regression",
    "href": "posts/2023-12-07/index.html#step-3-perform-piecewise-regression",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 3: Perform Piecewise Regression",
    "text": "Step 3: Perform Piecewise Regression\nNow, the exciting part! Let’s fit our piecewise regression model using the segmented package.\n\n# Fit the piecewise regression model\nmodel &lt;- lm(y ~ x, data = data)\nsegmented_model &lt;- segmented(model, seg.Z = ~x)"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-4-visualize-the-results",
    "href": "posts/2023-12-07/index.html#step-4-visualize-the-results",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 4: Visualize the Results:",
    "text": "Step 4: Visualize the Results:\nTo truly understand the magic happening, let’s visualize the fitted model and residuals.\n\nseg_preds &lt;- predict(segmented_model)\nseg_res &lt;- y - seg_preds\n\n# Plot the original data with the fitted model\nplot(\n  data$x, data$y,\n  main = \"Piecewise Regression Fit\",\n  xlab = \"Independent Variable (x)\",\n  ylab = \"Dependent Variable (y)\",\n  col = \"blue\"\n)\nlines(data$x, seg_preds,col = \"red\", lwd = 2)\n\n\n\n# Plot residuals\n# Plot the residuals for each segment\nplot(x, seg_res, main = \"Residuals\")\nabline(h = 0, col = \"red\")"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-5-interpret-the-breakpoints",
    "href": "posts/2023-12-07/index.html#step-5-interpret-the-breakpoints",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 5: Interpret the Breakpoints:",
    "text": "Step 5: Interpret the Breakpoints:\nInspecting the segmented model will reveal the breakpoints and the corresponding regression lines. It’s like deciphering the story your data is trying to tell.\n\n# View breakpoints and coefficients\nsummary(segmented_model)\n\n\n    ***Regression Model with Segmented Relationship(s)***\n\nCall: \nsegmented.lm(obj = model, seg.Z = ~x)\n\nEstimated Break-Point(s):\n          Est. St.Err\npsi1.x 24.757  3.074\n\nCoefficients of the linear terms:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  2.49825    2.55867   0.976    0.331\nx           -0.04055    0.17907  -0.226    0.821\nU1.x         0.93569    0.18186   5.145       NA\n\nResidual standard error: 6.073 on 96 degrees of freedom\nMultiple R-Squared: 0.9333,  Adjusted R-squared: 0.9312 \n\nBoot restarting based on 6 samples. Last fit:\nConvergence attained in 2 iterations (rel. change 2.9855e-12)"
  },
  {
    "objectID": "posts/2023-12-07/index.html#step-6-encourage-exploration",
    "href": "posts/2023-12-07/index.html#step-6-encourage-exploration",
    "title": "Unraveling Patterns: A Step-by-Step Guide to Piecewise Regression in R",
    "section": "Step 6: Encourage Exploration:",
    "text": "Step 6: Encourage Exploration:\nNow that you’ve conquered piecewise regression, encourage your fellow data explorers to try it themselves. Challenge them to apply this technique to their datasets and share their insights."
  },
  {
    "objectID": "posts/2023-12-08/index.html",
    "href": "posts/2023-12-08/index.html",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "",
    "text": "If you’re a data enthusiast diving into the world of regression analysis in R, you’ve likely encountered the challenges of managing code complexity and juggling different modeling engines. The good news is that there’s a powerful tool to streamline your regression workflow – the tidyAML R package."
  },
  {
    "objectID": "posts/2023-12-08/index.html#try-it-yourself",
    "href": "posts/2023-12-08/index.html#try-it-yourself",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nNow that you’ve seen the power of tidyAML in action, it’s time to try it yourself. Install the package, load your data, and adapt the script to your specific use case. TidyAML provides a clean and efficient way to explore different regression models, making your analysis more manageable and insightful.\ninstall.packages(\"tidyAML\")\nlibrary(tidyAML)\n# Your data loading and analysis code here\nHappy coding, and may your regression analyses be tidy and insightful!"
  },
  {
    "objectID": "posts/2023-12-08/index.html#setting-up-the-recipe",
    "href": "posts/2023-12-08/index.html#setting-up-the-recipe",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "Setting Up the Recipe",
    "text": "Setting Up the Recipe\n\ndf &lt;- mtcars\nrecipe &lt;- recipe(mpg ~ ., data = df)\n\nIn this snippet, we’re creating a recipe for our regression analysis. The response variable (mpg) is modeled against all other variables in the mtcars dataset."
  },
  {
    "objectID": "posts/2023-12-08/index.html#fast-regression-with-tidyaml",
    "href": "posts/2023-12-08/index.html#fast-regression-with-tidyaml",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "Fast Regression with TidyAML",
    "text": "Fast Regression with TidyAML\n\nfr_tbl &lt;- fast_regression(\n  .data = df,\n  .rec_obj = recipe,\n  .parsnip_fns = c(\"linear_reg\", \"mars\", \"bag_mars\", \"rand_forest\",\n                   \"boost_tree\", \"bag_tree\"),\n  .parsnip_eng = c(\"lm\", \"gee\", \"glm\", \"gls\", \"earth\", \"rpart\", \"lightgbm\")\n)\n\nThis is where the magic happens. The fast_regression function performs regression using various modeling functions (linear_reg, mars, etc.) and engines (lm, gee, etc.) specified. It’s a versatile approach to quickly explore different models."
  },
  {
    "objectID": "posts/2023-12-08/index.html#visualizing-residuals",
    "href": "posts/2023-12-08/index.html#visualizing-residuals",
    "title": "Exploring TidyAML: Simplifying Regression Analysis in R",
    "section": "Visualizing Residuals",
    "text": "Visualizing Residuals\n\nfr_tbl |&gt;\n  mutate(res = map(fitted_wflw, \\(x) x |&gt; \n                     broom::augment(new_data = df))) |&gt;\n  unnest(cols = res) |&gt;\n  mutate(pfe = paste0(.parsnip_engine, \" - \", .parsnip_fns)) |&gt;\n  mutate(.res = mpg - .pred) |&gt;\n  ggplot(aes(x = pfe, y = .res, fill = pfe)) +\n    geom_boxplot() +\n    theme_minimal() +\n    labs(title = \"Residuals by Fitted Model\",\n       subtitle = \"Residuals are mpg - .pred\",\n       x = \"Model\",\n       y = \"Residuals\",\n       fill = \"Engine + Function\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nThis block of code generates a boxplot visualizing residuals by model. Residuals are the differences between observed and predicted values. The plot helps you assess how well your models are performing."
  },
  {
    "objectID": "posts/2023-12-12/index.html",
    "href": "posts/2023-12-12/index.html",
    "title": "Conquering Unequal Variance with Weighted Least Squares in R: A Practical Guide",
    "section": "",
    "text": "Tired of your least-squares regression model giving wonky results because some data points shout louder than others? Meet Weighted Least Squares (WLS), the superhero of regression, ready to tackle unequal variance (heteroscedasticity) and give your model the justice it deserves! Today, we’ll dive into the world of WLS in R, using base functions for maximum transparency. Buckle up, data warriors!"
  },
  {
    "objectID": "posts/2023-12-12/index.html#steps",
    "href": "posts/2023-12-12/index.html#steps",
    "title": "Conquering Unequal Variance with Weighted Least Squares in R: A Practical Guide",
    "section": "Steps",
    "text": "Steps\nStep 1: Gathering the Troops (Data):\nLet’s create some simulated data:\n\n# Generate exam scores and study hours\nset.seed(123)\nscores &lt;- rnorm(100, mean = 70, sd = 10)\nhours &lt;- rnorm(100, mean = 20, sd = 5)\nhours &lt;- rnorm(100, mean = 0, sd = hours * 0.2) # Add heteroscedasticity\n\n# Create a data frame\ndata &lt;- data.frame(scores, hours)\n\nStep 2: Visualizing the Battlefield:\nA scatter plot is our trusty map:\n\nplot(data$hours, data$scores)\n\n\n\n\nDo you see those clusters of high-scoring students with more study hours? They’re the loud ones skewing the OLS line.\nStep 3: Building the WLS Wall:\nIt’s time to define our weights. We want to give less weight to observations with high variance (those loud students) and more weight to those with low variance. Here’s a simple approach:\n\n# Calculate inverse of variance\nweights &lt;- 1 / (data$hours)^2\n\n# Fit WLS model\nwls_model &lt;- lm(scores ~ hours, weights = weights, data = data)\n\nStep 4: Inspecting the Model’s Performance:\nLet’s see if WLS silenced the loud ones:\n\nsummary(wls_model)\n\n\nCall:\nlm(formula = scores ~ hours, data = data, weights = weights)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-75.854  -1.456   0.927   3.509  57.472 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   68.524      0.632 108.421   &lt;2e-16 ***\nhours         -1.085      1.480  -0.733    0.465    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.65 on 98 degrees of freedom\nMultiple R-squared:  0.00545,   Adjusted R-squared:  -0.004698 \nF-statistic: 0.537 on 1 and 98 DF,  p-value: 0.4654\n\n\nCompare this summary to your OLS model’s. Do the coefficients and residuals look more sensible?\nStep 5: Visualizing the Conquered Land:\nTime to see if WLS straightened the line:\n\nplot(data$hours, data$scores)\nlines(data$hours, wls_model$fitted, col = \"red\")\n\n\n\n\nNotice how the red WLS line now passes closer to the majority of data points, unlike the blue OLS line that chased the loud ones.\nStep 6: Residuals: The Echoes of Battle:\nLet’s see if the residuals (errors) are under control:\n\nplot(data$hours, wls_model$residuals)\n\n\n\n\nA random scatterplot of residuals is a good sign! No more funky patterns indicating heteroscedasticity.\nThe Victory Lap:\nWLS has restored justice to your regression model! Remember, this is just a basic example. You can customize your weights based on your specific data and needs.\nNow it’s your turn! Try WLS on your own data and see the magic unfold. Remember, data analysis is an adventure, and WLS is your trusty steed. Ride on, data warrior!\nBonus Tip: Check out the lmtest and sandwich packages for even more advanced WLS analysis.\nHappy coding!"
  },
  {
    "objectID": "posts/2023-12-14/index.html",
    "href": "posts/2023-12-14/index.html",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "",
    "text": "Ever run an R regression and stared at the output, feeling like you’re deciphering an ancient scroll? Fear not, fellow data enthusiasts! Today, we’ll crack the code and turn those statistics into meaningful insights.\nLet’s grab our trusty R arsenal and set up the scene:\n\nDataset: mtcars (a classic car dataset in R)\nRegression: Linear model with mpg as the dependent variable (miles per gallon) and all other variables as independent variables (predictors)"
  },
  {
    "objectID": "posts/2023-12-14/index.html#coefficients",
    "href": "posts/2023-12-14/index.html#coefficients",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "Coefficients",
    "text": "Coefficients\nThese tell you how much, on average, the dependent variable changes for a one-unit increase in the corresponding independent variable (holding other variables constant). For example, a coefficient of 0.05 for cyl means for every one more cylinder, mpg is expected to increase by 0.05 miles per gallon, on average.\n\nmodel$coefficients\n\n(Intercept)         cyl        disp          hp        drat          wt \n12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 \n       qsec          vs          am        gear        carb \n 0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925"
  },
  {
    "objectID": "posts/2023-12-14/index.html#p-values",
    "href": "posts/2023-12-14/index.html#p-values",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "P-values",
    "text": "P-values\nThese whisper secrets about significance. A p-value less than 0.05 would mean the observed relationship between the variable and mpg is unlikely to be due to chance. The following are the individual p-values for each variable:\n\nsummary(model)$coefficients[, 4]\n\n(Intercept)         cyl        disp          hp        drat          wt \n 0.51812440  0.91608738  0.46348865  0.33495531  0.63527790  0.06325215 \n       qsec          vs          am        gear        carb \n 0.27394127  0.88142347  0.23398971  0.66520643  0.81217871 \n\n\nNow the overall p-value for the model:\n\nmodel_p &lt;- function(.model) {\n  \n  # Get p-values\n  fstat &lt;- summary(.model)$fstatistic\n  p &lt;- pf(fstat[1], fstat[2], fstat[3], lower.tail = FALSE)\n  print(p)\n}\n\nmodel_p(.model = model)\n\n       value \n3.793152e-07"
  },
  {
    "objectID": "posts/2023-12-14/index.html#coefficients-1",
    "href": "posts/2023-12-14/index.html#coefficients-1",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "Coefficients",
    "text": "Coefficients\nThink of them as slopes. A positive coefficient means the dependent variable increases with the independent variable. Negative? The opposite! For example, disp has a negative coefficient, so bigger engines (larger displacement) tend to have lower mpg."
  },
  {
    "objectID": "posts/2023-12-14/index.html#p-values-1",
    "href": "posts/2023-12-14/index.html#p-values-1",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "P-values",
    "text": "P-values\nImagine a courtroom. A low p-value is like a strong witness, convincing you the relationship between the variables is real. High p-values (like for am!) are like unreliable witnesses, leaving us unsure."
  },
  {
    "objectID": "posts/2023-12-14/index.html#r-squared",
    "href": "posts/2023-12-14/index.html#r-squared",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "R-squared",
    "text": "R-squared\nThis tells you how well the model explains the variation in mpg. A value close to 1 is fantastic, while closer to 0 means the model needs work. In our case, it’s not bad, but there’s room for improvement.\n\nsummary(model)$r.squared\n\n[1] 0.8690158"
  },
  {
    "objectID": "posts/2023-12-14/index.html#residuals",
    "href": "posts/2023-12-14/index.html#residuals",
    "title": "Decoding the Mystery: How to Interpret Regression Output in R Like a Champ",
    "section": "Residuals",
    "text": "Residuals\nThese are the differences between the actual mpg values and the model’s predictions. Analyzing them can reveal hidden patterns and model issues.\n\ndata.frame(model$residuals)\n\n                    model.residuals\nMazda RX4              -1.599505761\nMazda RX4 Wag          -1.111886079\nDatsun 710             -3.450644085\nHornet 4 Drive          0.162595453\nHornet Sportabout       1.006565971\nValiant                -2.283039036\nDuster 360             -0.086256253\nMerc 240D               1.903988115\nMerc 230               -1.619089898\nMerc 280                0.500970058\nMerc 280C              -1.391654392\nMerc 450SE              2.227837890\nMerc 450SL              1.700426404\nMerc 450SLC            -0.542224699\nCadillac Fleetwood     -1.634013415\nLincoln Continental    -0.536437711\nChrysler Imperial       4.206370638\nFiat 128                4.627094192\nHonda Civic             0.503261089\nToyota Corolla          4.387630904\nToyota Corona          -2.143103442\nDodge Challenger       -1.443053221\nAMC Javelin            -2.532181498\nCamaro Z28             -0.006021976\nPontiac Firebird        2.508321011\nFiat X1-9              -0.993468693\nPorsche 914-2          -0.152953961\nLotus Europa            2.763727417\nFord Pantera L         -3.070040803\nFerrari Dino            0.006171846\nMaserati Bora           1.058881618\nVolvo 142E             -2.968267683\n\n\nBonus Tip: Visualize the data! Scatter plots and other graphs can make relationships between variables pop.\nRemember: Interpreting regression output is an art, not a science. Use your domain knowledge, consider the context, and don’t hesitate to explore further!\nSo next time you face regression output, channel your inner R wizard and remember:\n\nCoefficients whisper about slopes and changes.\nP-values tell tales of significance, true or false.\nR-squared unveils the model’s explanatory magic.\nResiduals hold hidden clues, waiting to be discovered.\n\nWith these tools in your belt, you’ll be interpreting regression output like a pro in no time! Now go forth and conquer the data, fellow R adventurers!\nNote: This is just a brief example. For a deeper dive, explore specific diagnostics, model selection techniques, and other advanced topics to truly master the art of regression interpretation."
  },
  {
    "objectID": "posts/2023-12-15/index.html",
    "href": "posts/2023-12-15/index.html",
    "title": "Demystifying Odds Ratios in Logistic Regression: Your R Recipe for Loan Defaults",
    "section": "",
    "text": "Introduction\nEver wondered why some individuals default on loans while others don’t? Logistic regression can shed light on this, and calculating odds ratios in R is the secret sauce. So, strap on your data aprons, folks, and let’s cook up some insights!\n\n\nWhat are Odds Ratios?\nImagine a loan officer flipping a coin to decide whether to approve your loan. Odds ratios tell you how much more likely one factor (like your income) makes the “heads” (approval) side appear compared to another (like your student status).\nIn logistic regression, odds ratios compare the odds of an event (loan default, in our case) for two groups defined by a specific variable. They’re like multipliers: greater than 1 means something increases the chances of default, while less than 1 means it decreases them.\n\n\nThe R Recipe (with ISLR Flavor)\n\nGather your ingredients: Load the ISLR package and the Default dataset. This data tells us whether individuals defaulted on loans, their student status, bank balance, and income.\nWhip up the model: Use the glm() function with family='binomial' to fit a logistic regression model that predicts loan defaults based on student status, balance, and income. Think of it as the base for your delicious insights.\nExtract the spices: Use the summary() function to access the estimated coefficients for each variable. These are the secret ingredients that give your model flavor.\nUnleash the magic of exponentiation: Apply the exp() function to transform the coefficients back to the odds ratio scale. Remember, logistic regression operates on log-odds, so we need to break the code.\nSavor the results: Analyze the odds ratios. Are they greater than 1? Those factors increase default odds. Less than 1? They decrease them. A value near 1 suggests little to no effect.\n\n\n\nExample Time\n\n# Load ISLR package and data\nlibrary(ISLR)\n\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n# Fit the model\nmodel &lt;- glm(default~student+balance+income, family='binomial', data=Default)\n\n#disable scientific notation for model summary\noptions(scipen=999)\n\n# Extract and exponentiate coefficients\nodds_ratios &lt;- exp(coef(model))\n\n# Print the odds ratios\ncat(\"Odds ratios:\")\n\nOdds ratios:\n\nprint(odds_ratios)\n\n  (Intercept)    studentYes       balance        income \n0.00001903854 0.52373166965 1.00575299051 1.00000303345 \n\ncat(\"Odds ratios with confidence intervals:\")\n\nOdds ratios with confidence intervals:\n\nexp(cbind(Odds_Ratio = coef(model), confint(model)))\n\nWaiting for profiling to be done...\n\n\n               Odds_Ratio          2.5 %       97.5 %\n(Intercept) 0.00001903854 0.000007074481 0.0000487808\nstudentYes  0.52373166965 0.329882707270 0.8334223982\nbalance     1.00575299051 1.005308940686 1.0062238757\nincome      1.00000303345 0.999986952969 1.0000191246\n\n\nInterpretation time! Being a student decreases default with log odds by -0.646, while higher income leaves log odds basically flat.\nGo Forth and Experiment!\nThis is just the tip of the iceberg! Play around with different models, variables, and visualizations using RStudio. Remember, the more you experiment, the better you’ll understand the magic of odds ratios and logistic regression. Now, go forth and analyze!\nBonus Tip: Check out the confint() function to calculate confidence intervals for your odds ratios. This adds another layer of spice to your statistical analysis!\nSo, there you have it! Odds ratios in R, made easy with the ISLR package and a dash of culinary magic. Remember, the key ingredients are understanding, practice, and a sprinkle of creativity. Bon appétit, data chefs!"
  },
  {
    "objectID": "posts/2023-12-18/index.html",
    "href": "posts/2023-12-18/index.html",
    "title": "Exploring Variance Inflation Factor (VIF) in R: A Practical Guide",
    "section": "",
    "text": "Introduction\nHey there fellow R enthusiasts! Today, we’re diving into the fascinating world of Variance Inflation Factor (VIF) and how to calculate it using R. VIF is a crucial metric that helps us understand the level of multicollinearity among predictors in a regression model. So, buckle up your seatbelts, and let’s embark on this coding adventure!\n\n\nSetting the Stage\nLet’s start by setting up our stage. We’ll use a linear regression model with the mtcars dataset. Here’s the model we’re going to work with:\n\n# Setting up the model\nmodel &lt;- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n\n\n\nCalculating VIF with car library\nNow, the exciting part! We’ll employ the car library to compute the VIF using the vif function. VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated. It’s a handy tool to identify collinearity issues in your model.\n\n# Installing and loading the 'car' library\n# install.packages(\"car\")\nlibrary(car)\n\n# Calculating VIF\nvif_values &lt;- vif(model)\nvif_values\n\n    disp       hp       wt     drat \n8.209402 2.894373 5.096601 2.279547 \n\n\n\n\nVisualizing the Model and Residuals\nTo gain deeper insights, let’s visualize our model and its residuals. Visualizations often provide a clearer picture of what’s happening under the hood.\n\n# Visualizing the model\nplot(model, which = 1, main = \"Model Fit\")\n\n\n\n\nThese plots will give us a sense of how well our model fits the data and whether there are any patterns in the residuals.\n\n\nVisualizing VIF\nNow, let’s bring our VIF into the spotlight. We’ll use a barplot to showcase the VIF values for each predictor.\n\n# Visualizing VIF\nbarplot(vif_values, col = \"skyblue\", main = \"Variance Inflation Factor (VIF)\")\n\n\n\n\nThis barplot will help us identify predictors that might be causing multicollinearity issues in our model.\n\n\nCorrelation Matrix and Visualization\nTo complete our journey, let’s create a correlation matrix of the predictors and visualize it. Understanding the correlations between variables is crucial in regression analysis.\n\n# Creating a correlation matrix\ncor_matrix &lt;- cor(mtcars[c(\"disp\", \"hp\", \"wt\", \"drat\")])\n\n# Visualizing the correlation matrix\nimage(cor_matrix, main = \"Correlation Matrix\", col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(20))\n\n\n\n\nThis visualization will give us a colorful snapshot of how our predictors are correlated.\n\n\nWrapping Up\nAnd there you have it, folks! We’ve explored the ins and outs of calculating VIF in R, visualized our model, checked residuals, and even took a colorful glance at predictor correlations. These tools are invaluable in ensuring the health and accuracy of our regression models.\nFeel free to tweak and play around with the code, and don’t forget to share your findings with the R community. Happy coding!\nKeep calm and code in R, Steve"
  },
  {
    "objectID": "posts/2023-12-19/index.html",
    "href": "posts/2023-12-19/index.html",
    "title": "A Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!",
    "section": "",
    "text": "Hey data enthusiasts! Today, we’re diving into the fascinating world of count data and its trusty sidekick, Poisson regression. Buckle up, because we’re about to explore how this statistical powerhouse helps us understand the factors influencing, you guessed it, counts.\nScenario: Imagine you’re an education researcher, eager to understand how a student’s GPA might influence their job offer count after graduation. But hold on, job offers aren’t continuous – they’re discrete, ranging from 0 to a handful. That’s where Poisson regression comes in!"
  },
  {
    "objectID": "posts/2023-12-19/index.html#generating-data",
    "href": "posts/2023-12-19/index.html#generating-data",
    "title": "Counting Blessings: A Gentle Dive into Poisson Regression for Job Offers",
    "section": "Generating Data",
    "text": "Generating Data\nWe’ll keep the data generation part the same, just adjusting the variables in our data frame.\n\nlibrary(tidyverse)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Creating data frame\ndata &lt;- data.frame(\n  School = sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE),\n  GPA = c(\n    round(runif(50, 1, 3), 1),\n    round(runif(30, 2, 3.5), 1),\n    round(runif(20, 3, 4), 1)\n  ),\n  JobOffers = c(rep(0, 50), rep(1, 30), rep(2, 10), rep(3, 7), rep(4, 3))\n)\n\nsummary(data)\n\n    School               GPA          JobOffers   \n Length:100         Min.   :1.000   Min.   :0.00  \n Class :character   1st Qu.:1.875   1st Qu.:0.00  \n Mode  :character   Median :2.600   Median :0.50  \n                    Mean   :2.532   Mean   :0.83  \n                    3rd Qu.:3.200   3rd Qu.:1.00  \n                    Max.   :4.000   Max.   :4.00  \n\ndata |&gt;\n  group_by(JobOffers) |&gt;\n  summarise(mean_gpa = mean(GPA))\n\n# A tibble: 5 × 2\n  JobOffers mean_gpa\n      &lt;dbl&gt;    &lt;dbl&gt;\n1         0     1.94\n2         1     2.87\n3         2     3.41\n4         3     3.5 \n5         4     3.8"
  },
  {
    "objectID": "posts/2023-12-19/index.html#visualizing-the-data",
    "href": "posts/2023-12-19/index.html#visualizing-the-data",
    "title": "A Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\nLet’s update the plots to reflect the change in the predictor and outcome.\n\nlibrary(ggplot2)\n\n# Plotting GPA distribution by school\nggplot(data, aes(JobOffers, fill = School)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  theme_minimal()\n\n\n\n\nThe density plot now showcases the distribution of GPA scores for each school.\nNext, let’s visualize the relationship between GPA and job offers.\n\n# Plotting Job Offers vs. GPA\nggplot(data, aes(x = GPA, y = JobOffers, color = School)) +\n  geom_point(aes(y = JobOffers), alpha = .628,\n             position = position_jitter(h = .2)) +\n  labs(title = \"Scatter Plot of Job Offers vs. GPA\",\n       x = \"GPA\", y = \"Job Offers\") +\n  theme_minimal()\n\n\n\n\nThis scatter plot gives us a visual cue that higher GPAs might correlate with more job offers."
  },
  {
    "objectID": "posts/2023-12-19/index.html#poisson-regression",
    "href": "posts/2023-12-19/index.html#poisson-regression",
    "title": "A Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nNow, let’s adjust the Poisson Regression model to reflect the change in predictor and outcome.\n\n# Fitting Poisson Regression model\npoisson_model &lt;- glm(JobOffers ~ GPA + School, data = data, family = \"poisson\")\n\n# Summary of the model\nsummary(poisson_model)\n\n\nCall:\nglm(formula = JobOffers ~ GPA + School, family = \"poisson\", data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2452  -0.5856  -0.3483   0.3221   1.6491  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.25817    0.70621  -7.446 9.65e-14 ***\nGPA          1.73169    0.21241   8.153 3.56e-16 ***\nSchoolB      0.03135    0.27524   0.114    0.909    \nSchoolC     -0.19137    0.27637  -0.692    0.489    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 138.07  on 99  degrees of freedom\nResidual deviance:  43.18  on 96  degrees of freedom\nAIC: 168.06\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe model summary will now provide insights into how GPA influences the number of job offers."
  },
  {
    "objectID": "posts/2023-12-19/index.html#visualizing-model-fits",
    "href": "posts/2023-12-19/index.html#visualizing-model-fits",
    "title": "A Gentle Introduction to Poisson Regression for Count Data: School’s Out, Job Offers Incoming!",
    "section": "Visualizing Model Fits",
    "text": "Visualizing Model Fits\nLet’s update the plot to reflect the relationship between GPA and predicted job offers.\n\n# Adding predicted values to the data frame\ndata$Predicted &lt;- predict(poisson_model, type = \"response\")\n\n# Plotting observed vs. predicted values\nggplot(data, aes(x = GPA, y = Predicted, color = School)) +\n  geom_point(aes(y = JobOffers), alpha = .628,\n             position = position_jitter(h = .2)) +\n  geom_line() +\n  labs(\n    title = \"Observed vs. Predicted Job Offers\",\n    x = \"GPA\", \n    y = \"Predicted Job Offers\",\n    color = \"School\") +\n  theme_minimal()\n\n\n\n\nThis plot now illustrates how the Poisson Regression model predicts job offers based on GPA."
  },
  {
    "objectID": "posts/2023-12-27/index.html",
    "href": "posts/2023-12-27/index.html",
    "title": "Creating Time Series in R with the ts() Function",
    "section": "",
    "text": "Time series analysis is a powerful tool in the hands of a data scientist or analyst. It allows us to uncover patterns, trends, and insights hidden within temporal data. In this blog post, we’ll explore how to create a time series in R using the base R function ts()."
  },
  {
    "objectID": "posts/2023-12-27/index.html#convert_to_ts-function-details",
    "href": "posts/2023-12-27/index.html#convert_to_ts-function-details",
    "title": "Creating Time Series in R with the ts() Function",
    "section": "convert_to_ts() Function Details",
    "text": "convert_to_ts() Function Details\nThe convert_to_ts() function takes the following arguments:\n\n.data: A data frame or tibble to be converted into a time series format.\n.return_ts: A logical value indicating whether to return the time series data. Default is TRUE.\n.pivot_longer: A logical value indicating whether to pivot the data into long format. Default is FALSE."
  },
  {
    "objectID": "posts/2023-12-27/index.html#how-it-works",
    "href": "posts/2023-12-27/index.html#how-it-works",
    "title": "Creating Time Series in R with the ts() Function",
    "section": "How It Works",
    "text": "How It Works\n\nThe function checks if the input is a data frame or tibble; otherwise, it raises an error.\nIt verifies if the data comes from a tidy_distribution function; otherwise, it raises an error.\nThe data is then converted into a time series format, grouping it by “sim_number” and transforming the “y” column into a time series."
  },
  {
    "objectID": "posts/2023-12-27/index.html#example-usage",
    "href": "posts/2023-12-27/index.html#example-usage",
    "title": "Creating Time Series in R with the ts() Function",
    "section": "Example Usage",
    "text": "Example Usage\n\nlibrary(TidyDensity)\n\n# Assuming you have a tidy data frame 'tidy_data'\ntidy_time_series &lt;- convert_to_ts(.data = tidy_normal(), \n                                  .return_ts = TRUE, \n                                  .pivot_longer = FALSE)\n\n# Display the result\nhead(tidy_time_series)\n\n              y\n[1,] -2.1617445\n[2,]  0.7630891\n[3,]  0.1951564\n[4,]  1.0558584\n[5,] -1.5169866\n[6,] -1.4532770\n\nplot(tidy_time_series)\n\n\n\nmultiple_simulations_series &lt;- convert_to_ts(.data = tidy_normal(.num_sims = 10),\n                                             .return_ts = TRUE, \n                                             .pivot_longer = TRUE)\nhead(multiple_simulations_series)\n\n              1          2           3           4          5          6\n[1,]  0.6591429  1.0850380 -1.41562870 -0.59330831 -0.2680326  0.6516654\n[2,] -1.7456947 -0.5792555  0.95670979  0.35232047  1.3702818 -1.0709930\n[3,]  0.2665711 -2.1701118  2.18141262  0.25480605  1.5762242 -0.8022482\n[4,]  0.3128563  0.4328502  0.55082256  0.06628991  0.7984409  0.3048087\n[5,]  0.6763225 -0.3997367 -0.09709908  1.13736623  1.0121689  0.3383476\n[6,] -0.1086352  1.3522350 -1.00235321  0.14722832  1.3395307 -0.1026343\n              7          8          9         10\n[1,] -0.1113009  1.6959992 -1.1897814 -0.2290430\n[2,] -0.7512943 -0.6969146  1.1334643  0.7554655\n[3,]  1.0782559  0.5296079 -1.0057891  1.1089107\n[4,] -1.8030557  1.5021519  0.7094383 -1.0848102\n[5,] -0.5539205  0.7127801 -1.3130555 -0.6742046\n[6,] -0.7625295 -1.1712384  0.8147821  0.8036737\n\nplot(multiple_simulations_series)\n\n\n\nconvert_to_ts(.data = tidy_normal(.num_sims = 10),\n              .return_ts = FALSE, \n              .pivot_longer = FALSE)\n\n# A tibble: 50 × 10\n       `1`    `2`    `3`      `4`    `5`    `6`     `7`    `8`     `9`   `10`\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  1.34    0.300  0.475 -3.07     1.35   1.18   0.403  -0.104 -0.347  -0.590\n 2  0.153  -0.563 -1.28  -0.0429   0.354  1.24  -0.184   0.850  0.338  -0.700\n 3 -0.448   0.160 -1.22   0.600   -0.532 -0.357 -0.759   0.659  0.691  -0.778\n 4  0.0384 -1.38  -0.918  1.12    -1.09   0.949  0.0276  0.456  0.510   1.10 \n 5 -0.0148 -0.663  0.401  0.00200 -0.790 -1.98   0.714   0.613  0.658   1.02 \n 6  0.528   0.164 -0.104 -0.977   -0.889 -0.589  1.39    0.916  0.496   0.326\n 7  1.08   -1.21   0.116  0.685    1.22   0.132  0.608  -0.322 -1.06   -0.624\n 8 -0.620   1.45   0.666 -1.39    -1.20  -0.175  0.0665  1.21   1.95   -0.237\n 9 -0.143  -1.93  -0.683  0.603   -1.24   0.623 -0.564  -0.417  0.0639  1.34 \n10  0.125   0.869 -1.47   0.953    0.608 -1.80   0.272   1.16   1.17   -1.09 \n# ℹ 40 more rows\n\nconvert_to_ts(.data = tidy_normal(.num_sims = 10),\n              .pivot_longer = TRUE)\n\nTime Series:\nStart = 1 \nEnd = 50 \nFrequency = 1 \n              1            2           3           4           5           6\n 1 -0.655132170  1.124119928 -1.68015013  0.27119005 -1.34222309 -0.85029821\n 2 -0.563753358  0.148496529 -0.08420323  0.52425342 -0.22838128  0.38009238\n 3  0.162642987  0.944409541 -0.31995660  1.62739294  0.04586743 -1.00996184\n 4 -1.946332513  1.693491637 -0.76559306  2.58711915 -0.67796334  0.73113802\n 5  0.009260305  0.563304432 -0.90409541 -0.74206033  1.18933630  1.19979835\n 6  0.422154075 -0.827368569  0.38440082  1.58106342  0.53744939 -0.01699670\n 7  0.672312916  1.917586521  0.18758750  0.52812847  0.04987565  0.42471806\n 8 -0.099748356  0.548332963 -0.44544054 -0.74466540  0.27741779  0.03754982\n 9  0.599374087  2.416047894  0.13083797 -0.71925129  1.46397148  0.21928699\n10  2.137103506  1.012679620  0.10946528 -1.21309250 -0.40298312 -0.06149162\n11 -0.584328794 -0.977150553  1.98763118  0.80100807  1.59761439  0.96962954\n12 -0.977410854  0.153227900 -0.89588458 -0.04738268 -0.99671233 -0.79875286\n13  0.348559098 -0.139254894  0.17014759 -0.41635428  1.16343543  0.45536856\n14  2.050755613  0.379570270 -1.57943509 -0.49670553 -0.49108776 -0.81654431\n15 -0.536403083  0.815378564  0.44909230 -0.17645908  2.14187118  0.85912010\n16  0.154173100 -0.009842145 -0.06548799  1.36411289 -0.26842334  1.69185208\n17  0.526008171 -1.021411558 -0.01515875 -0.39923678 -1.91505446  1.47060381\n18 -0.382628412  1.554043999 -0.99083886  0.03813862 -0.12122244  0.78097490\n19  0.068027520  1.559917873  1.56624132 -0.72556821  0.04364763 -1.08713622\n20 -0.375312760  0.182453162 -0.29118413  0.63647087  1.14593859  0.33562580\n21  0.527199674  0.514358677 -0.85199804 -0.65959289  1.34991539 -0.88580797\n22  0.606739583 -0.349281940 -0.42159565 -0.29988202  0.86897866  0.83607508\n23 -0.750044523 -0.721098218  0.81759595  1.81533167  1.67514533  0.49723656\n24 -0.167121348  1.050352702  1.70669358 -2.78197517  0.70872919 -1.19627981\n25 -0.442427162 -0.009639385  0.66120474  0.07333735  0.39692306  2.72030582\n26 -0.405109397 -0.463937639  2.92697938  0.64263562 -0.59588190  1.27812303\n27  0.554957737  1.508452736 -0.67384362 -1.14296113  0.25859866 -0.09516432\n28 -0.167333762 -0.154519359  1.29634414  2.05276305  1.22046958 -0.20261165\n29 -0.426751358  0.299516899 -0.10270470  0.45218446 -1.01077778  0.41069225\n30 -0.915369502 -1.134302489 -0.45195412 -0.02372924 -0.87979497 -2.22429752\n31  0.398618595 -0.246544276  0.63197257  0.04685569  0.46524825 -0.41017315\n32  0.001083079 -0.530058643  0.82139589  0.39120899  0.63881495  0.63570075\n33 -0.502176823 -0.642359996  0.42880920 -0.44803379 -0.01992917  0.38896456\n34  0.276462923 -1.042478900  1.14313112 -1.55697201  0.52390061  0.07794736\n35  0.923405153 -0.237080174  0.96970857  0.86964379 -0.91567940 -0.63612591\n36 -0.400504232  0.217544505 -0.63904106  0.91258477 -0.01156312 -0.41156245\n37  0.458010625 -1.456299801 -0.95303905 -1.01123779 -0.04321204 -0.84060963\n38 -0.202664022 -0.729410616  1.10544600 -1.54460728  0.48443033 -0.67777269\n39  0.041503245 -0.610875862 -0.45167645 -0.47658183 -2.46133081  0.26514433\n40 -0.121966128 -0.265657879 -0.40754380  0.41665215 -3.23015392 -0.11733447\n41  2.275054279 -0.140453274  1.01738854  1.08335318 -0.72963796 -0.07750213\n42  0.305032329 -0.912897889 -0.54486760 -0.06350812  0.35661866 -0.89575613\n43 -0.097368038  0.879480923  0.42349178  0.90800105  1.22592750 -1.93884437\n44 -0.053250772  0.590070911 -0.04377062  0.38001642 -0.56422962 -1.55652310\n45 -0.631346970  0.510970484  1.03953655 -0.52313314  0.66643930 -0.50328900\n46 -0.573835910  0.662271162 -0.94615866 -1.09348403  1.51469795  0.02769026\n47  2.056373176 -1.438372766  0.64932666 -1.17330573  0.41932438  1.53009528\n48 -2.353078785  0.487698963 -0.81844578 -0.92462341  0.27244456  0.42617475\n49 -3.231722103 -1.271841203  0.24348256 -1.36611307 -0.97603663 -1.95217754\n50  0.753719680 -0.366101153 -0.01044950 -1.59566595  0.08057617  1.11583833\n              7            8            9           10\n 1  0.171342136 -1.143037486  0.798526236 -0.244110488\n 2 -0.940732995 -0.098286237 -0.002464059 -1.329350897\n 3 -0.694053671 -0.861938231 -0.981141759 -0.016408426\n 4  0.715770296  0.891107430 -1.631236257 -0.812448587\n 5 -1.487322100  1.513993297  1.034899433 -1.562309837\n 6 -2.299581079 -1.372057771  1.141053483 -0.523175745\n 7 -1.551226431  0.584053401 -0.124530500  1.386795935\n 8  0.199088420  0.176433940  0.896122531  0.150326444\n 9 -2.610686399  1.619626479 -0.304107194  1.999026100\n10  0.993024200 -1.717659646 -0.936505161  0.249643134\n11  0.242493969 -1.104745018  2.139557395  1.308248416\n12  1.438262730 -0.371852512 -0.367182295 -1.589296525\n13 -0.149204186 -1.054119573 -0.465127766  0.423034528\n14  1.199604760 -0.295676868  1.818224237  1.651671457\n15  0.682116022  1.589055554  0.940553190  0.044546697\n16 -0.023887103 -0.544176304  0.078750649 -1.618718807\n17  0.783402254 -0.024077038  1.530981707  0.610937582\n18  0.840292783 -0.781554633  0.177714516 -0.059345413\n19 -1.313595307 -1.101811653  0.057190918  0.067426355\n20  0.005232829  0.145444788  1.066697084  1.068481723\n21 -0.554820885  0.380379950 -0.162190910  1.185489015\n22 -0.861222004  0.030283953  0.908438632 -0.231394452\n23  1.157935009  0.063995477  2.361496504 -0.396326692\n24 -0.897071507  0.369621973 -0.266053668 -0.131590687\n25  0.035629927 -0.084923255  0.003248558 -0.368614537\n26  0.509364566 -1.832084693  0.890542325  0.888462980\n27  1.207424021 -2.671878721  0.063299112 -0.878590418\n28  0.211237171  1.535283026  0.759650387  0.549046140\n29 -0.595276048 -2.514556134  0.445083701 -0.769968392\n30  1.348793576  0.004755218 -0.301946343 -2.037938159\n31  0.361619164 -1.340745382 -0.706048393 -0.003291719\n32  0.014851985 -0.249794267  0.741063865 -0.398728564\n33 -1.172677388 -0.193834398  1.018583201 -0.351067819\n34 -0.572769045 -2.072442096  0.577545791  1.284331483\n35  0.443800268 -0.108977727  1.866110069 -0.020469667\n36  0.926425998 -0.687618149  1.224365387 -0.096690188\n37 -0.460173605 -0.302608648  0.671541153 -2.696710002\n38  0.277085477  0.335125232 -0.754473314  0.619338071\n39  1.279310040 -0.842097806 -0.275860802 -0.768216600\n40  0.015055026  0.835779589 -0.535925622 -0.990428811\n41  0.690052418  1.488830535  0.318262300 -0.265301715\n42 -2.342151157 -0.587400371  1.794438099  1.190162522\n43 -1.284973383  0.976120498 -0.678730423  0.895248035\n44 -1.857641265 -0.484324204  1.312931115  1.671816010\n45 -0.828061584 -1.461679865  1.175113675  0.392315093\n46 -0.124694812  0.800465295 -0.328118006  0.170025963\n47  0.698593283  0.676449924  1.963221359 -0.477702054\n48  0.118048192  0.257227889 -0.600914093  0.908605679\n49 -0.101844218  0.458018251  0.177924006 -0.469079298\n50  0.249644640 -1.684424651  0.620835209  1.330859032\n\n\nThis example showcases how to leverage TidyDensity’s functionality to convert tidy data into a time series format effortlessly. At this point in time though, the parameters of the ts() function are not utilized, meaning you cannot also pass in a start, end or frequency, but that will be added in the future.\nIn conclusion, mastering the ts() function in base R and exploring additional tools like convert_to_ts() opens up new avenues for time series analysis. So, roll up your sleeves, experiment with your data, and unlock the insights hidden in the temporal dimension. Happy coding!"
  },
  {
    "objectID": "posts/2023-12-28/index.html",
    "href": "posts/2023-12-28/index.html",
    "title": "Unveiling the Time Traveler: Plotting Time Series in R",
    "section": "",
    "text": "Introduction\nReady to journey through time with R? Buckle up, because we’re about to explore the art of visualizing time-dependent data, known as time series analysis. Whether you’re tracking monthly sales patterns or analyzing yearly climate trends, R has your back with powerful tools to visualize these stories through time.\nOur Flight Plan:\n\nLoading Up with Data: Grabbing our trusty dataset, AirPassengers.\nTaking Off with Base R: Creating a basic time series plot using base R functions.\nSoaring with ggplot2: Crafting a visually stunning time series plot using the ggplot2 library.\nNavigating Date Formatting: Customizing axis labels with scale_x_date() for clarity.\nLanding with Your Own Exploration: Encouraging you to take the controls and create your own time series plots!\n\n1. Ready for Takeoff: Loading Data\nWe’ll start by loading the built-in AirPassengers dataset, which chronicles monthly passenger totals from 1949 to 1960:\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\n2. Base R: The Simple and Straightforward Route\nBase R offers a direct path to creating a time series plot:\n\nplot(AirPassengers)\n\n\n\n\nThis generates a basic line plot, revealing an upward trend in air passengers over time.\n3. ggplot2: The High-Flying, Visually Staggering Journey\nFor more customization and visual appeal, we’ll turn to the ggplot2 library and the healthyR.ts library to first convert the AirPassengers Data set into a tibble:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(healthyR.ts)\n\ndf &lt;- ts_to_tbl(AirPassengers)\n\nggplot(df, aes(x = date_col, y = value)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Monthly Air Passengers (1949-1960)\",\n       x = \"Year\",\n       y = \"Passengers\")\n\n\n\n\nThis creates a more refined plot with informative labels and a sleeker aesthetic.\n4. Mastering Time with scale_x_date()\nTo fine-tune the x-axis date labels, ggplot2 offers the versatile scale_x_date() function. Let’s display years and abbreviated months:\n\nggplot(df, aes(x = date_col, y = value)) +\n  geom_line() +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%b %Y\") +\n  labs(title = \"Monthly Air Passengers (1949-1960)\",\n       y = \"Passengers\")\n\n\n\n\n5. Your Turn to Pilot: Experiment and Explore!\nR is your playground for time series visualization! Try these challenges:\n\nExplore other time series datasets in R.\nCustomize plots further with colors, themes, and annotations.\nUse scale_x_date() to display different date formats.\nCombine multiple time series in a single plot.\n\nUnleash your creativity and uncover the captivating stories hidden within time series data! For a start here are some resources:\n\nscale_x_date()\n\nThe scale_x_date() functiontakes the following arguments:\n\n%d: Day as a number between 0 and 31\n%a: Abbreviated weekday (e.g. “Tue”)\n%A: Unabbreviated weekday (e.g. “Tuesday”)\n%m: Month between 0 and 12\n%b: Abbreviated month (e.g. “Jan”)\n%B: Unabbreviated month (e.g. “January”)\n%y: 2-digit year (e.g. “21”)\n%Y: 4-digit year (e.g. “2021”)\n%W: Week of the year between 0 and 52"
  },
  {
    "objectID": "posts/2023-12-29/index.html",
    "href": "posts/2023-12-29/index.html",
    "title": "Unlocking the Power of Time: Transforming Data Frames into Time Series in R",
    "section": "",
    "text": "Hey there, fellow R enthusiasts! Today, we’re diving into the realm of time series, where data dances along the temporal dimension. To join this rhythmic analysis, we’ll first learn how to convert our trusty data frames into time series objects—the heart of time-based exploration in R."
  },
  {
    "objectID": "posts/2023-12-29/index.html#gather-your-data",
    "href": "posts/2023-12-29/index.html#gather-your-data",
    "title": "Unlocking the Power of Time: Transforming Data Frames into Time Series in R",
    "section": "1. Gather Your Data",
    "text": "1. Gather Your Data\nEvery journey begins with preparation. Here’s our sample data frame containing daily sales:\n\ndf &lt;- data.frame(date = as.Date('2022-01-01') + 0:9,\n                 sales = runif(10, 10, 500) + seq(50, 59)^2)"
  },
  {
    "objectID": "posts/2023-12-29/index.html#choose-your-time-series-destination",
    "href": "posts/2023-12-29/index.html#choose-your-time-series-destination",
    "title": "Unlocking the Power of Time: Transforming Data Frames into Time Series in R",
    "section": "2. Choose Your Time Series Destination",
    "text": "2. Choose Your Time Series Destination\nR offers two primary time series classes:\n\n“ts”: Base R’s classic time series object, designed for regularly spaced data.\n“xts”: Part of the ‘xts’ package, offering enhanced flexibility and features."
  },
  {
    "objectID": "posts/2023-12-29/index.html#embark-on-the-conversion-quest",
    "href": "posts/2023-12-29/index.html#embark-on-the-conversion-quest",
    "title": "Unlocking the Power of Time: Transforming Data Frames into Time Series in R",
    "section": "3. Embark on the Conversion Quest",
    "text": "3. Embark on the Conversion Quest\nA. Transforming into “ts”:\n\nlibrary(stats)  # Package for 'ts' class\n\n# Unleash the time series magic!\nts_sales &lt;- ts(df$sales, start = c(2022, 1), frequency = 365)  # Daily data\n\n# Admire your creation:\nprint(ts_sales)\n\nTime Series:\nStart = c(2022, 1) \nEnd = c(2022, 10) \nFrequency = 365 \n [1] 2728.713 3026.967 2769.227 2928.872 3401.730 3129.780 3303.479 3414.551\n [9] 3584.525 3922.348\n\n\nExplanation:\n\nts() function creates the time series object.\ndf$sales specifies the data for conversion.\nstart = c(2022, 1) sets the starting year and month.\nfrequency = 365 indicates daily observations (365 days per year).\n\nB. Shaping into “xts”:\n\nlibrary(xts)  # Package for 'xts' class\n\n# Time to shine!\nxts_sales &lt;- xts(df$sales, order.by = df$date)\n\n# Behold your masterpiece:\nprint(xts_sales)\n\n               [,1]\n2022-01-01 2728.713\n2022-01-02 3026.967\n2022-01-03 2769.227\n2022-01-04 2928.872\n2022-01-05 3401.730\n2022-01-06 3129.780\n2022-01-07 3303.479\n2022-01-08 3414.551\n2022-01-09 3584.525\n2022-01-10 3922.348\n\n\nExplanation:\n\nxts() function constructs the time series object.\ndf$sales provides the data.\norder.by = df$date sets the time-based ordering.\n\n4. Your Time to Experiment!\nNow that you’ve mastered the conversion, unleash your creativity:\n\nVisualize trends with plots.\nForecast future values.\nAnalyze patterns and seasonality.\nDecompose time series into components.\nAnd much more!\n\nThe possibilities are as boundless as time itself.\nRemember:\n\nChoose the time series class that best suits your analysis needs.\nAlways ensure your data frame has a column with valid date or time values.\nExplore the rich functionalities of R’s time series packages.\n\nHappy time series adventures!"
  },
  {
    "objectID": "posts/2024-01-02/index.html",
    "href": "posts/2024-01-02/index.html",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "",
    "text": "Hey fellow R enthusiasts! Today, let’s dive into the fascinating world of Lowess smoothing and learn how to harness its power for creating smooth visualizations of your data. Whether you’re new to R or a seasoned pro, this step-by-step guide will walk you through the process of performing Lowess smoothing, generating data, visualizing the model, and comparing different models with varying smoother spans."
  },
  {
    "objectID": "posts/2024-01-02/index.html#step-1-generate-data",
    "href": "posts/2024-01-02/index.html#step-1-generate-data",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nBefore we can smooth anything, we need some data to work with. Let’s create a synthetic dataset using the rnorm function and introduce a non-linear trend:\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate data\nx &lt;- seq(1, 100, by = 1)\ny &lt;- sin(x/10) + rnorm(100, sd = 0.5)\n\n# Plot the raw data\nplot(x, y, main = \"Raw Data with Non-linear Trend\", col = \"blue\", pch = 16)"
  },
  {
    "objectID": "posts/2024-01-02/index.html#step-2-perform-lowess-smoothing",
    "href": "posts/2024-01-02/index.html#step-2-perform-lowess-smoothing",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "Step 2: Perform Lowess Smoothing",
    "text": "Step 2: Perform Lowess Smoothing\nNow that we have our data, let’s apply Lowess smoothing using the lowess function:\n\n# Apply Lowess smoothing\nsmoothed_data &lt;- lowess(x, y)\n\n# Plot the smoothed data\nplot(x, y, main = \"Lowess Smoothed\", col = \"blue\", pch = 16)\nlines(smoothed_data, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Raw Data\", \"Lowess Smoothed\"), col = c(\"blue\", \"red\"), lwd = 2)"
  },
  {
    "objectID": "posts/2024-01-02/index.html#step-3-visualize-the-model-and-residuals",
    "href": "posts/2024-01-02/index.html#step-3-visualize-the-model-and-residuals",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "Step 3: Visualize the Model and Residuals",
    "text": "Step 3: Visualize the Model and Residuals\nTo better understand our smoothed model, let’s visualize the fitted values along with the residuals:\n\n# Get fitted values and residuals\nfitted_values &lt;- smoothed_data$y\nresiduals &lt;- y - fitted_values\n\n# Plot the model\nplot(x, fitted_values, main = \"Lowess Smoothed Model with Residuals\", col = \"red\", type = \"l\", lwd = 2)\npoints(x, residuals, col = \"green\", pch = 16)\nlegend(\"topleft\", legend = c(\"Smoothed Model\", \"Residuals\"), col = c(\"red\", \"green\"), lwd = 2)"
  },
  {
    "objectID": "posts/2024-01-02/index.html#step-4-compare-different-models",
    "href": "posts/2024-01-02/index.html#step-4-compare-different-models",
    "title": "Mastering Lowess Smoothing in R: A Step-by-Step Guide",
    "section": "Step 4: Compare Different Models",
    "text": "Step 4: Compare Different Models\nNow, let’s take our Lowess smoothing to the next level by experimenting with different smoother spans. We’ll create three models with varying spans and visualize the differences:\n\n# Generate three smoothed models with different spans\nmodel_1 &lt;- lowess(x, y, f = 0.2)\nmodel_2 &lt;- lowess(x, y, f = 0.5)\nmodel_3 &lt;- lowess(x, y, f = 0.8)\n\n# Plot the original data\nplot(x, y, main = \"Comparison of Lowess Models\", col = \"blue\", pch = 16)\n\n# Plot the smoothed models\nlines(model_1, col = \"red\", lty = 2, lwd = 2)\nlines(model_2, col = \"green\", lty = 3, lwd = 2)\nlines(model_3, col = \"purple\", lty = 4, lwd = 2)\n\n# Add a legend\nlegend(\"bottomleft\", legend = c(\"Raw Data\", \"Model 1\", \"Model 2\", \"Model 3\"), col = c(\"blue\", \"red\", \"green\", \"purple\"), lwd = 2)"
  },
  {
    "objectID": "posts/2024-01-04/index.html",
    "href": "posts/2024-01-04/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review (2023)",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2023, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2024!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2023\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\nlibrary(knitr)\nlibrary(kableExtra)\n\nfp &lt;- \"linkedin_content.xlsx\"\n\nengagement_tbl &lt;- read_excel(fp, sheet = \"ENGAGEMENT\") %&gt;%\n  clean_names() %&gt;%\n  mutate(date = mdy(date)) %&gt;% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %&gt;%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2023-12-31\"\n  )\n\ntop_posts_tbl &lt;- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %&gt;%\n  clean_names()\n\nfollowers_tbl &lt;- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %&gt;%\n  clean_names() %&gt;%\n  mutate(date = mdy(date)) %&gt;%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2023-12-31\"\n  )\n\ndemographics_tbl &lt;- read_excel(fp, sheet = \"DEMOGRAPHICS\") %&gt;%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 365\nColumns: 4\n$ date              &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-01-04, 202…\n$ impressions       &lt;dbl&gt; 4872, 3735, 10360, 12217, 27036, 26084, 8720, 2753, …\n$ engagements       &lt;dbl&gt; 34, 17, 51, 80, 173, 124, 32, 17, 80, 54, 106, 135, …\n$ `Engagement Rate` &lt;dbl&gt; 0.6978654, 0.4551539, 0.4922780, 0.6548252, 0.639887…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 7\n$ post_url_1          &lt;chr&gt; \"https://www.linkedin.com/feed/update/urn:li:activ…\n$ post_publish_date_2 &lt;chr&gt; \"2/16/2023\", \"2/16/2023\", \"3/16/2023\", \"1/24/2023\"…\n$ engagements         &lt;dbl&gt; 281, 227, 220, 194, 181, 172, 160, 145, 138, 124, …\n$ x4                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_5          &lt;chr&gt; \"https://www.linkedin.com/feed/update/urn:li:activ…\n$ post_publish_date_6 &lt;chr&gt; \"2/16/2023\", \"1/5/2023\", \"1/17/2023\", \"1/24/2023\",…\n$ impressions         &lt;dbl&gt; 43951, 38656, 34402, 32505, 25205, 24916, 22656, 2…\n\nglimpse(followers_tbl)\n\nRows: 365\nColumns: 2\n$ date          &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-01-04, 2023-01…\n$ new_followers &lt;dbl&gt; 11, 13, 17, 16, 26, 15, 14, 18, 14, 9, 11, 23, 6, 13, 5,…\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics &lt;chr&gt; \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            &lt;chr&gt; \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       &lt;chr&gt; \"0.05210459977388382\", \"0.03567609563469887\", \"0.0223…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %&gt;%\n  pivot_longer(cols = -date) %&gt;%\n  mutate(name = str_to_title(name)) %&gt;%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %&gt;%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  theme_minimal()\n\n\n\n\nLet’s look at a cumulative view of things.\n\nengagement_tbl %&gt;%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %&gt;%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %&gt;%\n  slice(1:12) %&gt;%\n  pivot_longer(cols = -date) %&gt;%\n  mutate(name = str_to_title(name)) %&gt;%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %&gt;%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %&gt;%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %&gt;%\n  slice(1:12) %&gt;%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\n\n\nKey Stats and Tables\nNow we are going to look at some key stats and tables. First we will look at the top 10 posts by impressions.\n\ntop_posts_tbl %&gt;%\n  select(post_publish_date_2, impressions, post_url_1) %&gt;%\n  arrange(desc(impressions)) %&gt;%\n  head(10) %&gt;%\n  setNames(c(\"Post Date\", \"Impressions\", \"Post URL\")) %&gt;%\n  kable(caption = \"Top 10 Posts by Impressions\", align = \"c\")\n\n\nTop 10 Posts by Impressions\n\n\nPost Date\nImpressions\nPost URL\n\n\n\n\n2/16/2023\n43951\nhttps://www.linkedin.com/feed/update/urn:li:activity:7031977309351890944\n\n\n2/16/2023\n38656\nhttps://www.linkedin.com/feed/update/urn:li:activity:7031977249448820737\n\n\n3/16/2023\n34402\nhttps://www.linkedin.com/feed/update/urn:li:activity:7042173970149728257\n\n\n1/24/2023\n32505\nhttps://www.linkedin.com/feed/update/urn:li:activity:7023663053699207168\n\n\n8/24/2023\n25205\nhttps://www.linkedin.com/feed/update/urn:li:activity:7100535469263699968\n\n\n1/17/2023\n24916\nhttps://www.linkedin.com/feed/update/urn:li:activity:7021114787371614209\n\n\n1/5/2023\n22656\nhttps://www.linkedin.com/feed/update/urn:li:activity:7016769698595749888\n\n\n3/10/2023\n21943\nhttps://www.linkedin.com/feed/update/urn:li:activity:7039960379266883585\n\n\n6/23/2023\n20559\nhttps://www.linkedin.com/feed/update/urn:li:activity:7078000250656808960\n\n\n2/21/2023\n19730\nhttps://www.linkedin.com/feed/update/urn:li:activity:7033888693216018432\n\n\n\n\n\n\n\nNow we will look at the top 10 posts by engagements.\n\ntop_posts_tbl %&gt;%\n  select(post_publish_date_2, engagements, post_url_1) %&gt;%\n  arrange(desc(engagements)) %&gt;%\n  head(10) %&gt;%\n  setNames(c(\"Post Date\", \"Engagements\", \"Post URL\")) %&gt;%\n  kable(caption = \"Top 10 Posts by Engagements\", align = \"c\")\n\n\nTop 10 Posts by Engagements\n\n\nPost Date\nEngagements\nPost URL\n\n\n\n\n2/16/2023\n281\nhttps://www.linkedin.com/feed/update/urn:li:activity:7031977309351890944\n\n\n2/16/2023\n227\nhttps://www.linkedin.com/feed/update/urn:li:activity:7031977249448820737\n\n\n3/16/2023\n220\nhttps://www.linkedin.com/feed/update/urn:li:activity:7042173970149728257\n\n\n1/24/2023\n194\nhttps://www.linkedin.com/feed/update/urn:li:activity:7023663053699207168\n\n\n8/24/2023\n181\nhttps://www.linkedin.com/feed/update/urn:li:activity:7100535469263699968\n\n\n1/17/2023\n172\nhttps://www.linkedin.com/feed/update/urn:li:activity:7021114787371614209\n\n\n1/5/2023\n160\nhttps://www.linkedin.com/feed/update/urn:li:activity:7016769698595749888\n\n\n3/10/2023\n145\nhttps://www.linkedin.com/feed/update/urn:li:activity:7039960379266883585\n\n\n6/23/2023\n138\nhttps://www.linkedin.com/feed/update/urn:li:activity:7078000250656808960\n\n\n2/21/2023\n124\nhttps://www.linkedin.com/feed/update/urn:li:activity:7033888693216018432\n\n\n\n\n\n\n\nNow we will look at the top 10 posts by engagement rate.\n\ntop_posts_tbl %&gt;%\n  select(post_publish_date_2, engagements, impressions, post_url_1) %&gt;%\n  mutate(engagement_rate = engagements / impressions) %&gt;%\n  arrange(desc(engagement_rate)) %&gt;%\n  select(post_publish_date_2, engagement_rate, post_url_1) %&gt;%\n  head(10) %&gt;%\n  setNames(c(\"Post Date\", \"Engagement Rate\", \"Post URL\")) %&gt;%\n  kable(caption = \"Top 10 Posts by Engagement Rate\", align = \"c\")\n\n\nTop 10 Posts by Engagement Rate\n\n\nPost Date\nEngagement Rate\nPost URL\n\n\n\n\n8/24/2023\n0.0071811\nhttps://www.linkedin.com/feed/update/urn:li:activity:7100535469263699968\n\n\n1/5/2023\n0.0070621\nhttps://www.linkedin.com/feed/update/urn:li:activity:7016769698595749888\n\n\n1/17/2023\n0.0069032\nhttps://www.linkedin.com/feed/update/urn:li:activity:7021114787371614209\n\n\n5/10/2023\n0.0068692\nhttps://www.linkedin.com/feed/update/urn:li:activity:7062043144850137088\n\n\n2/6/2023\n0.0067925\nhttps://www.linkedin.com/feed/update/urn:li:activity:7028379188441010176\n\n\n7/17/2023\n0.0067441\nhttps://www.linkedin.com/feed/update/urn:li:activity:7086688278245961728\n\n\n11/29/2023\n0.0067365\nhttps://www.linkedin.com/feed/update/urn:li:activity:7135620032557940736\n\n\n6/23/2023\n0.0067124\nhttps://www.linkedin.com/feed/update/urn:li:activity:7078000250656808960\n\n\n1/25/2023\n0.0066225\nhttps://www.linkedin.com/feed/update/urn:li:activity:7024095415348133889\n\n\n3/10/2023\n0.0066080\nhttps://www.linkedin.com/feed/update/urn:li:activity:7039960379266883585\n\n\n\n\n\n\n\nTotal Impressions: 2,720,605\nTotal Engagements: 20,000\nMean Engagement Rate: 0.0073513\nNew Followers: 6,388\nAnd finally the demographics of people who typically interact with my posts:\n\ndemographics_tbl %&gt;%\n  mutate(percentage = substr(percentage, 1, 4)) %&gt;%\n  kable(\n    caption = \"Demographics of People Who Interact With My Posts\", \n    align = \"c\"\n    )\n\n\nDemographics of People Who Interact With My Posts\n\n\ntop_demographics\nvalue\npercentage\n\n\n\n\nJob titles\nData Scientist\n0.05\n\n\nJob titles\nData Analyst\n0.03\n\n\nJob titles\nSoftware Engineer\n0.02\n\n\nJob titles\nData Engineer\n0.01\n\n\nJob titles\nProfessor\n0.01\n\n\nLocations\nNew York City Metropolitan Area\n0.06\n\n\nLocations\nGreater Bengaluru Area\n0.03\n\n\nLocations\nGreater Delhi Area\n0.02\n\n\nLocations\nPune/Pimpri-Chinchwad Area\n0.02\n\n\nLocations\nMumbai Metropolitan Region\n0.01\n\n\nIndustries\nIT Services and IT Consulting\n0.22\n\n\nIndustries\nSoftware Development\n0.11\n\n\nIndustries\nHigher Education\n0.05\n\n\nIndustries\nFinancial Services\n0.05\n\n\nIndustries\nHospitals and Health Care\n0.05\n\n\nSeniority\nSenior\n0.33\n\n\nSeniority\nEntry\n0.27\n\n\nSeniority\nDirector\n0.03\n\n\nSeniority\nManager\n0.03\n\n\nSeniority\nTraining\n0.02\n\n\nCompany size\n10,001+ employees\n0.17\n\n\nCompany size\n1001-5000 employees\n0.10\n\n\nCompany size\n51-200 employees\n0.08\n\n\nCompany size\n11-50 employees\n0.08\n\n\nCompany size\n1-10 employees\n0.06\n\n\nCompanies\nTata Consultancy Services\n&lt; 1%\n\n\nCompanies\nLong Island Community Hospital\n&lt; 1%\n\n\nCompanies\nAmazon\n&lt; 1%\n\n\nCompanies\nEY\n&lt; 1%\n\n\nCompanies\nCiti\n&lt; 1%\n\n\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/2024-01-05/index.html",
    "href": "posts/2024-01-05/index.html",
    "title": "Unveiling the Smooth Operator: Rolling Averages in R",
    "section": "",
    "text": "Ever felt those data points were a bit too jittery? Smoothing out trends and revealing underlying patterns is a breeze with rolling averages in R. Ready to roll? Let’s dive in!"
  },
  {
    "objectID": "posts/2024-01-05/index.html#creating-a-simple-time-series",
    "href": "posts/2024-01-05/index.html#creating-a-simple-time-series",
    "title": "Unveiling the Smooth Operator: Rolling Averages in R",
    "section": "Creating a Simple Time Series",
    "text": "Creating a Simple Time Series\n\nset.seed(123)  # Set seed for reproducibility (optional\n# Let's imagine some daily sales data\nsales &lt;- trunc(runif(112, min = 100, max = 500))  # Generate some random sales\ndays &lt;- as.Date(1:112, origin = \"2022-12-31\")  # Add some dates!\ndata_zoo &lt;- zoo(sales, days)  # Convert to a zoo object"
  },
  {
    "objectID": "posts/2024-01-05/index.html#calculating-rolling-averages",
    "href": "posts/2024-01-05/index.html#calculating-rolling-averages",
    "title": "Unveiling the Smooth Operator: Rolling Averages in R",
    "section": "Calculating Rolling Averages",
    "text": "Calculating Rolling Averages\n\n# Say we want a 7-day rolling average:\nrolling_avg7 &lt;- rollmean(data_zoo, k = 7)\nrolling_avg7_left &lt;- rollmean(data_zoo, k = 7, align = \"left\")\nrolling_avg7_right &lt;- rollmean(data_zoo, k = 7, align = \"right\")\n\n# How about a 28-day one?\nrolling_avg28 &lt;- rollmean(data_zoo, k = 28)\nrolling_avg28_left &lt;- rollmean(data_zoo, k = 28, align = \"left\")\nrolling_avg28_right &lt;- rollmean(data_zoo, k = 28, align = \"right\")"
  },
  {
    "objectID": "posts/2024-01-05/index.html#visualizing-the-smoothness",
    "href": "posts/2024-01-05/index.html#visualizing-the-smoothness",
    "title": "Unveiling the Smooth Operator: Rolling Averages in R",
    "section": "Visualizing the Smoothness",
    "text": "Visualizing the Smoothness\n\nplot(data_zoo, type = \"l\", col = \"black\", lwd = 1, ylab = \"Sales\")\nlines(rolling_avg7, col = \"red\", lwd = 2, lty = 2)\nlines(rolling_avg7_left, col = \"green\", lwd = 2, lty = 2)\nlines(rolling_avg7_right, col = \"orange\", lwd = 2, lty = 2)\nlegend(\n  \"bottomleft\", \n  legend = c(\n    \"Original Data\", \"7-day Avg\", \"7-day Avg (left-aligned)\", \n    \"7-day Avg (right-aligned)\"\n    ),\n  col = c(\"black\", \"red\", \"green\", \"orange\"), \n  lwd = 1, lty = 1:2,\n  cex = 0.628\n  )\n\n\n\n\n\nplot(data_zoo, type = \"l\", col = \"black\", lwd = 1, ylab = \"Sales\")\nlines(rolling_avg28, col = \"green\", lwd = 2, lty = 2)\nlines(rolling_avg28_left, col = \"steelblue\", lwd = 2, lty = 2)\nlines(rolling_avg28_right, col = \"brown\", lwd = 2, lty = 2)\nlegend(\n  \"bottomleft\", \n  legend = c(\n    \"Original Data\", \"28-day Avg\", \"28-day Avg (left-aligned)\", \n    \"28-day Avg (right-aligned)\"\n    ),\n  col = c(\"black\", \"green\", \"steelblue\", \"brown\"), \n  lwd = 1, lty = 1:2,\n  cex = 0.628\n  )"
  },
  {
    "objectID": "posts/2024-01-08/index.html",
    "href": "posts/2024-01-08/index.html",
    "title": "Conquering Daily Data: How to Aggregate to Months and Years Like a Pro in R",
    "section": "",
    "text": "Introduction\nTaming the beast of daily data can be daunting. While it captures every detail, sometimes you need a bird’s-eye view. Enter aggregation, your secret weapon for transforming daily data into monthly and yearly insights. In this post, we’ll dive into the world of R, where you’ll wield powerful tools like dplyr and lubridate to master this data wrangling art.\n\n\nPackages: Gear Up with the Right Packages\nThink of R packages like your trusty toolbox. Today, we’ll need two essentials:\n\ndplyr: This swiss army knife lets you manipulate and summarize data like a boss.\nlubridate: Time is our domain, and lubridate helps us navigate it with precision, especially for dates.\n\n\n\nSample Data, Our Training Ground\nImagine you have daily sales data for a year. Each row represents a day, with columns for date, product, and sales amount. Let’s create a mini version:\n\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Generate random dates and sales\nset.seed(123)\ndates &lt;- seq(as.Date('2023-01-01'), as.Date('2023-12-31'), by = 'day')\nsales &lt;- runif(365, min=5000, max=10000)\n\n# Create our data frame\ndaily_data &lt;- data.frame(date = dates, sales = sales)\n\n# Peek at our data\nhead(daily_data)\n\n        date    sales\n1 2023-01-01 6437.888\n2 2023-01-02 8941.526\n3 2023-01-03 7044.885\n4 2023-01-04 9415.087\n5 2023-01-05 9702.336\n6 2023-01-06 5227.782\n\n\nThis code generates 10 random dates and sales figures, and stores them in a data frame called daily_data.\n\n\nMonthly Magic – From Days to Months\nNow, let’s transform this daily data into monthly insights. Here’s the incantation:\n\n# Group data by month\nmonthly_data &lt;- daily_data %&gt;%\n   # Group by month extracted from date\n  group_by(month = month(date)) %&gt;%\n  # Calculate total sales for each month\n  summarize(total_sales = sum(sales))\n\nhead(monthly_data)\n\n# A tibble: 6 × 2\n  month total_sales\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     1     245675.\n2     2     199109.\n3     3     233764.\n4     4     227888.\n5     5     230928.\n6     6     222015.\n\n\nLet’s break it down:\n\ngroup_by(month = month(date)): We tell R to group our data by the month extracted from the date column.\nsummarize(total_sales = sum(sales)): Within each month group, we calculate the total sales by summing the sales values.\n\n\n\nYearly Triumph – Conquering the Calendar\nYearning for yearly insights? Fear not! Modify the spell slightly:\n\n# Group data by year\nyearly_data &lt;- daily_data %&gt;%\n  # Group by year extracted from date\n  group_by(year = year(date)) %&gt;%\n  # Calculate average sales for each year\n  summarize(average_sales = mean(sales))\n\nhead(yearly_data)\n\n# A tibble: 1 × 2\n   year average_sales\n  &lt;dbl&gt;         &lt;dbl&gt;\n1  2023         7494.\n\n\nHere, we group by the year extracted from date and then calculate the average sales for each year.\n\n\nBut what about base R?\nSo far, we’ve used dplyr to group and summarize our data. But what if you don’t have dplyr? No problem! You can use base R functions like aggregate() to achieve the same results:\n\nmonthly_data &lt;- aggregate(\n  daily_data$sales, \n  by = list(month = format(daily_data$date, '%m')), \n  FUN = sum\n  )\nhead(monthly_data)\n\n  month        x\n1    01 245675.1\n2    02 199108.7\n3    03 233764.1\n4    04 227888.3\n5    05 230928.0\n6    06 222015.3\n\nyearly_data &lt;- aggregate(\n  daily_data$sales, \n  by = list(year = format(daily_data$date, '%Y')), \n  FUN = mean\n  )\nhead(yearly_data)\n\n  year      x\n1 2023 7493.8\n\n\n\n\nExperiment!\nThe magic doesn’t stop there! You can customize your aggregations to your heart’s content. Try these variations:\n\nCalculate maximum sales per month.\nFind the product with the highest average sales per year.\nGroup data by month and product to see which products perform best each month.\n\n\n\nRemember\n\nPlay around with different summarize() functions like min(), max(), or median().\nUse filter() before group_by() to focus on specific subsets of data.\nExplore other time units like weeks or quarters with lubridate’s powerful tools.\n\n\n\nThe Takeaway\nMastering daily data aggregation is a valuable skill for any data warrior. With the help of R and your newfound knowledge, you can transform mountains of daily data into insightful monthly and yearly summaries. So, go forth, conquer your data, and share your insights with the world!\nBonus Challenge: Share your own R code and insights in the comments below! Let’s learn from each other and become daily data aggregation masters together!"
  },
  {
    "objectID": "posts/2024-01-09/index.html",
    "href": "posts/2024-01-09/index.html",
    "title": "New Horizons for TidyDensity: Version 1.3.0 Release",
    "section": "",
    "text": "Introduction\nThe latest release of the TidyDensity R package brings some major changes and improvements that open up new possibilities for statistical analysis and data visualization. Version 1.3.0 includes breaking changes, new features, and a host of minor fixes and improvements that enhance performance and usability. Let’s dive into what’s new!\n\n\nBreaking Changes\nTwo key functions have been modified in this release:\n\ntidy_multi_single_dist() now requires passing the .return_tibble parameter to specify whether to return a tibble (TRUE) or a list (FALSE). This allows better control over the output.\nThe minimum R version has been bumped to 4.1.0 to leverage the native pipe operator |&gt; instead of %&gt;%.\n\n\n\nNew Features\nSeveral new functions expand the capabilities of TidyDensity:\n\ntidy_triangular() generates a tidy dataframe of points from a triangular distribution.\nutil_triangular_param_estimate() estimates the parameters of a triangular distribution.\nutil_triangular_stats_tbl() computes summary statistics for a triangular distribution.\ntriangle_plot() creates a triangular density plot.\ntidy_autoplot() now supports triangular distributions.\n\n\n\nPerformance Improvements\nMany functions have been optimized for speed:\n\ncvar() and csd() are now vectorized for over 100x speedup.\nUsing data.table in the tidy_ functions typically improves speed by 30% or more.\nOther vectorized improvements speed up cskewness() by 124x and ckurtosis() by 121x.\n\nThe minor fixes address deprecation warnings, documentation, and ensure consistency across functions.\nVersion 1.3.0 takes TidyDensity to the next level with expanded capabilities and boosted performance. Whether you need to model triangular distributions or crunch large datasets, this release has you covered. The pipe workflow makes analyses simpler and faster. Check out the full details in the GitHub repository. Let us know if you have any issues or feature requests!"
  },
  {
    "objectID": "posts/2024-01-10/index.html",
    "href": "posts/2024-01-10/index.html",
    "title": "Exploring the Peaks: A Dive into the Triangular Distribution in TidyDensity",
    "section": "",
    "text": "Welcome back, fellow data enthusiasts! Today, we embark on an exciting journey into the world of statistical distributions with a special focus on the latest addition to the TidyDensity package – the triangular distribution. Tightly packed and versatile, this distribution brings a unique flavor to your data simulations and analyses. In this blog post, we’ll delve into the functions provided, understand their arguments, and explore the wonders of the triangular distribution."
  },
  {
    "objectID": "posts/2024-01-10/index.html#using-tidy_triangular-for-simulations",
    "href": "posts/2024-01-10/index.html#using-tidy_triangular-for-simulations",
    "title": "Exploring the Peaks: A Dive into the Triangular Distribution in TidyDensity",
    "section": "Using tidy_triangular for Simulations",
    "text": "Using tidy_triangular for Simulations\nSuppose you want to simulate a triangular distribution with 100 x values, a minimum of 0, a maximum of 1, and a mode at 0.5. You’d use the following code:\n\nlibrary(TidyDensity)\n\ntriangular_data &lt;- tidy_triangular(\n  .n = 100, \n  .min = 0, \n  .max = 1, \n  .mode = 0.5, \n  .num_sims = 1, \n  .return_tibble = TRUE\n  )\n\ntriangular_data\n\n# A tibble: 100 × 7\n   sim_number     x     y      dx      dy     p     q\n   &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1              1 0.853 -0.140  0.00158 0.957 0.853\n 2 1              2 0.697 -0.128  0.00282 0.816 0.697\n 3 1              3 0.656 -0.116  0.00484 0.764 0.656\n 4 1              4 0.518 -0.103  0.00805 0.536 0.518\n 5 1              5 0.635 -0.0909 0.0130  0.733 0.635\n 6 1              6 0.838 -0.0786 0.0202  0.948 0.838\n 7 1              7 0.645 -0.0662 0.0304  0.748 0.645\n 8 1              8 0.482 -0.0539 0.0444  0.464 0.482\n 9 1              9 0.467 -0.0416 0.0627  0.437 0.467\n10 1             10 0.599 -0.0293 0.0859  0.678 0.599\n# ℹ 90 more rows\n\n\nThis generates a tidy tibble with simulated data, ready for your analysis."
  },
  {
    "objectID": "posts/2024-01-10/index.html#estimating-parameters-and-creating-stats-tables",
    "href": "posts/2024-01-10/index.html#estimating-parameters-and-creating-stats-tables",
    "title": "Exploring the Peaks: A Dive into the Triangular Distribution in TidyDensity",
    "section": "Estimating Parameters and Creating Stats Tables",
    "text": "Estimating Parameters and Creating Stats Tables\nUtilize the util_triangular_param_estimate function to estimate parameters and create tidy empirical data:\n\nparam_estimate &lt;- util_triangular_param_estimate(.x = triangular_data$y)\n\nt(param_estimate$parameter_tbl)\n\n          [,1]        \ndist_type \"Triangular\"\nsamp_size \"100\"       \nmin       \"0.0572515\" \nmax       \"0.8822025\" \nmode      \"0.8822025\" \nmethod    \"Basic\"     \n\n\nFor statistics table creation:\n\nstats_table &lt;- util_triangular_stats_tbl(.data = triangular_data)\nt(stats_table)\n\n                  [,1]                     \ntidy_function     \"tidy_triangular\"        \nfunction_call     \"Triangular c(0, 1, 0.5)\"\ndistribution      \"Triangular\"             \ndistribution_type \"continuous\"             \npoints            \"100\"                    \nsimulations       \"1\"                      \nmean              \"0.5\"                    \nmedian            \"0.3535534\"              \nmode              \"1\"                      \nrange_low         \"0.0572515\"              \nrange_high        \"0.8822025\"              \nvariance          \"0.04166667\"             \nskewness          \"0\"                      \nkurtosis          \"-0.6\"                   \nentropy           \"-0.6931472\"             \ncomputed_std_skew \"-0.1870017\"             \ncomputed_std_kurt \"2.778385\"               \nci_lo             \"0.08311609\"             \nci_hi             \"0.8476985\"              \n\n\nVisualizing the Triangular Distribution: Now, let’s visualize the triangular distribution using the triangle_plot function:\n\ntriangle_plot(.data = triangular_data, .interactive = TRUE)\n\n\n\n\n\n\ntriangle_plot(.data = triangular_data, .interactive = FALSE)\n\n\n\n\nThis will generate an informative plot, and if you set .interactive to TRUE, you can explore the distribution interactively using plotly."
  },
  {
    "objectID": "posts/2023-01-11/index.html",
    "href": "posts/2023-01-11/index.html",
    "title": "Benchmarking the Speed of Cumulative Functions in TidyDensity",
    "section": "",
    "text": "Introduction\nStatistical analysis often involves calculating various measures on large datasets. Speed and efficiency are crucial, especially when dealing with real-time analytics or massive data volumes. The TidyDensity package in R provides a set of fast cumulative functions for common statistical measures like mean, standard deviation, skewness, and kurtosis. But just how fast are these cumulative functions compared to doing the computations directly? In this post, I benchmark the cumulative functions against the base R implementations using the rbenchmark package.\n\n\nSetting the bench\nTo assess the performance of TidyDensity’s cumulative functions, we’ll employ the rbenchmark package for benchmarking and the ggplot2 package for visualization. I’ll benchmark the following cumulative functions on random samples of increasing size:\n\ncgmean() - Cumulative geometric mean\nchmean() - Cumulative harmonic mean\nckurtosis() - Cumulative kurtosis\ncskewness() - Cumulative skewness\ncmean() - Cumulative mean\ncsd() - Cumulative standard deviation\ncvar() - Cumulative variance\n\n\nlibrary(TidyDensity)\nlibrary(rbenchmark)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nset.seed(123)\n\nx1 &lt;- sample(1e2) + 1e2\nx2 &lt;- sample(1e3) + 1e3 \nx3 &lt;- sample(1e4) + 1e4\nx4 &lt;- sample(1e5) + 1e5\nx5 &lt;- sample(1e6) + 1e6\n\ncg_bench &lt;- benchmark(\n  \"100\" = cgmean(x1),\n  \"1000\" = cgmean(x2),\n  \"10000\" = cgmean(x3),\n  \"100000\" = cgmean(x4),\n  \"1000000\" = cgmean(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\n# Run benchmarks for other functions\nch_bench &lt;- benchmark(\n  \"100\" = chmean(x1),\n  \"1000\" = chmean(x2),\n  \"10000\" = chmean(x3),\n  \"100000\" = chmean(x4),\n  \"1000000\" = chmean(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\nck_bench &lt;- benchmark(\n  \"100\" = ckurtosis(x1),\n  \"1000\" = ckurtosis(x2),\n  \"10000\" = ckurtosis(x3),\n  \"100000\" = ckurtosis(x4),\n  \"1000000\" = ckurtosis(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")  \n)\n\ncs_bench &lt;- benchmark(\n  \"100\" = cskewness(x1),\n  \"1000\" = cskewness(x2), \n  \"10000\" = cskewness(x3),\n  \"100000\" = cskewness(x4),\n  \"1000000\" = cskewness(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\ncm_bench &lt;- benchmark(\n  \"100\" = cmean(x1),\n  \"1000\" = cmean(x2),\n  \"10000\" = cmean(x3),\n  \"100000\" = cmean(x4),\n  \"1000000\" = cmean(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\ncsd_bench &lt;- benchmark(\n  \"100\" = csd(x1),\n  \"1000\" = csd(x2),\n  \"10000\" = csd(x3),\n  \"100000\" = csd(x4),\n  \"1000000\" = csd(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")  \n)\n\ncv_bench &lt;- benchmark(\n  \"100\" = cvar(x1),\n  \"1000\" = cvar(x2),\n  \"10000\" = cvar(x3),\n  \"100000\" = cvar(x4), \n  \"1000000\" = cvar(x5),\n  replications = 100L,\n  columns = c(\"test\",\"replications\",\"elapsed\", \"relative\",\"user.self\",\"sys.self\")\n)\n\nbenchmarks &lt;- rbind(cg_bench, ch_bench, ck_bench, cs_bench, cm_bench, csd_bench, cv_bench)\n\n# Arrange benchmarks and plot\nbench_tbl &lt;- benchmarks |&gt; \n  mutate(func = c(\n    rep(\"cgmean\", 5), \n    rep(\"chmean\", 5),\n    rep(\"ckurtosis\", 5),\n    rep(\"cskewness\", 5),\n    rep(\"cmean\", 5),\n    rep(\"csd\", 5),\n    rep(\"cvar\", 5)\n    )\n  ) |&gt;\n  arrange(func, test) |&gt;\n  select(func, test, everything())\n\nbench_tbl |&gt;\n  ggplot(aes(x=test, y=elapsed, group = func, color = func)) +\n    geom_line() +\n    facet_wrap(~func, scales=\"free_y\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(title=\"Cumulative Function Speed Comparison\",\n       x=\"Sample Size\",\n       y=\"Elapsed Time (sec)\",\n       color = \"Function\")\n\n\n\n\nThe results show that the TidyDensity cumulative functions scale extremely well as the sample size increases. The elapsed time remains very low even at 1 million observations. The base R implementations like var() and sd() perform significantly worse when used inside of an sapply at large sample sizes. What was not tested however is cmedian() and this is because the performance is very slow once we reach 1e4 compared to the other functions as such that it would take too long to run the benchmark if it ran at all.\nSo if you need fast statistical functions that can scale to big datasets, the TidyDensity cumulative functions are a great option! They provide massive speedups over base R while returning the same final result.\nLet me know in the comments if you have any other benchmark ideas for comparing R packages! I’m always looking for interesting performance comparisons to test out."
  },
  {
    "objectID": "posts/2024-01-12/index.html",
    "href": "posts/2024-01-12/index.html",
    "title": "TidyDensity Powers Up with Data.table: Speedier Distributions for Your Data Exploration",
    "section": "",
    "text": "Calling all R enthusiasts who love tidy data and crave efficiency!\nI’m thrilled to announce a major upgrade to the TidyDensity package that’s sure to accelerate your data analysis workflows. We’ve integrated the lightning-fast data.table package for generating tidy distribution data, resulting in a jaw-dropping 30% speed boost.\nHere is one of the tests ran during development where v1 was the current and v2 was the version using data.table:\nn &lt;- 10000\nbenchmark(\n \"tidy_bernoulli_v2\" = {\n   tidy_bernoulli_v2(n, .5, 1, FALSE)\n },\n \"tidy_bernoulli_v1\" = {\n   TidyDensity::tidy_bernoulli(n, .5, 1)\n },\n replications = 100,\n columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\")\n) |&gt;\n arrange(relative)\n               test replications elapsed relative user.self sys.self\n1 tidy_bernoulli_v2          100    2.50    1.000      2.22     0.26\n2 tidy_bernoulli_v1          100    4.67    1.868      4.34     0.31\n\n\nHere’s what this means for you\n\nFaster Generation of Distribution Data: Whether you’re working with normal, binomial, Poisson, or other distributions, TidyDensity now produces results more swiftly than ever. This means less waiting and more time for exploring insights.\nFlexible Output Formats: Choose the format that best suits your needs:\n\nTibbles for Seamless Integration with Tidyverse: Set .return_tibble = TRUE to receive the data as a tibble, ready for seamless interaction with your favorite tidyverse tools.\ndata.table for Enhanced Performance: Set .return_tibble = FALSE to harness the raw power of data.table objects for memory-efficient and lightning-fast operations.\n\nEnjoy the Speed Boost, No Matter Your Choice: The speed enhancement shines through regardless of your preferred output format, as the data generation itself leverages data.table under the hood.\n\n\n\nHow to experience this boost\n\nUpdate TidyDensity: Ensure you have the latest version installed: install.packages(\"TidyDensity\")\nChoose Your Output Format: Indicate your preference with the .return_tibble parameter:\n# For a tibble:\ntidy_data &lt;- tidy_normal(.return_tibble = TRUE)\n\n# For a data.table:\ntidy_data &lt;- tidy_normal(.return_tibble = FALSE)\nNo matter which output you choose you will still enjoy the speedup because data.table is used to create the data and the conversion to a tibble is done afterwards if that is the output you want.\n\n\n\nLet’s see the output\n\nlibrary(TidyDensity)\n\n# Generate data\nnormal_tibble &lt;- tidy_normal(.return_tibble = TRUE)\nhead(normal_tibble)\n\n# A tibble: 6 × 7\n  sim_number     x       y    dx       dy      p       q\n  &lt;fct&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 1              1  1.05   -2.97 0.000398 0.854   1.05  \n2 1              2  0.0168 -2.84 0.00104  0.507   0.0168\n3 1              3  1.77   -2.72 0.00244  0.961   1.77  \n4 1              4 -1.81   -2.59 0.00518  0.0353 -1.81  \n5 1              5  0.447  -2.46 0.00997  0.673   0.447 \n6 1              6  1.05   -2.33 0.0174   0.854   1.05  \n\nclass(normal_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnormal_dt &lt;- tidy_normal(.return_tibble = FALSE)\nhead(normal_dt)\n\n   sim_number x           y        dx           dy         p           q\n1:          1 1  2.24103518 -3.424949 0.0002787401 0.9874881  2.24103518\n2:          1 2 -0.12769603 -3.286892 0.0008586864 0.4491948 -0.12769603\n3:          1 3 -0.39666069 -3.148835 0.0022824304 0.3458088 -0.39666069\n4:          1 4  0.89626001 -3.010778 0.0052656793 0.8149430  0.89626001\n5:          1 5  0.04267757 -2.872721 0.0105661984 0.5170207  0.04267757\n6:          1 6  0.53424808 -2.734664 0.0185083421 0.7034150  0.53424808\n\nclass(normal_dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\n\n\nReady to unleash the power of TidyDensity and data.table?\nDive into your next data exploration project and experience the efficiency firsthand! Share your discoveries and feedback with the community—we’re eager to hear how this upgrade empowers your analysis.\nHappy tidy data exploration!"
  },
  {
    "objectID": "posts/2024-01-16/index.html",
    "href": "posts/2024-01-16/index.html",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "",
    "text": "Greetings, fellow data enthusiasts! Today, we’re diving into the exciting world of tidyAML 0.0.4, where innovation meets efficiency in the realm of R programming. As we unpack the latest release, we’ll explore the new features, enhancements, and the overall impact of this powerful tool on your data science endeavors."
  },
  {
    "objectID": "posts/2024-01-16/index.html#introducing-extract_regression_residuals",
    "href": "posts/2024-01-16/index.html#introducing-extract_regression_residuals",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Introducing extract_regression_residuals()",
    "text": "Introducing extract_regression_residuals()\nOne of the standout features in this release is the addition of extract_regression_residuals(). This function empowers users to delve deeper into regression models, providing a valuable tool for analyzing and understanding residuals. Whether you’re fine-tuning your models or gaining insights into data patterns, this enhancement adds a crucial layer to your analytical arsenal."
  },
  {
    "objectID": "posts/2024-01-16/index.html#enhanced-classification-with-.drop_na",
    "href": "posts/2024-01-16/index.html#enhanced-classification-with-.drop_na",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "2. Enhanced Classification with .drop_na",
    "text": "2. Enhanced Classification with .drop_na\nResponding to user feedback and aiming for seamless user experience, tidyAML 0.0.4 brings forth an important addition to fast_classification() and fast_regression(). The introduction of the .drop_na parameter allows users to handle missing data more efficiently, streamlining the classification and regression processes."
  },
  {
    "objectID": "posts/2024-01-16/index.html#core-package-expansion",
    "href": "posts/2024-01-16/index.html#core-package-expansion",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Core Package Expansion",
    "text": "Core Package Expansion\nAcknowledging the diverse needs of data scientists, tidyAML now incorporates additional core packages. The inclusion of discrim, mda, sda, sparsediscrim, liquidSVM, kernlab, and klaR extends the scope of possibilities. These additions enhance the versatility of tidyAML, making it an even more comprehensive solution for your modeling requirements."
  },
  {
    "objectID": "posts/2024-01-16/index.html#refined-internal-predictions",
    "href": "posts/2024-01-16/index.html#refined-internal-predictions",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Refined Internal Predictions",
    "text": "Refined Internal Predictions\nThe update addresses #190 by refining the internal_make_wflw_predictions() function. Now, it includes all essential data elements: the actual data, training predictions, and testing predictions. This refinement ensures a more holistic view of your model’s performance, facilitating a comprehensive evaluation of its predictive capabilities."
  },
  {
    "objectID": "posts/2024-01-16/index.html#streamlined-regression-analysis",
    "href": "posts/2024-01-16/index.html#streamlined-regression-analysis",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Streamlined Regression Analysis",
    "text": "Streamlined Regression Analysis\nWith the introduction of extract_regression_residuals(), tidyAML empowers users to conduct in-depth regression analyses with ease. Uncover hidden patterns, identify outliers, and fine-tune your models for optimal performance."
  },
  {
    "objectID": "posts/2024-01-16/index.html#improved-data-handling-in-classification",
    "href": "posts/2024-01-16/index.html#improved-data-handling-in-classification",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Improved Data Handling in Classification",
    "text": "Improved Data Handling in Classification\nThe new .drop_na parameter in fast_classification() and fast_regression() simplifies the management of missing data. Enhance the robustness of your classification models by seamlessly handling missing values, resulting in more reliable and accurate predictions."
  },
  {
    "objectID": "posts/2024-01-16/index.html#comprehensive-core-packages",
    "href": "posts/2024-01-16/index.html#comprehensive-core-packages",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Comprehensive Core Packages",
    "text": "Comprehensive Core Packages\nThe expansion of core packages broadens the toolkit at your disposal. Whether you’re exploring discriminant analysis, support vector machines, or kernel methods, tidyAML now supports an extended range of algorithms, catering to diverse modeling needs."
  },
  {
    "objectID": "posts/2024-01-16/index.html#holistic-model-evaluation",
    "href": "posts/2024-01-16/index.html#holistic-model-evaluation",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Holistic Model Evaluation",
    "text": "Holistic Model Evaluation\nThe refined internal_make_wflw_predictions() ensures that you have all the necessary components for a comprehensive model evaluation. Analyze the actual data alongside training and testing predictions, gaining a 360-degree view of your model’s performance."
  },
  {
    "objectID": "posts/2024-01-16/index.html#enhanced-classificationregression-build-with-.drop_na",
    "href": "posts/2024-01-16/index.html#enhanced-classificationregression-build-with-.drop_na",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Enhanced Classification/Regression build with .drop_na",
    "text": "Enhanced Classification/Regression build with .drop_na\nResponding to user feedback and aiming for seamless user experience, tidyAML 0.0.4 brings forth an important addition to fast_classification() and fast_regression(). The introduction of the .drop_na parameter allows users to handle missing data more efficiently, streamlining the classification and regression processes."
  },
  {
    "objectID": "posts/2024-01-16/index.html#improved-data-handling-in-classification-and-regression",
    "href": "posts/2024-01-16/index.html#improved-data-handling-in-classification-and-regression",
    "title": "Exploring the Power of tidyAML 0.0.4: Unleashing New Features and Enhancements",
    "section": "Improved Data Handling in Classification and Regression",
    "text": "Improved Data Handling in Classification and Regression\nThe new .drop_na parameter in fast_classification() and fast_regression() simplifies the management of missing data. Enhance the robustness of your classification models by seamlessly handling missing values, resulting in more reliable and accurate predictions."
  },
  {
    "objectID": "posts/2024-01-17/UntitledQMD.html",
    "href": "posts/2024-01-17/UntitledQMD.html",
    "title": "Using .drop_na in Fast Classification and Regression",
    "section": "",
    "text": "In the newest release of tidyAML there has been an addition of a new parameter to the functions fast_classification() and fast_regression(). The parameter is .drop_na and it is a logical value that defaults to TRUE. This parameter is used to determine if the function should drop rows with missing values from the output if a model cannot be built for some reason. Let’s take a look at the function and it’s arguments.\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL,\n  .drop_na = TRUE\n)\n\n\n.data - The data being passed to the function for the regression problem .rec_obj - The recipe object being passed. .parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported. .parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported. .split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample .split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type. .drop_na - The default is TRUE, which will drop all NA’s from the data.\nNow let’s see this in action."
  },
  {
    "objectID": "posts/2024-01-17/UntitledQMD.html#arguments",
    "href": "posts/2024-01-17/UntitledQMD.html#arguments",
    "title": "Using .drop_na in Fast Classification and Regression",
    "section": "",
    "text": ".data - The data being passed to the function for the regression problem .rec_obj - The recipe object being passed. .parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported. .parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported. .split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample .split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type. .drop_na - The default is TRUE, which will drop all NA’s from the data.\nNow let’s see this in action."
  },
  {
    "objectID": "posts/2024-01-17/index.html",
    "href": "posts/2024-01-17/index.html",
    "title": "Using .drop_na in Fast Classification and Regression",
    "section": "",
    "text": "In the newest release of tidyAML there has been an addition of a new parameter to the functions fast_classification() and fast_regression(). The parameter is .drop_na and it is a logical value that defaults to TRUE. This parameter is used to determine if the function should drop rows with missing values from the output if a model cannot be built for some reason. Let’s take a look at the function and it’s arguments.\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL,\n  .drop_na = TRUE\n)\n\n\n.data - The data being passed to the function for the regression problem .rec_obj - The recipe object being passed. .parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported. .parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported. .split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample .split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type. .drop_na - The default is TRUE, which will drop all NA’s from the data.\nNow let’s see this in action."
  },
  {
    "objectID": "posts/2024-01-17/index.html#arguments",
    "href": "posts/2024-01-17/index.html#arguments",
    "title": "Using .drop_na in Fast Classification and Regression",
    "section": "",
    "text": ".data - The data being passed to the function for the regression problem .rec_obj - The recipe object being passed. .parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported. .parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported. .split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample .split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type. .drop_na - The default is TRUE, which will drop all NA’s from the data.\nNow let’s see this in action."
  },
  {
    "objectID": "posts/2024-01-18/index.html",
    "href": "posts/2024-01-18/index.html",
    "title": "Exploring the Enhanced Features of tidyAML’s internal_make_wflw_predictions()",
    "section": "",
    "text": "Hey R enthusiasts! Steve here, and today I’m excited to share some fantastic updates about a key function in the tidyAML package – internal_make_wflw_predictions(). The latest version addresses issue #190, ensuring that all crucial data is now included in the predictions. Let’s dive into the details!"
  },
  {
    "objectID": "posts/2024-01-18/index.html#arguments",
    "href": "posts/2024-01-18/index.html#arguments",
    "title": "Exploring the Enhanced Features of tidyAML’s internal_make_wflw_predictions()",
    "section": "Arguments:",
    "text": "Arguments:\n\n.model_tbl: The model table generated from a function like fast_regression_parsnip_spec_tbl(). Ensure that it has a class of “tidyaml_mod_spec_tbl.” This is typically used after running the internal_make_fitted_wflw() function and saving the resulting tibble.\n.splits_obj: The splits object obtained from the auto_ml function. It is internal to the auto_ml function."
  },
  {
    "objectID": "posts/2024-01-18/index.html#why-it-matters",
    "href": "posts/2024-01-18/index.html#why-it-matters",
    "title": "Exploring the Enhanced Features of tidyAML’s internal_make_wflw_predictions()",
    "section": "Why It Matters",
    "text": "Why It Matters\nBy including actual data along with training and testing predictions, the internal_make_wflw_predictions() function empowers you to perform a more thorough evaluation of your models. This is a significant step towards ensuring the reliability and generalization capability of your machine learning models.\nSo, R enthusiasts, update your tidyAML package, explore the enhanced features, and let us know how these improvements elevate your modeling experience. Happy coding!"
  },
  {
    "objectID": "posts/2024-01-19/index.html",
    "href": "posts/2024-01-19/index.html",
    "title": "The new function on the block with tidyAML extract_regression_residuals()",
    "section": "",
    "text": "Introduction\nYesterday I discussed the use of the function internal_make_wflw_predictions() in the tidyAML R package. Today I will discuss the use of the function extract_wflw_pred() and the brand new function extract_regression_residuals() in the tidyAML R package. We breifly saw yesterday the output of the function internal_make_wflw_predictions() which is a list of tibbles that are typically inside of a list column in the final output of fast_regression() and fast_classification(). The function extract_wflw_pred() takes this list of tibbles and extracts them from that output. The function extract_regression_residuals() also extracts those tibbles and has the added feature of also returning the residuals. Let’s see how these functions work.\n\n\nThe new function\nFirst, we will go over the syntax of the new function extract_regression_residuals().\nextract_regression_residuals(.model_tbl, .pivot_long = FALSE)\nThe function takes two arguments. The first argument is .model_tbl which is the output of fast_regression() or fast_classification(). The second argument is .pivot_long which is a logical argument that defaults to FALSE. If TRUE then the output will be in a long format. If FALSE then the output will be in a wide format. Let’s see how this works.\n\n\nExample\n\n# Load packages\nlibrary(tidyAML)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(multilevelmod) # for the gee model\n\ntidymodels_prefer() # good practice when using tidyAML\n\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\nfrt_tbl &lt;- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"stan\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n  )\n\nLet’s break down the R code step by step:\n\nLoading Libraries:\n\nlibrary(tidyAML)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(multilevelmod) # for the gee model\nHere, the code is loading several R packages. These packages provide functions and tools for data analysis, modeling, and visualization. tidyAML and tidymodels are particularly relevant for modeling, while tidyverse is a collection of packages for data manipulation and visualization. multilevelmod is included for the Generalized Estimating Equations (gee) model.\n\nSetting Preferences:\ntidymodels_prefer() # good practice when using tidyAML\n\nThis line of code is setting preferences for the tidy modeling workflow using tidymodels_prefer(). It ensures that when using tidyAML, the tidy modeling conventions are followed. Tidy modeling involves an organized and consistent approach to modeling in R.\n\nCreating a Recipe Object:\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\n\nHere, a recipe object (rec_obj) is created using the recipe function from the tidymodels package. The formula mpg ~ . specifies that we want to predict the mpg variable based on all other variables in the dataset (mtcars).\n\nPerforming Fast Regression:\nfrt_tbl &lt;- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\",\"stan\",\"gee\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nThis part involves using the fast_regression function. It performs a fast regression analysis using various engines specified by .parsnip_eng and specific functions specified by .parsnip_fns. In this case, it includes linear models (lm), generalized linear models (glm), Stan models (stan), and the Generalized Estimating Equations model (gee). The results are stored in the frt_tbl table.\nIn summary, the code is setting up a tidy modeling workflow, creating a recipe for predicting mpg based on other variables in the mtcars dataset, and then performing a fast regression using different engines and functions. The choice of engines and functions allows flexibility in exploring different modeling approaches.\nNow that we have the output of fast_regression() stored in frt_tbl, we can use the function extract_wflw_pred() to extract the predictions and from the output. Let’s see how this works. First, the syntax:\nextract_wflw_pred(.data, .model_id = NULL)\nThe function takes two arguments. The first argument is .data which is the output of fast_regression() or fast_classification(). The second argument is .model_id which is a numeric vector that defaults to NULL. If NULL then the function will extract none of the predictions from the output. If a numeric vector is provided then the function will extract the predictions for the models specified by the numeric vector. Let’s see how this works.\n\nextract_wflw_pred(frt_tbl, 1)\n\n# A tibble: 64 × 4\n   .model_type     .data_category .data_type .value\n   &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 54 more rows\n\nextract_wflw_pred(frt_tbl, 1:2)\n\n# A tibble: 128 × 4\n   .model_type     .data_category .data_type .value\n   &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 118 more rows\n\nextract_wflw_pred(frt_tbl, 1:nrow(frt_tbl))\n\n# A tibble: 256 × 4\n   .model_type     .data_category .data_type .value\n   &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n 1 lm - linear_reg actual         actual       15.2\n 2 lm - linear_reg actual         actual       10.4\n 3 lm - linear_reg actual         actual       33.9\n 4 lm - linear_reg actual         actual       32.4\n 5 lm - linear_reg actual         actual       16.4\n 6 lm - linear_reg actual         actual       21.5\n 7 lm - linear_reg actual         actual       15.8\n 8 lm - linear_reg actual         actual       15  \n 9 lm - linear_reg actual         actual       14.7\n10 lm - linear_reg actual         actual       10.4\n# ℹ 246 more rows\n\n\nThe first line of code extracts the predictions for the first model in the output. The second line of code extracts the predictions for the first two models in the output. The third line of code extracts the predictions for all models in the output.\nNow, let’s visualize the predictions for the models in the output and the actual values. We will use the ggplot2 package for visualization. First, we will extract the predictions for all models in the output and store them in a table called pred_tbl. Then, we will use ggplot2 to visualize the predictions and actual values.\n\npred_tbl &lt;- extract_wflw_pred(frt_tbl, 1:nrow(frt_tbl))\n\npred_tbl |&gt;\n  group_split(.model_type) |&gt;\n  map(\\(x) x |&gt;\n        group_by(.data_category) |&gt;\n        mutate(x = row_number()) |&gt;\n        ungroup() |&gt;\n        pivot_wider(names_from = .data_type, values_from = .value) |&gt;\n        ggplot(aes(x = x, y = actual, group = .data_category)) +\n        geom_line(color = \"black\") +\n        geom_line(aes(x = x, y = training), linetype = \"dashed\", color = \"red\",\n                  linewidth = 1) +\n        geom_line(aes(x = x, y = testing), linetype = \"dashed\", color = \"blue\",\n                  linewidth = 1) +\n        theme_minimal() +\n        labs(\n          x = \"\",\n          y = \"Observed/Predicted Value\",\n          title = \"Observed vs. Predicted Values by Model Type\",\n          subtitle = x$.model_type[1]\n        )\n      )\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nOr we can facet them by model type:\n\npred_tbl |&gt;\n  group_by(.model_type, .data_category) |&gt;\n  mutate(x = row_number()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = x, y = .value)) +\n  geom_line(data = . %&gt;% filter(.data_type == \"actual\"), color = \"black\") +\n  geom_line(data = . %&gt;% filter(.data_type == \"training\"), \n            linetype = \"dashed\", color = \"red\") +\n  geom_line(data = . %&gt;% filter(.data_type == \"testing\"), \n            linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ .model_type, ncol = 2, scales = \"free\") +\n  labs(\n    x = \"\",\n    y = \"Observed/Predicted Value\",\n    title = \"Observed vs. Predicted Values by Model Type\"\n  ) +\n  theme_minimal()\n\n\n\n\nOk, so what about this new function I talked about above? Well let’s go over it here. We have already discussed it’s syntax so no need to go over it again. Let’s just jump right into an example. This function will return the residuals for all models. We will slice off just the first model for demonstration purposes.\n\nextract_regression_residuals(.model_tbl = frt_tbl, .pivot_long = FALSE)[[1]]\n\n# A tibble: 32 × 4\n   .model_type     .actual .predicted .resid\n   &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 lm - linear_reg    15.2       17.3 -2.09 \n 2 lm - linear_reg    10.4       11.9 -1.46 \n 3 lm - linear_reg    33.9       30.8  3.06 \n 4 lm - linear_reg    32.4       28.0  4.35 \n 5 lm - linear_reg    16.4       15.0  1.40 \n 6 lm - linear_reg    21.5       22.3 -0.779\n 7 lm - linear_reg    15.8       17.2 -1.40 \n 8 lm - linear_reg    15         15.1 -0.100\n 9 lm - linear_reg    14.7       10.9  3.85 \n10 lm - linear_reg    10.4       10.8 -0.445\n# ℹ 22 more rows\n\n\nNow let’s set .pivot_long = TRUE:\n\nextract_regression_residuals(.model_tbl = frt_tbl, .pivot_long = TRUE)[[1]]\n\n# A tibble: 96 × 3\n   .model_type     name       value\n   &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;\n 1 lm - linear_reg .actual    15.2 \n 2 lm - linear_reg .predicted 17.3 \n 3 lm - linear_reg .resid     -2.09\n 4 lm - linear_reg .actual    10.4 \n 5 lm - linear_reg .predicted 11.9 \n 6 lm - linear_reg .resid     -1.46\n 7 lm - linear_reg .actual    33.9 \n 8 lm - linear_reg .predicted 30.8 \n 9 lm - linear_reg .resid      3.06\n10 lm - linear_reg .actual    32.4 \n# ℹ 86 more rows\n\n\nNow let’s visualize the data:\n\nresid_tbl &lt;- extract_regression_residuals(frt_tbl, TRUE)\n\nresid_tbl |&gt;\n  map(\\(x) x |&gt;\n        group_by(name) |&gt;\n        mutate(x = row_number()) |&gt;\n        ungroup() |&gt;\n        mutate(plot_group = ifelse(name == \".resid\", \"Residuals\", \"Actual and Predictions\")) |&gt;\n        ggplot(aes(x = x, y = value, group = name, color = name)) +\n        geom_line() +\n        theme_minimal() +\n        facet_wrap(~ plot_group, ncol = 1, scales = \"free\") +\n        labs(\n          x = \"\",\n          y = \"Value\",\n          title = \"Actual, Predicted, and Residual Values by Model Type\",\n          subtitle = x$.model_type[1],\n          color = \"Data Type\"\n        )\n      )\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nAnd that’s it!\nThank you for reading and I would love to hear your feedback. Please feel free to reach out to me."
  },
  {
    "objectID": "posts/2024-01-22/index.html",
    "href": "posts/2024-01-22/index.html",
    "title": "Demystifying bootstrap_stat_plot(): Your Ticket to Insightful Data Exploration",
    "section": "",
    "text": "Ever feel like your data is hiding secrets? Like it’s whispering truths but you just can’t quite grasp them? Well, fear not, fellow data sleuths! Today, we’ll crack the code of an R function that’s like a magnifying glass for your statistical investigations: bootstrap_stat_plot() from the TidyDensity package.\nImagine this: You have a dataset, say, car mileage (MPG) from the classic mtcars dataset. You want to understand the average MPG, but what if that average is just a mirage? What if it’s skewed by a few outliers or doesn’t capture the full story?\nEnter bootstrapping, a statistical technique that’s like taking your data on a wild ride. It creates multiple copies of your data, each with a slight twist, and then calculates the statistic you’re interested in (e.g., average MPG) for each copy. This gives you a distribution of possible averages, revealing the variability and potential biases lurking beneath the surface.\nbootstrap_stat_plot() takes this magic a step further. It not only calculates the distribution but also visualizes it, giving you a clear picture of how the statistic fluctuates across different versions of your data. It’s like a magnifying glass for your statistical investigations!"
  },
  {
    "objectID": "posts/2024-01-22/index.html#syntax",
    "href": "posts/2024-01-22/index.html#syntax",
    "title": "Demystifying bootstrap_stat_plot(): Your Ticket to Insightful Data Exploration",
    "section": "Syntax",
    "text": "Syntax\nLet’s take a look at the function:\nbootstrap_stat_plot(\n  .data,\n  .value,\n  .stat = \"cmean\",\n  .show_groups = FALSE,\n  .show_ci_labels = TRUE,\n  .interactive = FALSE\n)"
  },
  {
    "objectID": "posts/2024-01-22/index.html#arguments",
    "href": "posts/2024-01-22/index.html#arguments",
    "title": "Demystifying bootstrap_stat_plot(): Your Ticket to Insightful Data Exploration",
    "section": "Arguments",
    "text": "Arguments\n1. The Data:\n\n.data: The data frame containing your data.\n\n2. The Value:\n\n.value: The variable you want to calculate the statistic for.\n\n3. The Statistic:\n\n.stat: The statistic you want to calculate. Options include:\n\ncmean: The mean\ncmedian: The median\ncmin: The minimum\ncmax: The maximum\ncsd: The standard deviation\ncvar: The variance\nand many others!\n\n\n4. Show Groups:\n\n.show_groups: Whether to show the groups in the plot. Default is FALSE.\n\n5. Show Confidence Interval Labels:\n\n.show_ci_labels: Whether to show the confidence interval labels in the plot. Default is TRUE.\n\n6. Interactive:\n\n.interactive: Whether to make the plot interactive. Default is FALSE."
  },
  {
    "objectID": "posts/2024-01-23/index.html",
    "href": "posts/2024-01-23/index.html",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "Hey folks! Today, we’re diving into the world of R programming, and our star of the show is the lengths() function. This little gem might not be as famous as some other R functions, but it’s incredibly handy when it comes to exploring the lengths of elements in your data structures.\n\n\nIn a nutshell, lengths() is a function in R that returns a vector of the lengths of the elements in a list, vector, or other data structure. It’s like a measuring tape for your data, allowing you to quickly assess the size of different components.\n\n\n\n\n\n\n# Create a numeric vector\nnumeric_vector &lt;- c(10, 20, 30, 40, 50)\n\n# Use lengths() to get the lengths of elements\nelement_lengths &lt;- lengths(list(numeric_vector))\n\n# Print the result\nprint(element_lengths)\n\n[1] 5\n\n\nIn this example, we create a numeric vector and use lengths() to find out how many elements it contains. The output will be a vector with a single value, representing the length of our numeric vector.\n\n\n\n\n# Create a list with elements of different lengths\nmixed_list &lt;- list(c(1, 2, 3), \"Hello\", matrix(1:6, ncol = 2))\n\n# Use lengths() to get the lengths of elements\nelement_lengths &lt;- lengths(mixed_list)\n\n# Print the result\nprint(element_lengths)\n\n[1] 3 1 6\n\n\nHere, we’ve crafted a list with diverse elements – a numeric vector, a character string, and a matrix. lengths() now gives us a vector containing the lengths of each element in the list.\n\n\n\n\n# Create a data frame\ndata_frame_example &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n                                 Age = c(25, 30, 22),\n                                 Score = c(90, 85, 95))\n\n# Use lengths() to get the lengths of columns in the data frame\ncolumn_lengths &lt;- lengths(data_frame_example)\n\n# Print the result\nprint(column_lengths)\n\n Name   Age Score \n    3     3     3 \n\n\nIn this example, we’re working with a data frame. lengths() allows us to check the number of elements in each column, providing insights into the structure of our data.\n\n\n\n\nUnderstanding the lengths of elements in your data is crucial for efficient data manipulation. Whether you’re dealing with lists, vectors, or data frames, knowing the sizes of different components can guide your analysis and help you avoid unexpected surprises.\n\n\n\nNow that you’ve seen some examples, I encourage you to grab your own datasets, create different structures, and experiment with lengths(). It’s a fantastic tool for quickly grasping the dimensions of your data.\nRemember, the best way to learn is by doing. So fire up your R console, start experimenting, and feel the satisfaction of mastering yet another powerful tool in your R toolkit!\nHappy coding! 🚀✨"
  },
  {
    "objectID": "posts/2024-01-23/index.html#what-is-lengths-and-why-should-you-care",
    "href": "posts/2024-01-23/index.html#what-is-lengths-and-why-should-you-care",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "In a nutshell, lengths() is a function in R that returns a vector of the lengths of the elements in a list, vector, or other data structure. It’s like a measuring tape for your data, allowing you to quickly assess the size of different components."
  },
  {
    "objectID": "posts/2024-01-23/index.html#lets-get-started-with-examples",
    "href": "posts/2024-01-23/index.html#lets-get-started-with-examples",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "# Create a numeric vector\nnumeric_vector &lt;- c(10, 20, 30, 40, 50)\n\n# Use lengths() to get the lengths of elements\nelement_lengths &lt;- lengths(list(numeric_vector))\n\n# Print the result\nprint(element_lengths)\n\n[1] 5\n\n\nIn this example, we create a numeric vector and use lengths() to find out how many elements it contains. The output will be a vector with a single value, representing the length of our numeric vector.\n\n\n\n\n# Create a list with elements of different lengths\nmixed_list &lt;- list(c(1, 2, 3), \"Hello\", matrix(1:6, ncol = 2))\n\n# Use lengths() to get the lengths of elements\nelement_lengths &lt;- lengths(mixed_list)\n\n# Print the result\nprint(element_lengths)\n\n[1] 3 1 6\n\n\nHere, we’ve crafted a list with diverse elements – a numeric vector, a character string, and a matrix. lengths() now gives us a vector containing the lengths of each element in the list.\n\n\n\n\n# Create a data frame\ndata_frame_example &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n                                 Age = c(25, 30, 22),\n                                 Score = c(90, 85, 95))\n\n# Use lengths() to get the lengths of columns in the data frame\ncolumn_lengths &lt;- lengths(data_frame_example)\n\n# Print the result\nprint(column_lengths)\n\n Name   Age Score \n    3     3     3 \n\n\nIn this example, we’re working with a data frame. lengths() allows us to check the number of elements in each column, providing insights into the structure of our data."
  },
  {
    "objectID": "posts/2024-01-23/index.html#why-should-you-experiment",
    "href": "posts/2024-01-23/index.html#why-should-you-experiment",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "Understanding the lengths of elements in your data is crucial for efficient data manipulation. Whether you’re dealing with lists, vectors, or data frames, knowing the sizes of different components can guide your analysis and help you avoid unexpected surprises."
  },
  {
    "objectID": "posts/2024-01-23/index.html#your-turn-to-play",
    "href": "posts/2024-01-23/index.html#your-turn-to-play",
    "title": "Exploring Data Lengths with R’s lengths() Function",
    "section": "",
    "text": "Now that you’ve seen some examples, I encourage you to grab your own datasets, create different structures, and experiment with lengths(). It’s a fantastic tool for quickly grasping the dimensions of your data.\nRemember, the best way to learn is by doing. So fire up your R console, start experimenting, and feel the satisfaction of mastering yet another powerful tool in your R toolkit!\nHappy coding! 🚀✨"
  },
  {
    "objectID": "posts/2024-01-24/index.html",
    "href": "posts/2024-01-24/index.html",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Greetings fellow R enthusiasts! Today, let’s dive into the fascinating world of date calculations. Whether you’re a data scientist, analyst, or just someone who loves coding in R, understanding how to calculate the number of months between dates is a valuable skill. In this blog post, we’ll explore two approaches using both base R and the lubridate package, ensuring you have the tools to tackle any date-related challenge that comes your way."
  },
  {
    "objectID": "posts/2024-01-24/index.html#base-r-method",
    "href": "posts/2024-01-24/index.html#base-r-method",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Base R Method",
    "text": "Base R Method\nLet’s start with the basics – base R. The difftime function will be our trusty companion in this method. The idea is to find the time difference between two dates and then convert it into months.\n\n# Sample dates\nstart_date &lt;- as.Date(\"2022-01-15\")\nend_date &lt;- as.Date(\"2023-07-20\")\n\n# Calculate time difference in days\ntime_diff_days &lt;- end_date - start_date\n\n# Convert days to months\nmonths_diff_base &lt;- as.numeric(time_diff_days) / 30.44  # average days in a month\n\ncat(\"Number of months using base R:\", round(months_diff_base, 2), \"\\n\")\n\nNumber of months using base R: 18.1"
  },
  {
    "objectID": "posts/2024-01-24/index.html#explanation",
    "href": "posts/2024-01-24/index.html#explanation",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Explanation",
    "text": "Explanation\n\nWe define our start and end dates using the as.Date function.\nCalculate the time difference in days using the subtraction operator.\nConvert the time difference to months by dividing by the average days in a month (30.44)."
  },
  {
    "objectID": "posts/2024-01-24/index.html#lubridate-package-method",
    "href": "posts/2024-01-24/index.html#lubridate-package-method",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Lubridate Package Method",
    "text": "Lubridate Package Method\nNow, let’s add a touch of elegance to our date calculations with the lubridate package. This package simplifies working with dates and times in R, making our code more readable and intuitive.\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Sample dates\nstart_date &lt;- ymd(\"2022-01-15\")\nend_date &lt;- ymd(\"2023-07-20\")\n\n# Calculate months difference using lubridate\nmonths_diff_lubridate &lt;- interval(start_date, end_date) %/% months(1)\n\ncat(\"Number of months using lubridate:\", months_diff_lubridate, \"\\n\")\n\nNumber of months using lubridate: 18"
  },
  {
    "objectID": "posts/2024-01-24/index.html#explanation-1",
    "href": "posts/2024-01-24/index.html#explanation-1",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Explanation",
    "text": "Explanation\n\nWe load the lubridate package to leverage its convenient date functions.\nUse the ymd function to convert our dates into lubridate date objects.\nCreate an interval between the start and end dates and use %/% to get the floor division by months."
  },
  {
    "objectID": "posts/2024-01-24/index.html#handling-partial-months",
    "href": "posts/2024-01-24/index.html#handling-partial-months",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "Handling Partial Months",
    "text": "Handling Partial Months\nLife isn’t always about whole months, and our date calculations should reflect that reality. Let’s modify our examples to include partial months.\n\n# Sample dates with partial months\nstart_date_partial &lt;- as.Date(\"2022-01-15\")\nend_date_partial &lt;- as.Date(\"2023-07-20\") - 15  # subtract 15 days for a partial month\n\n# Base R with partial months\ntime_diff_days_partial &lt;- end_date_partial - start_date_partial\nmonths_diff_base_partial &lt;- as.numeric(time_diff_days_partial) / 30.44\n\ncat(\"Number of months (with partial) using base R:\", round(months_diff_base_partial, 2), \"\\n\")\n\nNumber of months (with partial) using base R: 17.61 \n\n# Lubridate with partial months\nmonths_diff_lubridate_partial &lt;- interval(start_date_partial, end_date_partial) / months(1)\n\ncat(\"Number of months (with partial) using lubridate:\", months_diff_lubridate_partial, \"\\n\")\n\nNumber of months (with partial) using lubridate: 17.66667"
  },
  {
    "objectID": "posts/2024-01-24/index.html#more-lubridate-with-interval",
    "href": "posts/2024-01-24/index.html#more-lubridate-with-interval",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "More lubridate with interval()",
    "text": "More lubridate with interval()\nThe lubridate package makes working with dates in R much easier. It provides the interval function to calculate the time difference between two dates:\n\ndate1 &lt;- ymd(\"2023-01-15\")\ndate2 &lt;- ymd(\"2024-04-30\")\n\ninterval(date1, date2) / months(1) \n\n[1] 15.5\n\n\nThis returns the number of months including the partial:\n[1] 15.870968\nTo get just the full months:\n\ninterval(date1, date2) %/% months(1)\n\n[1] 15\n\n\nWhich gives:\n[1] 15\nThe interval function combined with lubridate’s months makes this a very clean way to calculate both full and partial months between dates."
  },
  {
    "objectID": "posts/2024-01-25/index.html",
    "href": "posts/2024-01-25/index.html",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Welcome back, fellow R enthusiasts! Today, we’re diving into a common task in data manipulation: subtracting hours from time objects in R. Whether you’re working with timestamps, time durations, or time series data, knowing how to subtract hours can be incredibly useful. In this post, we’ll explore two popular methods: using base R functions and the lubridate package.\n\n\nBefore we jump into the code, let’s quickly discuss why you might need to subtract hours from time objects. This operation is handy in various scenarios, such as:\n\nAdjusting timestamps for different time zones.\nCalculating time differences between events.\nShifting time points in time series analysis.\n\nNow, let’s get our hands dirty with some code!\n\n\n\nIn base R, we can perform basic arithmetic operations on time objects. To subtract hours from a time object, we’ll use the POSIXct class, which represents date and time information. Here’s a simple example:\n\n# Create a POSIXct object representing a specific time\nmy_time &lt;- as.POSIXct(\"2024-01-25 10:00:00\")\n\n# Subtract 2 hours from the original time\nnew_time &lt;- my_time - (2 * 60 * 60)\n\n# Print the original and modified times\nprint(my_time)\n\n[1] \"2024-01-25 10:00:00 EST\"\n\nprint(new_time)\n\n[1] \"2024-01-25 08:00:00 EST\"\n\n\nIn this code snippet, we first create a POSIXct object my_time representing 10:00 AM on January 25, 2024. Then, we subtract 2 hours and assign the result to new_time. Finally, we print both the original and modified times to see the difference.\n\n\n\nThe lubridate package provides convenient functions for handling date-time data in R. It simplifies common tasks like parsing dates, extracting components, and performing arithmetic operations. Let’s see how we can subtract hours using lubridate:\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Create a POSIXct object representing a specific time\nmy_time &lt;- ymd_hms(\"2024-01-25 10:00:00\")\n\n# Subtract 2 hours from the original time\nnew_time &lt;- my_time - hours(2)\n\n# Print the original and modified times\nprint(my_time)\n\n[1] \"2024-01-25 10:00:00 UTC\"\n\nprint(new_time)\n\n[1] \"2024-01-25 08:00:00 UTC\"\n\n\nIn this example, we start by loading the lubridate package. Then, we use the ymd_hms() function to create a POSIXct object my_time. Next, we subtract 2 hours using the hours() function and assign the result to new_time. Finally, we print both times to compare the changes.\n\n\n\nLet’s explore a few more examples to solidify our understanding:\n\n\n\n# Create a vector of POSIXct times\ntimes &lt;- as.POSIXct(c(\"2024-01-25 08:00:00\", \"2024-01-25 12:00:00\"))\n\n# Subtract 1 hour from each time\nadjusted_times &lt;- times - hours(1)\n\n# Print the original and modified times\nprint(times)\n\n[1] \"2024-01-25 08:00:00 EST\" \"2024-01-25 12:00:00 EST\"\n\nprint(adjusted_times)\n\n[1] \"2024-01-25 07:00:00 EST\" \"2024-01-25 11:00:00 EST\"\n\n\nIn this example, we have a vector of two times, and we subtract 1 hour from each using the hours() function.\n\n\n\n\n# Create a time interval from 9:00 AM to 5:00 PM\ntime_interval &lt;- interval(ymd_hms(\"2024-01-25 09:00:00\"), ymd_hms(\"2024-01-25 17:00:00\"))\n\n# Subtract 2 hours from the interval\nadjusted_interval &lt;- int_shift(time_interval, - hours(2))\n\n# Print the original and modified intervals\nprint(time_interval)\n\n[1] 2024-01-25 09:00:00 UTC--2024-01-25 17:00:00 UTC\n\nprint(adjusted_interval)\n\n[1] 2024-01-25 07:00:00 UTC--2024-01-25 15:00:00 UTC\n\n\nIn this example, we create a time interval representing working hours and subtract 2 hours from it.\n\n\n\n\nSubtracting hours from time objects is a fundamental operation in data manipulation and time series analysis. In this post, we explored two methods: using base R functions and the lubridate package. Whether you prefer the simplicity of base R or the convenience of lubridate, mastering this skill will undoubtedly enhance your R programming repertoire.\nNow it’s your turn! Try out these examples with your own time data and experiment with different hour values. Don’t hesitate to reach out if you have any questions or want to share your experiences. Happy coding!"
  },
  {
    "objectID": "posts/2024-01-25/index.html#why-subtract-hours",
    "href": "posts/2024-01-25/index.html#why-subtract-hours",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Before we jump into the code, let’s quickly discuss why you might need to subtract hours from time objects. This operation is handy in various scenarios, such as:\n\nAdjusting timestamps for different time zones.\nCalculating time differences between events.\nShifting time points in time series analysis.\n\nNow, let’s get our hands dirty with some code!"
  },
  {
    "objectID": "posts/2024-01-25/index.html#using-base-r-functions",
    "href": "posts/2024-01-25/index.html#using-base-r-functions",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "In base R, we can perform basic arithmetic operations on time objects. To subtract hours from a time object, we’ll use the POSIXct class, which represents date and time information. Here’s a simple example:\n\n# Create a POSIXct object representing a specific time\nmy_time &lt;- as.POSIXct(\"2024-01-25 10:00:00\")\n\n# Subtract 2 hours from the original time\nnew_time &lt;- my_time - (2 * 60 * 60)\n\n# Print the original and modified times\nprint(my_time)\n\n[1] \"2024-01-25 10:00:00 EST\"\n\nprint(new_time)\n\n[1] \"2024-01-25 08:00:00 EST\"\n\n\nIn this code snippet, we first create a POSIXct object my_time representing 10:00 AM on January 25, 2024. Then, we subtract 2 hours and assign the result to new_time. Finally, we print both the original and modified times to see the difference."
  },
  {
    "objectID": "posts/2024-01-25/index.html#using-lubridate-package",
    "href": "posts/2024-01-25/index.html#using-lubridate-package",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "The lubridate package provides convenient functions for handling date-time data in R. It simplifies common tasks like parsing dates, extracting components, and performing arithmetic operations. Let’s see how we can subtract hours using lubridate:\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Create a POSIXct object representing a specific time\nmy_time &lt;- ymd_hms(\"2024-01-25 10:00:00\")\n\n# Subtract 2 hours from the original time\nnew_time &lt;- my_time - hours(2)\n\n# Print the original and modified times\nprint(my_time)\n\n[1] \"2024-01-25 10:00:00 UTC\"\n\nprint(new_time)\n\n[1] \"2024-01-25 08:00:00 UTC\"\n\n\nIn this example, we start by loading the lubridate package. Then, we use the ymd_hms() function to create a POSIXct object my_time. Next, we subtract 2 hours using the hours() function and assign the result to new_time. Finally, we print both times to compare the changes."
  },
  {
    "objectID": "posts/2024-01-25/index.html#additional-examples",
    "href": "posts/2024-01-25/index.html#additional-examples",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Let’s explore a few more examples to solidify our understanding:\n\n\n\n# Create a vector of POSIXct times\ntimes &lt;- as.POSIXct(c(\"2024-01-25 08:00:00\", \"2024-01-25 12:00:00\"))\n\n# Subtract 1 hour from each time\nadjusted_times &lt;- times - hours(1)\n\n# Print the original and modified times\nprint(times)\n\n[1] \"2024-01-25 08:00:00 EST\" \"2024-01-25 12:00:00 EST\"\n\nprint(adjusted_times)\n\n[1] \"2024-01-25 07:00:00 EST\" \"2024-01-25 11:00:00 EST\"\n\n\nIn this example, we have a vector of two times, and we subtract 1 hour from each using the hours() function.\n\n\n\n\n# Create a time interval from 9:00 AM to 5:00 PM\ntime_interval &lt;- interval(ymd_hms(\"2024-01-25 09:00:00\"), ymd_hms(\"2024-01-25 17:00:00\"))\n\n# Subtract 2 hours from the interval\nadjusted_interval &lt;- int_shift(time_interval, - hours(2))\n\n# Print the original and modified intervals\nprint(time_interval)\n\n[1] 2024-01-25 09:00:00 UTC--2024-01-25 17:00:00 UTC\n\nprint(adjusted_interval)\n\n[1] 2024-01-25 07:00:00 UTC--2024-01-25 15:00:00 UTC\n\n\nIn this example, we create a time interval representing working hours and subtract 2 hours from it."
  },
  {
    "objectID": "posts/2024-01-25/index.html#conclusion",
    "href": "posts/2024-01-25/index.html#conclusion",
    "title": "Mastering Date Calculations in R: A Guide to Calculating Months with Base R and lubridate",
    "section": "",
    "text": "Subtracting hours from time objects is a fundamental operation in data manipulation and time series analysis. In this post, we explored two methods: using base R functions and the lubridate package. Whether you prefer the simplicity of base R or the convenience of lubridate, mastering this skill will undoubtedly enhance your R programming repertoire.\nNow it’s your turn! Try out these examples with your own time data and experiment with different hour values. Don’t hesitate to reach out if you have any questions or want to share your experiences. Happy coding!"
  },
  {
    "objectID": "posts/2024-01-26/index.html",
    "href": "posts/2024-01-26/index.html",
    "title": "Unveiling the Earliest Date: A Journey Through R",
    "section": "",
    "text": "Greetings, fellow data enthusiasts! Today, we embark on a quest to uncover the earliest date lurking within a column of dates using the power of R. Whether you’re a seasoned R programmer or a curious newcomer, fear not, for we shall navigate through this journey step by step, unraveling the mysteries of date manipulation along the way.\nImagine you have a dataset filled with dates, and you’re tasked with finding the earliest one among them. How would you tackle this challenge? Fear not, for R comes to our rescue with its arsenal of functions and packages."
  },
  {
    "objectID": "posts/2024-01-26/index.html#example-1-finding-the-earliest-date-in-a-simple-dataset",
    "href": "posts/2024-01-26/index.html#example-1-finding-the-earliest-date-in-a-simple-dataset",
    "title": "Unveiling the Earliest Date: A Journey Through R",
    "section": "Example 1: Finding the earliest date in a simple dataset:",
    "text": "Example 1: Finding the earliest date in a simple dataset:\n\n# Sample dataset\ndates &lt;- as.Date(c(\"2023-01-15\", \"2023-02-20\", \"2022-12-10\"))\n\n# Finding the earliest date\nearliest_date &lt;- min(dates)\nprint(earliest_date)\n\n[1] \"2022-12-10\""
  },
  {
    "objectID": "posts/2024-01-26/index.html#example-2-handling-missing-values-gracefully",
    "href": "posts/2024-01-26/index.html#example-2-handling-missing-values-gracefully",
    "title": "Unveiling the Earliest Date: A Journey Through R",
    "section": "Example 2: Handling missing values gracefully:",
    "text": "Example 2: Handling missing values gracefully:\n\n# Sample dataset with missing values\ndates_with_na &lt;- as.Date(c(\"2023-01-15\", NA, \"2022-12-10\"))\n\n# Finding the earliest date, ignoring missing values\nearliest_date &lt;- min(dates_with_na, na.rm = TRUE)\nprint(earliest_date)\n\n[1] \"2022-12-10\""
  },
  {
    "objectID": "posts/2024-01-29/index.html",
    "href": "posts/2024-01-29/index.html",
    "title": "How to Extract Month from Date in R (With Examples)",
    "section": "",
    "text": "Greetings fellow R enthusiasts! Today, we’re diving into a fundamental task: extracting the month from a date in R. Whether you’re new to R or a seasoned pro, understanding how to manipulate dates is essential. We’ll explore two popular methods: using base R and the powerful lubridate package. So, let’s roll up our sleeves and get started!\n\n\nFirst up, let’s tackle the task with base R. We’ll use the format() function to extract the month from a date.\n\n\n\n\n# Create a vector of dates\ndates_vector &lt;- as.Date(c(\"2023-01-15\", \"2023-05-20\", \"2023-09-10\"))\n\n# Extract the month\nmonths &lt;- format(dates_vector, \"%m\")\n\n# Print the result\nprint(months)\n\n[1] \"01\" \"05\" \"09\"\n\n\nIn this example, we have a vector of dates. We use the format() function to specify that we want to extract the month (%m), and voila! We get the months corresponding to each date.\n\n\n\n# Create a sample data frame\ndf &lt;- data.frame(date = as.Date(c(\"2023-01-15\", \"2023-05-20\", \"2023-09-10\")))\n\n# Extract the month from the 'date' column\ndf$month &lt;- format(df$date, \"%m\")\n\n# Print the data frame with the new 'month' column\nprint(df)\n\n        date month\n1 2023-01-15    01\n2 2023-05-20    05\n3 2023-09-10    09\n\n\nHere, we’re working with a data frame. We use the $ operator to access the ‘date’ column and apply the format() function to extract the month. The result is a data frame with an additional ‘month’ column containing the extracted months.\n\n\n\n\n# Single date\nsingle_date &lt;- as.Date(\"2023-07-04\")\n\n# Extract the month\nmonth &lt;- format(single_date, \"%m\")\n\n# Print the result\nprint(month)\n\n[1] \"07\"\n\n\nEven if you have just one date, you can still use the format() function to extract the month. Simple and effective!\n\n\n\n\nNow, let’s switch gears and explore how to achieve the same task using the lubridate package, known for its user-friendly date-time functions.\n\n\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Create a sample date\ndate &lt;- ymd(\"2023-11-30\")\n\n# Extract the month using lubridate's month() function\nmonth &lt;- month(date)\n\n# Print the result\nprint(month)\n\n[1] 11\n\n\nWith lubridate, we simplify the process using the month() function directly on the date object. It’s clean, concise, and effortlessly extracts the month."
  },
  {
    "objectID": "posts/2024-01-29/index.html#using-base-r",
    "href": "posts/2024-01-29/index.html#using-base-r",
    "title": "How to Extract Month from Date in R (With Examples)",
    "section": "",
    "text": "First up, let’s tackle the task with base R. We’ll use the format() function to extract the month from a date."
  },
  {
    "objectID": "posts/2024-01-29/index.html#example-1-extracting-month-from-a-vector-of-dates",
    "href": "posts/2024-01-29/index.html#example-1-extracting-month-from-a-vector-of-dates",
    "title": "How to Extract Month from Date in R (With Examples)",
    "section": "",
    "text": "# Create a vector of dates\ndates_vector &lt;- as.Date(c(\"2023-01-15\", \"2023-05-20\", \"2023-09-10\"))\n\n# Extract the month\nmonths &lt;- format(dates_vector, \"%m\")\n\n# Print the result\nprint(months)\n\n[1] \"01\" \"05\" \"09\"\n\n\nIn this example, we have a vector of dates. We use the format() function to specify that we want to extract the month (%m), and voila! We get the months corresponding to each date.\n\n\n\n# Create a sample data frame\ndf &lt;- data.frame(date = as.Date(c(\"2023-01-15\", \"2023-05-20\", \"2023-09-10\")))\n\n# Extract the month from the 'date' column\ndf$month &lt;- format(df$date, \"%m\")\n\n# Print the data frame with the new 'month' column\nprint(df)\n\n        date month\n1 2023-01-15    01\n2 2023-05-20    05\n3 2023-09-10    09\n\n\nHere, we’re working with a data frame. We use the $ operator to access the ‘date’ column and apply the format() function to extract the month. The result is a data frame with an additional ‘month’ column containing the extracted months.\n\n\n\n\n# Single date\nsingle_date &lt;- as.Date(\"2023-07-04\")\n\n# Extract the month\nmonth &lt;- format(single_date, \"%m\")\n\n# Print the result\nprint(month)\n\n[1] \"07\"\n\n\nEven if you have just one date, you can still use the format() function to extract the month. Simple and effective!"
  },
  {
    "objectID": "posts/2024-01-29/index.html#using-lubridate-package",
    "href": "posts/2024-01-29/index.html#using-lubridate-package",
    "title": "How to Extract Month from Date in R (With Examples)",
    "section": "",
    "text": "Now, let’s switch gears and explore how to achieve the same task using the lubridate package, known for its user-friendly date-time functions.\n\n\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Create a sample date\ndate &lt;- ymd(\"2023-11-30\")\n\n# Extract the month using lubridate's month() function\nmonth &lt;- month(date)\n\n# Print the result\nprint(month)\n\n[1] 11\n\n\nWith lubridate, we simplify the process using the month() function directly on the date object. It’s clean, concise, and effortlessly extracts the month."
  },
  {
    "objectID": "posts/2024-01-30/index.html",
    "href": "posts/2024-01-30/index.html",
    "title": "Mastering Time Manipulation in R: Subtracting Hours with Ease",
    "section": "",
    "text": "Ever wished you could rewind time in R, not just for debugging, but for actual data analysis? Well, you don’t need plutonium and flux capacitors! Let’s dive into the fascinating world of time manipulation in R, specifically subtracting hours from timestamps. We’ll explore two approaches: one using base R’s time-bending tricks, and another powered by the lubridate package, our time-traveling companion."
  },
  {
    "objectID": "posts/2024-01-30/index.html#base-r-back-to-the-basics",
    "href": "posts/2024-01-30/index.html#base-r-back-to-the-basics",
    "title": "Mastering Time Manipulation in R: Subtracting Hours with Ease",
    "section": "Base R: Back to the Basics",
    "text": "Base R: Back to the Basics\nImagine a timestamp like a ticking clock. Each second is another notch on the gears, and we want to turn those gears backward a few hours. Base R lets us do this by treating time as numbers. Remember, there are 3600 seconds in an hour, so to subtract 2 hours, we simply:\n\nmy_time &lt;- as.POSIXct(\"2024-01-30 10:00:00\") # Create a time object\nnew_time &lt;- my_time - (2 * 3600) # Subtract 2 hours (2 * 3600 seconds)\nprint(my_time) # See the original time\n\n[1] \"2024-01-30 10:00:00 EST\"\n\nprint(new_time) # Voila! 2 hours back!\n\n[1] \"2024-01-30 08:00:00 EST\"\n\n\nThis code tells R to:\n\nCreate a time object my_time representing “January 30, 2024, 10:00 AM”.\nDefine new_time by subtracting 2 hours from my_time. We multiply 2 by 3600 because, well, you get the point.\nPrint both times to see the magic unfold."
  },
  {
    "objectID": "posts/2024-01-30/index.html#lubridate-time-travel-made-easy",
    "href": "posts/2024-01-30/index.html#lubridate-time-travel-made-easy",
    "title": "Mastering Time Manipulation in R: Subtracting Hours with Ease",
    "section": "Lubridate: Time Travel Made Easy",
    "text": "Lubridate: Time Travel Made Easy\nBut what if you want a fancier ride? This is where lubridate comes in! This package adds superpowers to our time-traveling toolkit. Let’s rewrite the above using its hours() function:\n\nlibrary(lubridate) # Load the lubridate package\n\nmy_time &lt;- as.POSIXct(\"2024-01-30 10:00:00\")\nnew_time &lt;- my_time - hours(2) # Subtract 2 hours with the `hours()` function\nprint(my_time)\n\n[1] \"2024-01-30 10:00:00 EST\"\n\nprint(new_time)\n\n[1] \"2024-01-30 08:00:00 EST\"\n\n\nThis code does the same thing, but with less math and more clarity. We simply tell R to subtract 2 hours using the hours(2) function, making the code cleaner and more readable."
  },
  {
    "objectID": "posts/2024-01-31/index.html",
    "href": "posts/2024-01-31/index.html",
    "title": "Time Flies? Time Travels! Adding Days to Dates in R (Like a Pro)",
    "section": "",
    "text": "Ever wished you could skip ahead a few days for that weekend getaway, or rewind to relive a magical moment? While real-life time travel remains a sci-fi dream, in R, adding days to dates is a breeze! Today, we’ll explore both base R and the powerful lubridate and timetk packages to master this handy skill."
  },
  {
    "objectID": "posts/2024-01-31/index.html#example-1-base-r-basics",
    "href": "posts/2024-01-31/index.html#example-1-base-r-basics",
    "title": "Time Flies? Time Travels! Adding Days to Dates in R (Like a Pro)",
    "section": "Example 1: Base R Basics",
    "text": "Example 1: Base R Basics\nLet’s start with the classic. Imagine you have a date stored as my_date &lt;- \"2024-01-31\" (yes, today!). To add, say, 5 days, you can simply use my_date + 5. Voila! You’ve time-jumped to February 5th, 2024. But wait, this doesn’t handle months or leap years like a pro.\n\n# Create a date object\ndate &lt;- as.Date(\"2024-01-31\")\n\n# Add 5 days to the date\nnew_date &lt;- date + 5\n\nprint(date)\n\n[1] \"2024-01-31\"\n\nprint(new_date)\n\n[1] \"2024-02-05\"\n\nclass(date)\n\n[1] \"Date\"\n\nclass(new_date)\n\n[1] \"Date\""
  },
  {
    "objectID": "posts/2024-01-31/index.html#example-2-enter-lubridate",
    "href": "posts/2024-01-31/index.html#example-2-enter-lubridate",
    "title": "Time Flies? Time Travels! Adding Days to Dates in R (Like a Pro)",
    "section": "Example 2: Enter lubridate",
    "text": "Example 2: Enter lubridate\nThis superhero package offers functions like as.Date() and days() that understand the nuances of dates. Let’s revisit our example:\n\nlibrary(lubridate)\n\nmy_date &lt;- as.Date(\"2024-01-31\") # Convert string to Date object\nfuture_date &lt;- my_date + days(5) # Add 5 days using days()\n\nfuture_date # \"2024-02-05\"\n\n[1] \"2024-02-05\"\n\n\nSee the magic? days(5) tells R to add 5 days specifically. You can even subtract days (imagine reliving that delicious pizza!):\n\npizza_day &lt;- as.Date(\"2024-01-27\") # Date of pizza bliss\nrelive_pizza &lt;- pizza_day - days(2) # Travel back 2 days\n\nrelive_pizza # \"2024-01-25\"\n\n[1] \"2024-01-25\""
  },
  {
    "objectID": "posts/2024-01-31/index.html#example-3-beyond-days-timetk-takes-the-wheel",
    "href": "posts/2024-01-31/index.html#example-3-beyond-days-timetk-takes-the-wheel",
    "title": "Time Flies? Time Travels! Adding Days to Dates in R (Like a Pro)",
    "section": "Example 3: Beyond Days: timetk Takes the Wheel",
    "text": "Example 3: Beyond Days: timetk Takes the Wheel\nWant to add weeks, months, or even years? timetk takes things to the next level with functions like years(), wednesdays(), and more. Check this out:\n\nlibrary(timetk)\n\ngraduation &lt;- as.Date(\"2025-06-15\") # Your graduation date (hopefully!)\n\ngraduation %+time% \"1 hour 34 seconds\"\n\n[1] \"2025-06-15 01:00:34 UTC\"\n\ngraduation %+time% \"3 months\"\n\n[1] \"2025-09-15\"\n\ngraduation %+time% \"1 year 3 months 6 days\"\n\n[1] \"2026-09-21\"\n\n# Backward (Minus Time)\ngraduation %-time% \"1 hour 34 seconds\"\n\n[1] \"2025-06-14 22:59:26 UTC\"\n\ngraduation %-time% \"3 months\"\n\n[1] \"2025-03-15\"\n\ngraduation %-time% \"1 year 3 months 6 days\"\n\n[1] \"2024-03-09\"\n\n\nBonus Tip: Don’t forget about formatting! Use format() with options like \"%Y-%m-%d\" to display your dates in your preferred format."
  },
  {
    "objectID": "posts/2024-02-01/index.html",
    "href": "posts/2024-02-01/index.html",
    "title": "R for the Real World: Counting those Business Days like a Pro!",
    "section": "",
    "text": "Hi fellow coders, data wranglers, and all-around R enthusiasts! Have you ever been stuck calculating the number of business days between two dates? You know, like figuring out how long that project actually took, excluding weekends (because let’s be honest, who works on those?). Well, fret no more! Today, we’re diving into the wonderful world of business day calculations in R with some easy-to-follow examples. Buckle up, it’s gonna be a productive ride!"
  },
  {
    "objectID": "posts/2024-02-01/index.html#step-1-grabbing-the-toolkit",
    "href": "posts/2024-02-01/index.html#step-1-grabbing-the-toolkit",
    "title": "R for the Real World: Counting those Business Days like a Pro!",
    "section": "Step 1: Grabbing the Toolkit",
    "text": "Step 1: Grabbing the Toolkit\nFirst things first, we need the right tools. We’ll be using the mighty bizdays package. Think of it as your personal business day calculator, always ready to lend a hand (or rather, some code). Install it with this magic spell:\n\n# install.packages(\"bizdays\")\nlibrary(bizdays)"
  },
  {
    "objectID": "posts/2024-02-01/index.html#step-2-the-basic-count",
    "href": "posts/2024-02-01/index.html#step-2-the-basic-count",
    "title": "R for the Real World: Counting those Business Days like a Pro!",
    "section": "Step 2: The Basic Count",
    "text": "Step 2: The Basic Count\nAlright, let’s say you want to know how many business days there were between January 1st and December 31st, 2023. Simple, right? Here’s the code:\n\nstart_date &lt;- as.Date(\"2023-01-01\")\nend_date &lt;- as.Date(\"2023-12-31\")\n\nbusiness_days &lt;- bizdays(start_date, end_date, \"weekends\")\n\nprint(paste0(\"There were \", business_days, \" business days in 2023!\"))\n\n[1] \"There were 259 business days in 2023!\"\n\n\nWhat’s happening here? We define the start and end dates, feed them to the bizdays function, and voila! It counts the business days for us, excluding weekends by default. The print function just displays the result with a fun message."
  },
  {
    "objectID": "posts/2024-02-01/index.html#step-3-get-creative-and-explore",
    "href": "posts/2024-02-01/index.html#step-3-get-creative-and-explore",
    "title": "R for the Real World: Counting those Business Days like a Pro!",
    "section": "Step 3: Get Creative and Explore!",
    "text": "Step 3: Get Creative and Explore!\nRemember, this is just the tip of the bizdays iceberg. You can explore its other features like:\n\nAdding or subtracting business days from a date\nHandling custom holiday lists\nWorking with different time zones\n\nBut wait, there’s more! The most important step is to experiment and try things out yourself. Play with different dates, holidays, and weekend definitions. See what results you get and how they fit your specific needs. R is all about exploration and making it work for you!\nSo, fellow coders, go forth and conquer those business day calculations with confidence! And if you get stuck, remember, the R community is always here to help. Happy coding!"
  },
  {
    "objectID": "posts/2024-02-02/index.html",
    "href": "posts/2024-02-02/index.html",
    "title": "Accounts Recievables Pathways in SQL",
    "section": "",
    "text": "Yesterday I was working on a project that required me to create a SQL query to generate a table of accounts receivables pathways. I thought it would be interesting to share the SQL code I wrote for this task. The code is as follows:\n-- Create the table in the specified schema\n-- Create a new table called 'c_tableau_collector_pathway_tbl' in schema 'dbo'\n-- Drop the table if it already exists\nIF OBJECT_ID('dbo.c_tableau_collector_pathway_tbl', 'U') IS NOT NULL\nDROP TABLE dbo.c_tableau_collector_pathway_tbl\nGO\n-- Create the table in the specified schema\nCREATE TABLE dbo.c_tableau_collector_pathway_tbl\n(\n    c_tableau_collector_pathway_tblId INT NOT NULL IDENTITY(1, 1) PRIMARY KEY, -- primary key column\n    pt_no VARCHAR(50) NOT NULL,\n    collector_dept_path VARCHAR(MAX)\n);\n\nWITH tmp AS (\n    SELECT DISTINCT pt_no\n    FROM sms.dbo.c_tableau_times_with_worklist_tbl\n    )\nINSERT INTO sms.dbo.c_tableau_collector_pathway_tbl (\n    pt_no,\n    collector_dept_path\n    )\nSELECT rtrim(ltrim(tmp.pt_no)) AS [pt_no],\n    stuff((\n            SELECT ', ' + z.collector_dept\n            FROM sms.dbo.c_tableau_times_with_worklist_tbl AS z\n            WHERE z.pt_no = tmp.pt_no\n            GROUP BY z.collector_dept\n            ORDER BY max(event_number)\n            FOR XML path('')\n            ), 1, 2, '') AS [collector_dept_path]\nFROM tmp AS tmp;\n\nselect pt_no,\n    [collector_dept_path],  \n    [number_of_distinct_collector_dept] = (LEN(REPLACE(collector_dept_path, ',', '**')) - LEN(collector_dept_path)) + 1\nfrom dbo.c_tableau_collector_pathway_tbl\nSo what does it do? Let’s break it down step by step:\n\nIF OBJECT_ID('dbo.c_tableau_collector_pathway_tbl', 'U') IS NOT NULL\n\nThis part checks if a table named c_tableau_collector_pathway_tbl exists in the dbo schema. If it does, it proceeds to the next step.\n\nDROP TABLE dbo.c_tableau_collector_pathway_tbl\n\nIf the table exists, it drops (deletes) the table c_tableau_collector_pathway_tbl.\n\nCREATE TABLE dbo.c_tableau_collector_pathway_tbl (...)\n\nThis part creates a new table named c_tableau_collector_pathway_tbl in the dbo schema with three columns:\n\nc_tableau_collector_pathway_tblId of type INT, which is the primary key and automatically increments by 1 for each new row.\npt_no of type VARCHAR(50), which stores values up to 50 characters long and cannot be NULL.\ncollector_dept_path of type VARCHAR(MAX), which can store large amounts of text.\n\n\nWITH tmp AS (...)\n\nThis part defines a temporary table (tmp) that contains distinct values of pt_no from another table named sms.dbo.c_tableau_times_with_worklist_tbl.\n\nINSERT INTO sms.dbo.c_tableau_collector_pathway_tbl (...) SELECT ...\n\nThis part inserts data into the newly created c_tableau_collector_pathway_tbl table. It selects distinct pt_no values from the temporary table tmp and concatenates corresponding collector_dept values into a single string, separated by commas. The FOR XML path('') part formats the result as XML, and stuff(..., 1, 2, '') removes the leading comma and space.\n\nSELECT pt_no, [collector_dept_path], [number_of_distinct_collector_dept] = (...)\n\nFinally, this part selects data from the c_tableau_collector_pathway_tbl table. It selects pt_no, collector_dept_path, and calculates the number of distinct collector departments by counting the commas in the collector_dept_path string.\n\n\nIn summary, this SQL code drops an existing table (if it exists), creates a new table with specific columns, inserts data into the new table by concatenating values from another table, and then selects data from the new table along with a calculated value for the number of distinct collector departments."
  },
  {
    "objectID": "posts/2024-02-05/index.html",
    "href": "posts/2024-02-05/index.html",
    "title": "Taming Excel Dates in R: From Numbers to Meaningful Dates!",
    "section": "",
    "text": "Introduction\nHave you ever battled with Excel’s quirky date formats in your R projects? If so, you’re not alone! Those cryptic numbers can be a real headache, but fear not, fellow R warriors! Today, we’ll conquer this challenge and transform those numbers into beautiful, usable dates.\nOur Mission: We’ll convert two date columns in a tibble named “df”:\n\ndate: Stored as numbers, representing days since some mysterious date.\ndatetime: Also in numberland, but with an additional decimal for time.\n\nOur Weapons:\n\nas.Date(): This built-in R function is our date-conversion hero, but we need to give it a secret weapon: origin = \"1899-12-30\". This tells as.Date() where the Excel date system starts counting days from.\nopenxlsx library: This package helps us deal with Excel files. We’ll use its convertToDateTime() function to handle the datetime column, which includes both date and time information.\n\n\n\nLet’s Code!\n\n# Install and load the openxlsx library (if needed)\nif (!require(openxlsx)) install.packages(\"openxlsx\")\nlibrary(openxlsx)\n\n# Our example data\ndf &lt;- data.frame(\n  date = c(44563, 44566, 44635, 44670, 44706, 44716, 44761, 44782, 44864, 44919),\n  datetime = c(44563.17, 44566.51, 44635.64, 44670.40,\n               44706.43, 44716.42, 44761.05, 44782.09,\n               44864.19, 44919.89),\n  sales = c(14, 19, 22, 29, 24, 25, 25, 30, 35, 28)\n)\n\ndf\n\n    date datetime sales\n1  44563 44563.17    14\n2  44566 44566.51    19\n3  44635 44635.64    22\n4  44670 44670.40    29\n5  44706 44706.43    24\n6  44716 44716.42    25\n7  44761 44761.05    25\n8  44782 44782.09    30\n9  44864 44864.19    35\n10 44919 44919.89    28\n\n# Convert \"date\" column using as.Date() and the magic origin\ndf$date &lt;- as.Date(df$date, origin = \"1899-12-30\")\n\n# Convert \"datetime\" column using openxlsx and convertToDateTime()\ndf$datetime &lt;- convertToDateTime(df$datetime)\n\n\n\nBreaking it Down\n\nThe first line checks if openxlsx is installed and loads it if needed.\nWe create our sample data frame df with the date and datetime columns.\nThe magic happens! We use as.Date() on df$date, specifying the origin as “1899-12-30”. This tells R to interpret the numbers as days since that date.\nFor df$datetime, we use convertToDateTime() from the openxlsx package. This function handles both date and time information stored as decimals.\n\nVoila! Our df now has proper date and datetime columns, ready for further analysis and visualization. Let’s see the results:\n\nhead(df, 1)\n\n        date            datetime sales\n1 2022-01-02 2022-01-02 04:04:48    14\n\n\n\n\nYou’re Turn!\nNow it’s your turn! Grab your own Excel data with mysterious date formats and try this code. Play with different origin values if needed (depending on your Excel version). Remember, R is a playground, so have fun exploring and taming those dates!\nBonus Tip: Want to format your dates for readability? Use the format() function, like this:\n\ndf$date &lt;- format(df$date, \"%d/%m/%Y\")\ndf\n\n         date            datetime sales\n1  02/01/2022 2022-01-02 04:04:48    14\n2  05/01/2022 2022-01-05 12:14:24    19\n3  15/03/2022 2022-03-15 15:21:36    22\n4  19/04/2022 2022-04-19 09:36:00    29\n5  25/05/2022 2022-05-25 10:19:12    24\n6  04/06/2022 2022-06-04 10:04:48    25\n7  19/07/2022 2022-07-19 01:12:00    25\n8  09/08/2022 2022-08-09 02:09:36    30\n9  30/10/2022 2022-10-30 04:33:36    35\n10 24/12/2022 2022-12-24 21:21:36    28\n\n\nThis will display your dates in the familiar “day/month/year” format.\nSo there you have it, fellow R enthusiasts! With these tools, you can confidently handle Excel’s date quirks and unleash the power of your data. Happy coding!"
  },
  {
    "objectID": "posts/2024-02-06/index.html",
    "href": "posts/2024-02-06/index.html",
    "title": "Simplifying Date Manipulation: How to Get Week Numbers in R",
    "section": "",
    "text": "When working with dates in R, you may need to extract the week number for any given date. This can be useful for doing time series analysis or visualizations by week.\nIn this post, I’ll demonstrate how to get the week number from dates in R using both base R and the lubridate package. I’ll provide simple examples so you can try it yourself.\n\n\nIn base R, the strftime() function is used to format dates and extract different date components like day, month, year etc.\nThe syntax for strftime() is:\nstrftime(x, format, tz = \"\")\nWhere:\n\nx: is the date object\n\nformat: is the format string specifying which date components to extract\ntz: is an optional time zone string\n\nTo get the week number, we need to use \"%V\" in the format string. This tells strftime() to return the ISO 8601 standard week number.\nLet’s see an example:\n\ndate &lt;- as.Date(\"2023-01-15\")\n\nstrftime(date, format = \"%V\") \n\n[1] \"02\"\n\n\nThis returns the week number as a string. In this case, it’s the second week of the year.\nWe passed the date object to strftime() along with the format string containing \"%V\".\nLet’s try another example on a vector of dates:\n\ndates &lt;- as.Date(c(\"2023-01-15\", \"2023-02-28\", \"2023-12-31\"))\n\nstrftime(dates, format = \"%V\")\n\n[1] \"02\" \"09\" \"52\"\n\n\nThis returns the week number for each date. So with base R, we can use strftime() and %V to easily extract week numbers from dates.\n\n\n\nThe lubridate package provides a wrapper function called week() to get the week number from a date.\nThe syntax for week() is simple:\nweek(x)\nWhere x is the date object.\nLet’s see an example:\n\nlibrary(lubridate)\n\ndate &lt;- ymd(\"2023-01-15\")\n\nweek(date)\n\n[1] 3\n\n\nThis returns a numeric value representing the week number. In this case, it’s the third week of the year.\nFor a vector of dates:\n\ndates &lt;- ymd(c(\"2023-01-15\", \"2023-02-28\", \"2023-12-31\"))\n\nweek(dates) \n\n[1]  3  9 53\n\n\nSo week() makes it easy to extract the week number from dates in lubridate. You will also notice that strftime() returns “52” for the last date of the year, while week() returns “53”. This is because week() follows the ISO 8601 standard for week numbers.\n\n\n\nTo quickly recap the key points:\n\nBase R: strftime(date, format = \"%V\")\n\nlubridate: week(date)\n\nI encourage you to try these functions out on some sample dates in R. Being able to wrangle dates is an important skill for handling temporal data.\nLet me know if you have any other questions!"
  },
  {
    "objectID": "posts/2024-02-06/index.html#using-base-r",
    "href": "posts/2024-02-06/index.html#using-base-r",
    "title": "Simplifying Date Manipulation: How to Get Week Numbers in R",
    "section": "",
    "text": "In base R, the strftime() function is used to format dates and extract different date components like day, month, year etc.\nThe syntax for strftime() is:\nstrftime(x, format, tz = \"\")\nWhere:\n\nx: is the date object\n\nformat: is the format string specifying which date components to extract\ntz: is an optional time zone string\n\nTo get the week number, we need to use \"%V\" in the format string. This tells strftime() to return the ISO 8601 standard week number.\nLet’s see an example:\n\ndate &lt;- as.Date(\"2023-01-15\")\n\nstrftime(date, format = \"%V\") \n\n[1] \"02\"\n\n\nThis returns the week number as a string. In this case, it’s the second week of the year.\nWe passed the date object to strftime() along with the format string containing \"%V\".\nLet’s try another example on a vector of dates:\n\ndates &lt;- as.Date(c(\"2023-01-15\", \"2023-02-28\", \"2023-12-31\"))\n\nstrftime(dates, format = \"%V\")\n\n[1] \"02\" \"09\" \"52\"\n\n\nThis returns the week number for each date. So with base R, we can use strftime() and %V to easily extract week numbers from dates."
  },
  {
    "objectID": "posts/2024-02-06/index.html#using-lubridate",
    "href": "posts/2024-02-06/index.html#using-lubridate",
    "title": "Simplifying Date Manipulation: How to Get Week Numbers in R",
    "section": "",
    "text": "The lubridate package provides a wrapper function called week() to get the week number from a date.\nThe syntax for week() is simple:\nweek(x)\nWhere x is the date object.\nLet’s see an example:\n\nlibrary(lubridate)\n\ndate &lt;- ymd(\"2023-01-15\")\n\nweek(date)\n\n[1] 3\n\n\nThis returns a numeric value representing the week number. In this case, it’s the third week of the year.\nFor a vector of dates:\n\ndates &lt;- ymd(c(\"2023-01-15\", \"2023-02-28\", \"2023-12-31\"))\n\nweek(dates) \n\n[1]  3  9 53\n\n\nSo week() makes it easy to extract the week number from dates in lubridate. You will also notice that strftime() returns “52” for the last date of the year, while week() returns “53”. This is because week() follows the ISO 8601 standard for week numbers."
  },
  {
    "objectID": "posts/2024-02-06/index.html#wrap-up",
    "href": "posts/2024-02-06/index.html#wrap-up",
    "title": "Simplifying Date Manipulation: How to Get Week Numbers in R",
    "section": "",
    "text": "To quickly recap the key points:\n\nBase R: strftime(date, format = \"%V\")\n\nlubridate: week(date)\n\nI encourage you to try these functions out on some sample dates in R. Being able to wrangle dates is an important skill for handling temporal data.\nLet me know if you have any other questions!"
  },
  {
    "objectID": "posts/2024-02-07/index.html",
    "href": "posts/2024-02-07/index.html",
    "title": "How to Check if Date is Between Two Dates in R",
    "section": "",
    "text": "Hello fellow R enthusiasts! Today, we’re diving into a common task in data analysis and manipulation: checking if a date falls between two given dates. Whether you’re working with time-series data, financial data, or any other type of data that includes dates, being able to filter or flag data based on date ranges is an essential skill.\nIn this blog post, we’ll explore two approaches to accomplish this task using base R syntax. We’ll use simple examples and explain the code in easy-to-understand terms. So, let’s get started!"
  },
  {
    "objectID": "posts/2024-02-07/index.html#method-1-using-ifelse-to-create-a-new-column",
    "href": "posts/2024-02-07/index.html#method-1-using-ifelse-to-create-a-new-column",
    "title": "How to Check if Date is Between Two Dates in R",
    "section": "Method 1: Using ifelse() to Create a New Column",
    "text": "Method 1: Using ifelse() to Create a New Column\nOne straightforward way to check if a date is between two dates is by using the ifelse() function to create a new column with an indicator variable.\nHere’s how you can do it:\n\n# Sample data frame with dates\ndf &lt;- data.frame(date = as.Date(c(\"2022-01-01\", \"2022-03-15\", \n                                  \"2022-07-10\", \"2022-11-30\")),\n                 value = c(10, 20, 30, 40))\n\n# Define start and end dates\nstart_date &lt;- as.Date(\"2022-02-01\")\nend_date &lt;- as.Date(\"2022-10-01\")\n\n# Create a new column indicating if date falls between start_date and end_date\ndf$between &lt;- ifelse(df$date &gt;= start_date & df$date &lt;= end_date, 1, 0)\n\n# View the updated data frame\nprint(df)\n\n        date value between\n1 2022-01-01    10       0\n2 2022-03-15    20       1\n3 2022-07-10    30       1\n4 2022-11-30    40       0\n\n\nIn this code snippet, we first define a sample data frame df containing a column of dates. Then, we specify the start_date and end_date between which we want to check if each date falls. We use the ifelse() function to create a new column between, where a value of 1 indicates that the date falls between the specified range, and 0 otherwise."
  },
  {
    "objectID": "posts/2024-02-07/index.html#method-2-using-subsetting-to-filter-data",
    "href": "posts/2024-02-07/index.html#method-2-using-subsetting-to-filter-data",
    "title": "How to Check if Date is Between Two Dates in R",
    "section": "Method 2: Using Subsetting to Filter Data",
    "text": "Method 2: Using Subsetting to Filter Data\nAnother approach is to directly subset the data frame based on the date range. This method can be useful when you want to retrieve or manipulate the subset of data that falls within the specified range.\nHere’s how you can do it:\n\n# Sample data frame with dates\ndf &lt;- data.frame(date = as.Date(c(\"2022-01-01\", \"2022-03-15\", \n                                  \"2022-07-10\", \"2022-11-30\")),\n                 value = c(10, 20, 30, 40))\n\n# Define start and end dates\nstart_date &lt;- as.Date(\"2022-02-01\")\nend_date &lt;- as.Date(\"2022-10-01\")\n\n# Subset data where date falls between start_date and end_date\nsubset_df &lt;- df[df$date &gt;= start_date & df$date &lt;= end_date, ]\n\n# View the subsetted data frame\nprint(subset_df)\n\n        date value\n2 2022-03-15    20\n3 2022-07-10    30\n\n\nIn this code snippet, we use subsetting to filter the df data frame, retaining only the rows where the date falls between start_date and end_date."
  },
  {
    "objectID": "posts/2024-02-08/index.html",
    "href": "posts/2024-02-08/index.html",
    "title": "How to Check if a Column is a Date in R: A Comprehensive Guide with Examples",
    "section": "",
    "text": "As an R programmer, you may often encounter datasets where you need to determine whether a column contains date values. This task is crucial for data cleaning, manipulation, and analysis. In this blog post, we’ll explore various methods to check if a column is a date in R, with a focus on using the lubridate package and the ts_is_date_class() function from the healthyR.ts package."
  },
  {
    "objectID": "posts/2024-02-08/index.html#using-lubridate",
    "href": "posts/2024-02-08/index.html#using-lubridate",
    "title": "How to Check if a Column is a Date in R: A Comprehensive Guide with Examples",
    "section": "Using lubridate",
    "text": "Using lubridate\nlubridate is a powerful package in R for handling date and time data. It provides intuitive functions to parse, manipulate, and work with date-time objects. Let’s see how we can use lubridate to check if a column is a date.\n\n# Load the lubridate package\nlibrary(lubridate)\nlibrary(dplyr)\n\n# Sample data frame\ndf &lt;- data.frame(\n  Date_Column = c(\"2022-01-01\", \"2022-02-15\", \"not a date\", \"2022-03-30\")\n)\n\n# Check if Date_Column is a date\nis_date &lt;- is.Date(df$Date_Column)\n\n# Print the result\nprint(is_date)\n\n[1] FALSE\n\n\nIn this example, we created a sample data frame df with a column named Date_Column. We used the is.Date() function from lubridate to check if the values in Date_Column are dates. The result is a logical with either a value of (TRUE) or (FALSE). In this instance the result is FALSE because the entire vector is not a date. This can change to TRUE if the entire vector is a date. See below:\n\ndf |&gt; \n  mutate(Date_Column = as.Date(Date_Column)) |&gt; \n  pull(Date_Column) |&gt; \n  is.Date()\n\n[1] TRUE\n\n# OR\ndf |&gt;\n  mutate(Date_Column = as.Date(Date_Column) |&gt; is.Date())\n\n  Date_Column\n1        TRUE\n2        TRUE\n3        TRUE\n4        TRUE"
  },
  {
    "objectID": "posts/2024-02-08/index.html#using-ts_is_date_class-from-healthyr.ts",
    "href": "posts/2024-02-08/index.html#using-ts_is_date_class-from-healthyr.ts",
    "title": "How to Check if a Column is a Date in R: A Comprehensive Guide with Examples",
    "section": "Using ts_is_date_class() from healthyR.ts",
    "text": "Using ts_is_date_class() from healthyR.ts\nNow, let’s explore how to achieve the same task using the ts_is_date_class() function from the healthyR.ts package. This function is specifically designed to check if a column is a date class, providing an alternative method for date validation.\n\n# Install and load the healthyR.ts package\n# install.packages(\"healthyR.ts\")\nlibrary(healthyR.ts)\n\n# Check if Date_Column is a date using ts_is_date_class()\nis_date_class &lt;- ts_is_date_class(as.Date(df$Date_Column))\n\n# Print the result\nprint(is_date_class)\n\n[1] TRUE\n\n# OR\n\ndf |&gt;\n  mutate(is_date = ts_is_date_class(as.Date(Date_Column)))\n\n  Date_Column is_date\n1  2022-01-01    TRUE\n2  2022-02-15    TRUE\n3  not a date    TRUE\n4  2022-03-30    TRUE\n\n\nIn this example, we installed and loaded the healthyR.ts package, which contains the ts_is_date_class() function. We then applied this function to df$Date_Column to check if the values are of date class.\nYou will notice both methods incorrectly identify the row “not a date” as a date because the as.Date() function coerces the string “not a date” to an NA inside of the mutate function. If you use rowwise() before the mutate it will fail out completely, this can be a pitfall and is something to watch out for."
  },
  {
    "objectID": "posts/2024-02-09/index.html",
    "href": "posts/2024-02-09/index.html",
    "title": "Demystifying Dates: Finding the Day of the Week in R with lubridate",
    "section": "",
    "text": "Have you ever stared at a date in R and wondered, “What day of the week was this?!” Fear not, fellow data wranglers! Today, we embark on a journey to conquer this seemingly simple, yet surprisingly tricky, task. Buckle up, because we’re about to become date whisperers with the help of the lubridate package."
  },
  {
    "objectID": "posts/2024-02-09/index.html#example-1-using-wday",
    "href": "posts/2024-02-09/index.html#example-1-using-wday",
    "title": "Demystifying Dates: Finding the Day of the Week in R with lubridate",
    "section": "Example 1: Using wday()",
    "text": "Example 1: Using wday()\nThis function is your go-to for both numeric and character representations of the day. Let’s break it down:\n\nlibrary(lubridate)\n\n# Sample date\ndate &lt;- ymd(\"2024-02-09\")\n\n# Numeric day (Monday = 1, Sunday = 7)\nnumeric_day &lt;- wday(date)\nprint(numeric_day)  # Output: 6 (Friday)\n\n[1] 6\n\nclass(numeric_day)\n\n[1] \"numeric\"\n\n# Character day (full name)\nfull_day &lt;- wday(date, label = TRUE)\nprint(full_day)  # Output: Friday\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\nclass(full_day)\n\n[1] \"ordered\" \"factor\" \n\n# Character day (abbreviated)\nabbrev_day &lt;- wday(date, label = TRUE, abbr = TRUE)\nprint(abbrev_day)  # Output: Fri\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\nclass(abbrev_day)\n\n[1] \"ordered\" \"factor\""
  },
  {
    "objectID": "posts/2024-02-09/index.html#example-2.-using-strftime",
    "href": "posts/2024-02-09/index.html#example-2.-using-strftime",
    "title": "Demystifying Dates: Finding the Day of the Week in R with lubridate",
    "section": "Example 2. Using strftime()",
    "text": "Example 2. Using strftime()\nThis function offers more flexibility in formatting dates, including extracting the day of the week.\n\n# Same date as before\ndate &lt;- ymd(\"2024-02-09\")\nclass(date)\n\n[1] \"Date\"\n\n# Day of the week (full name)\nfull_day &lt;- strftime(date, format = \"%A\")\nprint(full_day)  # Output: Friday\n\n[1] \"Friday\"\n\nclass(full_day)\n\n[1] \"character\"\n\n# Day of the week (abbreviated)\nabbrev_day &lt;- strftime(date, format = \"%a\")\nprint(abbrev_day)  # Output: Fri\n\n[1] \"Fri\"\n\nclass(abbrev_day)\n\n[1] \"character\"\n\n\n\nBeyond the Basics: Customizing Your Output\nBoth wday() and strftime() offer options to personalize your results. For example, you can change the starting day of the week (default is Monday) or use different formatting codes for the day name.\nBonus Tip: Check out the lubridate documentation for more advanced options and functionalities!"
  },
  {
    "objectID": "posts/2024-02-12/index.html",
    "href": "posts/2024-02-12/index.html",
    "title": "From Chaos to Clarity: Mastering Weekly Data Wrangling in R with strftime()",
    "section": "",
    "text": "Introduction\nGrouping data by week is a common task in data analysis. It allows you to summarize and analyze your data on a weekly basis. In R, there are a few different ways to group data by week, but one easy method is using the strftime() function.\nThe strftime() function converts a date-time object to a string in a specified format. By using the format %V, we can extract the week number from a date. Let’s walk through an example:\nFirst, let’s create a data frame with some date values:\n\ndates &lt;- as.Date(c(\"2023-01-01\", \"2023-01-15\", \"2023-02-05\", \"2023-02-17\", \"2023-03-01\"))\nvalues &lt;- c(1.5, 3.2, 2.7, 4.1, 2.3) \n\ndf &lt;- data.frame(dates, values)\ndf\n\n       dates values\n1 2023-01-01    1.5\n2 2023-01-15    3.2\n3 2023-02-05    2.7\n4 2023-02-17    4.1\n5 2023-03-01    2.3\n\n\nNow we can use strftime() to extract the week number as follows:\n\ndf$week &lt;- strftime(df$dates, format = \"%V\")\ndf\n\n       dates values week\n1 2023-01-01    1.5   52\n2 2023-01-15    3.2   02\n3 2023-02-05    2.7   05\n4 2023-02-17    4.1   07\n5 2023-03-01    2.3   09\n\n\nThis adds a new column week to the data frame containing the week number for each date.\nWe can now easily group the data by week and summarize the values column:\n\naggregate(values ~ week, df, mean)\n\n  week values\n1   02    3.2\n2   05    2.7\n3   07    4.1\n4   09    2.3\n5   52    1.5\n\n\nAnd there we have it! The data neatly summarized by week. The %V format in strftime() makes it easy to group by week in R.\nI encourage you to try this on your own data. Converting dates to week numbers enables all sorts of weekly time series analyses. Let me know if you have any other questions!"
  },
  {
    "objectID": "posts/2024-02-13/index.html",
    "href": "posts/2024-02-13/index.html",
    "title": "How to Get First or Last Day of Month in R with lubridate and base R",
    "section": "",
    "text": "When working with dates in R, you’ll often need to find the first or last day of the current month or any given month. There are a couple easy ways to do this using the lubridate package and base R functions. In this post, I’ll show you how."
  },
  {
    "objectID": "posts/2024-02-13/index.html#using-lubridate",
    "href": "posts/2024-02-13/index.html#using-lubridate",
    "title": "How to Get First or Last Day of Month in R with lubridate and base R",
    "section": "Using lubridate",
    "text": "Using lubridate\nThe lubridate package makes working with dates in R much easier. It has a number of helper functions for manipulating and extracting info from Date and POSIXct objects.\nTo get the first day of the current month, you can use floor_date() and pass it the current date:\n\nlibrary(lubridate)\n\ntoday &lt;- Sys.Date() # or Sys.time() for POSIXct\nfirst_day &lt;- floor_date(today, unit = \"month\")\nfirst_day\n\n[1] \"2024-02-01\"\n\n\nThis will return a Date object with the first day of the month.\nTo get the last day, use ceiling_date() instead:\n\nlast_day &lt;- ceiling_date(today, unit = \"month\") - days(1)\nlast_day\n\n[1] \"2024-02-29\"\n\n\nYou can also pass any Date object to these functions to get the first or last day of that month:\n\ndate &lt;- ymd(\"2023-06-15\")\nfloor_date(date, \"month\") # 2023-06-01\n\n[1] \"2023-06-01\"\n\nceiling_date(date, \"month\") - days(1) # 2023-06-30\n\n[1] \"2023-06-30\"\n\n\nThe lubridate functions make this really easy!"
  },
  {
    "objectID": "posts/2024-02-13/index.html#base-r-methods",
    "href": "posts/2024-02-13/index.html#base-r-methods",
    "title": "How to Get First or Last Day of Month in R with lubridate and base R",
    "section": "Base R Methods",
    "text": "Base R Methods\nYou can also get the first and last day of month using just base R functions.\nFor the first day, use as.Date() with format() and pass it the year, month, and day 1:\n\nfirst_day &lt;- as.Date(format(today, \"%Y-%m-01\"))\nfirst_day\n\n[1] \"2024-02-01\"\n\n\nFor the last day, we can use 0 as the day which will give the last day of the month:\n\nlast_day &lt;- as.Date((format(today + months(1), \"%Y-%m-01\")))-1\nlast_day\n\n[1] \"2024-02-29\"\n\n\nA bit more work than lubridate, but good to know you can do this with just base R.\nI hope this helps you easily get the first and last day of the month in your own date analyses in R! Let me know if you have any other questions."
  },
  {
    "objectID": "posts/2024-02-14/index.html",
    "href": "posts/2024-02-14/index.html",
    "title": "Mastering Date Sequences in R: A Comprehensive Guide",
    "section": "",
    "text": "In the world of data analysis and manipulation, working with dates is a common and crucial task. Whether you’re analyzing financial data, tracking trends over time, or forecasting future events, understanding how to generate date sequences efficiently is essential. In this blog post, we’ll explore three powerful R packages—lubridate, timetk, and base R—that make working with dates a breeze. By the end of this guide, you’ll be equipped with the knowledge to generate date sequences effortlessly and efficiently in R."
  },
  {
    "objectID": "posts/2024-02-14/index.html#generating-date-sequences-with-lubridate",
    "href": "posts/2024-02-14/index.html#generating-date-sequences-with-lubridate",
    "title": "Mastering Date Sequences in R: A Comprehensive Guide",
    "section": "Generating Date Sequences with lubridate:",
    "text": "Generating Date Sequences with lubridate:\nLubridate is a popular R package that simplifies working with dates and times. Let’s start by generating a sequence of dates using lubridate’s seq function.\n\nlibrary(lubridate)\n\n# Generate a sequence of dates from January 1, 2022 to January 10, 2022\ndate_seq_lubridate &lt;- seq(ymd(\"2022-01-01\"), ymd(\"2022-01-10\"), by = \"days\")\n\nprint(date_seq_lubridate)\n\n [1] \"2022-01-01\" \"2022-01-02\" \"2022-01-03\" \"2022-01-04\" \"2022-01-05\"\n [6] \"2022-01-06\" \"2022-01-07\" \"2022-01-08\" \"2022-01-09\" \"2022-01-10\"\n\n\nExplanation: - library(lubridate): Loads the lubridate package into the R session. - seq(ymd(\"2022-01-01\"), ymd(\"2022-01-10\"), by = \"days\"): Generates a sequence of dates starting from January 1, 2022, to January 10, 2022, with a step size of one day. - print(date_seq_lubridate): Prints the generated sequence of dates."
  },
  {
    "objectID": "posts/2024-02-14/index.html#generating-date-sequences-with-timetk",
    "href": "posts/2024-02-14/index.html#generating-date-sequences-with-timetk",
    "title": "Mastering Date Sequences in R: A Comprehensive Guide",
    "section": "Generating Date Sequences with timetk",
    "text": "Generating Date Sequences with timetk\nTimetk is another fantastic R package for working with date-time data. Let’s use timetk’s tk_make_seq function to generate a sequence of dates.\n\n# Load the timetk package\nlibrary(timetk)\n\n# Generate a sequence of dates from January 1, 2022 to January 10, 2022\ndate_seq_timetk &lt;- tk_make_timeseries(\n  start_date = \"2022-01-01\", \n  end_date = \"2022-01-10\", \n  by = \"days\"\n  )\n\nprint(date_seq_timetk)\n\n [1] \"2022-01-01\" \"2022-01-02\" \"2022-01-03\" \"2022-01-04\" \"2022-01-05\"\n [6] \"2022-01-06\" \"2022-01-07\" \"2022-01-08\" \"2022-01-09\" \"2022-01-10\"\n\n\nExplanation: - library(timetk): Loads the timetk package into the R session. - tk_make_seq(from = \"2022-01-01\", to = \"2022-01-10\", by = \"days\"): Generates a sequence of dates starting from January 1, 2022, to January 10, 2022, with a step size of one day. - print(date_seq_timetk): Prints the generated sequence of dates."
  },
  {
    "objectID": "posts/2024-02-14/index.html#generating-date-sequences-with-base-r",
    "href": "posts/2024-02-14/index.html#generating-date-sequences-with-base-r",
    "title": "Mastering Date Sequences in R: A Comprehensive Guide",
    "section": "Generating Date Sequences with base R:",
    "text": "Generating Date Sequences with base R:\nFinally, let’s explore how to generate a sequence of dates using base R’s seq function.\n\n# Generate a sequence of dates from January 1, 2022 to January 10, 2022\ndate_seq_base &lt;- seq(\n  as.Date(\"2022-01-01\"), \n  as.Date(\"2022-01-10\"), \n  by = \"days\"\n  )\n\nprint(date_seq_base)\n\n [1] \"2022-01-01\" \"2022-01-02\" \"2022-01-03\" \"2022-01-04\" \"2022-01-05\"\n [6] \"2022-01-06\" \"2022-01-07\" \"2022-01-08\" \"2022-01-09\" \"2022-01-10\"\n\n\nExplanation: - seq(as.Date(\"2022-01-01\"), as.Date(\"2022-01-10\"), by = \"days\"): Generates a sequence of dates starting from January 1, 2022, to January 10, 2022, with a step size of one day. - print(date_seq_base): Prints the generated sequence of dates.\nHere is another example of generating a sequence of dates using base R’s seq function with a different frequency:\n\nstart_date &lt;- as.Date(\"2023-01-01\")\nend_date &lt;- as.Date(\"2023-12-31\")\n\nday_count &lt;- as.numeric(end_date - start_date) + 1\ndate_seq &lt;- start_date + 0:day_count\nmin(date_seq)\n\n[1] \"2023-01-01\"\n\nmax(date_seq)\n\n[1] \"2024-01-01\"\n\nhead(date_seq)\n\n[1] \"2023-01-01\" \"2023-01-02\" \"2023-01-03\" \"2023-01-04\" \"2023-01-05\"\n[6] \"2023-01-06\"\n\ntail(date_seq)\n\n[1] \"2023-12-27\" \"2023-12-28\" \"2023-12-29\" \"2023-12-30\" \"2023-12-31\"\n[6] \"2024-01-01\"\n\nhealthyR.ts::ts_info_tbl(as.ts(date_seq))\n\n# A tibble: 1 × 7\n  name            class frequency start end   var        length\n  &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n1 as.ts(date_seq) ts            1 1 1   366 1 univariate    366"
  },
  {
    "objectID": "posts/2024-02-15/index.html",
    "href": "posts/2024-02-15/index.html",
    "title": "Conquering R’s Apply Family: Your Guide to apply(), lapply(), sapply(), and tapply()",
    "section": "",
    "text": "Welcome, fellow R warriors! Today, we delve into the heart of vectorized operations with R’s “apply” family: apply(), lapply(), sapply(), and tapply(). These functions are your secret weapons for efficiency and elegance, so buckle up and prepare to be amazed!\nBut first, the “why”: Loops are great, but for repetitive tasks on data structures, vectorization reigns supreme. It’s faster, cleaner, and lets you focus on the “what” instead of the “how” of your analysis. Enter the apply family, each member offering a unique twist on applying functions to your data."
  },
  {
    "objectID": "posts/2024-02-15/index.html#example-1.-the-grandparent-apply",
    "href": "posts/2024-02-15/index.html#example-1.-the-grandparent-apply",
    "title": "Conquering R’s Apply Family: Your Guide to apply(), lapply(), sapply(), and tapply()",
    "section": "Example 1. The Grandparent: apply()",
    "text": "Example 1. The Grandparent: apply()\nThink of apply() as the customizable grandfather. It takes three arguments:\n\nX: Your data (matrix, array, data frame).\nMARGIN: Where to apply the function (rows = 1, columns = 2, both = c(1, 2)).\nFUN: The function to apply (e.g., mean, sum, your custom function).\n\nCalculate the mean of each column in the iris dataset:\n\ncolumn_means &lt;- apply(iris[, 1:4], 2, mean)\nprint(column_means)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\nExplanation: We apply mean (FUN) to each column (MARGIN = 2) of the first four columns (iris[, 1:4]) of the iris data frame, storing the results in column_means."
  },
  {
    "objectID": "posts/2024-02-15/index.html#example-2.-the-speedy-sibling-lapply",
    "href": "posts/2024-02-15/index.html#example-2.-the-speedy-sibling-lapply",
    "title": "Conquering R’s Apply Family: Your Guide to apply(), lapply(), sapply(), and tapply()",
    "section": "Example 2. The Speedy Sibling: lapply()",
    "text": "Example 2. The Speedy Sibling: lapply()\nlapply() is the speed demon, applying a function to each element of a list or vector and returning a list of results.\nCalculate the median of each petal length in a list of lists:\n\npetal_lengths &lt;- list(c(1.4, 1.5, 1.6), c(4.4, 4.5, 4.6))\npetal_medians &lt;- lapply(petal_lengths, median)\nprint(petal_medians)\n\n[[1]]\n[1] 1.5\n\n[[2]]\n[1] 4.5\n\n\nExplanation: We apply median to each sub-list in petal_lengths, returning a list (petal_medians) containing the medians."
  },
  {
    "objectID": "posts/2024-02-15/index.html#example-3.-the-streamlined-cousin-sapply",
    "href": "posts/2024-02-15/index.html#example-3.-the-streamlined-cousin-sapply",
    "title": "Conquering R’s Apply Family: Your Guide to apply(), lapply(), sapply(), and tapply()",
    "section": "Example 3. The Streamlined Cousin: sapply()",
    "text": "Example 3. The Streamlined Cousin: sapply()\nsapply() is like lapply(), but it tries to simplify the output. If all results are of the same type (e.g., numeric), it returns a vector instead of a list.\nFind the minimum value in each row of a matrix:\n\nmatrix &lt;- matrix(1:12, nrow = 3)\nrow_mins &lt;- sapply(1:nrow(matrix), function(i) min(matrix[i, ]))\nprint(row_mins)\n\n[1] 1 2 3\n\n\nExplanation: We use an anonymous function to find the minimum in each row (matrix[i, ]) and apply it to each row number (1:nrow(matrix)). sapply() simplifies the output to a vector of minimum values (row_mins)."
  },
  {
    "objectID": "posts/2024-02-15/index.html#example-4.-the-grouping-guru-tapply",
    "href": "posts/2024-02-15/index.html#example-4.-the-grouping-guru-tapply",
    "title": "Conquering R’s Apply Family: Your Guide to apply(), lapply(), sapply(), and tapply()",
    "section": "Example 4. The Grouping Guru: tapply()",
    "text": "Example 4. The Grouping Guru: tapply()\ntapply() groups data based on another variable and applies a function to each group. Perfect for summarizing data by categories!\nCalculate the average sepal length for each species in the iris dataset:\n\nsepal_length_by_species &lt;- tapply(iris$Sepal.Length, iris$Species, mean)\nprint(sepal_length_by_species)\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588 \n\n\nExplanation: We group the Sepal.Length column by the Species column (using iris$Species) and calculate the mean (mean) for each group. The results are stored in sepal_length_by_species."
  },
  {
    "objectID": "posts/2024-02-16/index.html",
    "href": "posts/2024-02-16/index.html",
    "title": "Level Up Your Data Wrangling: Adding Index Columns in R like a Pro!",
    "section": "",
    "text": "Data wrangling in R is like cooking: you have your ingredients (data), and you use tools (functions) to prepare them (clean, transform) for analysis (consumption!). One essential tool is adding an “index column” – a unique identifier for each row. This might seem simple, but there are several ways to do it in base R and tidyverse packages like dplyr and tibble. Let’s explore and spice up your data wrangling skills!"
  },
  {
    "objectID": "posts/2024-02-16/index.html#adding-heat-with-base-r",
    "href": "posts/2024-02-16/index.html#adding-heat-with-base-r",
    "title": "Level Up Your Data Wrangling: Adding Index Columns in R like a Pro!",
    "section": "Adding Heat with Base R",
    "text": "Adding Heat with Base R\n\nEx 1: The Sequencer:\nImagine lining up your rows. cbind(df, 1:nrow(df)) adds a new column with numbers 1 to n, where n is the number of rows in your data frame (df).\n\n# Sample data\ndf &lt;- data.frame(name = c(\"Alice\", \"Bob\", \"Charlie\"), age = c(25, 30, 28))\n\n# Add index using cbind\ndf_with_index &lt;- cbind(index = 1:nrow(df), df)\ndf_with_index\n\n  index    name age\n1     1   Alice  25\n2     2     Bob  30\n3     3 Charlie  28\n\n\n\n\nEx 2: Row Name Shuffle:\nPrefer names over numbers? rownames(df) &lt;- 1:nrow(df) assigns row numbers as your index, replacing existing row names.\n\n# Sample data\ndf &lt;- data.frame(name = c(\"Alice\", \"Bob\", \"Charlie\"), age = c(25, 30, 28))\n\ndf_with_index &lt;- cbind(index = rownames(df), df)\ndf_with_index\n\n  index    name age\n1     1   Alice  25\n2     2     Bob  30\n3     3 Charlie  28\n\n\n\n\nEx 3: The All-Seeing Eye:\nseq_len(nrow(df)) generates a sequence of numbers, perfect for adding as a new column named “index”.\n\n# Sample data\ndf &lt;- data.frame(name = c(\"Alice\", \"Bob\", \"Charlie\"), age = c(25, 30, 28))\n\ndf_with_index &lt;- cbind(index = seq_len(nrow(df)), df)\ndf_with_index\n\n  index    name age\n1     1   Alice  25\n2     2     Bob  30\n3     3 Charlie  28"
  },
  {
    "objectID": "posts/2024-02-16/index.html#the-tidyverse-twist",
    "href": "posts/2024-02-16/index.html#the-tidyverse-twist",
    "title": "Level Up Your Data Wrangling: Adding Index Columns in R like a Pro!",
    "section": "The Tidyverse Twist:",
    "text": "The Tidyverse Twist:\nThe tidyverse offers unique approaches:\n\nEx 1: Tibble Magic:\ntibble::rowid_to_column(df) adds a column named “row_id” with unique row identifiers.\n\nlibrary(tibble)\n\n# Convert df to tibble\ndf_tib &lt;- as_tibble(df)\n\n# Add row_id\ndf_tib_indexed &lt;- rowid_to_column(df_tib)\ndf_tib_indexed\n\n# A tibble: 3 × 3\n  rowid name      age\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 Alice      25\n2     2 Bob        30\n3     3 Charlie    28\n\n\n\n\nEx 2: dplyr’s Ranking System:\ndplyr::row_number() assigns ranks (starting from 1) based on the order of your data.\n\nlibrary(dplyr)\n# Add row number\ndf_tib_ranked &lt;- df_tib |&gt;\n  mutate(rowid = row_number()) |&gt;\n  select(rowid, everything())\n\ndf_tib_ranked\n\n# A tibble: 3 × 3\n  rowid name      age\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 Alice      25\n2     2 Bob        30\n3     3 Charlie    28"
  },
  {
    "objectID": "posts/2023-02-21/index.html",
    "href": "posts/2023-02-21/index.html",
    "title": "Enhancing Your Plots in R: Adding Superscripts & Subscripts",
    "section": "",
    "text": "Hey R enthusiasts! Are you looking to take your data visualization skills to the next level? Well, you’re in the right place because today, we’re diving into the world of superscripts and subscripts in R plots. Whether you’re a seasoned R user or just getting started, adding these little details can make your plots more informative and visually appealing."
  },
  {
    "objectID": "posts/2023-02-21/index.html#example-1-adding-superscripts-to-axis-labels",
    "href": "posts/2023-02-21/index.html#example-1-adding-superscripts-to-axis-labels",
    "title": "Enhancing Your Plots in R: Adding Superscripts & Subscripts",
    "section": "Example 1: Adding Superscripts to Axis Labels",
    "text": "Example 1: Adding Superscripts to Axis Labels\n\n# Create some sample data\nx &lt;- 1:10\ny &lt;- x^2\n\n# Plot the data\nplot(x, y, xlab = expression(paste(\"X Axis Label with Superscript: \", italic(\"x\")^2)))\n\n\n\n\nIn this example, we’re using the expression() function to create a plot with a customized x-axis label that includes a superscript (in this case, “x squared”)."
  },
  {
    "objectID": "posts/2023-02-21/index.html#example-2-adding-subscripts-to-axis-labels",
    "href": "posts/2023-02-21/index.html#example-2-adding-subscripts-to-axis-labels",
    "title": "Enhancing Your Plots in R: Adding Superscripts & Subscripts",
    "section": "Example 2: Adding Subscripts to Axis Labels",
    "text": "Example 2: Adding Subscripts to Axis Labels\n\n# Create some sample data\nx &lt;- 1:10\ny &lt;- x^2\n\n# Plot the data\nplot(x, y, ylab = expression(paste(\"Y Axis Label with Subscript: \", italic(\"y\")[i])))\n\n\n\n\nHere, we’re using the expression() function again to create a plot with a customized y-axis label that includes a subscript (in this case, “y subscript i”)."
  },
  {
    "objectID": "posts/2023-02-22/index.html",
    "href": "posts/2023-02-22/index.html",
    "title": "Demystifying Data Types in R: A Beginner’s Guide with Code Examples",
    "section": "",
    "text": "Ever wondered what kind of information your data holds in R? Knowing the data type is crucial for performing the right analysis and avoiding errors. This post will equip you with the skills to check data types in R, making your coding journey smoother and more efficient."
  },
  {
    "objectID": "posts/2023-02-22/index.html#example-1-checking-the-type-of-a-single-variable",
    "href": "posts/2023-02-22/index.html#example-1-checking-the-type-of-a-single-variable",
    "title": "Demystifying Data Types in R: A Beginner’s Guide with Code Examples",
    "section": "Example 1: Checking the type of a single variable:",
    "text": "Example 1: Checking the type of a single variable:\n\n# Create a variable with different data types\nage &lt;- 25\nname &lt;- \"Alice\"\nis_employed &lt;- TRUE\n\n# Check the data types using class()\nclass(age)  # Output: \"numeric\"\n\n[1] \"numeric\"\n\nclass(name) # Output: \"character\"\n\n[1] \"character\"\n\nclass(is_employed) # Output: \"logical\"\n\n[1] \"logical\"\n\n# Check for even more details using typeof()\ntypeof(age)  # Output: \"double\"\n\n[1] \"double\"\n\ntypeof(name) # Output: \"character\"\n\n[1] \"character\"\n\ntypeof(is_employed) # Output: \"logical\"\n\n[1] \"logical\""
  },
  {
    "objectID": "posts/2023-02-22/index.html#example-2-examining-data-types-within-a-data-frame",
    "href": "posts/2023-02-22/index.html#example-2-examining-data-types-within-a-data-frame",
    "title": "Demystifying Data Types in R: A Beginner’s Guide with Code Examples",
    "section": "Example 2: Examining data types within a data frame:*",
    "text": "Example 2: Examining data types within a data frame:*\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  ID = 1:5,\n  Name = c(\"Bob\", \"Charlie\", \"David\", \"Emily\", \"Fiona\"),\n  Age = c(28, 32, 41, 25, 37)\n)\n\n# Peek into the data frame's structure using str()\nstr(data)\n\n'data.frame':   5 obs. of  3 variables:\n $ ID  : int  1 2 3 4 5\n $ Name: chr  \"Bob\" \"Charlie\" \"David\" \"Emily\" ...\n $ Age : num  28 32 41 25 37\n\n\nThe str() function displays a detailed summary of the data frame, including the names and data types of each column."
  },
  {
    "objectID": "posts/2024-02-23/index.html",
    "href": "posts/2024-02-23/index.html",
    "title": "Taming the Data Jungle: Filtering data.tables and data.frames in R",
    "section": "",
    "text": "Ah, data! The lifeblood of many an analysis, but sometimes it can feel like you’re lost in a tangled jungle. Thankfully, R offers powerful tools to navigate this data wilderness, and filtering is one of the most essential skills in your arsenal. Today, we’ll explore how to filter both data.tables and data.frames, making your data exploration a breeze!"
  },
  {
    "objectID": "posts/2024-02-23/index.html#example-1.-filtering-by-a-single-condition",
    "href": "posts/2024-02-23/index.html#example-1.-filtering-by-a-single-condition",
    "title": "Taming the Data Jungle: Filtering data.tables and data.frames in R",
    "section": "Example 1. Filtering by a single condition:",
    "text": "Example 1. Filtering by a single condition:\n\n# Sample data.table\nlibrary(data.table)\nmtcars_dt &lt;- as.data.table(mtcars)\n\n# Filter cars with MPG greater than 25\nfiltered_cars &lt;- mtcars_dt[mpg &gt; 25]\nfiltered_cars\n\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:  32.4     4  78.7    66  4.08 2.200 19.47     1     1     4     1\n2:  30.4     4  75.7    52  4.93 1.615 18.52     1     1     4     2\n3:  33.9     4  71.1    65  4.22 1.835 19.90     1     1     4     1\n4:  27.3     4  79.0    66  4.08 1.935 18.90     1     1     4     1\n5:  26.0     4 120.3    91  4.43 2.140 16.70     0     1     5     2\n6:  30.4     4  95.1   113  3.77 1.513 16.90     1     1     5     2\n\n\nExplanation:\n\nmtcars_dt[mpg &gt; 25] selects rows where the mpg column is greater than 25.\nThe result, stored in filtered_cars, is a new data.table containing only those rows."
  },
  {
    "objectID": "posts/2024-02-23/index.html#example-2.-combining-conditions",
    "href": "posts/2024-02-23/index.html#example-2.-combining-conditions",
    "title": "Taming the Data Jungle: Filtering data.tables and data.frames in R",
    "section": "Example 2. Combining conditions:**",
    "text": "Example 2. Combining conditions:**\n\n# Filter cars with 4 cylinders and horsepower over 150\nfiltered_cars &lt;- mtcars_dt[(cyl == 4) & (hp &gt; 150)]\nfiltered_cars\n\nEmpty data.table (0 rows and 11 cols): mpg,cyl,disp,hp,drat,wt...\n\n\nExplanation:\n\n(cyl == 4) & (hp &gt; 150) combines two conditions using the & operator (AND).\nOnly rows meeting both conditions are included in the filtered data.table."
  },
  {
    "objectID": "posts/2024-02-23/index.html#example-1.-filtering-with-logical-operators",
    "href": "posts/2024-02-23/index.html#example-1.-filtering-with-logical-operators",
    "title": "Taming the Data Jungle: Filtering data.tables and data.frames in R",
    "section": "Example 1. Filtering with logical operators:",
    "text": "Example 1. Filtering with logical operators:\n\n# Filter irises with Sepal.Length less than 5 and Petal.Width greater than 2\nfiltered_iris &lt;- iris[iris$Sepal.Length &lt; 5 & iris$Petal.Width &gt; 2,]\nfiltered_iris\n\n[1] Sepal.Length Sepal.Width  Petal.Length Petal.Width  Species     \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nExplanation:\n\nThis approach is similar to data.tables, using logical operators (&lt;, &gt;, &) to define conditions.\nThe filtered data.frame is stored in filtered_iris."
  },
  {
    "objectID": "posts/2024-02-23/index.html#example-2.-subsetting-with-row-indices",
    "href": "posts/2024-02-23/index.html#example-2.-subsetting-with-row-indices",
    "title": "Taming the Data Jungle: Filtering data.tables and data.frames in R",
    "section": "Example 2. Subsetting with row indices:",
    "text": "Example 2. Subsetting with row indices:\n\n# Filter the first 3 and last 2 rows\nfiltered_iris &lt;- iris[1:3, ] # First 3 rows\nfiltered_iris\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\nfiltered_iris &lt;- iris[nrow(iris) - 0:1, ] # Last 2 rows\nfiltered_iris\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n150          5.9         3.0          5.1         1.8 virginica\n149          6.2         3.4          5.4         2.3 virginica\n\n\nExplanation:\n\nYou can directly specify row indices within square brackets [].\nThis is useful for selecting specific rows based on their position."
  },
  {
    "objectID": "posts/2024-02-23/index.html#example-3.-filtering-by-values-in-a-list",
    "href": "posts/2024-02-23/index.html#example-3.-filtering-by-values-in-a-list",
    "title": "Taming the Data Jungle: Filtering data.tables and data.frames in R",
    "section": "Example 3. Filtering by values in a list:",
    "text": "Example 3. Filtering by values in a list:\n\n# Filter cars with carb in 1 or 2\nfiltered_cars &lt;- mtcars_dt[carb %in% c(1, 2)]\nfiltered_cars\n\n      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n    &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:  22.8     4 108.0    93  3.85 2.320 18.61     1     1     4     1\n 2:  21.4     6 258.0   110  3.08 3.215 19.44     1     0     3     1\n 3:  18.7     8 360.0   175  3.15 3.440 17.02     0     0     3     2\n 4:  18.1     6 225.0   105  2.76 3.460 20.22     1     0     3     1\n 5:  24.4     4 146.7    62  3.69 3.190 20.00     1     0     4     2\n 6:  22.8     4 140.8    95  3.92 3.150 22.90     1     0     4     2\n 7:  32.4     4  78.7    66  4.08 2.200 19.47     1     1     4     1\n 8:  30.4     4  75.7    52  4.93 1.615 18.52     1     1     4     2\n 9:  33.9     4  71.1    65  4.22 1.835 19.90     1     1     4     1\n10:  21.5     4 120.1    97  3.70 2.465 20.01     1     0     3     1\n11:  15.5     8 318.0   150  2.76 3.520 16.87     0     0     3     2\n12:  15.2     8 304.0   150  3.15 3.435 17.30     0     0     3     2\n13:  19.2     8 400.0   175  3.08 3.845 17.05     0     0     3     2\n14:  27.3     4  79.0    66  4.08 1.935 18.90     1     1     4     1\n15:  26.0     4 120.3    91  4.43 2.140 16.70     0     1     5     2\n16:  30.4     4  95.1   113  3.77 1.513 16.90     1     1     5     2\n17:  21.4     4 121.0   109  4.11 2.780 18.60     1     1     4     2\n\n\nExplanation:\n\n%in% checks if a value belongs to a list.\nHere, we filter for cars where the carb is either 1 or 2."
  },
  {
    "objectID": "posts/2024-02-26/index.html",
    "href": "posts/2024-02-26/index.html",
    "title": "Unveiling the Magic of dcast Function in R’s data.table Package",
    "section": "",
    "text": "Here is a draft blog post on using the dcast function from the data.table package in R:"
  },
  {
    "objectID": "posts/2024-02-26/index.html#example-1-converting-rows-to-columns",
    "href": "posts/2024-02-26/index.html#example-1-converting-rows-to-columns",
    "title": "Unveiling the Magic of dcast Function in R’s data.table Package",
    "section": "Example 1: Converting Rows to Columns",
    "text": "Example 1: Converting Rows to Columns\nSuppose we have a dataset my_data with columns Year, Month, and Sales. We want to pivot this data so that each unique value in the Year column becomes a column, and each unique value in the Month column becomes a row, with Sales as the values.\n\nlibrary(data.table)\n\n# Sample data\nmy_data &lt;- data.table(\n  Year = c(2019, 2019, 2020, 2020),\n  Month = c(\"Jan\", \"Feb\", \"Jan\", \"Feb\"),\n  Sales = c(1000, 1500, 1200, 1800)\n)\n\n# Using dcast\ndcast(my_data, Month ~ Year, value.var = \"Sales\")\n\nKey: &lt;Month&gt;\n    Month  2019  2020\n   &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:    Feb  1500  1800\n2:    Jan  1000  1200\n\n\nIn this example, Month ~ Year tells dcast to use Month as rows and create columns for each unique value in Year. value.var = \"Sales\" specifies that the Sales column contains the values to populate the new table."
  },
  {
    "objectID": "posts/2024-02-26/index.html#example-2-aggregating-data",
    "href": "posts/2024-02-26/index.html#example-2-aggregating-data",
    "title": "Unveiling the Magic of dcast Function in R’s data.table Package",
    "section": "Example 2: Aggregating Data",
    "text": "Example 2: Aggregating Data\nNow, let’s say we have a dataset my_data_2 with columns Category, Year, and Sales. We want to pivot this data to see the total sales for each category across different years.\n\n# Sample data\nmy_data_2 &lt;- data.table(\n  Category = c(\"A\", \"A\", \"B\", \"B\"),\n  Year = c(2019, 2020, 2019, 2020),\n  Sales = c(1000, 1500, 1200, 1800)\n)\n\n# Using dcast for aggregation\ndcast(my_data_2, Category ~ Year, value.var = \"Sales\", fun.aggregate = sum)\n\nKey: &lt;Category&gt;\n   Category  2019  2020\n     &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:        A  1000  1500\n2:        B  1200  1800\n\n\nHere, fun.aggregate = sum specifies that we want to calculate the sum of Sales for each combination of Category and Year."
  },
  {
    "objectID": "posts/2024-02-26/index.html#reshaping-from-long-to-wide",
    "href": "posts/2024-02-26/index.html#reshaping-from-long-to-wide",
    "title": "Unveiling the Magic of dcast Function in R’s data.table Package",
    "section": "Reshaping from Long to Wide",
    "text": "Reshaping from Long to Wide\nLet’s walk through an example with the mtcars dataset. First we convert mtcars to a data.table:\n\ndt &lt;- as.data.table(mtcars)\n\nSay we want to reshape the data from long to wide, aggregating the hp values by cyl. We can use dcast:\n\ndcast(dt, cyl ~ ., value.var=\"hp\", fun.aggregate=mean)\n\nKey: &lt;cyl&gt;\n     cyl         .\n   &lt;num&gt;     &lt;num&gt;\n1:     4  82.63636\n2:     6 122.28571\n3:     8 209.21429\n\n\nThis aggregates the hp by cyl, casting the other columns as identifiers. The result is a table with one row per cyl, and columns for mean hp and all other variables."
  },
  {
    "objectID": "posts/2024-02-26/index.html#aggregating-multiple-columns",
    "href": "posts/2024-02-26/index.html#aggregating-multiple-columns",
    "title": "Unveiling the Magic of dcast Function in R’s data.table Package",
    "section": "Aggregating Multiple Columns",
    "text": "Aggregating Multiple Columns\nYou can also aggregate multiple value columns in one call. Let’s add aggregating disp by the mean:\n\ndcast(dt, cyl ~ ., value.var=c(\"hp\", \"disp\"), fun.aggregate=mean)\n\nKey: &lt;cyl&gt;\n     cyl        hp     disp\n   &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:     4  82.63636 105.1364\n2:     6 122.28571 183.3143\n3:     8 209.21429 353.1000\n\n\nNow we have mean hp and mean disp aggregated by cyl in the wide format."
  },
  {
    "objectID": "posts/2024-02-26/index.html#using-multiple-formulas",
    "href": "posts/2024-02-26/index.html#using-multiple-formulas",
    "title": "Unveiling the Magic of dcast Function in R’s data.table Package",
    "section": "Using Multiple Formulas",
    "text": "Using Multiple Formulas\nAnother common operation is aggregating over several formulas separately. For example, aggregating hp by cyl and gear.\nWe can pass a list of formulas to dcast:\n\ndcast(dt, cyl ~ ., value.var=\"hp\", fun.aggregate=mean) + \n  dcast(dt, gear ~ ., value.var=\"hp\", fun.aggregate=mean)\n\n     cyl        .\n   &lt;num&gt;    &lt;num&gt;\n1:     7 258.7697\n2:    10 211.7857\n3:    13 404.8143\n\n\nThis outputs two sets of aggregations, by cyl and gear, in a single wide table."
  },
  {
    "objectID": "posts/2024-02-26/index.html#reshaping-from-wide-to-long",
    "href": "posts/2024-02-26/index.html#reshaping-from-wide-to-long",
    "title": "Unveiling the Magic of dcast Function in R’s data.table Package",
    "section": "Reshaping from Wide to Long",
    "text": "Reshaping from Wide to Long\nThe melt function from data.table can reshape from wide to long format. For example:\n\nmelt(dt, id.vars = \"cyl\", measure.vars = c(\"hp\", \"disp\"))\n\n      cyl variable value\n    &lt;num&gt;   &lt;fctr&gt; &lt;num&gt;\n 1:     6       hp 110.0\n 2:     6       hp 110.0\n 3:     4       hp  93.0\n 4:     6       hp 110.0\n 5:     8       hp 175.0\n 6:     6       hp 105.0\n 7:     8       hp 245.0\n 8:     4       hp  62.0\n 9:     4       hp  95.0\n10:     6       hp 123.0\n11:     6       hp 123.0\n12:     8       hp 180.0\n13:     8       hp 180.0\n14:     8       hp 180.0\n15:     8       hp 205.0\n16:     8       hp 215.0\n17:     8       hp 230.0\n18:     4       hp  66.0\n19:     4       hp  52.0\n20:     4       hp  65.0\n21:     4       hp  97.0\n22:     8       hp 150.0\n23:     8       hp 150.0\n24:     8       hp 245.0\n25:     8       hp 175.0\n26:     4       hp  66.0\n27:     4       hp  91.0\n28:     4       hp 113.0\n29:     8       hp 264.0\n30:     6       hp 175.0\n31:     8       hp 335.0\n32:     4       hp 109.0\n33:     6     disp 160.0\n34:     6     disp 160.0\n35:     4     disp 108.0\n36:     6     disp 258.0\n37:     8     disp 360.0\n38:     6     disp 225.0\n39:     8     disp 360.0\n40:     4     disp 146.7\n41:     4     disp 140.8\n42:     6     disp 167.6\n43:     6     disp 167.6\n44:     8     disp 275.8\n45:     8     disp 275.8\n46:     8     disp 275.8\n47:     8     disp 472.0\n48:     8     disp 460.0\n49:     8     disp 440.0\n50:     4     disp  78.7\n51:     4     disp  75.7\n52:     4     disp  71.1\n53:     4     disp 120.1\n54:     8     disp 318.0\n55:     8     disp 304.0\n56:     8     disp 350.0\n57:     8     disp 400.0\n58:     4     disp  79.0\n59:     4     disp 120.3\n60:     4     disp  95.1\n61:     8     disp 351.0\n62:     6     disp 145.0\n63:     8     disp 301.0\n64:     4     disp 121.0\n      cyl variable value\n\n\nThis melts the data to long form based on the id and measure columns."
  },
  {
    "objectID": "posts/2024-02-26/index.html#additional-tips",
    "href": "posts/2024-02-26/index.html#additional-tips",
    "title": "Unveiling the Magic of dcast Function in R’s data.table Package",
    "section": "Additional Tips",
    "text": "Additional Tips\n\nUse fun.aggregate=length to get counts per group\nSet fill=NA to output NA for combinations without data instead of 0\nUse variable.name to set custom column names"
  },
  {
    "objectID": "posts/2024-02-27/index.html",
    "href": "posts/2024-02-27/index.html",
    "title": "Demystifying the melt() Function in R",
    "section": "",
    "text": "The melt() function in the data.table package is an extremely useful tool for reshaping datasets in R. However, for beginners, understanding how to use melt() can be tricky. In this post, I’ll walk through several examples to demonstrate how to use melt() to move from wide to long data formats."
  },
  {
    "objectID": "posts/2024-02-27/index.html#casting-data-back-into-wide-format",
    "href": "posts/2024-02-27/index.html#casting-data-back-into-wide-format",
    "title": "Demystifying the melt() Function in R",
    "section": "Casting data back into wide format",
    "text": "Casting data back into wide format\nOnce data is in long format, you can cast it back into wide format using dcast() from data.table:\n\nmelted &lt;- melt(WideTable, id.vars=\"Id\") \n\ndcast(melted, Id ~ variable)\n\nKey: &lt;Id&gt;\n      Id  Var1  Var2\n   &lt;int&gt; &lt;num&gt; &lt;num&gt;\n1:     1    10   100\n2:     2    20   200\n3:     3    30   300\n\n\nThis flexibility allows for easy data manipulation as needed for analysis and visualization."
  },
  {
    "objectID": "posts/2024-02-29/index.html",
    "href": "posts/2024-02-29/index.html",
    "title": "Unlocking Efficiency: How to Set a Data Frame Column as Index in R",
    "section": "",
    "text": "In the realm of data manipulation and analysis, efficiency is paramount. One powerful technique to enhance your workflow is setting a column in a data frame as the index. This seemingly simple task can unlock a plethora of benefits, from faster data access to streamlined operations. In this blog post, we’ll delve into the why and how of setting a data frame column as the index in R, with practical examples to illustrate its importance and ease of implementation."
  },
  {
    "objectID": "posts/2024-02-29/index.html#using-data.table-package",
    "href": "posts/2024-02-29/index.html#using-data.table-package",
    "title": "Unlocking Efficiency: How to Set a Data Frame Column as Index in R",
    "section": "Using data.table package",
    "text": "Using data.table package\n\nlibrary(data.table)\n\n# Sample data frame\ndf &lt;- data.frame(ID = c(1, 2, 3),\n                 Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n                 Score = c(85, 90, 75))\n\n# Set 'ID' column as index\nsetDT(df, key = \"ID\")\n\n# Check the updated data frame\nprint(df)\n\nKey: &lt;ID&gt;\n      ID    Name Score\n   &lt;num&gt;  &lt;char&gt; &lt;num&gt;\n1:     1   Alice    85\n2:     2     Bob    90\n3:     3 Charlie    75"
  },
  {
    "objectID": "posts/2024-02-29/index.html#using-tibble-package",
    "href": "posts/2024-02-29/index.html#using-tibble-package",
    "title": "Unlocking Efficiency: How to Set a Data Frame Column as Index in R",
    "section": "Using tibble package:",
    "text": "Using tibble package:\n\nlibrary(tibble)\n\n# Sample data frame\ndf &lt;- data.frame(ID = c(101, 202, 303),\n                 Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n                 Score = c(85, 90, 75))\n\n# Set 'ID' column as index\ndf &lt;- df |&gt; column_to_rownames(var = 'ID')\n\n# Check the updated data frame\nprint(df)\n\n       Name Score\n101   Alice    85\n202     Bob    90\n303 Charlie    75"
  },
  {
    "objectID": "posts/2024-03-01/index.html",
    "href": "posts/2024-03-01/index.html",
    "title": "Filtering Rows in R Where Column Value is Between Two Values",
    "section": "",
    "text": "Filtering data frames in R is a common task in data analysis. Often we want to subset a data frame to only keep rows that meet certain criteria. A useful filtering technique is keeping rows where a column value falls between two specified values.\nIn this post, we’ll walk through how to filter rows in R where a column value is between two values using base R syntax.\n\n\nOne way to filter rows is by using bracket notation [] and specifying a logical vector.\nLet’s create a sample data frame:\n\ndf &lt;- data.frame(\n  id = 1:10,\n  value = c(5, 3, 6, 9, 2, 4, 7, 1, 8, 10)\n)\n\nWe can filter df to only keep rows where value is between 5 and 8 with:\n\ndf[df$value &gt;= 5 & df$value &lt;= 8,]\n\n  id value\n1  1     5\n3  3     6\n7  7     7\n9  9     8\n\n\nThis filters for rows where value is greater than or equal to 5 df$value &gt;= 5 AND less than or equal to 8 df$value &lt;= 8. The comma after the logical vector tells R to return the filtered rows.\n\n\n\nAnother option is using the subset() function:\n\nsubset(df, value &gt;= 5 & value &lt;= 8)\n\n  id value\n1  1     5\n3  3     6\n7  7     7\n9  9     8\n\n\nsubset() takes a data frame as the first argument, then a logical expression similar to the bracket notation.\n\n\n\nWe can filter on different columns and value ranges:\n\n# id between 3 and 7\ndf[df$id &gt;= 3 & df$id &lt;= 7,] \n\n  id value\n3  3     6\n4  4     9\n5  5     2\n6  6     4\n7  7     7\n\n# value less than 5\nsubset(df, value &lt; 5)\n\n  id value\n2  2     3\n5  5     2\n6  6     4\n8  8     1\n\n\nIt’s also possible to filter rows outside a range by flipping the logical operators:\n\n# id NOT between 3 and 7\ndf[!(df$id &gt;= 3 & df$id &lt;= 7),]\n\n   id value\n1   1     5\n2   2     3\n8   8     1\n9   9     8\n10 10    10\n\n# value greater than 5  \nsubset(df, value &gt; 5) \n\n   id value\n3   3     6\n4   4     9\n7   7     7\n9   9     8\n10 10    10\n\n\n\n\n\nFiltering data frames where a column is between two values is straightforward in R. The key steps are:\n\nUse bracket notation df[logical,] or subset(df, logical)\nCreate a logical expression with & and &gt;=, &lt;= operators\nSpecify the column name and range of values to filter between\n\nI encourage you to try filtering data frames on your own! Subsetting by logical expressions is an important skill for efficient R programming."
  },
  {
    "objectID": "posts/2024-03-01/index.html#filtering-with-bracket-notation",
    "href": "posts/2024-03-01/index.html#filtering-with-bracket-notation",
    "title": "Filtering Rows in R Where Column Value is Between Two Values",
    "section": "",
    "text": "One way to filter rows is by using bracket notation [] and specifying a logical vector.\nLet’s create a sample data frame:\n\ndf &lt;- data.frame(\n  id = 1:10,\n  value = c(5, 3, 6, 9, 2, 4, 7, 1, 8, 10)\n)\n\nWe can filter df to only keep rows where value is between 5 and 8 with:\n\ndf[df$value &gt;= 5 & df$value &lt;= 8,]\n\n  id value\n1  1     5\n3  3     6\n7  7     7\n9  9     8\n\n\nThis filters for rows where value is greater than or equal to 5 df$value &gt;= 5 AND less than or equal to 8 df$value &lt;= 8. The comma after the logical vector tells R to return the filtered rows."
  },
  {
    "objectID": "posts/2024-03-01/index.html#filtering-with-subset",
    "href": "posts/2024-03-01/index.html#filtering-with-subset",
    "title": "Filtering Rows in R Where Column Value is Between Two Values",
    "section": "",
    "text": "Another option is using the subset() function:\n\nsubset(df, value &gt;= 5 & value &lt;= 8)\n\n  id value\n1  1     5\n3  3     6\n7  7     7\n9  9     8\n\n\nsubset() takes a data frame as the first argument, then a logical expression similar to the bracket notation."
  },
  {
    "objectID": "posts/2024-03-01/index.html#additional-examples",
    "href": "posts/2024-03-01/index.html#additional-examples",
    "title": "Filtering Rows in R Where Column Value is Between Two Values",
    "section": "",
    "text": "We can filter on different columns and value ranges:\n\n# id between 3 and 7\ndf[df$id &gt;= 3 & df$id &lt;= 7,] \n\n  id value\n3  3     6\n4  4     9\n5  5     2\n6  6     4\n7  7     7\n\n# value less than 5\nsubset(df, value &lt; 5)\n\n  id value\n2  2     3\n5  5     2\n6  6     4\n8  8     1\n\n\nIt’s also possible to filter rows outside a range by flipping the logical operators:\n\n# id NOT between 3 and 7\ndf[!(df$id &gt;= 3 & df$id &lt;= 7),]\n\n   id value\n1   1     5\n2   2     3\n8   8     1\n9   9     8\n10 10    10\n\n# value greater than 5  \nsubset(df, value &gt; 5) \n\n   id value\n3   3     6\n4   4     9\n7   7     7\n9   9     8\n10 10    10"
  },
  {
    "objectID": "posts/2024-03-01/index.html#summary",
    "href": "posts/2024-03-01/index.html#summary",
    "title": "Filtering Rows in R Where Column Value is Between Two Values",
    "section": "",
    "text": "Filtering data frames where a column is between two values is straightforward in R. The key steps are:\n\nUse bracket notation df[logical,] or subset(df, logical)\nCreate a logical expression with & and &gt;=, &lt;= operators\nSpecify the column name and range of values to filter between\n\nI encourage you to try filtering data frames on your own! Subsetting by logical expressions is an important skill for efficient R programming."
  },
  {
    "objectID": "posts/2024-03-04/index.html",
    "href": "posts/2024-03-04/index.html",
    "title": "A Beginner’s Guide to Renaming Data Frame Columns in R",
    "section": "",
    "text": "Welcome back, fellow R enthusiasts! Today, we’re diving into a fundamental yet crucial aspect of data manipulation: renaming data frame columns. Whether you’re just starting out with R or looking to refresh your skills, this guide will walk you through the process step by step using base R."
  },
  {
    "objectID": "posts/2024-03-04/index.html#method-1-using-names",
    "href": "posts/2024-03-04/index.html#method-1-using-names",
    "title": "A Beginner’s Guide to Renaming Data Frame Columns in R",
    "section": "Method 1: Using names()",
    "text": "Method 1: Using names()\n\n# Create a sample data frame\ndata &lt;- data.frame(A = c(1, 2, 3), B = c(4, 5, 6))\ncat(\"Column Names: \", names(data))\n\nColumn Names:  A B\n\n# Rename columns using names()\nnames(data) &lt;- c(\"Column_1\", \"Column_2\")\ncat(\"New Column Names: \", names(data))\n\nNew Column Names:  Column_1 Column_2\n\n\nExplanation: In this method, we use the names() function to assign new column names to the data frame. We provide a vector of new names in the desired order, matching the number of columns in the data frame."
  },
  {
    "objectID": "posts/2024-03-04/index.html#method-2-using-colnames",
    "href": "posts/2024-03-04/index.html#method-2-using-colnames",
    "title": "A Beginner’s Guide to Renaming Data Frame Columns in R",
    "section": "Method 2: Using colnames()",
    "text": "Method 2: Using colnames()\n\n# Create a sample data frame\ndata &lt;- data.frame(A = c(1, 2, 3), B = c(4, 5, 6))\ncat(\"Column Names: \", names(data))\n\nColumn Names:  A B\n\n# Rename columns using colnames()\ncolnames(data) &lt;- c(\"Column_1\", \"Column_2\")\ncat(\"New Column Names: \", names(data))\n\nNew Column Names:  Column_1 Column_2\n\n\nExplanation: Similar to names(), the colnames() function is used to rename columns in a data frame. We provide a vector of new names matching the number of columns in the data frame."
  },
  {
    "objectID": "posts/2024-03-04/index.html#method-3-using-setnames",
    "href": "posts/2024-03-04/index.html#method-3-using-setnames",
    "title": "A Beginner’s Guide to Renaming Data Frame Columns in R",
    "section": "Method 3: Using setNames()",
    "text": "Method 3: Using setNames()\n\n# Create a sample data frame\ndata &lt;- data.frame(A = c(1, 2, 3), B = c(4, 5, 6))\ncat(\"Column Names: \", names(data))\n\nColumn Names:  A B\n\n# Rename columns using setNames()\ndata &lt;- setNames(data, c(\"Column_1\", \"Column_2\"))\ncat(\"New Column Names: \", names(data))\n\nNew Column Names:  Column_1 Column_2\n\n\nExplanation: The setNames() function allows us to assign new column names to a data frame and return a new data frame with the updated names. We provide the original data frame as the first argument and a vector of new names as the second argument."
  },
  {
    "objectID": "posts/2024-03-05/index.html",
    "href": "posts/2024-03-05/index.html",
    "title": "How to Rename Factor Levels in R",
    "section": "",
    "text": "Hey there, fellow R enthusiasts! Today, we’re diving into the world of factors in R and learning how to rename their levels. Factors are essential data structures in R, often used to represent categorical variables. However, sometimes the default factor levels might not be as informative or user-friendly as we’d like them to be. Fear not! In this blog post, I’ll guide you through various methods to rename factor levels in R, accompanied by simple explanations and examples."
  },
  {
    "objectID": "posts/2024-03-05/index.html#example-1-using-levels-function",
    "href": "posts/2024-03-05/index.html#example-1-using-levels-function",
    "title": "Title: How to Rename Factor Levels in R (With Examples)",
    "section": "Example 1 Using levels() Function:",
    "text": "Example 1 Using levels() Function:\nThe levels() function allows us to view and modify the levels of a factor. To rename factor levels using this method, we simply assign new names to the existing levels.\n::: {.cell}\n# Create a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Male\", \"Female\"))\n\n# View original levels\nlevels(gender)\n::: {.cell-output .cell-output-stdout} [1] \"Female\" \"Male\" :::\n# Rename levels\nlevels(gender) &lt;- c(\"M\", \"F\")\n\n# View modified levels\nlevels(gender)\n::: {.cell-output .cell-output-stdout} [1] \"M\" \"F\" ::: :::"
  },
  {
    "objectID": "posts/2024-03-05/index.html#example-2-using-revalue-function-from-plyr-package",
    "href": "posts/2024-03-05/index.html#example-2-using-revalue-function-from-plyr-package",
    "title": "Title: How to Rename Factor Levels in R (With Examples)",
    "section": "Example 2 Using revalue() Function from plyr Package",
    "text": "Example 2 Using revalue() Function from plyr Package\nThe revalue() function from the plyr package provides a convenient way to rename factor levels by specifying old and new values as pairs.\n\n# Install and load the plyr package\n#   install.packages(\"plyr\")\nlibrary(plyr)\n\n# Create a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Male\", \"Female\"))\nlevels(gender)\n\n[1] \"Female\" \"Male\"  \n\n# Rename levels\ngender &lt;- revalue(gender, c(\"Male\" = \"M\", \"Female\" = \"F\"))\n\n# View modified levels\nlevels(gender)\n\n[1] \"F\" \"M\""
  },
  {
    "objectID": "posts/2024-03-05/index.html#example-3-using-fct_recode-function-from-forcats-package",
    "href": "posts/2024-03-05/index.html#example-3-using-fct_recode-function-from-forcats-package",
    "title": "How to Rename Factor Levels in R",
    "section": "Example 3 Using fct_recode() Function from forcats Package",
    "text": "Example 3 Using fct_recode() Function from forcats Package\nThe forcats package provides powerful tools for working with factors in R. The fct_recode() function allows us to rename factor levels by specifying old and new values.\n\n# Install and load the forcats package\n#   install.packages(\"forcats\")\nlibrary(forcats)\n\n# Create a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Male\", \"Female\"))\nlevels(gender)\n\n[1] \"Female\" \"Male\"  \n\n# Rename levels\ngender &lt;- fct_recode(gender, \"M\" = \"Male\", \"F\" = \"Female\")\n\n# View modified levels\nlevels(gender)\n\n[1] \"F\" \"M\""
  },
  {
    "objectID": "posts/2024-03-05/index.html#example-1---using-levels-function",
    "href": "posts/2024-03-05/index.html#example-1---using-levels-function",
    "title": "How to Rename Factor Levels in R",
    "section": "Example 1 - Using levels() Function:",
    "text": "Example 1 - Using levels() Function:\nThe levels() function allows us to view and modify the levels of a factor. To rename factor levels using this method, we simply assign new names to the existing levels.\n\n# Create a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Male\", \"Female\"))\n\n# View original levels\nlevels(gender)\n\n[1] \"Female\" \"Male\"  \n\n# Rename levels\nlevels(gender) &lt;- c(\"M\", \"F\")\n\n# View modified levels\nlevels(gender)\n\n[1] \"M\" \"F\""
  },
  {
    "objectID": "posts/2024-03-05/index.html#example-2---using-revalue-function-from-plyr-package",
    "href": "posts/2024-03-05/index.html#example-2---using-revalue-function-from-plyr-package",
    "title": "How to Rename Factor Levels in R",
    "section": "Example 2 - Using revalue() Function from plyr Package",
    "text": "Example 2 - Using revalue() Function from plyr Package\nThe revalue() function from the plyr package provides a convenient way to rename factor levels by specifying old and new values as pairs.\n\n# Install and load the plyr package\n#   install.packages(\"plyr\")\nlibrary(plyr)\n\n# Create a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Male\", \"Female\"))\nlevels(gender)\n\n[1] \"Female\" \"Male\"  \n\n# Rename levels\ngender &lt;- revalue(gender, c(\"Male\" = \"M\", \"Female\" = \"F\"))\n\n# View modified levels\nlevels(gender)\n\n[1] \"F\" \"M\""
  },
  {
    "objectID": "posts/2024-03-06/index.html",
    "href": "posts/2024-03-06/index.html",
    "title": "How to Add New Level to Factor in R",
    "section": "",
    "text": "Introduction\nAs an R programmer, working with categorical data is a common task, and factors (a data type in R) are used to represent categorical variables. However, sometimes you may encounter a situation where you need to add a new level to an existing factor. This could happen when you have new data that includes a category not present in your original dataset.\nIn this blog post, we’ll explore how to add a new level to a factor in R using base R functions. Let’s dive in!\n\n\nExample\nFirst, let’s create a sample dataset:\n\n# Create a sample dataset\nanimal &lt;- c(\"dog\", \"cat\", \"bird\", \"dog\", \"cat\", \"fish\")\nanimal_factor &lt;- factor(animal)\n\nanimal\n\n[1] \"dog\"  \"cat\"  \"bird\" \"dog\"  \"cat\"  \"fish\"\n\nlevels(animal_factor)\n\n[1] \"bird\" \"cat\"  \"dog\"  \"fish\"\n\n\nHere, we’ve created a character vector called animal and converted it into a factor called animal_factor.\nNow, let’s say we want to add a new level “reptile” to our animal_factor. We can do this using the levels() function:\n\n# Add a new level to the factor\nnew_levels &lt;- c(levels(animal_factor), \"reptile\")\nanimal_factor &lt;- factor(animal_factor, levels = new_levels)\nlevels(animal_factor)\n\n[1] \"bird\"    \"cat\"     \"dog\"     \"fish\"    \"reptile\"\n\n\nHere’s what the code does:\n\nnew_levels &lt;- c(levels(animal_factor), \"reptile\"): This line creates a new vector called new_levels that contains all the existing levels from animal_factor plus the new level “reptile”.\nanimal_factor &lt;- factor(animal_factor, levels = new_levels): This line recreates the animal_factor object as a factor, but with the levels specified in new_levels.\nlevels(animal_factor): This line prints the updated levels of the animal_factor, which now includes “reptile”.\n\nYou see that the output is:\n[1] \"bird\" \"cat\"  \"dog\"  \"fish\" \"reptile\"\nAs you can see, the new level “reptile” has been added to the factor animal_factor.\nIt’s important to note that adding a new level to a factor doesn’t change the existing data; it simply allows for the possibility of including the new level in future data.\nNow that you’ve learned how to add a new level to a factor in R, it’s your turn to practice! Try creating your own dataset and experiment with adding new levels to factors. You can also explore other related functions, such as levels&lt;-() and addNA(), which can be useful when working with factors.\nRemember, practice makes perfect, so keep coding and exploring the world of R!"
  },
  {
    "objectID": "posts/2024-03-07/index.html",
    "href": "posts/2024-03-07/index.html",
    "title": "How to Subset Data Frame in R by Multiple Conditions",
    "section": "",
    "text": "In data analysis with R, subsetting data frames based on multiple conditions is a common task. It allows us to extract specific subsets of data that meet certain criteria. In this blog post, we will explore how to subset a data frame using three different methods: base R’s subset() function, dplyr’s filter() function, and the data.table package."
  },
  {
    "objectID": "posts/2024-03-07/index.html#using-base-rs-subset-function",
    "href": "posts/2024-03-07/index.html#using-base-rs-subset-function",
    "title": "How to Subset Data Frame in R by Multiple Conditions",
    "section": "Using Base R’s subset() Function",
    "text": "Using Base R’s subset() Function\nBase R provides a handy function called subset() that allows us to subset data frames based on one or more conditions.\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Subset data frame using subset() function\nsubset_mtcars &lt;- subset(mtcars, mpg &gt; 20 & cyl == 4)\n\n# View the resulting subset\nprint(subset_mtcars)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nIn the above code, we first load the mtcars dataset. Then, we use the subset() function to create a subset of the data frame where the miles per gallon (mpg) is greater than 20 and the number of cylinders (cyl) is equal to 4. Finally, we print the resulting subset."
  },
  {
    "objectID": "posts/2024-03-07/index.html#using-dplyrs-filter-function",
    "href": "posts/2024-03-07/index.html#using-dplyrs-filter-function",
    "title": "How to Subset Data Frame in R by Multiple Conditions",
    "section": "Using dplyr’s filter() Function",
    "text": "Using dplyr’s filter() Function\ndplyr is a powerful package for data manipulation, and it provides the filter() function for subsetting data frames based on conditions.\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Subset data frame using filter() function\nfilter_mtcars &lt;- mtcars %&gt;%\n  filter(mpg &gt; 20, cyl == 4)\n\n# View the resulting subset\nprint(filter_mtcars)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nIn this code snippet, we load the dplyr package and use the %&gt;% operator, also known as the pipe operator, to pipe the mtcars dataset into the filter() function. We specify the conditions within the filter() function to create the subset, and then print the resulting subset."
  },
  {
    "objectID": "posts/2024-03-07/index.html#using-data.table-package",
    "href": "posts/2024-03-07/index.html#using-data.table-package",
    "title": "How to Subset Data Frame in R by Multiple Conditions",
    "section": "Using data.table Package",
    "text": "Using data.table Package\nThe data.table package is known for its speed and efficiency in handling large datasets. We can use data.table’s syntax to subset data frames as well.\n\n# Load the data.table package\nlibrary(data.table)\n\n# Convert mtcars to data.table\ndt_mtcars &lt;- as.data.table(mtcars)\n\n# Subset data frame using data.table syntax\ndt_subset_mtcars &lt;- dt_mtcars[mpg &gt; 20 & cyl == 4]\n\n# Convert back to data frame (optional)\nsubset_mtcars_dt &lt;- as.data.frame(dt_subset_mtcars)\n\n# View the resulting subset\nprint(subset_mtcars_dt)\n\n    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n2  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n3  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n4  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n5  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n6  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n7  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n8  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n9  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n10 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n11 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nIn this code block, we first load the data.table package and convert the mtcars data frame into a data.table using the as.data.table() function. Then, we subset the data using data.table’s syntax, specifying the conditions within square brackets. Optionally, we can convert the resulting subset back to a data frame using as.data.frame() function before printing it."
  },
  {
    "objectID": "posts/2024-03-08/index.html",
    "href": "posts/2024-03-08/index.html",
    "title": "Taming the Nameless: Using the names() Function in R",
    "section": "",
    "text": "Have you ever created a dataset in R and ended up with a bunch of unnamed elements? It can make your code clunky and hard to read. Fear not, fellow R wranglers! The names() function is here to save the day."
  },
  {
    "objectID": "posts/2024-03-08/index.html#example-1-naming-a-vector",
    "href": "posts/2024-03-08/index.html#example-1-naming-a-vector",
    "title": "Taming the Nameless: Using the names() Function in R",
    "section": "Example 1: Naming a Vector",
    "text": "Example 1: Naming a Vector\n\n# Create an unnamed vector\nmy_data &lt;- c(23, 5, 99)\n\n# Check the names (there are none!)\nnames(my_data)\n\nNULL\n\n# Assign names using c()\nnames(my_data) &lt;- c(\"age\", \"height\", \"iq\")\n\n# Print the data with names\nmy_data\n\n   age height     iq \n    23      5     99 \n\n\nIn this example, we started with an unnamed vector. We then used names() to see there were no existing names. Finally, we assigned clear names using c() and the assignment operator."
  },
  {
    "objectID": "posts/2024-03-08/index.html#example-2-naming-a-list",
    "href": "posts/2024-03-08/index.html#example-2-naming-a-list",
    "title": "Taming the Nameless: Using the names() Function in R",
    "section": "Example 2: Naming a List",
    "text": "Example 2: Naming a List\n\n# Create an unnamed list\nmy_info &lt;- list(score = 87, games = 10)\n\n# Peek at the names (default is numeric order)\nmy_info\n\n$score\n[1] 87\n\n$games\n[1] 10\n\n# Assign new names\nnames(my_info) &lt;- c(\"exam_score\", \"num_games\")\n\n# Print the list with names\nmy_info\n\n$exam_score\n[1] 87\n\n$num_games\n[1] 10\n\n\nHere, we created a list with default numeric names. We used names() to see these, then replaced them with more descriptive names."
  },
  {
    "objectID": "posts/2024-03-08/index.html#example-3-renaming-data-frame-columns",
    "href": "posts/2024-03-08/index.html#example-3-renaming-data-frame-columns",
    "title": "Taming the Nameless: Using the names() Function in R",
    "section": "Example 3: Renaming Data Frame Columns",
    "text": "Example 3: Renaming Data Frame Columns\n\n# Sample data frame (mtcars comes with R)\nhead(mtcars)  # Peek at the data\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Rename the \"cyl\" column\nnames(mtcars)[[3]] &lt;- \"cylinders\"  # Access by position\n\n# Print the data frame with renamed column\nhead(mtcars)\n\n                   mpg cyl cylinders  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6       160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6       160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4       108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6       258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8       360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6       225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThis example shows how names() can be used with data frames. We access the column position (index 3) and assign a new name using double square brackets ([[ ]])."
  },
  {
    "objectID": "posts/2024-03-11/index.html",
    "href": "posts/2024-03-11/index.html",
    "title": "Wrangling Names in R: Your Guide to the make.names() Function",
    "section": "",
    "text": "Introduction\nEver tried to use a number or special character as a name for a variable or column in R, only to be met with an error? R has specific rules for what constitutes a valid name, and the make.names function is your knight in shining armor when it comes to wrangling these names into something R understands.\n\n\nWhat is make.names?\nThink of make.names as a name janitor. It takes a vector of characters (potential names) and ensures they comply with R’s naming conventions. These conventions say a valid name:\n\nMust start with a letter or a dot (“.”)\nCan only contain letters, numbers, periods, and underscores\nCannot be a reserved word in R (like if, else, or for)\n\n\n\nHow to Use make.names\nUsing make.names is straightforward. You simply provide it with a character vector containing your desired names, and it returns a new vector with valid names. Here’s the basic syntax:\nnew_names &lt;- make.names(old_names)\n\n\nMaking Names Unique (Optional)\nBy default, make.names doesn’t guarantee unique names. If you have duplicates, it might just keep them. To ensure unique names, add the unique = TRUE argument:\nunique_names &lt;- make.names(old_names, unique = TRUE)\nThis will modify duplicate names slightly to make them distinct.\n\n\nExamples in Action!\nLet’s see make.names in action with some examples:\n\n# Example 1: Fix numeric names\nnumbers &lt;- c(10, 20, 30)\nvalid_names &lt;- make.names(numbers)\nprint(valid_names)\n\n[1] \"X10\" \"X20\" \"X30\"\n\n\nIn this case, make.names prepends an “X” to each number to make them valid names.\n\n# Example 2: Handle special characters\nspecial_chars &lt;- c(\"data#1\", \"result$\", \"graph!\")\nclean_names &lt;- make.names(special_chars)\nprint(clean_names)\n\n[1] \"data.1\"  \"result.\" \"graph.\" \n\n\nHere, make.names removes special characters and replaces them with periods (except for “$” which is removed).\n\n\nGive it a Try!\nR is a playground for exploration. Here are some challenges to try with make.names:\n\nCreate a vector with names containing spaces and underscores. Use make.names to see how it handles them.\nTry using make.names on a data frame’s column names. What happens?\nExplore the unique = TRUE argument. Can you think of situations where it might be necessary?\n\nRemember, make.names is your friend when dealing with non-standard names in R. By understanding its purpose and using it effectively, you can keep your R code clean and error-free. Happy coding!"
  },
  {
    "objectID": "posts/2024-03-12/index.html",
    "href": "posts/2024-03-12/index.html",
    "title": "Mastering Random Sampling in R with the sample() Function",
    "section": "",
    "text": "The sample() function in R is a powerful tool that allows you to generate random samples from a given dataset or vector. It’s an essential function for tasks such as data analysis, Monte Carlo simulations, and randomized experiments. In this blog post, we’ll explore the sample() function in detail and provide examples to help you understand how to use it effectively."
  },
  {
    "objectID": "posts/2024-03-12/index.html#simple-random-sampling",
    "href": "posts/2024-03-12/index.html#simple-random-sampling",
    "title": "Mastering Random Sampling in R with the sample() Function",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nLet’s start with a basic example of simple random sampling without replacement:\n\n# Creating a vector\nnumbers &lt;- 1:10\n\n# Drawing a sample of 5 elements without replacement\nsample_without_replacement &lt;- sample(numbers, 5)\n\nThis code will generate a random sample of 5 unique elements from the numbers vector. The output might look something like:\n\nprint(sample_without_replacement)\n\n[1] 7 8 3 6 1"
  },
  {
    "objectID": "posts/2024-03-12/index.html#sampling-with-replacement",
    "href": "posts/2024-03-12/index.html#sampling-with-replacement",
    "title": "Mastering Random Sampling in R with the sample() Function",
    "section": "Sampling with Replacement",
    "text": "Sampling with Replacement\nSometimes, you may want to sample with replacement, which means that an element can be selected multiple times. To do this, you can set the replace argument to TRUE:\n\n# Drawing a sample of 5 elements with replacement\nsample_with_replacement &lt;- sample(numbers, 5, replace = TRUE)\n\nThis code might produce an output like:\n\nprint(sample_with_replacement)\n\n[1] 1 3 6 6 2\n\n\nNotice that the number 2 appears twice in the sample, since we’re sampling with replacement."
  },
  {
    "objectID": "posts/2024-03-12/index.html#weighted-random-sampling",
    "href": "posts/2024-03-12/index.html#weighted-random-sampling",
    "title": "Mastering Random Sampling in R with the sample() Function",
    "section": "Weighted Random Sampling",
    "text": "Weighted Random Sampling\nThe prob argument in the sample() function allows you to perform weighted random sampling. This means that elements have different probabilities of being selected based on the provided weights. Here’s an example:\n\n# Creating a vector of weights\nweights &lt;- c(0.1, 0.2, 0.3, 0.4)\n\n# Drawing a weighted sample of 3 elements without replacement\nweighted_sample &lt;- sample(1:4, 3, replace = FALSE, prob = weights)\n\nIn this example, the numbers 1, 2, 3, and 4 have weights of 0.1, 0.2, 0.3, and 0.4, respectively. The output might look like:\n\nprint(weighted_sample)\n\n[1] 4 3 2\n\n\nNotice how the elements with higher weights (4 and 3) are more likely to be selected in the sample."
  },
  {
    "objectID": "posts/2024-03-13/index.html",
    "href": "posts/2024-03-13/index.html",
    "title": "🚀 Exciting News! 🚀",
    "section": "",
    "text": "I’m thrilled to announce the latest release of tidyAML, version 0.0.5, now available for download on CRAN or GitHub! 🎉\nIn this release, we’ve introduced some fantastic new features and made minor fixes and improvements to enhance your experience with tidyAML.\nNew Features:\n📈 plot_regression_residuals(): Dive deeper into your data analysis with this new function that allows you to visualize residuals, providing valuable insights into your regression models.\n📊 plot_regression_predictions(): Want to see predictions from your model? Now you can with this handy function, making it easier than ever to understand your model’s performance.\nMinor Fixes and Improvements:\n🛠️ load_deps(): We’ve listened to your feedback and dropped the selection message from this function for a smoother user experience.\n🔄 fast_regression() and fast_classification(): Say goodbye to NULL predictions! We’ve updated these functions to ensure more accurate results for your analyses.\nWith tidyAML 0.0.5, we’re committed to providing you with the tools you need to streamline your data analysis and make informed decisions. Whether you’re a seasoned data scientist or just starting out, tidyAML has something for everyone.\nDownload tidyAML 0.0.5 today and take your data analysis to the next level! Don’t forget to share your feedback and experiences with us - we love hearing from our users.\nHappy analyzing! 📊✨"
  },
  {
    "objectID": "posts/2024-03-14/index.html",
    "href": "posts/2024-03-14/index.html",
    "title": "Unleash the Power of Your Data: Extend Excel with Python and R!",
    "section": "",
    "text": "Introduction\nHave you ever felt limited by Excel’s capabilities? Sure, it’s fantastic for basic tasks and creating clear spreadsheets, but what if your data craves something more? What if you have complex analyses or stunning visualizations in mind? This is where my new book, Extending Excel with Python and R: Unlock the Potential of Analytics Languages for Advanced Data Manipulation and Visualization, comes in!\nIn this book, I’ll be your guide on a journey to unlock the true potential of your data. We’ll delve into the world of Python and R, two powerful programming languages that can supercharge your Excel expertise.\nWhy Python and R?\nThese languages aren’t Excel replacements; they’re superpowers! Python and R are designed for heavy-duty data analysis and manipulation. They can handle massive datasets, automate complex tasks, and create mind-blowing visualizations that would leave Excel speechless.\nBut I don’t know how to code!\nDon’t worry! This book is designed for users at all levels. Even if you’ve never written a line of code before, I’ll break down the basics of Python and R in a way that’s easy to understand. We’ll start with simple examples and gradually build your skills, so you’ll be conquering complex tasks in no time.\nWhat will you learn?\n\nExtracting and Importing Data: Learn how to effortlessly bring data from various sources into your Python or R environment for seamless analysis.\nData Cleaning and Manipulation: Master the art of transforming your data into a usable format. No more messy spreadsheets holding you back!\nAdvanced Data Analysis: Unleash the power of statistical functions and modeling techniques to uncover hidden insights within your data.\nCreating Stunning Visualizations: Go beyond basic charts and graphs. We’ll create interactive and informative visualizations that will bring your data to life.\nBringing it Back to Excel: Seamlessly integrate your Python and R results back into Excel, so you can leverage the best of both worlds.\n\nExtending Excel with Python and R is more than just a book; it’s your gateway to a whole new level of data expertise. Imagine the possibilities! You’ll be able to:\n\nAutomate tedious tasks: Free up your time for what matters – strategic analysis and data-driven decision making.\nTackle complex datasets: No dataset is too big or too messy for your new skillset.\nImpress your audience: Create presentations and reports that will leave a lasting impression.\n\nReady to unlock the true potential of your data? Get your copy of Extending Excel with Python and R: Unlock the Potential of Analytics Languages for Advanced Data Manipulation and Visualization today! Available on Amazon: https://www.amazon.com/dp/1804610690/ref=tsm_1_fb_lk\nLet’s embark on this data adventure together!"
  },
  {
    "objectID": "posts/2024-03-15/idnex.html",
    "href": "posts/2024-03-15/idnex.html",
    "title": "Plotting Training and Testing Predictions with tidyAML",
    "section": "",
    "text": "Introduction\nIn the realm of machine learning, visualizing model predictions is essential for understanding the performance and behavior of our algorithms. When it comes to regression tasks, plotting predictions alongside actual values provides valuable insights into how well our model is capturing the underlying patterns in the data. With the plot_regression_predictions() function in tidyAML, this process becomes seamless and informative.\n\n\nIntroducing plot_regression_predictions()\nThe plot_regression_predictions() function is a powerful tool for visualizing regression predictions in R. Developed as part of the tidyAML package, it leverages the capabilities of ggplot2 to create insightful plots that compare actual values with model predictions, both for training and testing datasets.\n\n\nSyntax and Arguments\nLet’s break down the syntax and arguments of plot_regression_predictions():\nplot_regression_predictions(.data, .output = \"list\")\n\n.data: This argument takes the data from the output of the extract_regression_residuals() function.\n.output: By default, this argument is set to “list”, which returns a list of plots. Alternatively, you can choose “facet”, which returns a single faceted plot.\n\n\n\nExample Usage\nTo illustrate how plot_regression_predictions() works in practice, let’s consider an example using the mtcars dataset and a simple linear regression model.\n\nlibrary(tidyAML)\nlibrary(recipes)\n\n# Define the recipe\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\n\n# Train the model\nfrt_tbl &lt;- fast_regression(\n  mtcars,\n  rec_obj\n)\n\n Setting default kernel parameters  \n Setting default kernel parameters  \n\n\nIn this example, we’ve created a recipe for predicting mpg based on other variables in the mtcars dataset. We then trained a fast regression model using fast_regression() from the recipes package.\nNow, let’s use extract_wflw_pred() to extract the predictions:\n\n# Extract predictions\npreds &lt;- extract_wflw_pred(frt_tbl, 1:nrow(frt_tbl))\nhead(preds)\n\n# A tibble: 6 × 4\n  .model_type     .data_category .data_type .value\n  &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n1 lm - linear_reg actual         actual       15.2\n2 lm - linear_reg actual         actual       30.4\n3 lm - linear_reg actual         actual       21.4\n4 lm - linear_reg actual         actual       33.9\n5 lm - linear_reg actual         actual       19.7\n6 lm - linear_reg actual         actual       10.4\n\nunique(preds$.model_type)\n\n [1] \"lm - linear_reg\"            \"brulee - linear_reg\"       \n [3] \"glm - linear_reg\"           \"stan - linear_reg\"         \n [5] \"dbarts - bart\"              \"xgboost - boost_tree\"      \n [7] \"rpart - decision_tree\"      \"earth - mars\"              \n [9] \"nnet - mlp\"                 \"brulee - mlp\"              \n[11] \"kknn - nearest_neighbor\"    \"ranger - rand_forest\"      \n[13] \"randomForest - rand_forest\" \"LiblineaR - svm_linear\"    \n[15] \"kernlab - svm_linear\"       \"kernlab - svm_poly\"        \n[17] \"kernlab - svm_rbf\"         \n\n\nWith the predictions extracted, we can now plot the regression predictions using plot_regression_predictions():\n\n# Plot regression predictions\nextract_wflw_pred(frt_tbl, 1:6) |&gt;\n  plot_regression_predictions(.output = \"facet\")\n\n\n\n\n\n\n\nextract_wflw_pred(frt_tbl, 1:6) |&gt;\n  plot_regression_predictions(.output = \"list\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\nThis will generate a set of plots comparing actual values with model predictions for both the training and testing datasets.\n\n\nInterpreting the Plots\nThe plots produced by plot_regression_predictions() offer valuable insights into the performance of our regression model. Here’s what you can expect to see:\n\nActual vs. Predicted Values: The main plot compares the actual values (y-axis) with the predicted values also (y-axis). This allows you to see how the model performs across the range of observed values both in training and in testing.\nTraining vs. Testing: If you choose the “facet” output option, you’ll see separate plots for training and testing data sets by model type.\n\n\n\nConclusion\nIn summary, plot_regression_predictions() is a valuable tool for visualizing regression predictions in R. Whether you’re assessing model performance, diagnosing errors, or communicating results to stakeholders, these plots provide a clear and intuitive way to understand how well your model is capturing the underlying patterns in the data. So next time you’re working on a regression task with tidyAML, don’t forget to leverage the power of visualization with plot_regression_predictions()!"
  },
  {
    "objectID": "posts/2024-03-18/index.html",
    "href": "posts/2024-03-18/index.html",
    "title": "Introducing plot_regression_residuals() from tidyAML: Unveiling the Power of Visualizing Regression Residuals",
    "section": "",
    "text": "Introduction\nGreetings, fellow R enthusiasts! Today, we’re diving into the depths of tidyAML, specifically exploring a new gem in its arsenal: plot_regression_residuals(). Strap in as we embark on a journey to unravel the mysteries of regression residuals and witness how this function revolutionizes the way we visualize and understand our regression models.\n\n\nUnderstanding the Essence of Regression Residuals\nBefore we delve into the intricacies of plot_regression_residuals(), let’s take a moment to appreciate the significance of regression residuals. In the realm of statistical modeling, residuals are like breadcrumbs left behind by our regression models. They represent the discrepancies between observed and predicted values, serving as crucial indicators of model performance and areas for improvement.\n\n\nUnveiling the Functionality\nAt its core, plot_regression_residuals() is designed to provide us with intuitive visualizations of regression residuals. Armed with the output from extract_regression_residuals(), this function empowers us to generate insightful ggplot2 plots effortlessly.\n\n\nSyntax Demystified\nThe syntax of plot_regression_residuals() is elegantly simple:\nplot_regression_residuals(.data)\nHere, .data refers to the data extracted from the output of extract_regression_residuals(). It’s like feeding the function with the raw material it needs to work its magic.\n\n\nBringing Theory to Life: An Example\nLet’s put theory into practice with a hands-on example:\n\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(earth)\n\n# Create a recipe\nrec_obj &lt;- recipe(mpg ~ ., data = mtcars)\n\n# Perform fast regression\nfrt_tbl &lt;- fast_regression(\n  mtcars,\n  rec_obj,\n  .parsnip_eng = c(\"lm\",\"glm\",\"earth\"),\n  .parsnip_fns = c(\"linear_reg\",\"mars\")\n)\n\n# Extract regression residuals and plot\nextract_regression_residuals(frt_tbl, FALSE) |&gt;\n  plot_regression_residuals()\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\nIn this snippet, we prepare our data with a recipe, perform fast regression, extract the residuals, and finally, visualize them using plot_regression_residuals(). It’s like crafting a masterpiece with just a few strokes of the keyboard.\n\n\nUnlocking Insights with Visualization\nWhat makes plot_regression_residuals() truly remarkable is its ability to unlock hidden insights within our data. With a single function call, we can uncover patterns, detect outliers, and assess the homoscedasticity of our model—all through the lens of beautifully crafted plots.\n\n\nConclusion: Empowering Data Exploration\nAs we draw the curtains on our exploration of plot_regression_residuals(), it’s evident that tidyAML continues to push the boundaries of data exploration and analysis. By democratizing the visualization of regression residuals, this function empowers R users of all skill levels to gain deeper insights into their models and make more informed decisions.\nSo, next time you find yourself knee-deep in regression analysis, remember the power that lies within plot_regression_residuals(). With just a single function call, you can transform raw residuals into actionable insights, propelling your data analysis journey to new heights.\nTo dive deeper into the world of plot_regression_residuals() and unleash its full potential, check out the official documentation here.\nUntil next time, happy coding and may your residuals always lead you to new discoveries!"
  },
  {
    "objectID": "posts/2024-03-19/index.html",
    "href": "posts/2024-03-19/index.html",
    "title": "How to Replicate Rows in a Data Frame in R",
    "section": "",
    "text": "Introduction\nAre you working with a dataset where you need to duplicate certain rows multiple times? Perhaps you want to create synthetic data by replicating existing observations, or you need to handle imbalanced data by oversampling minority classes. Whatever the reason, replicating rows in a data frame is a handy skill to have in your R programming toolkit.\nIn this post, we’ll explore how to replicate rows in a data frame using base R functions. We’ll cover replicating each row the same number of times, as well as replicating rows a different number of times based on a specified pattern.\nLet’s start by creating a sample data frame:\n\n# Create a sample data frame\ndf &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\"),\n  Age = c(25, 30, 35, 40),\n  City = c(\"New York\", \"London\", \"Paris\", \"Tokyo\")\n)\n\ndf\n\n     Name Age     City\n1   Alice  25 New York\n2     Bob  30   London\n3 Charlie  35    Paris\n4   David  40    Tokyo\n\n\n\n\nReplicating Each Row the Same Number of Times\nTo replicate each row in a data frame the same number of times, we can use the rep() function in combination with row.names() and cbind(). Here’s an example where we replicate each row twice:\n\n# Replicate each row twice\nreplicated_df &lt;- cbind(df, rep(row.names(df), each = 2))\n\nOutput:\n\nreplicated_df\n\n     Name Age     City rep(row.names(df), each = 2)\n1   Alice  25 New York                            1\n2     Bob  30   London                            1\n3 Charlie  35    Paris                            2\n4   David  40    Tokyo                            2\n5   Alice  25 New York                            3\n6     Bob  30   London                            3\n7 Charlie  35    Paris                            4\n8   David  40    Tokyo                            4\n\n\nIn this example, we use the rep() function to repeat the row names of the original data frame df twice for each row (using the each argument). We then combine the original data frame with the repeated row names using cbind() to create a new data frame replicated_df.\n\n\nReplicating Rows a Different Number of Times\nWhat if you want to replicate each row a different number of times? You can achieve this by creating a vector that specifies the number of times to replicate each row. Let’s say we want to replicate the first row twice, the second row three times, the third row once, and the fourth row four times:\n\n# Vector specifying the number of times to replicate each row\nreplication_times &lt;- c(2, 3, 1, 4)\n\n# Replicate rows according to the specified pattern\nreplicated_df &lt;- df[rep(row.names(df), times = replication_times), ]\n\nOutput:\n\nreplicated_df\n\n       Name Age     City\n1     Alice  25 New York\n1.1   Alice  25 New York\n2       Bob  30   London\n2.1     Bob  30   London\n2.2     Bob  30   London\n3   Charlie  35    Paris\n4     David  40    Tokyo\n4.1   David  40    Tokyo\n4.2   David  40    Tokyo\n4.3   David  40    Tokyo\n\n\nIn this example, we create a vector replication_times that specifies the number of times to replicate each row. We then use the rep() function with the times argument to repeat the row names according to the specified pattern. Finally, we subset the original data frame df using the repeated row names to create the new data frame replicated_df.\n\n\nTry It Yourself!\nReplicating rows in a data frame is a useful skill to have, and the best way to solidify your understanding is to practice. Why not try replicating rows in your own datasets or create a new data frame and experiment with different replication patterns?\nRemember, the syntax for replicating rows is:\n# Replicate each row the same number of times\nreplicated_df &lt;- cbind(df, rep(row.names(df), each = n))\n\n# Replicate rows a different number of times\nreplication_times &lt;- c(n1, n2, n3, ...)\nreplicated_df &lt;- df[rep(row.names(df), times = replication_times), ]\nReplace n with the number of times you want to replicate each row, and replace n1, n2, n3, etc., with the desired number of times to replicate each row individually.\nHappy coding!"
  },
  {
    "objectID": "posts/2024-03-20/index.html",
    "href": "posts/2024-03-20/index.html",
    "title": "Mastering Data Segmentation: A Guide to Using the cut() Function in R",
    "section": "",
    "text": "In the realm of data analysis, understanding how to effectively segment your data is paramount. Whether you’re dealing with age groups, income brackets, or any other continuous variable, the ability to categorize your data can provide invaluable insights. In R, the cut() function is a powerful tool for precisely this purpose. In this guide, we’ll explore how to harness the full potential of cut() to slice and dice your data with ease."
  },
  {
    "objectID": "posts/2024-03-20/index.html#example-1-basic-usage",
    "href": "posts/2024-03-20/index.html#example-1-basic-usage",
    "title": "Mastering Data Segmentation: A Guide to Using the cut() Function in R",
    "section": "Example 1: Basic Usage",
    "text": "Example 1: Basic Usage\nLet’s start with a simple example. Suppose we have a vector representing ages:\n\nages &lt;- c(21, 35, 42, 18, 65, 28, 51, 40, 22, 60)\n\nNow, let’s use the cut() function to divide these ages into three categories: “Young”, “Middle-aged”, and “Elderly”:\n\nage_groups &lt;- cut(\n  ages, \n  breaks = c(0, 30, 50, Inf), \n  labels = c(\"Young\", \"Middle-aged\", \"Elderly\")\n  )\n\nprint(age_groups)\n\n [1] Young       Middle-aged Middle-aged Young       Elderly     Young      \n [7] Elderly     Middle-aged Young       Elderly    \nLevels: Young Middle-aged Elderly\n\n\nIn this code: - breaks = c(0, 30, 50, Inf) specifies the breakpoints for the age groups. - labels = c(\"Young\", \"Middle-aged\", \"Elderly\") assigns labels to each category."
  },
  {
    "objectID": "posts/2024-03-20/index.html#example-2-customized-breakpoints",
    "href": "posts/2024-03-20/index.html#example-2-customized-breakpoints",
    "title": "Mastering Data Segmentation: A Guide to Using the cut() Function in R",
    "section": "Example 2: Customized Breakpoints",
    "text": "Example 2: Customized Breakpoints\nNow, let’s say we want more granular age groups. We can specify custom breakpoints:\n\ncustom_breaks &lt;- c(0, 20, 30, 40, 50, 60, Inf)\ncustom_labels &lt;- c(\"0-20\", \"21-30\", \"31-40\", \"41-50\", \"51-60\", \"61+\")\ncustom_age_groups &lt;- cut(ages, \n                         breaks = custom_breaks, \n                         labels = custom_labels\n                         )\n\nprint(custom_age_groups)\n\n [1] 21-30 31-40 41-50 0-20  61+   21-30 51-60 31-40 21-30 51-60\nLevels: 0-20 21-30 31-40 41-50 51-60 61+\n\n\nThis will create age groups such as “0-20”, “21-30”, and so on, making our analysis more detailed."
  },
  {
    "objectID": "posts/2024-03-21/index.html",
    "href": "posts/2024-03-21/index.html",
    "title": "Mastering Replacement: Using the replace() Function in R",
    "section": "",
    "text": "The replace() function is a handy tool in your R toolbox for modifying specific elements within vectors and data frames. It allows you to swap out unwanted values with new ones, making data cleaning and manipulation a breeze."
  },
  {
    "objectID": "posts/2024-03-21/index.html#example-1-replacing-a-single-value",
    "href": "posts/2024-03-21/index.html#example-1-replacing-a-single-value",
    "title": "Mastering Replacement: Using the replace() Function in R",
    "section": "Example 1: Replacing a Single Value",
    "text": "Example 1: Replacing a Single Value\nImagine you have a vector of temperatures (temp) with an outlier you want to fix. Here’s how to replace it:\n\ntemp &lt;- c(15, 22, 30, 10, 18)  # Our temperature data\nnew_temp &lt;- replace(temp, 3, 25)  # Replace the value at position 3 (30) with 25\nprint(temp)  # Output: [15, 22, 30, 10, 18]\n\n[1] 15 22 30 10 18\n\nprint(new_temp)  # Output: [15, 22, 25, 10, 18]\n\n[1] 15 22 25 10 18"
  },
  {
    "objectID": "posts/2024-03-21/index.html#example-2-replacing-multiple-values-based-on-conditions",
    "href": "posts/2024-03-21/index.html#example-2-replacing-multiple-values-based-on-conditions",
    "title": "Mastering Replacement: Using the replace() Function in R",
    "section": "Example 2: Replacing Multiple Values Based on Conditions",
    "text": "Example 2: Replacing Multiple Values Based on Conditions\nSuppose you want to replace all values below 15 in temp with 0. Here’s how to achieve that:\n\nreplace(temp, temp &lt; 15, 0)  # Replace values less than 15 with 0\n\n[1] 15 22 30  0 18\n\n\nIn this case, temp &lt; 15 creates a logical vector where TRUE indicates elements below 15."
  },
  {
    "objectID": "posts/2024-03-21/index.html#example-3-replacing-values-in-data-frames",
    "href": "posts/2024-03-21/index.html#example-3-replacing-values-in-data-frames",
    "title": "Mastering Replacement: Using the replace() Function in R",
    "section": "Example 3: Replacing Values in Data Frames",
    "text": "Example 3: Replacing Values in Data Frames\nreplace() can also work with data frames! Let’s say you have a data frame (weather) with a “wind_speed” column and want to replace missing values with the average speed.\n\nweather &lt;- data.frame(\n  temperature = c(18, 20, NA, 25), \n  wind_speed = c(5, 10, NA, 12)\n  )\navg_wind &lt;- mean(weather$wind_speed, na.rm = TRUE)  # Calculate average excluding NA\nnew_weather &lt;- replace(\n  weather$wind_speed, \n  is.na(weather$wind_speed), \n  avg_wind\n  )\nweather$wind_speed &lt;- new_weather  # Update the data frame\nprint(weather)\n\n  temperature wind_speed\n1          18          5\n2          20         10\n3          NA          9\n4          25         12\n\n\nHere, is.na(weather$wind_speed) creates a logical vector to identify missing values (NA) in the “wind_speed” column."
  },
  {
    "objectID": "posts/2024-03-22/index.html",
    "href": "posts/2024-03-22/index.html",
    "title": "Mastering Data Manipulation in R with the Sweep Function",
    "section": "",
    "text": "Welcome to another exciting journey into the world of data manipulation in R! In this blog post, we’re going to explore a powerful tool in R’s arsenal: the sweep function. Whether you’re a seasoned R programmer or just starting out, understanding how to leverage sweep can significantly enhance your data analysis capabilities. So, let’s dive in and unravel the magic of sweep!"
  },
  {
    "objectID": "posts/2024-03-22/index.html#example-1-scaling-data",
    "href": "posts/2024-03-22/index.html#example-1-scaling-data",
    "title": "Mastering Data Manipulation in R with the Sweep Function",
    "section": "Example 1: Scaling Data",
    "text": "Example 1: Scaling Data\nSuppose we have a matrix data containing numerical values, and we want to scale each column by subtracting its mean and dividing by its standard deviation.\n\n# Create sample data\ndata &lt;- matrix(rnorm(20), nrow = 5)\nprint(data)\n\n           [,1]       [,2]        [,3]       [,4]\n[1,] -0.0345423  0.5671910  0.64555547 -1.4316793\n[2,]  0.2124999  0.7805793 -2.03254741 -0.4705828\n[3,]  1.1442591  0.6055960  0.41827804 -0.7136599\n[4,]  0.4727024  0.9285763 -0.27855411  0.1741202\n[5,]  0.1429103 -0.9512931 -0.01988827 -0.4070733\n\n# Scale each column\nscaled_data &lt;- sweep(data, 2, colMeans(data), FUN = \"-\")\nprint(scaled_data)\n\n           [,1]       [,2]        [,3]        [,4]\n[1,] -0.4221082  0.1810611  0.89898672 -0.86190434\n[2,] -0.1750660  0.3944494 -1.77911615  0.09919224\n[3,]  0.7566932  0.2194661  0.67170929 -0.14388487\n[4,]  0.0851365  0.5424464 -0.02512285  0.74389523\n[5,] -0.2446556 -1.3374230  0.23354299  0.16270174\n\nscaled_data &lt;- sweep(scaled_data, 2, apply(data, 2, sd), FUN = \"/\")\n\n# View scaled data\nprint(scaled_data)\n\n           [,1]       [,2]       [,3]       [,4]\n[1,] -0.9164833  0.2377712  0.8494817 -1.4818231\n[2,] -0.3801042  0.5179946 -1.6811446  0.1705356\n[3,]  1.6429362  0.2882050  0.6347199 -0.2473731\n[4,]  0.1848488  0.7123457 -0.0237394  1.2789367\n[5,] -0.5311974 -1.7563166  0.2206823  0.2797238\n\n\nIn this example, we first subtracted the column means from each column and then divided by the column standard deviations."
  },
  {
    "objectID": "posts/2024-03-22/index.html#example-2-centering-data",
    "href": "posts/2024-03-22/index.html#example-2-centering-data",
    "title": "Mastering Data Manipulation in R with the Sweep Function",
    "section": "Example 2: Centering Data",
    "text": "Example 2: Centering Data\nLet’s say we have a matrix scores representing student exam scores, and we want to center each row by subtracting the row means.\n\n# Create sample data\nscores &lt;- matrix(\n  c(80, 75, 85, 90, 95, 85, 70, 80, 75), \n  nrow = 3, \n  byrow = TRUE\n  )\nprint(scores)\n\n     [,1] [,2] [,3]\n[1,]   80   75   85\n[2,]   90   95   85\n[3,]   70   80   75\n\n# Center each row\ncentered_scores &lt;- sweep(scores, 1, rowMeans(scores), FUN = \"-\")\n\n# View centered data\nprint(centered_scores)\n\n     [,1] [,2] [,3]\n[1,]    0   -5    5\n[2,]    0    5   -5\n[3,]   -5    5    0\n\n\nHere, we subtracted the row means from each row, effectively centering the data around zero."
  },
  {
    "objectID": "posts/2024-03-22/index.html#example-3-custom-operations",
    "href": "posts/2024-03-22/index.html#example-3-custom-operations",
    "title": "Mastering Data Manipulation in R with the Sweep Function",
    "section": "Example 3: Custom Operations",
    "text": "Example 3: Custom Operations\nYou can also apply custom functions using sweep. Let’s say we want to cube each element in a matrix nums.\n\n# Create sample data\nnums &lt;- matrix(1:9, nrow = 3)\nprint(nums)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# Custom operation: cube each element\ncubed_nums &lt;- sweep(nums, 1:2, 3, FUN = \"^\")\n\n# View result\nprint(cubed_nums)\n\n     [,1] [,2] [,3]\n[1,]    1   64  343\n[2,]    8  125  512\n[3,]   27  216  729\n\n\nIn this example, we defined a custom function to cube each element and applied it across all elements of the matrix."
  },
  {
    "objectID": "posts/2024-03-25/index.html",
    "href": "posts/2024-03-25/index.html",
    "title": "Wrangling Data with R: A Guide to the tapply() Function",
    "section": "",
    "text": "Hey R enthusiasts! Today we’re diving into the world of data manipulation with a fantastic function called tapply(). This little gem lets you apply a function of your choice to different subgroups within your data.\nImagine you have a dataset on trees, with a column for tree height and another for species. You might want to know the average height for each species. tapply() comes to the rescue!"
  },
  {
    "objectID": "posts/2024-03-25/index.html#example-1-average-tree-height-by-species",
    "href": "posts/2024-03-25/index.html#example-1-average-tree-height-by-species",
    "title": "Wrangling Data with R: A Guide to the tapply() Function",
    "section": "Example 1: Average Tree Height by Species",
    "text": "Example 1: Average Tree Height by Species\nLet’s say we have a data frame trees with columns “height” (numeric) and “species” (factor):\n\n# Sample data\ntrees &lt;- data.frame(height = c(20, 30, 25, 40, 15, 28),\n                    species = c(\"Oak\", \"Oak\", \"Maple\", \"Pine\", \"Maple\", \"Pine\"))\n\n# Average height per species\naverage_height &lt;- tapply(trees$height, trees$species, mean)\nprint(average_height)\n\nMaple   Oak  Pine \n   20    25    34 \n\n\nThis code calculates the average height for each species in the “species” column and stores the results in average_height. The output will be a named vector showing the average height for each unique species."
  },
  {
    "objectID": "posts/2024-03-25/index.html#example-2-exploring-distribution-with-summary-statistics",
    "href": "posts/2024-03-25/index.html#example-2-exploring-distribution-with-summary-statistics",
    "title": "Wrangling Data with R: A Guide to the tapply() Function",
    "section": "Example 2: Exploring Distribution with Summary Statistics",
    "text": "Example 2: Exploring Distribution with Summary Statistics\nWe can use tapply() with summary() to get a quick overview of how a variable is distributed within groups. Here, we’ll see the distribution of height within each species:\n\nsummary_by_species &lt;- tapply(trees$height, trees$species, summary)\nprint(summary_by_species)\n\n$Maple\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   15.0    17.5    20.0    20.0    22.5    25.0 \n\n$Oak\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   20.0    22.5    25.0    25.0    27.5    30.0 \n\n$Pine\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     28      31      34      34      37      40 \n\n\nThis code applies the summary() function to each subgroup defined by the “species” factor. The output will be a data frame showing various summary statistics (like minimum, maximum, quartiles) for the height of each species."
  },
  {
    "objectID": "posts/2024-03-25/index.html#example-3-custom-function-for-identifying-tall-trees",
    "href": "posts/2024-03-25/index.html#example-3-custom-function-for-identifying-tall-trees",
    "title": "Wrangling Data with R: A Guide to the tapply() Function",
    "section": "Example 3: Custom Function for Identifying Tall Trees",
    "text": "Example 3: Custom Function for Identifying Tall Trees\nLet’s create a custom function to find trees that are taller than the average height of their species:\n\ntall_trees &lt;- function(height, avg_height) {\n    height &gt; avg_height\n}\n\n# Find tall trees within each species\ntall_trees_by_species &lt;- tapply(trees$height, trees$species, mean(trees$height),FUN=tall_trees)\nprint(tall_trees_by_species)\n\n$Maple\n[1] FALSE FALSE\n\n$Oak\n[1] FALSE  TRUE\n\n$Pine\n[1] TRUE TRUE\n\n\nHere, we define a function tall_trees() that takes a tree’s height and the average height (passed as arguments) and returns TRUE if the tree’s height is greater. We then use tapply() with this custom function. The crucial difference here is that we use mean(trees$height) within the FUN argument to calculate the average height for each group outside of the custom function. This ensures the average height is calculated correctly for each subgroup before being compared to individual tree heights. The output will be a logical vector for each species, indicating which trees are taller than the average."
  },
  {
    "objectID": "posts/2023-03-26/index.html",
    "href": "posts/2023-03-26/index.html",
    "title": "Title: Mastering the map() Function in R: A Comprehensive Guide",
    "section": "",
    "text": "In the world of data manipulation and analysis with R, efficiency and simplicity are paramount. One function that epitomizes these qualities is map(). Whether you’re a novice or a seasoned R programmer, mastering map() can significantly streamline your workflow and enhance your code readability. In this guide, we’ll delve into the syntax, usage, and numerous examples to help you harness the full power of map().\nSyntax:\nmap(.x, .f, ...)\n\n.x: A list or atomic vector.\n.f: A function to apply to each element of .x.\n...: Additional arguments to be passed to .f."
  },
  {
    "objectID": "posts/2023-03-26/index.html#example-1-applying-a-function-to-each-element-of-a-vector",
    "href": "posts/2023-03-26/index.html#example-1-applying-a-function-to-each-element-of-a-vector",
    "title": "Title: Mastering the map() Function in R: A Comprehensive Guide",
    "section": "Example 1: Applying a Function to Each Element of a Vector",
    "text": "Example 1: Applying a Function to Each Element of a Vector\n\n# Define a vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Square each element using map()\nlibrary(purrr)\nsquared_numbers &lt;- map(numbers, ~ .x^2)\n\n# Print the result\nprint(squared_numbers)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nIn this example, we utilize map() to apply the square function to each element of the vector numbers. The result is a new vector squared_numbers containing the squared values."
  },
  {
    "objectID": "posts/2023-03-26/index.html#example-2-working-with-lists",
    "href": "posts/2023-03-26/index.html#example-2-working-with-lists",
    "title": "Title: Mastering the map() Function in R: A Comprehensive Guide",
    "section": "Example 2: Working with Lists",
    "text": "Example 2: Working with Lists\n\n# Define a list\nnames &lt;- list(\"John\", \"Alice\", \"Bob\")\n\n# Convert each name to uppercase using map()\nlibrary(purrr)\nuppercase_names &lt;- map(names, toupper)\n\n# Print the result\nprint(uppercase_names)\n\n[[1]]\n[1] \"JOHN\"\n\n[[2]]\n[1] \"ALICE\"\n\n[[3]]\n[1] \"BOB\"\n\n\nHere, map() transforms each element of the list names to uppercase using the toupper() function."
  },
  {
    "objectID": "posts/2023-03-26/index.html#example-3-passing-additional-arguments",
    "href": "posts/2023-03-26/index.html#example-3-passing-additional-arguments",
    "title": "Title: Mastering the map() Function in R: A Comprehensive Guide",
    "section": "Example 3: Passing Additional Arguments",
    "text": "Example 3: Passing Additional Arguments\n\n# Define a list of strings\nwords &lt;- list(\"apple\", \"banana\", \"orange\")\n\n# Extract substrings using map()\nlibrary(purrr)\nsubstring_list &lt;- map(words, substr, start = 1, stop = 3)\n\n# Print the result\nprint(substring_list)\n\n[[1]]\n[1] \"app\"\n\n[[2]]\n[1] \"ban\"\n\n[[3]]\n[1] \"ora\"\n\n\nIn this example, we pass additional arguments start and stop to the substr() function within map(). This extracts the first three characters of each word in the list words.\nExplanation:\nThe map() function iterates over each element of the input data structure (vector or list) and applies the specified function to each element. It then returns the results as a list.\n\nInput Data (.x): This is the data structure (vector or list) over which the function will iterate.\nFunction (.f): The function to be applied to each element of the input data.\nAdditional Arguments (…): Any additional arguments required by the function can be passed here."
  },
  {
    "objectID": "posts/2023-03-26/index.html#example-4-mapping-a-function-to-a-vector",
    "href": "posts/2023-03-26/index.html#example-4-mapping-a-function-to-a-vector",
    "title": "Title: Mastering the map() Function in R: A Comprehensive Guide",
    "section": "Example 4: Mapping a function to a vector",
    "text": "Example 4: Mapping a function to a vector\n\ndata &lt;- 1:3\n\ndata |&gt; map(\\(x) rnorm(5, x))\n\n[[1]]\n[1] -0.2490827 -0.2498850  0.5439327  1.0617387  1.6211063\n\n[[2]]\n[1] 1.2767685 1.2854668 1.1979807 0.4309475 1.2817245\n\n[[3]]\n[1] 3.136021 2.699936 1.956743 4.685243 3.923138\n\n\nIn this example, we use the pipe operator to pass the vector data to the map() function. We then apply the rnorm() function to each element of the vector, generating a list of random numbers."
  },
  {
    "objectID": "posts/2024-03-27/index.html",
    "href": "posts/2024-03-27/index.html",
    "title": "Mastering Text Manipulation in R: A Guide to Using gsub() for Multiple Pattern Replacement",
    "section": "",
    "text": "In the realm of text manipulation in R, the gsub() function stands as a powerful tool, allowing you to replace specific patterns within strings effortlessly. Whether you’re cleaning messy data or transforming text for analysis, mastering gsub() can significantly streamline your workflow. In this tutorial, we’ll focus on how to effectively utilize gsub() to replace multiple patterns, equipping you with the skills to tackle various text manipulation tasks with ease."
  },
  {
    "objectID": "posts/2024-03-27/index.html#replacing-single-patterns",
    "href": "posts/2024-03-27/index.html#replacing-single-patterns",
    "title": "Mastering Text Manipulation in R: A Guide to Using gsub() for Multiple Pattern Replacement",
    "section": "Replacing Single Patterns",
    "text": "Replacing Single Patterns\nFirst, let’s start with a simple example of replacing a single pattern within a string:\n\ntext &lt;- \"Hello, world!\"\nnew_text &lt;- gsub(\"world\", \"R community\", text)\nprint(new_text)\n\n[1] \"Hello, R community!\"\n\n\nIn this example, \"world\" is replaced with \"R community\", resulting in \"Hello, R community!\"."
  },
  {
    "objectID": "posts/2024-03-27/index.html#replacing-multiple-patterns",
    "href": "posts/2024-03-27/index.html#replacing-multiple-patterns",
    "title": "Mastering Text Manipulation in R: A Guide to Using gsub() for Multiple Pattern Replacement",
    "section": "Replacing Multiple Patterns",
    "text": "Replacing Multiple Patterns\nNow, let’s move on to replacing multiple patterns using gsub(). This can be achieved by providing vectors of patterns and replacements:\n\ntext &lt;- \"Data science is amazing, but coding can be challenging.\"\npatterns &lt;- c(\"Data science|coding\")\nreplacements &lt;- c(\"Statistics\")\nnew_text &lt;- gsub(patterns, replacements, text)\nprint(new_text)\n\n[1] \"Statistics is amazing, but Statistics can be challenging.\"\n\n\nHere, \"Data science\" is replaced with \"Statistics\", and \"coding\" is also replaced with \"Statistics\", yielding \"Statistics is amazing, but Statistics can be challenging.\"."
  },
  {
    "objectID": "posts/2024-03-27/index.html#handling-case-sensitivity",
    "href": "posts/2024-03-27/index.html#handling-case-sensitivity",
    "title": "Mastering Text Manipulation in R: A Guide to Using gsub() for Multiple Pattern Replacement",
    "section": "Handling Case Sensitivity",
    "text": "Handling Case Sensitivity\nBy default, gsub() is case sensitive. However, you can make it case insensitive by specifying the ignore.case argument:\n\ntext &lt;- \"R programming is Fun!\"\npattern &lt;- \"R\"\nreplacement &lt;- \"Python\"\nnew_text &lt;- gsub(pattern, replacement, text, ignore.case = FALSE)\nprint(new_text)\n\n[1] \"Python programming is Fun!\"\n\n\nWith ignore.case = TRUE, \"R\" is replaced with \"Python\", resulting in \"Python programming is Fun!\"."
  },
  {
    "objectID": "posts/2024-03-27/index.html#using-regular-expressions",
    "href": "posts/2024-03-27/index.html#using-regular-expressions",
    "title": "Mastering Text Manipulation in R: A Guide to Using gsub() for Multiple Pattern Replacement",
    "section": "Using Regular Expressions",
    "text": "Using Regular Expressions\ngsub() supports regular expressions, providing advanced pattern matching capabilities. Let’s see how to leverage regular expressions for multiple pattern replacement:\n\ntext &lt;- \"Today is 2024-03-27, tomorrow will be 2024-03-28.\"\npattern &lt;- \"\\\\d{4}-\\\\d{2}-\\\\d{2}\"\nreplacement &lt;- \"DATE\"\nnew_text &lt;- gsub(pattern, replacement, text)\nprint(new_text)\n\n[1] \"Today is DATE, tomorrow will be DATE.\"\n\n\nHere, the regular expression \"\\\\d{4}-\\\\d{2}-\\\\d{2}\" matches dates in the format YYYY-MM-DD and replaces them with \"DATE\", resulting in \"Today is DATE, tomorrow will be DATE.\"."
  },
  {
    "objectID": "posts/2024-03-26/index.html",
    "href": "posts/2024-03-26/index.html",
    "title": "Mastering the map() Function in R: A Comprehensive Guide",
    "section": "",
    "text": "In the world of data manipulation and analysis with R, efficiency and simplicity are paramount. One function that epitomizes these qualities is map(). Whether you’re a novice or a seasoned R programmer, mastering map() can significantly streamline your workflow and enhance your code readability. In this guide, we’ll delve into the syntax, usage, and numerous examples to help you harness the full power of map().\nSyntax:\nmap(.x, .f, ...)\n\n.x: A list or atomic vector.\n.f: A function to apply to each element of .x.\n...: Additional arguments to be passed to .f."
  },
  {
    "objectID": "posts/2024-03-26/index.html#example-1-applying-a-function-to-each-element-of-a-vector",
    "href": "posts/2024-03-26/index.html#example-1-applying-a-function-to-each-element-of-a-vector",
    "title": "Mastering the map() Function in R: A Comprehensive Guide",
    "section": "Example 1: Applying a Function to Each Element of a Vector",
    "text": "Example 1: Applying a Function to Each Element of a Vector\n\n# Define a vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Square each element using map()\nlibrary(purrr)\nsquared_numbers &lt;- map(numbers, ~ .x^2)\n\n# Print the result\nprint(squared_numbers)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nIn this example, we utilize map() to apply the square function to each element of the vector numbers. The result is a new vector squared_numbers containing the squared values."
  },
  {
    "objectID": "posts/2024-03-26/index.html#example-2-working-with-lists",
    "href": "posts/2024-03-26/index.html#example-2-working-with-lists",
    "title": "Mastering the map() Function in R: A Comprehensive Guide",
    "section": "Example 2: Working with Lists",
    "text": "Example 2: Working with Lists\n\n# Define a list\nnames &lt;- list(\"John\", \"Alice\", \"Bob\")\n\n# Convert each name to uppercase using map()\nlibrary(purrr)\nuppercase_names &lt;- map(names, toupper)\n\n# Print the result\nprint(uppercase_names)\n\n[[1]]\n[1] \"JOHN\"\n\n[[2]]\n[1] \"ALICE\"\n\n[[3]]\n[1] \"BOB\"\n\n\nHere, map() transforms each element of the list names to uppercase using the toupper() function."
  },
  {
    "objectID": "posts/2024-03-26/index.html#example-3-passing-additional-arguments",
    "href": "posts/2024-03-26/index.html#example-3-passing-additional-arguments",
    "title": "Mastering the map() Function in R: A Comprehensive Guide",
    "section": "Example 3: Passing Additional Arguments",
    "text": "Example 3: Passing Additional Arguments\n\n# Define a list of strings\nwords &lt;- list(\"apple\", \"banana\", \"orange\")\n\n# Extract substrings using map()\nlibrary(purrr)\nsubstring_list &lt;- map(words, substr, start = 1, stop = 3)\n\n# Print the result\nprint(substring_list)\n\n[[1]]\n[1] \"app\"\n\n[[2]]\n[1] \"ban\"\n\n[[3]]\n[1] \"ora\"\n\n\nIn this example, we pass additional arguments start and stop to the substr() function within map(). This extracts the first three characters of each word in the list words.\nExplanation:\nThe map() function iterates over each element of the input data structure (vector or list) and applies the specified function to each element. It then returns the results as a list.\n\nInput Data (.x): This is the data structure (vector or list) over which the function will iterate.\nFunction (.f): The function to be applied to each element of the input data.\nAdditional Arguments (…): Any additional arguments required by the function can be passed here."
  },
  {
    "objectID": "posts/2024-03-26/index.html#example-4-mapping-a-function-to-a-vector",
    "href": "posts/2024-03-26/index.html#example-4-mapping-a-function-to-a-vector",
    "title": "Mastering the map() Function in R: A Comprehensive Guide",
    "section": "Example 4: Mapping a function to a vector",
    "text": "Example 4: Mapping a function to a vector\n\ndata &lt;- 1:3\n\ndata |&gt; map(\\(x) rnorm(5, x))\n\n[[1]]\n[1] -0.5899048  0.6927321  0.9609231  1.5313738  2.8812876\n\n[[2]]\n[1] 2.786631 1.378856 2.649387 1.362483 0.939132\n\n[[3]]\n[1] 1.383364 3.400441 3.722030 2.109162 3.393745\n\n\nIn this example, we use the pipe operator to pass the vector data to the map() function. We then apply the rnorm() function to each element of the vector, generating a list of random numbers."
  },
  {
    "objectID": "posts/2024-03-28/index.html",
    "href": "posts/2024-03-28/index.html",
    "title": "Mastering Quantile Normalization in R: A Step-by-Step Guide",
    "section": "",
    "text": "Quantile normalization is a crucial technique in data preprocessing, especially in fields like genomics and bioinformatics. It ensures that the distributions of different samples are aligned, making them directly comparable. In this tutorial, we’ll walk through the process step by step, demystifying the syntax and empowering you to apply this technique confidently in your projects."
  },
  {
    "objectID": "posts/2024-03-28/index.html#step-1-load-your-data",
    "href": "posts/2024-03-28/index.html#step-1-load-your-data",
    "title": "Mastering Quantile Normalization in R: A Step-by-Step Guide",
    "section": "Step 1: Load Your Data",
    "text": "Step 1: Load Your Data\nFirst things first, you’ll need some data to work with. For the sake of this tutorial, let’s say you have a dataframe called df containing your datasets.\n\nset.seed(42)  # For reproducibility\ndf &lt;- data.frame(\n  sample1 = rnorm(100, mean = 5, sd = 2),\n  sample2 = rnorm(100, mean = 10, sd = 1),\n  sample3 = rnorm(100)\n)\n\nhead(df)\n\n   sample1   sample2    sample3\n1 7.741917 11.200965 -2.0009292\n2 3.870604 11.044751  0.3337772\n3 5.726257  8.996791  1.1713251\n4 6.265725 11.848482  2.0595392\n5 5.808537  9.333227 -1.3768616\n6 4.787751 10.105514 -1.1508556\n\nhist(df$sample1, col = 'red', xlim=c(min(df), max(df)), \n     main = 'Distribution of Sample 1')\nhist(df$sample2, col = 'blue', add = TRUE)\nhist(df$sample3, col = 'green', add = TRUE)\n#add legend\nlegend('topright', \n       c('Sample 1', 'Sample 2','Sample 3'), \n       fill=c('red','blue', 'green'))"
  },
  {
    "objectID": "posts/2024-03-28/index.html#step-2-perform-quantile-normalization",
    "href": "posts/2024-03-28/index.html#step-2-perform-quantile-normalization",
    "title": "Mastering Quantile Normalization in R: A Step-by-Step Guide",
    "section": "Step 2: Perform Quantile Normalization",
    "text": "Step 2: Perform Quantile Normalization\nNow, it’s time to perform quantile normalization using R’s built-in quantile() function. This function calculates quantiles for a given dataset, which is essential for aligning the distributions. Function from: https://lifewithdata.com/2023/09/02/how-to-perform-quantile-normalization-in-r/\n\n# Perform quantile normalization\nqn &lt;- function(.data){\n data_sort &lt;- apply(.data, 2, sort)\n row_means &lt;- rowMeans(data_sort)\n data_sort &lt;- matrix(row_means, \n                     nrow = nrow(data_sort), \n                     ncol = ncol(data_sort), \n                     byrow = TRUE\n                     )\n index_rank &lt;- apply(.data, 2, order)\n normalized_data &lt;- matrix(nrow = nrow(.data), ncol = ncol(.data))\n for(i in 1:ncol(.data)){\n   normalized_data[,i] &lt;- data_sort[index_rank[,i], i]\n }\n return(normalized_data)\n}\n\nnormalized_data &lt;- qn(df)\n\nLet’s break down this code snippet:\nAbsolutely, let’s break down this R code block piece by piece:\n1. Function Definition:\nqn &lt;- function(.data){\n  # ... function body here ...\n}\nThis defines a function named qn that takes a data frame (data) as input. This data frame is most likely your dataset you want to normalize.\n2. Sorting Each Column:\ndata_sort &lt;- apply(.data, 2, sort)\nThis line sorts each column of the data frame data independently. Imagine sorting rows of data like sorting words in a dictionary. Here, we are sorting each column (each variable) from smallest to largest values. The result is stored in data_sort.\n3. Calculating Row Means:\nrow_means &lt;- rowMeans(data_sort)\nThis line calculates the average value for each row in the sorted data frame (data_sort). So, for each row (each data point), it finds the mean of the sorted values across all variables. The result is stored in row_means.\n4. Replicating Row Means into a Matrix:\ndata_sort &lt;- matrix(row_means, \n                    nrow = nrow(data_sort), \n                    ncol = ncol(data_sort), \n                    byrow = TRUE\n                    )\nThis part is a bit trickier. It creates a new matrix (data_sort) with the same dimensions (number of rows and columns) as the original sorted data. Then, it fills each row of this new matrix with the corresponding row mean calculated earlier (row_means). The byrow argument ensures this replication happens row-wise.\n5. Ranking Each Value’s Position:\nindex_rank &lt;- apply(.data, 2, order)\nSimilar to sorting, this line assigns a rank (position) to each value within its column (variable) in the original data frame (data). Imagine a race where the first place gets rank 1, second place gets rank 2, and so on. Here, the rank indicates the original position of each value after everything was sorted in step 2. The result is stored in index_rank.\n6. Building the Normalized Data Frame:\nnormalized_data &lt;- matrix(nrow = nrow(.data), ncol = ncol(.data))\nThis line creates an empty matrix (normalized_data) with the same dimensions as the original data frame. This will eventually hold the normalized data.\n7. Looping Through Columns and Assigning Ranked Values:\nfor(i in 1:ncol(.data)){\n  normalized_data[,i] &lt;- data_sort[index_rank[,i], i]\n}\nThis is the core of the normalization process. It loops through each column (variable) of the original data frame (data). For each column, it uses the ranks (index_rank) as indices to pick values from the sorted data with row means (data_sort). Basically, it replaces each value in the original data with the value from the sorted data that has the same rank (original position). This effectively replaces the original values with their corresponding row means (representing the center point) based on their original order.\n8. Returning the Normalized Data:\nreturn(normalized_data)\nFinally, the function returns the normalized_data matrix, which contains the quantile normalized version of your original data frame.\nIn essence, this code performs a type of rank-based normalization where each value is replaced with the row mean that corresponds to its original position after sorting all the data together. This approach ensures that the distribution of values across columns becomes more consistent."
  },
  {
    "objectID": "posts/2024-03-28/index.html#step-3-explore-the-results",
    "href": "posts/2024-03-28/index.html#step-3-explore-the-results",
    "title": "Mastering Quantile Normalization in R: A Step-by-Step Guide",
    "section": "Step 3: Explore the Results",
    "text": "Step 3: Explore the Results\nAfter quantile normalization, you’ll have a list of normalized datasets ready for further analysis. Take a moment to explore the results and ensure that the distributions are aligned as expected.\n\nsummary(df)\n\n    sample1           sample2          sample3        \n Min.   :-0.9862   Min.   : 7.975   Min.   :-2.69993  \n 1st Qu.: 3.7666   1st Qu.: 9.409   1st Qu.:-0.71167  \n Median : 5.1796   Median : 9.931   Median :-0.02474  \n Mean   : 5.0650   Mean   : 9.913   Mean   :-0.01037  \n 3rd Qu.: 6.3231   3rd Qu.:10.462   3rd Qu.: 0.65254  \n Max.   : 9.5733   Max.   :12.702   Max.   : 2.45959  \n\n# Explore the results\nsummary(normalized_data)\n\n       V1              V2              V3       \n Min.   :1.430   Min.   :1.430   Min.   :1.430  \n 1st Qu.:4.154   1st Qu.:4.154   1st Qu.:4.154  \n Median :5.029   Median :5.029   Median :5.029  \n Mean   :4.989   Mean   :4.989   Mean   :4.989  \n 3rd Qu.:5.812   3rd Qu.:5.812   3rd Qu.:5.812  \n Max.   :8.245   Max.   :8.245   Max.   :8.245"
  },
  {
    "objectID": "posts/2024-03-28/index.html#step-4-obtain-quantiles",
    "href": "posts/2024-03-28/index.html#step-4-obtain-quantiles",
    "title": "Mastering Quantile Normalization in R: A Step-by-Step Guide",
    "section": "Step 4: Obtain Quantiles",
    "text": "Step 4: Obtain Quantiles\nNow that the data is normalized, we can extract the quantiles to compare the distributions across datasets. This will help you confirm that the normalization process was successful.\n\nas.data.frame(normalized_data) |&gt; \n  sapply(function(x) quantile(x, probs = seq(0,1,1/4)))\n\n           V1       V2       V3\n0%   1.429737 1.429737 1.429737\n25%  4.154481 4.154481 4.154481\n50%  5.028521 5.028521 5.028521\n75%  5.812480 5.812480 5.812480\n100% 8.244925 8.244925 8.244925\n\n\nAs we can see, the quantiles of the normalized data are consistent across the different datasets. This indicates that the distributions have been aligned through quantile normalization.\nLet’s visuzlize for another confirmation\n\ndf_normalized &lt;- as.data.frame(normalized_data)\n\nhist(df_normalized$V1, col = 'red')\nhist(df_normalized$V2, col = 'blue', add = TRUE)\nhist(df_normalized$V3, col = 'green', add = TRUE)\n\nlegend('topright', c('Sample 1', 'Sample 2','Sample 3'), fill=c('red','blue', 'green'))"
  },
  {
    "objectID": "posts/2024-04-02/index.html",
    "href": "posts/2024-04-02/index.html",
    "title": "A Practical Guide to Data Normalization in R",
    "section": "",
    "text": "Data normalization is a crucial preprocessing step in data analysis and machine learning workflows. It helps in standardizing the scale of numeric features, ensuring fair treatment to all variables regardless of their magnitude. In this tutorial, we’ll explore how to normalize data in R using practical examples and step-by-step explanations."
  },
  {
    "objectID": "posts/2024-04-02/index.html#step-1-prepare-your-data",
    "href": "posts/2024-04-02/index.html#step-1-prepare-your-data",
    "title": "A Practical Guide to Data Normalization in R",
    "section": "Step 1: Prepare Your Data",
    "text": "Step 1: Prepare Your Data\nFor demonstration purposes, let’s create a sample dataset. Suppose we have a dataset called my_data with three numeric variables: age, income, and education.\n\nset.seed(42) # reproducible\n# Create a sample dataset\nmy_data &lt;- data.frame(\n  age = trunc(runif(250, 25, 65)),\n  income = round(rlnorm(250, log(71000))),\n  education = trunc(runif(250, 12, 20))\n)"
  },
  {
    "objectID": "posts/2024-04-02/index.html#step-2-normalize-the-data",
    "href": "posts/2024-04-02/index.html#step-2-normalize-the-data",
    "title": "A Practical Guide to Data Normalization in R",
    "section": "Step 2: Normalize the Data",
    "text": "Step 2: Normalize the Data\nNow, let’s normalize the numeric variables in our dataset. We’ll use the scale() function to standardize each variable to have a mean of 0 and a standard deviation of 1.\n\n# Normalize the data\nnormalized_data &lt;- data.frame(\n  age_normalized = scale(my_data$age),\n  income_normalized = scale(my_data$income),\n  education_normalized = scale(my_data$education)\n)"
  },
  {
    "objectID": "posts/2024-04-02/index.html#step-3-understand-the-normalized-data",
    "href": "posts/2024-04-02/index.html#step-3-understand-the-normalized-data",
    "title": "A Practical Guide to Data Normalization in R",
    "section": "Step 3: Understand the Normalized Data",
    "text": "Step 3: Understand the Normalized Data\nAfter normalization, each variable will have a mean of approximately 0 and a standard deviation of 1. This ensures that all variables are on the same scale, making them comparable and suitable for various analytical techniques.\n\n# View the normalized data\nhead(normalized_data)\n\n  age_normalized income_normalized education_normalized\n1     1.38435717        -0.5141139           -0.9663645\n2     1.47019281        -0.5829717           -1.3865230\n3    -0.76153378        -0.8385455           -0.1260475\n4     1.12685026        -0.7375278           -0.9663645\n5     0.44016515        -0.1738354           -0.9663645\n6     0.01098696         0.1804609           -0.5462060"
  },
  {
    "objectID": "posts/2024-04-02/index.html#step-4-interpret-the-results",
    "href": "posts/2024-04-02/index.html#step-4-interpret-the-results",
    "title": "A Practical Guide to Data Normalization in R",
    "section": "Step 4: Interpret the Results",
    "text": "Step 4: Interpret the Results\nIn the output, you’ll notice that each variable now has its normalized counterpart. For example:\n\nage_normalized represents the standardized values of the age variable.\nincome_normalized represents the standardized values of the income variable.\neducation_normalized represents the standardized values of the education variable."
  },
  {
    "objectID": "posts/2024-04-02/index.html#step-5-visualize-the-normalized-data-optional",
    "href": "posts/2024-04-02/index.html#step-5-visualize-the-normalized-data-optional",
    "title": "A Practical Guide to Data Normalization in R",
    "section": "Step 5: Visualize the Normalized Data (Optional)",
    "text": "Step 5: Visualize the Normalized Data (Optional)\nTo gain a better understanding of the normalization process, you can visualize the distribution of the original and normalized variables using histograms or density plots.\n\n# Visualize the original and normalized data (Optional)\npar(mfrow = c(2, 3)) # Arrange plots in a 2x3 grid\nhist(my_data$age, main = \"Age\", xlab = \"Age\")\nhist(normalized_data$age_normalized, main = \"Normalized Age\", xlab = \"Age (Normalized)\")\n\nhist(my_data$income, main = \"Income\", xlab = \"Income\")\nhist(normalized_data$income_normalized, main = \"Normalized Income\", xlab = \"Income (Normalized)\")\n\nhist(my_data$education, main = \"Education\", xlab = \"Education\")\nhist(normalized_data$education_normalized, main = \"Normalized Education\", xlab = \"Education (Normalized)\")\n\n\n\n\n\n\n\n\nConclusion\nCongratulations! You’ve successfully normalized your data in R. By standardizing the scale of numeric variables, you’ve prepared your data for further analysis, ensuring fair treatment to all variables. Feel free to explore more advanced techniques or apply normalization to your own datasets.\nI encourage you to try this process on your own datasets and experiment with different normalization techniques. Happy analyzing!"
  },
  {
    "objectID": "posts/2024-04-03/index.html",
    "href": "posts/2024-04-03/index.html",
    "title": "Scaling Your Data in R: Understanding the Range",
    "section": "",
    "text": "Introduction\nToday, we’re diving into a fundamental data pre-processing technique: scaling values. This might sound simple, but it can significantly impact how your data behaves in analyses.\n\n\nWhy Scale?\nImagine you have data on customer ages (in years) and purchase amounts (in dollars). The age range might be 18-80, while purchase amounts could vary from $10 to $1000. If you use these values directly in a model, the analysis might be biased towards the purchase amount due to its larger scale. Scaling brings both features (age and purchase amount) to a common ground, ensuring neither overpowers the other.\n\n\nThe scale() Function\nR offers a handy function called scale() to achieve this. Here’s the basic syntax:\nscaled_data &lt;- scale(x, center = TRUE, scale = TRUE)\n\ndata: This is the vector or data frame containing the values you want to scale. A numeric matrix(like object)\ncenter: Either a logical value or numeric-alike vector of length equal to the number of columns of x, where ‘numeric-alike’ means that as.numeric(.) will be applied successfully if is.numeric(.) is not true.\nscale: Either a logical value or numeric-alike vector of length equal to the number of columns of x.\nscaled_data: This stores the new data frame with scaled values (typically one standard deviation from the mean).\n\n\n\nExample in Action!\nLet’s see scale() in action. We’ll generate some sample data for height (in cm) and weight (in kg) of individuals:\n\nset.seed(123)  # For reproducibility\nheight &lt;- rnorm(100, mean = 170, sd = 10)\nweight &lt;- rnorm(100, mean = 70, sd = 15)\ndata &lt;- data.frame(height, weight)\n\nThis creates a data frame (data) with 100 rows, where height has values around 170 cm with a standard deviation of 10 cm, and weight is centered around 70 kg with a standard deviation of 15 kg.\n\n\nVisualizing Before and After\nNow, let’s visualize the distribution of both features before and after scaling. We’ll use the ggplot2 package for this:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Make Scaled data and cbind to original\nscaled_data &lt;- scale(data)\nsetNames(cbind(data, scaled_data), c(\"height\", \"weight\", \"height_scaled\", \"weight_scaled\")) -&gt; data\n\n# Tidy data for facet plotting\ndata_long &lt;- pivot_longer(\n  data, \n  cols = c(height, weight, height_scaled, weight_scaled), \n  names_to = \"variable\", \n  values_to = \"value\"\n  )\n\n# Visualize\ndata_long |&gt;\n  ggplot(aes(x = value, fill = variable)) +\n  geom_histogram(\n    bins = 30, \n    alpha = 0.328) +\n  facet_wrap(~variable, scales = \"free\") +\n  labs(\n    title = \"Distribution of Height and Weight Before and After Scaling\"\n    ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nRun this code and see the magic! The histograms before scaling will show a clear difference in spread between height and weight. After scaling, both distributions will have a similar shape, centered around 0 with a standard deviation of 1.\n\n\nTry it Yourself!\nThis is just a basic example. Get your hands dirty! Try scaling data from your own projects and see how it affects your analysis. Remember, scaling is just one step in data pre-processing. Explore other techniques like centering or normalization depending on your specific needs.\nSo, the next time you have features with different scales, consider using scale() to bring them to a level playing field and unlock the full potential of your models!"
  },
  {
    "objectID": "posts/2024-04-04/index.html",
    "href": "posts/2024-04-04/index.html",
    "title": "Unveiling Car Specs with Multidimensional Scaling in R",
    "section": "",
    "text": "Introduction\nVisualizing similarities between data points can be tricky, especially when dealing with many features. This is where multidimensional scaling (MDS) comes in handy. It allows us to explore these relationships in a lower-dimensional space, typically 2D or 3D for easier interpretation. In R, the cmdscale() function from base R and is a great tool for performing classical MDS.\n\n\ncmdscale()\nHere’s a breakdown of its arguments:\n\ndistance_matrix: This is the key argument. It represents a matrix containing the pairwise distances between your data points. You can calculate this using the dist() function.\neig: A logical value indicating whether you want the function to return the eigenvalues (default is FALSE). Eigenvalues help assess the quality of the dimensionality reduction.\nk: This specifies the number of dimensions for the resulting low-dimensional space (default is 2). You can choose higher values for more complex data, but visualization becomes trickier.\n...: Additional arguments can be used for fine-tuning the MDS process, but these are less common for basic applications.\n\n\n\nCar Specs with MDS: A Step-by-Step Example\nLet’s use the built-in mtcars dataset in R to demonstrate the power of MDS. This dataset contains information about various car models, including aspects like horsepower, mileage, and weight. While these features provide valuable insights, visualizing all of them simultaneously can be challenging. MDS will help us explore the relationships between these car specifications in a 2D space.\nHere’s the code with explanations:\n\n# Select relevant numerical features (exclude car names)\ncar_features &lt;- mtcars[, c(3:11)]\n\n# Calculate pairwise distances between car features\ndistance_matrix &lt;- dist(car_features)\nhead(distance_matrix, 3)\n\n[1]  0.6153251 54.8426385 98.1117059\n\n# Perform MDS to get a 2D representation\nmds_results &lt;- cmdscale(distance_matrix, k = 2)\nhead(mds_results, 3)\n\n                    [,1]      [,2]\nMazda RX4      -79.62307  2.157120\nMazda RX4 Wag  -79.62522  2.172370\nDatsun 710    -133.87165 -5.033323\n\n# Create a base R plot\nplot(mds_results[, 1], mds_results[, 2], \n     xlab = \"Dimension 1\", ylab = \"Dimension 2\",\n     main = \"MDS of Car Specs (mtcars)\")\n\n# Add text labels for car names (optional)\ntext(mds_results, labels = rownames(mtcars), col = \"blue\", cex = 0.62,\n     pos = 1)\n\n\n\n\n\n\n\n\n\nWe load the mtcars dataset using data(mtcars).\nWe select relevant numerical features from the dataset (excluding car names) and store them in car_features.\nThe dist() function calculates the pairwise distances between data points based on the chosen features and stores them in the distance_matrix.\nWe run cmdscale() on the distance matrix, specifying two dimensions (k = 2) for the output. The results are stored in mds_results.\nFinally, we use the base R plot() function to create a scatter plot. We set axis labels and a main title for the plot.\n\nOptional Step:\n\nWe can add text labels for each car model (using car names from mtcars$mpg) on the plot using the text() function. We set the pos argument to 1 to position the text labels above the data points and we set the cex argument to 0.62 so the size of the text decreases.\n\nThis plot can reveal interesting patterns. Cars closer together might share similar characteristics in terms of horsepower, weight, and other specifications. You might also observe some separation based on fuel efficiency reflected by the optional text labels.\n\n\nExperiment and Discover!\nMDS is a powerful tool for exploring data similarity in R. Now that you’ve seen the basics of cmdscale() and base R plotting functions, why not try it on your dataset? Remember to calculate the distance matrix appropriately based on the features you’re interested in. Play around with the number of dimensions (k) to see how it affects the visualization. By experimenting with MDS, you might uncover hidden relationships within your car data or any other dataset you choose to explore!"
  },
  {
    "objectID": "posts/2024-04-05/index.html",
    "href": "posts/2024-04-05/index.html",
    "title": "A Practical Guide to Merging Data Frames Based on Multiple Columns in R",
    "section": "",
    "text": "As a data scientist or analyst, you often encounter situations where you need to combine data from multiple sources. One common task is merging data frames based on multiple columns. In this guide, we’ll walk through several step-by-step examples of how to accomplish this efficiently using R."
  },
  {
    "objectID": "posts/2024-04-05/index.html#example-data",
    "href": "posts/2024-04-05/index.html#example-data",
    "title": "A Practical Guide to Merging Data Frames Based on Multiple Columns in R",
    "section": "Example Data",
    "text": "Example Data\nFor demonstration purposes, let’s create two sample data frames:\n\n# Sample Data Frame 1\ndf1 &lt;- data.frame(ID = c(1, 2, 3),\n                  Year = c(2019, 2020, 2021),\n                  Value1 = c(10, 20, 30))\n\n# Sample Data Frame 2\ndf2 &lt;- data.frame(ID = c(1, 2, 3),\n                  Year = c(2019, 2020, 2022),\n                  Value2 = c(100, 200, 300))"
  },
  {
    "objectID": "posts/2024-04-05/index.html#example-1-inner-join",
    "href": "posts/2024-04-05/index.html#example-1-inner-join",
    "title": "A Practical Guide to Merging Data Frames Based on Multiple Columns in R",
    "section": "Example 1: Inner Join",
    "text": "Example 1: Inner Join\nAn inner join combines rows from both data frames where there is a match based on the specified columns (ID and Year in this case). Rows with unmatched values are excluded.\n\n# Merge based on ID and Year using inner join\nmerged_inner &lt;- merge(df1, df2, by = c(\"ID\", \"Year\"))"
  },
  {
    "objectID": "posts/2024-04-05/index.html#example-2-left-join",
    "href": "posts/2024-04-05/index.html#example-2-left-join",
    "title": "A Practical Guide to Merging Data Frames Based on Multiple Columns in R",
    "section": "Example 2: Left Join",
    "text": "Example 2: Left Join\nA left join retains all rows from the left data frame (df1), and includes matching rows from the right data frame (df2). If there is no match, NA values are filled in for the columns from df2.\n\n# Merge based on ID and Year using left join\nmerged_left &lt;- merge(df1, df2, by = c(\"ID\", \"Year\"), all.x = TRUE)"
  },
  {
    "objectID": "posts/2024-04-05/index.html#example-3-right-join",
    "href": "posts/2024-04-05/index.html#example-3-right-join",
    "title": "A Practical Guide to Merging Data Frames Based on Multiple Columns in R",
    "section": "Example 3: Right Join",
    "text": "Example 3: Right Join\nA right join retains all rows from the right data frame (df2), and includes matching rows from the left data frame (df1). If there is no match, NA values are filled in for the columns from df1.\n\n# Merge based on ID and Year using right join\nmerged_right &lt;- merge(df1, df2, by = c(\"ID\", \"Year\"), all.y = TRUE)"
  },
  {
    "objectID": "posts/2024-04-05/index.html#example-4-full-join",
    "href": "posts/2024-04-05/index.html#example-4-full-join",
    "title": "A Practical Guide to Merging Data Frames Based on Multiple Columns in R",
    "section": "Example 4: Full Join",
    "text": "Example 4: Full Join\nA full join retains all rows from both data frames, filling in NA values for columns where there is no match.\n\n# Merge based on ID and Year using full join\nmerged_full &lt;- merge(df1, df2, by = c(\"ID\", \"Year\"), all = TRUE)"
  },
  {
    "objectID": "posts/2024-04-08/index.html",
    "href": "posts/2024-04-08/index.html",
    "title": "Data Frame Merging in R (With Examples)",
    "section": "",
    "text": "Merging multiple data frames is a pivotal skill in data manipulation. Whether you’re handling small-scale datasets or large-scale ones, mastering the art of merging can significantly enhance your efficiency. In this tutorial, we’ll delve into various methods of merging data frames in R, using straightforward examples to demystify the process."
  },
  {
    "objectID": "posts/2024-04-08/index.html#method-1-using-cbind-and-rbind",
    "href": "posts/2024-04-08/index.html#method-1-using-cbind-and-rbind",
    "title": "Data Frame Merging in R (With Examples)",
    "section": "Method 1: Using cbind() and rbind()",
    "text": "Method 1: Using cbind() and rbind()\nOne approach to merge data frames is by combining them column-wise using cbind() or row-wise using rbind().\n\n# Creating data frames from the list\ndf1 &lt;- data.frame(ID = 1:50, Value = random_list$sample1)\ndf2 &lt;- data.frame(ID = 1:50, Value = random_list$sample2)\ndf3 &lt;- data.frame(ID = 1:50, Value = random_list$sample3)\n\n# Merging data frames column-wise\ncbined_df &lt;- cbind(df1, df2$Value, df3$Value)\nhead(cbined_df)\n\n  ID      Value  df2$Value  df3$Value\n1  1 -0.8828435 -1.5116620  1.4729716\n2  2  0.7371127  0.1140000  0.6455959\n3  3  0.7611256  0.9740632 -0.2355084\n4  4  2.0613462 -1.0748615 -0.4654242\n5  5  0.1966095 -0.2415080  0.1059656\n6  6  0.3217213 -1.3252347  0.9432906\n\n# Merging data frames row-wise\nrbined_df &lt;- rbind(df1, df2, df3)\nhead(rbined_df)\n\n  ID      Value\n1  1 -0.8828435\n2  2  0.7371127\n3  3  0.7611256\n4  4  2.0613462\n5  5  0.1966095\n6  6  0.3217213\n\n\nIn the first example, cbind() combines df1, df2, and df3 column-wise, creating a new data frame combined_df. In the second example, rbind() stacks df1, df2, and df3 row-wise, appending the rows to create combined_df."
  },
  {
    "objectID": "posts/2024-04-08/index.html#method-2-using-purrrmap-and-data.frame",
    "href": "posts/2024-04-08/index.html#method-2-using-purrrmap-and-data.frame",
    "title": "Data Frame Merging in R (With Examples)",
    "section": "Method 2: Using purrr::map() and data.frame()",
    "text": "Method 2: Using purrr::map() and data.frame()\nWith the purrr package, you can efficiently merge data frames within a list using map() and data.frame().\n\nlibrary(purrr)\n\n# Merging data frames within the list\nmerged_list &lt;- map(random_list, data.frame)\n\n# Combining data frames row-wise\ncombined_df &lt;- do.call(rbind, merged_list)\nhead(combined_df)\n\n             .x..i..\nsample1.1 -0.8828435\nsample1.2  0.7371127\nsample1.3  0.7611256\nsample1.4  2.0613462\nsample1.5  0.1966095\nsample1.6  0.3217213\n\n\nHere, map() iterates over each element of random_list and converts them into data frames using data.frame(). Then, do.call(rbind, merged_list) combines the data frames row-wise, creating combined_df."
  },
  {
    "objectID": "posts/2024-04-08/index.html#method-3-using-purrrmap_df",
    "href": "posts/2024-04-08/index.html#method-3-using-purrrmap_df",
    "title": "Data Frame Merging in R (With Examples)",
    "section": "Method 3: Using purrr::map_df()",
    "text": "Method 3: Using purrr::map_df()\nAnother purrr function, map_df(), directly merges data frames within a list, producing a single combined data frame.\n\n# Merging data frames within the list\ncombined_df &lt;- map_df(random_list, cbind)\nhead(combined_df)\n\n# A tibble: 6 × 3\n  sample1[,1] sample2[,1] sample3[,1]\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1      -0.883      -1.51        1.47 \n2       0.737       0.114       0.646\n3       0.761       0.974      -0.236\n4       2.06       -1.07       -0.465\n5       0.197      -0.242       0.106\n6       0.322      -1.33        0.943\n\n\nBy employing map_df() with cbind, we merge data frames within random_list, resulting in combined_df, which is a single merged data frame."
  },
  {
    "objectID": "posts/2024-04-09/index.html",
    "href": "posts/2024-04-09/index.html",
    "title": "How to Remove Rows with Some or All NAs in R",
    "section": "",
    "text": "Handling missing values is a crucial aspect of data preprocessing in R. Often, datasets contain missing values, which can adversely affect the analysis or modeling process. One common task is to remove rows containing missing values entirely. In this tutorial, we’ll explore different methods to accomplish this task in R, catering to scenarios where we want to remove rows with either some or all missing values."
  },
  {
    "objectID": "posts/2024-04-09/index.html#example-1---using-complete.cases-function",
    "href": "posts/2024-04-09/index.html#example-1---using-complete.cases-function",
    "title": "How to Remove Rows with Some or All NAs in R",
    "section": "Example 1 - Using complete.cases() Function:",
    "text": "Example 1 - Using complete.cases() Function:\nThe complete.cases() function is a handy tool in R for removing rows with any missing values. It returns a logical vector indicating which rows in a data frame are complete (i.e., have no missing values).\n\n# Example data frame\ndf &lt;- data.frame(\n  x = c(1, 2, NA, 4),\n  y = c(NA, 2, 3, NA)\n)\ndf\n\n   x  y\n1  1 NA\n2  2  2\n3 NA  3\n4  4 NA\n\n# Remove rows with any missing values\ncomplete_rows &lt;- df[complete.cases(df), ]\ncomplete_rows\n\n  x y\n2 2 2"
  },
  {
    "objectID": "posts/2024-04-09/index.html#explanation",
    "href": "posts/2024-04-09/index.html#explanation",
    "title": "How to Remove Rows with Some or All NAs in R",
    "section": "Explanation:",
    "text": "Explanation:\n\nWe create a sample data frame df with some missing values.\nThe complete.cases(df) function returns a logical vector indicating complete cases (rows with no missing values).\nWe subset the data frame df using this logical vector to retain only the complete rows."
  },
  {
    "objectID": "posts/2024-04-09/index.html#example-2---using-na.omit-function",
    "href": "posts/2024-04-09/index.html#example-2---using-na.omit-function",
    "title": "How to Remove Rows with Some or All NAs in R",
    "section": "Example 2 - Using na.omit() Function:",
    "text": "Example 2 - Using na.omit() Function:\nSimilar to complete.cases(), the na.omit() function also removes rows with any missing values from a data frame. However, it directly returns the data frame without the incomplete rows.\n\n# Example data frame\ndf &lt;- data.frame(\n  x = c(1, 2, NA, 4),\n  y = c(NA, 2, 3, NA)\n)\ndf\n\n   x  y\n1  1 NA\n2  2  2\n3 NA  3\n4  4 NA\n\n# Remove rows with any missing values\ncomplete_df &lt;- na.omit(df)\ncomplete_df\n\n  x y\n2 2 2\n\n\n##Explanation:\n\nWe define a sample data frame df with missing values.\nThe na.omit(df) function directly removes rows with any missing values and returns the cleaned data frame."
  },
  {
    "objectID": "posts/2024-04-09/index.html#example-3---removing-rows-with-all-nas",
    "href": "posts/2024-04-09/index.html#example-3---removing-rows-with-all-nas",
    "title": "How to Remove Rows with Some or All NAs in R",
    "section": "Example 3 - Removing Rows with All NAs:",
    "text": "Example 3 - Removing Rows with All NAs:\nIn some cases, we may want to remove rows where all values are missing. We can achieve this by using the complete.cases() function along with the rowSums() function.\n\n# Example data frame\ndf &lt;- data.frame(\n  x = c(1, NA, NA),\n  y = c(NA, NA, NA)\n)\ndf\n\n   x  y\n1  1 NA\n2 NA NA\n3 NA NA\n\n# Remove rows with all missing values\nnon_na_rows &lt;- df[rowSums(is.na(df)) &lt; ncol(df), ]\nnon_na_rows\n\n  x  y\n1 1 NA"
  },
  {
    "objectID": "posts/2024-04-09/index.html#explanation-1",
    "href": "posts/2024-04-09/index.html#explanation-1",
    "title": "How to Remove Rows with Some or All NAs in R",
    "section": "Explanation:",
    "text": "Explanation:\n\nWe create a data frame df with all missing values.\nis.na(df) generates a logical matrix indicating NA values.\nrowSums(is.na(df)) calculates the total number of NA values in each row.\nWe compare this sum to the total number of columns ncol(df) to identify rows with all missing values.\nFinally, we subset the data frame to retain rows with at least one non-missing value."
  },
  {
    "objectID": "posts/2024-04-10/index.html",
    "href": "posts/2024-04-10/index.html",
    "title": "A Guide to Removing Multiple Rows in R Using Base R",
    "section": "",
    "text": "As data analysts and scientists, we often find ourselves working with large datasets where data cleaning becomes a crucial step in our analysis pipeline. One common task is removing unwanted rows from our data. In this guide, we’ll explore how to efficiently remove multiple rows in R using the base R package."
  },
  {
    "objectID": "posts/2024-04-10/index.html#understanding-the-subset-function",
    "href": "posts/2024-04-10/index.html#understanding-the-subset-function",
    "title": "A Guide to Removing Multiple Rows in R Using Base R",
    "section": "Understanding the subset() Function",
    "text": "Understanding the subset() Function\nOne handy function for removing rows based on certain conditions is subset(). This function allows us to filter rows based on logical conditions. Here’s how it works:\n\n# Example DataFrame\ndata &lt;- data.frame(\n  id = 1:6,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\"),\n  score = c(75, 82, 90, 68, 95, 60)\n)\ndata\n\n  id    name score\n1  1   Alice    75\n2  2     Bob    82\n3  3 Charlie    90\n4  4   David    68\n5  5     Eve    95\n6  6   Frank    60\n\n# Remove rows where score is less than 80\nfiltered_data &lt;- subset(data, score &gt;= 80)\nfiltered_data\n\n  id    name score\n2  2     Bob    82\n3  3 Charlie    90\n5  5     Eve    95\n\n\nIn this example, we have a DataFrame data with columns for id, name, and score. We use the subset() function to filter rows where the score column is greater than or equal to 80, effectively removing rows where the score is less than 80."
  },
  {
    "objectID": "posts/2024-04-10/index.html#using-logical-indexing",
    "href": "posts/2024-04-10/index.html#using-logical-indexing",
    "title": "A Guide to Removing Multiple Rows in R Using Base R",
    "section": "Using Logical Indexing",
    "text": "Using Logical Indexing\nAnother approach to remove multiple rows is by using logical indexing. We create a logical vector indicating which rows to keep or remove based on certain conditions. Here’s how it’s done:\n\n# Example DataFrame\ndata &lt;- data.frame(\n  id = 1:6,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\"),\n  score = c(75, 82, 90, 68, 95, 60)\n)\ndata\n\n  id    name score\n1  1   Alice    75\n2  2     Bob    82\n3  3 Charlie    90\n4  4   David    68\n5  5     Eve    95\n6  6   Frank    60\n\n# Create a logical vector\nkeep_rows &lt;- data$score &gt;= 80\nkeep_rows\n\n[1] FALSE  TRUE  TRUE FALSE  TRUE FALSE\n\n# Subset the DataFrame based on the logical vector\nfiltered_data &lt;- data[keep_rows, ]\nfiltered_data\n\n  id    name score\n2  2     Bob    82\n3  3 Charlie    90\n5  5     Eve    95\n\n\nIn this example, we create a logical vector keep_rows indicating which rows have a score greater than or equal to 80. We then subset the DataFrame data using this logical vector to keep only the rows that meet our condition."
  },
  {
    "objectID": "posts/2024-04-10/index.html#removing-rows-by-index",
    "href": "posts/2024-04-10/index.html#removing-rows-by-index",
    "title": "A Guide to Removing Multiple Rows in R Using Base R",
    "section": "Removing Rows by Index",
    "text": "Removing Rows by Index\nSometimes, we may want to remove rows by their index position rather than based on a condition. This can be achieved using negative indexing. Here’s how it’s done:\n\n# Example DataFrame\ndata &lt;- data.frame(\n  id = 1:6,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\"),\n  score = c(75, 82, 90, 68, 95, 60)\n)\ndata\n\n  id    name score\n1  1   Alice    75\n2  2     Bob    82\n3  3 Charlie    90\n4  4   David    68\n5  5     Eve    95\n6  6   Frank    60\n\n# Remove rows by index\nfiltered_data &lt;- data[-c(2, 4), ]\nfiltered_data\n\n  id    name score\n1  1   Alice    75\n3  3 Charlie    90\n5  5     Eve    95\n6  6   Frank    60\n\n\nIn this example, we use negative indexing to remove the second and fourth rows from the DataFrame data, effectively eliminating rows with indices 2 and 4."
  },
  {
    "objectID": "posts/2024-04-11/index.html",
    "href": "posts/2024-04-11/index.html",
    "title": "Mastering Rows: Selecting by Index in R",
    "section": "",
    "text": "Let’s jump into data manipulation with R! Selecting specific rows from our datasets is an important skill. Today, we’ll focus on subsetting rows by index, using the trusty square brackets ([]).\nFirst, we’ll load a dataset containing car characteristics:\n\nmtcars.data &lt;- mtcars\n\nThis code loads the mtcars dataset (containing car data) into a new variable, mtcars.data. Now, we’ll explore how to target specific rows."
  },
  {
    "objectID": "posts/2024-04-11/index.html#example-1-selecting-a-single-row-by-index",
    "href": "posts/2024-04-11/index.html#example-1-selecting-a-single-row-by-index",
    "title": "Mastering Rows: Selecting by Index in R",
    "section": "Example 1: Selecting a Single Row by Index",
    "text": "Example 1: Selecting a Single Row by Index\nImagine you want to analyze the fuel efficiency (miles per gallon) of a particular car. Here’s how to grab a single row by its index (row number):\n\n# Select the 5th row (remember indexing starts from 1!)\nspecific.car &lt;- mtcars.data[5,]\nspecific.car\n\n                   mpg cyl disp  hp drat   wt  qsec vs am gear carb\nHornet Sportabout 18.7   8  360 175 3.15 3.44 17.02  0  0    3    2\n\n\n\nExplanation:\n\nmtcars.data: This is our data frame, containing all the car information.\n[]: These are the square brackets, used for subsetting.\n5: This is the index of the row we want. Since indexing starts from 1, the 5th row will be selected.\n,: The comma tells R to select all columns (everything) from that row.\n\nTry it yourself! Select the 10th row and see what car it represents."
  },
  {
    "objectID": "posts/2024-04-11/index.html#example-2-selecting-multiple-rows-by-index",
    "href": "posts/2024-04-11/index.html#example-2-selecting-multiple-rows-by-index",
    "title": "Mastering Rows: Selecting by Index in R",
    "section": "Example 2: Selecting Multiple Rows by Index",
    "text": "Example 2: Selecting Multiple Rows by Index\nLet’s say you’re interested in comparing fuel efficiency (miles per gallon) of a few specific cars. We can use a vector of indices to grab multiple rows at once:\n\n# Select the 3rd, 7th, and 12th rows\nfew.cars &lt;- mtcars.data[c(3, 7, 12),]\nfew.cars\n\n            mpg cyl  disp  hp drat   wt  qsec vs am gear carb\nDatsun 710 22.8   4 108.0  93 3.85 2.32 18.61  1  1    4    1\nDuster 360 14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4\nMerc 450SE 16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3\n\n\n\nExplanation:\n\nWe use c() to create a vector containing the desired row indices: 3, 7, and 12.\nEverything else remains the same as the previous example.\n\nChallenge yourself! Create a vector to select the last 5 rows and analyze their horsepower."
  },
  {
    "objectID": "posts/2024-04-11/index.html#example-3-selecting-rows-using-a-range-of-indices",
    "href": "posts/2024-04-11/index.html#example-3-selecting-rows-using-a-range-of-indices",
    "title": "Mastering Rows: Selecting by Index in R",
    "section": "Example 3: Selecting Rows Using a Range of Indices",
    "text": "Example 3: Selecting Rows Using a Range of Indices\nSometimes, you want to analyze a group of consecutive cars. Here’s how to select a range using the colon (:) operator:\n\n# Select rows from 8 to 15 (inclusive)\ncar.slice &lt;- mtcars.data[8:15,]\ncar.slice\n\n                    mpg cyl  disp  hp drat   wt  qsec vs am gear carb\nMerc 240D          24.4   4 146.7  62 3.69 3.19 20.00  1  0    4    2\nMerc 230           22.8   4 140.8  95 3.92 3.15 22.90  1  0    4    2\nMerc 280           19.2   6 167.6 123 3.92 3.44 18.30  1  0    4    4\nMerc 280C          17.8   6 167.6 123 3.92 3.44 18.90  1  0    4    4\nMerc 450SE         16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3\nMerc 450SL         17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3\nMerc 450SLC        15.2   8 275.8 180 3.07 3.78 18.00  0  0    3    3\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4\n\n\n\nExplanation:\n\n8:15: This specifies the range of rows we want. Here, we select from row 8 (inclusive) to row 15 (inclusive).\n\nNow it’s your turn! Select rows 1 to 10 and explore the distribution of the number of cylinders.\nRemember, practice is key! Experiment with different indices and ranges to become comfortable with subsetting rows in R. As you work with more datasets, you’ll master these techniques and become a data wrangling pro.\nHappy coding!"
  },
  {
    "objectID": "posts/2024-04-12/index.html",
    "href": "posts/2024-04-12/index.html",
    "title": "Taking the data out of the glue with regex in R",
    "section": "",
    "text": "Introduction\nRegular expressions, or regex, are incredibly powerful tools for pattern matching and extracting specific information from text data. Today, we’ll explore how to harness the might of regex in R with a practical example.\nLet’s dive into a scenario where we have data that needs cleaning and extracting numerical values from strings. Our data, stored in a dataframe named df, consists of four columns (x1, x2, x3, x4) with strings containing numerical values along with percentage values enclosed in parentheses. Our goal is to extract these numerical values and compute a total for each row.\n\n\nLoading Libraries\nBefore we begin, we need to load the necessary libraries. We’ll be using the tidyverse package for data manipulation, along with glue and unglue for string manipulation.\n\n# Library Loading\npacman::p_load(tidyverse, glue, unglue)\n\n\n\nExploring the Data\nLet’s take a sneak peek at our data using the head() function to understand its structure.\n\ndf &lt;- tibble(\n  x1 = rep(\"Unit A\", 11),\n  x2 = c(glue(\"{11:20} ({1:10}%)\"),  glue(\"{251} ({13}%)\")),\n  x3 = c(glue(\"{21:30} ({11:20}%)\"), glue(\"{252} ({14}%)\")),\n  x4 = c(glue(\"{31:40} ({21:30}%)\"), glue(\"{253} ({15}%)\"))\n)\n\nhead(df, 3)\n\n# A tibble: 3 × 4\n  x1     x2      x3       x4      \n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n1 Unit A 11 (1%) 21 (11%) 31 (21%)\n2 Unit A 12 (2%) 22 (12%) 32 (22%)\n3 Unit A 13 (3%) 23 (13%) 33 (23%)\n\n\nThis command displays the first three rows of our dataframe df, giving us an idea of how our data looks like.\n\n\nCreating a Regex Function\nNow, we’ll define a custom function named reg_val_fns to extract numerical values from strings using regular expressions. This function takes two parameters: .col_data (column data) and .pattern (regex pattern). If no pattern is provided, it defaults to extracting any sequence of digits followed by non-word characters or the end of the string.\n\n# Make regex function\nreg_val_fns &lt;- function(.col_data, .pattern = NULL){\n  ptrn &lt;- .pattern\n  if(is.null(ptrn)){\n    ptrn &lt;- \"\\\\d+(?=\\\\W|$)\"\n  }\n  \n  reged_val &lt;- .col_data |&gt;\n    str_extract(ptrn) |&gt;\n    as.numeric()\n\n  return(reged_val)\n}\n\n\n\nApplying the Regex Function\nWith our regex function defined, we apply it across desired columns using the mutate(across()) function from the dplyr package. This extracts numerical values from strings in each column, converting them into numeric format. Additionally, we compute the total value for each row using rowSums().\n\n# Apply the function across the desired columns\ndf |&gt;\n  mutate(across(-x1, reg_val_fns)) |&gt;\n  mutate(total_val = rowSums(across(-x1)))\n\n# A tibble: 11 × 5\n   x1        x2    x3    x4 total_val\n   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 Unit A    11    21    31        63\n 2 Unit A    12    22    32        66\n 3 Unit A    13    23    33        69\n 4 Unit A    14    24    34        72\n 5 Unit A    15    25    35        75\n 6 Unit A    16    26    36        78\n 7 Unit A    17    27    37        81\n 8 Unit A    18    28    38        84\n 9 Unit A    19    29    39        87\n10 Unit A    20    30    40        90\n11 Unit A   251   252   253       756\n\n\n\n\nAlternative Approach: Using unglue\nAn alternative method to extract values from strings is using the unglue package. Here, we apply the unglue_data() function across columns (excluding x1) to extract values and percentages separately, then unnest the resulting dataframe and compute the total value for each row.\n\n# Use unglue\ndf |&gt;\n  mutate(across(-x1, \\(x) unglue_data(x, \"{val} ({pct}%)\"))) |&gt; \n  unnest(cols = everything(), names_sep = \"_\") |&gt;\n  mutate(across(.cols = contains(\"val\"), \\(x) as.numeric(x))) |&gt;\n  mutate(total_val = rowSums(across(where(is.numeric))))\n\n# A tibble: 11 × 8\n   x1     x2_val x2_pct x3_val x3_pct x4_val x4_pct total_val\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Unit A     11 1          21 11         31 21            63\n 2 Unit A     12 2          22 12         32 22            66\n 3 Unit A     13 3          23 13         33 23            69\n 4 Unit A     14 4          24 14         34 24            72\n 5 Unit A     15 5          25 15         35 25            75\n 6 Unit A     16 6          26 16         36 26            78\n 7 Unit A     17 7          27 17         37 27            81\n 8 Unit A     18 8          28 18         38 28            84\n 9 Unit A     19 9          29 19         39 29            87\n10 Unit A     20 10         30 20         40 30            90\n11 Unit A    251 13        252 14        253 15           756\n\n\n\n\nConclusion\nIn this tutorial, we’ve explored how to leverage the power of regular expressions in R to extract numerical values from strings within a dataframe. By defining custom regex functions and using packages like dplyr and unglue, we can efficiently clean and manipulate text data for further analysis.\nI encourage you to try out these techniques on your own datasets and explore the endless possibilities of regex in R. Happy coding!"
  },
  {
    "objectID": "posts/2024-04-15/index.html",
    "href": "posts/2024-04-15/index.html",
    "title": "Estimating Chi-Square Distribution Parameters Using R",
    "section": "",
    "text": "In the world of statistics and data analysis, understanding and accurately estimating the parameters of probability distributions is crucial. One such distribution is the chi-square distribution, often encountered in various statistical analyses. In this blog post, we’ll dive into how we can estimate the degrees of freedom (“df”) and the non-centrality parameter (“ncp”) of a chi-square distribution using R programming language."
  },
  {
    "objectID": "posts/2024-04-15/index.html#setting-the-stage-libraries-and-data",
    "href": "posts/2024-04-15/index.html#setting-the-stage-libraries-and-data",
    "title": "Estimating Chi-Square Distribution Parameters Using R",
    "section": "Setting the Stage: Libraries and Data",
    "text": "Setting the Stage: Libraries and Data\nFirst, we load the necessary libraries: tidyverse for data manipulation and bbmle for maximum likelihood estimation. We then generate a grid of parameters (degrees of freedom and non-centrality parameter) and sample sizes to create a diverse set of chi-square distributed data.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(bbmle)\n\n# Data ----\n# Make parameters and grid\ndf &lt;- 1:10\nncp &lt;- 1:10\nn &lt;- runif(10, 250, 500) |&gt; trunc()\nparam_grid &lt;- expand_grid(n = n, df = df, ncp = ncp)\n\nhead(param_grid)\n\n# A tibble: 6 × 3\n      n    df   ncp\n  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1   284     1     1\n2   284     1     2\n3   284     1     3\n4   284     1     4\n5   284     1     5\n6   284     1     6"
  },
  {
    "objectID": "posts/2024-04-15/index.html#function-exploration-unveiling-the-estimation-process",
    "href": "posts/2024-04-15/index.html#function-exploration-unveiling-the-estimation-process",
    "title": "Estimating Chi-Square Distribution Parameters Using R",
    "section": "Function Exploration: Unveiling the Estimation Process",
    "text": "Function Exploration: Unveiling the Estimation Process\nThe core of our exploration lies in several functions designed to estimate the chi-square parameters:\ndof/k Functions: These functions focus on estimating the degrees of freedom (df) using different approaches:\n\nmean_x: Calculates the mean of the data.\nmean_minus_1: Subtracts 1 from the mean.\nvar_div_2: Divides the variance of the data by 2.\nlength_minus_1: Subtracts 1 from the length of the data.\n\nncp Functions: These functions aim to estimate the non-centrality parameter (ncp) using various methods:\n\nmean_minus_mean_minus_1: A seemingly trivial calculation that serves as a baseline.\nie_mean_minus_var_div_2: Subtracts half the variance from the mean, ensuring the result is non-negative.\nie_optim: Utilizes optimization techniques to find the ncp that maximizes the likelihood of observing the data.\nestimate_chisq_params: This is the main function that employs maximum likelihood estimation (MLE) via the bbmle package to estimate both df and ncp simultaneously. It defines a negative log-likelihood function based on the chi-square distribution and uses mle2 to find the parameter values that minimize this function.\n\n\n# Functions ----\n# functions to estimate the parameters of a chisq distribution\n# dof\nmean_x &lt;- function(x) mean(x)\nmean_minus_1 &lt;- function(x) mean(x) - 1\nvar_div_2 &lt;- function(x) var(x) / 2\nlength_minus_1 &lt;- function(x) length(x) - 1\n# ncp\nmean_minus_mean_minus_1 &lt;- function(x) mean(x) - (mean(x) - 1)\nie_mean_minus_var_div_2 &lt;- function(x) ifelse((mean(x) - (var(x) / 2)) &lt; 0, 0, mean(x) - var(x)/2)\nie_optim &lt;- function(x) optim(par = 0,\n                             fn = function(ncp) {\n                               -sum(dchisq(x, df = var(x)/2, ncp = ncp, log = TRUE))\n                             },\n                             method = \"Brent\",\n                             lower = 0,\n                             upper = 10 * var(x)/2)$par\n# both\nestimate_chisq_params &lt;- function(data) {\n  # Negative log-likelihood function\n  negLogLik &lt;- function(df, ncp) {\n    -sum(dchisq(data, df = df, ncp = ncp, log = TRUE))\n  }\n  \n  # Initial values (adjust based on your data if necessary)\n  start_vals &lt;- list(df = trunc(var(data)/2), ncp = trunc(mean(data)))\n  \n  # MLE using bbmle\n  mle_fit &lt;- bbmle::mle2(negLogLik, start = start_vals)\n  # Return estimated parameters as a named vector\n  df &lt;- dplyr::tibble(\n    est_df = coef(mle_fit)[1],\n    est_ncp = coef(mle_fit)[2]\n  )\n  return(df)\n}\n\nsafe_estimates &lt;- {\n  purrr::possibly(\n    estimate_chisq_params,\n    otherwise = NA_real_,\n    quiet = TRUE\n  )\n}"
  },
  {
    "objectID": "posts/2024-04-15/index.html#simulating-and-evaluating-putting-the-functions-to-the-test",
    "href": "posts/2024-04-15/index.html#simulating-and-evaluating-putting-the-functions-to-the-test",
    "title": "Estimating Chi-Square Distribution Parameters Using R",
    "section": "Simulating and Evaluating: Putting the Functions to the Test",
    "text": "Simulating and Evaluating: Putting the Functions to the Test\nTo assess the performance of our functions, we simulate chi-square data using the parameter grid and apply each function to estimate the parameters. We then compare these estimates to the true values and visualize the results using boxplots.\n\n# Simulate data ----\nset.seed(123)\ndff &lt;- param_grid |&gt;\n  mutate(x = pmap(pick(everything()), match.fun(\"rchisq\"))) |&gt;\n  mutate(\n    safe_est_parms = map(x, safe_estimates),\n    dfa = map_dbl(x, mean_minus_1),\n    dfb = map_dbl(x, var_div_2),\n    dfc = map_dbl(x, length_minus_1),\n    ncpa = map_dbl(x, mean_minus_mean_minus_1),\n    ncpb = map_dbl(x, ie_mean_minus_var_div_2),\n    ncpc = map_dbl(x, ie_optim)\n  ) |&gt;\n  select(-x) |&gt;\n  filter(map_lgl(safe_est_parms, ~ any(is.na(.x))) == FALSE) |&gt;\n  unnest(cols = safe_est_parms) |&gt;\n  mutate(\n    dfa_resid = dfa - df,\n    dfb_resid = dfb - df,\n    dfc_resid = dfc - df,\n    dfd_resid = est_df - df,\n    ncpa_resid = ncpa - ncp,\n    ncpb_resid = ncpb - ncp,\n    ncpc_resid = ncpc - ncp,\n    ncpd_resid = est_ncp - ncp\n  )\n\nglimpse(dff)\n\nRows: 987\nColumns: 19\n$ n          &lt;dbl&gt; 284, 284, 284, 284, 284, 284, 284, 284, 284, 284, 284, 284,…\n$ df         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ ncp        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ est_df     &lt;dbl&gt; 1.1770904, 0.9905994, 0.9792179, 0.7781877, 1.5161669, 0.82…\n$ est_ncp    &lt;dbl&gt; 0.7231638, 1.9462325, 3.0371756, 4.2347494, 3.7611119, 6.26…\n$ dfa        &lt;dbl&gt; 0.9050589, 1.9826153, 3.0579375, 4.0515312, 4.2022289, 6.15…\n$ dfb        &lt;dbl&gt; 2.626501, 5.428382, 7.297746, 9.265272, 8.465838, 14.597976…\n$ dfc        &lt;dbl&gt; 283, 283, 283, 283, 283, 283, 283, 283, 283, 283, 283, 283,…\n$ ncpa       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ncpb       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ncpc       &lt;dbl&gt; 5.382789e-09, 8.170550e-09, 6.017177e-09, 8.618892e-09, 7.7…\n$ dfa_resid  &lt;dbl&gt; -0.09494109, 0.98261533, 2.05793748, 3.05153121, 3.20222890…\n$ dfb_resid  &lt;dbl&gt; 1.626501, 4.428382, 6.297746, 8.265272, 7.465838, 13.597976…\n$ dfc_resid  &lt;dbl&gt; 282, 282, 282, 282, 282, 282, 282, 282, 282, 282, 281, 281,…\n$ dfd_resid  &lt;dbl&gt; 0.177090434, -0.009400632, -0.020782073, -0.221812344, 0.51…\n$ ncpa_resid &lt;dbl&gt; 0, -1, -2, -3, -4, -5, -6, -7, -8, -9, 0, -1, -2, -3, -4, -…\n$ ncpb_resid &lt;dbl&gt; -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -1, -2, -3, -4, -5…\n$ ncpc_resid &lt;dbl&gt; -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -1, -2, -3, -4, -5…\n$ ncpd_resid &lt;dbl&gt; -0.27683618, -0.05376753, 0.03717560, 0.23474943, -1.238888…"
  },
  {
    "objectID": "posts/2024-04-15/index.html#visual-insights-assessing-estimation-accuracy",
    "href": "posts/2024-04-15/index.html#visual-insights-assessing-estimation-accuracy",
    "title": "Estimating Chi-Square Distribution Parameters Using R",
    "section": "Visual Insights: Assessing Estimation Accuracy",
    "text": "Visual Insights: Assessing Estimation Accuracy\nThe boxplots reveal interesting insights:\n\npar(mfrow = c(1, 2))\nboxplot(dff$dfa ~ dff$df, main = \"mean(x) -1 ~ df\")\nboxplot(dff$dfa_resid ~ dff$df, main = \"mean(x) -1 ~ df Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$dfb ~ dff$df, main = \"var(x) / 2 ~ df\")\nboxplot(dff$dfb_resid ~ dff$df, main = \"var(x) / 2 ~ df Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$dfc ~ dff$df, main = \"length(x) - 1 ~ df\")\nboxplot(dff$dfc_resid ~ dff$df, main = \"length(x) - 1 ~ df Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$est_df ~ dff$df, main = \"negloglik ~ df - Looks Good\")\nboxplot(dff$dfd_resid ~ dff$df, main = \"negloglik ~ df Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$ncpa ~ dff$ncp, main = \"mean(x) - (mean(x) - 1) ~ ncp\")\nboxplot(dff$ncpa_resid ~ dff$ncp, main = \"mean(x) - (mean(x) - 1) ~ ncp Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$ncpb ~ dff$ncp, main = \"mean(x) - var(x)/2 ~ nc\")\nboxplot(dff$ncpb_resid ~ dff$ncp, main = \"mean(x) - var(x)/2 ~ ncp Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$ncpc ~ dff$ncp, main = \"optim ~ ncp\")\nboxplot(dff$ncpc_resid ~ dff$ncp, main = \"optim ~ ncp Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\npar(mfrow = c(1, 2))\nboxplot(dff$est_ncp ~ dff$ncp, main = \"negloglik ~ ncp - Looks Good\")\nboxplot(dff$ncpd_resid ~ dff$ncp, main = \"negloglik ~ ncp Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\ndf Estimation:\n\nmean_x - 1 and var(x) / 2 show potential as df estimators but exhibit bias depending on the true df value.\nlength(x) - 1 performs poorly, consistently underestimating df.\nThe MLE approach from estimate_chisq_params demonstrates the most accurate and unbiased estimates across different df values.\n\nncp Estimation:\n\nThe simple methods (mean(x) - mean(x) - 1 and mean(x) - var(x) / 2) show substantial bias and variability.\nThe optimization-based method (optim) performs better but still exhibits some bias.\nThe MLE approach again emerges as the most reliable option, providing accurate and unbiased estimates across various ncp values."
  },
  {
    "objectID": "posts/2024-04-16/index.html",
    "href": "posts/2024-04-16/index.html",
    "title": "Selecting Rows with Specific Values: Exploring Options in R",
    "section": "",
    "text": "In R, we often need to filter data frames based on whether a specific value appears within any of the columns. Both base R and the dplyr package offer efficient ways to achieve this. Let’s delve into both approaches and see how they work!"
  },
  {
    "objectID": "posts/2024-04-16/index.html#example-1---use-dplyr",
    "href": "posts/2024-04-16/index.html#example-1---use-dplyr",
    "title": "Selecting Rows with Specific Values: Exploring Options in R",
    "section": "Example 1 - Use dplyr",
    "text": "Example 1 - Use dplyr\nThe dplyr package provides a concise and readable syntax for data manipulation. We can achieve our goal using the filter() function in conjunction with if_any().\nlibrary(dplyr)\n\nfiltered_data &lt;- data %&gt;%\n  filter(if_any(everything(), ~ .x == \"your_value\"))\nLet’s break down the code:\n\ndata: This represents your data frame.\nfilter(): This function keeps rows that meet a specified condition.\nif_any(): This checks if the condition is true for any of the columns.\neverything(): This indicates we want to consider all columns.\n.x: This represents each individual column within the everything() selection.\n== \"your_value\": This is the condition to check. Here, we are looking for rows where the value in any column is equal to “your_value”.\n\nExample:\n\nlibrary(dplyr)\n\ndata &lt;- data.frame(\n  fruit = c(\"apple\", \"banana\", \"orange\"),\n  color = c(\"red\", \"yellow\", \"orange\"),\n  price = c(0.5, 0.75, 0.6)\n)\n\ndata %&gt;%\n  filter(if_any(everything(), ~ .x == \"apple\"))\n\n  fruit color price\n1 apple   red   0.5\n\n\nThis code will return the row where “apple” appears in the “fruit” column."
  },
  {
    "objectID": "posts/2024-04-16/index.html#example-2---base-r-approach",
    "href": "posts/2024-04-16/index.html#example-2---base-r-approach",
    "title": "Selecting Rows with Specific Values: Exploring Options in R",
    "section": "Example 2 - Base R Approach",
    "text": "Example 2 - Base R Approach\nBase R offers its own set of functions for data manipulation. We can achieve the same row filtering using apply() and logical operations.\n# Identify rows with the value\nrow_indices &lt;- apply(data, 1, function(row) any(row == \"your_value\"))\n\n# Subset the data\nfiltered_data &lt;- data[row_indices, ]\nExplanation:\n\napply(data, 1, ...): This applies a function to each row of the data frame. The 1 indicates row-wise application.\nfunction(row) any(row == \"your_value\"): This anonymous function checks if “your_value” is present in any element of the row using the any() function and returns TRUE or FALSE.\nrow_indices: This stores the logical vector indicating which rows meet the condition.\ndata[row_indices, ]: We subset the data frame using the logical vector, keeping only the rows where the condition is TRUE.\n\nExample:\n\ndata &lt;- data.frame(\n  fruit = c(\"apple\", \"banana\", \"orange\"),\n  color = c(\"red\", \"yellow\", \"orange\"),\n  price = c(0.5, 0.75, 0.6)\n)\n\nrow_indices &lt;- apply(data, 1, function(row) any(row == \"apple\"))\nfiltered_data &lt;- data[row_indices, ]\nfiltered_data\n\n  fruit color price\n1 apple   red   0.5\n\n\nThis code will also return the row where “apple” appears."
  },
  {
    "objectID": "posts/2024-04-16/index.html#example-3---base-r-approach-2",
    "href": "posts/2024-04-16/index.html#example-3---base-r-approach-2",
    "title": "Selecting Rows with Specific Values: Exploring Options in R",
    "section": "Example 3 - Base R Approach 2",
    "text": "Example 3 - Base R Approach 2\nAnother base R approach involves using the rowSums() function to identify rows with the specified value.\n# Identify rows with the value\nfiltered_rows &lt;- which(rowSums(data == \"your_value\") &gt; 0, arr.ind = TRUE)\ndf_filtered &lt;- data[filtered_rows, ]\nWhile dplyr offers a concise approach, base R also provides solutions using loops. Here’s one way to achieve the same result:\n\nwhich(rowSums(df == value) &gt; 0, arr.ind = TRUE): This part finds the row indices where the sum of elements in each row being equal to the value is greater than zero (indicating at least one match).\nrowSums(df == value): Calculates the sum across rows, checking if any value in the row matches the target value.\n&gt; 0: Filters rows where the sum is greater than zero (i.e., at least one match).\narr.ind = TRUE: Ensures the output includes both row and column indices (useful for debugging but not required here).\ndf[filtered_rows, ]: Subsets the original data frame (df) based on the identified row indices (filtered_rows), creating the filtered data frame (df_filtered).\n\nExample:\n\nfiltered_rows &lt;- which(rowSums(data == \"apple\") &gt; 0, arr.ind = TRUE)\ndf_filtered &lt;- data[filtered_rows, ]\ndf_filtered\n\n  fruit color price\n1 apple   red   0.5\n\n\nThis code will return the row where “apple” appears in any column."
  },
  {
    "objectID": "posts/2024-04-17/index.html",
    "href": "posts/2024-04-17/index.html",
    "title": "A Guide to Selecting Rows with NA Values in R Using Base R",
    "section": "",
    "text": "Introduction\nDealing with missing data is a common challenge in data analysis and machine learning projects. In R, missing values are represented by NA. Being able to identify and handle these missing values is crucial for accurate analysis and model building. In this guide, we’ll explore how to select rows with NA values in R using base R functions.\n\n\nUnderstanding NA Values\nNA stands for “Not Available” and is used in R to represent missing or undefined data. When working with datasets, it’s essential to identify and handle NA values appropriately to avoid biased analysis or incorrect results.\n\n\nCreating a Sample Dataset\nLet’s start by creating a simple dataset with NA values to demonstrate the selection process. We’ll use the data.frame function to create a dataframe named “sample_data” with three columns: “ID”, “Age”, and “Income”.\n\n# Creating sample dataset\nsample_data &lt;- data.frame(\n  ID = 1:5,\n  Age = c(25, NA, 30, 35, 40),\n  Income = c(50000, 60000, NA, 70000, 80000)\n)\n\nsample_data\n\n  ID Age Income\n1  1  25  50000\n2  2  NA  60000\n3  3  30     NA\n4  4  35  70000\n5  5  40  80000\n\n\nNow, “sample_data” contains five rows and three columns, with some NA values in the “Age” and “Income” columns.\n\n\nSelecting Rows with NA Values\nTo select rows with NA values in R, we can use logical indexing combined with the is.na function. The is.na function returns a logical vector indicating which elements are NA.\n\n# Selecting rows with NA values in any column\nrows_with_na &lt;- sample_data[apply(\n  sample_data, \n  1, \n  function(x) any(is.na(x))\n  ), ]\n\nIn this code snippet, we use the apply function to apply the any and is.na functions row-wise. This returns a logical vector indicating whether each row contains any NA values. Finally, we use this logical vector to index the rows containing NA values in any column.\n\n\nVisualizing Selected Rows:\nLet’s print the selected rows to see which rows contain NA values.\n\n# Printing selected rows\nprint(rows_with_na)\n\n  ID Age Income\n2  2  NA  60000\n3  3  30     NA\n\n\nAs shown in the output, rows 2 and 3 contain NA values either in the “Age” or “Income” column.\n\n\nAlternative Method\nAnother approach to select rows with NA values is by using the complete.cases function. This function returns a logical vector indicating which rows are complete (i.e., have no missing values).\n\n# Selecting rows with NA values using complete.cases\nrows_with_na &lt;- sample_data[!complete.cases(sample_data), ]\nrows_with_na\n\n  ID Age Income\n2  2  NA  60000\n3  3  30     NA\n\n\nIn this code snippet, we use the complete.cases function to identify rows with missing values and then negate (!) the result to select rows with NA values.\n\n\nConclusion\nIn this guide, we’ve demonstrated how to select rows with NA values in R using base R functions. By using logical indexing and the is.na or complete.cases functions, you can efficiently identify rows containing missing data in your datasets. Handling missing values appropriately is crucial for ensuring the integrity and accuracy of your data analysis and modeling efforts. Experiment with different datasets and scenarios to deepen your understanding of handling missing values in R. Happy coding!"
  },
  {
    "objectID": "posts/2024-04-18/index.html",
    "href": "posts/2024-04-18/index.html",
    "title": "Extracting the Last N’th Row in R Data Frames",
    "section": "",
    "text": "Ever wrangled with a data frame and needed just the final row? Fear not, R warriors! Today’s quest unveils three mighty tools to conquer this task: base R, the dplyr package, and the data.table package."
  },
  {
    "objectID": "posts/2024-04-18/index.html#method-1-using-base-r",
    "href": "posts/2024-04-18/index.html#method-1-using-base-r",
    "title": "Extracting the Last N’th Row in R Data Frames",
    "section": "Method 1: Using Base R",
    "text": "Method 1: Using Base R\n\n# Create a sample data frame\nmy_df &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = c(25, 30, 22)\n)\n\n# Extract the last row using nrow() and indexing\nlast_row_base &lt;- my_df[nrow(my_df), ]\nprint(last_row_base)\n\n     Name Age\n3 Charlie  22\n\n\nExplanation: - We use nrow(my_df) to get the total number of rows in the data frame. - Then, we use indexing ([nrow(my_df), ]) to extract the last row."
  },
  {
    "objectID": "posts/2024-04-18/index.html#method-2-using-dplyr",
    "href": "posts/2024-04-18/index.html#method-2-using-dplyr",
    "title": "Extracting the Last N’th Row in R Data Frames",
    "section": "Method 2: Using dplyr",
    "text": "Method 2: Using dplyr\n\nlibrary(dplyr)\n\n# Extract the last row using tail()\nlast_row_dplyr &lt;- my_df %&gt;% tail(1)\nprint(last_row_dplyr)\n\n     Name Age\n3 Charlie  22\n\n\nExplanation: - The tail() function from dplyr returns the last n rows of a data frame (default is 6). - We use tail(my_df, 1) to get only the last row."
  },
  {
    "objectID": "posts/2024-04-18/index.html#method-3-using-data.table",
    "href": "posts/2024-04-18/index.html#method-3-using-data.table",
    "title": "Extracting the Last N’th Row in R Data Frames",
    "section": "Method 3: Using data.table",
    "text": "Method 3: Using data.table\n\nlibrary(data.table)\n\n# Convert data frame to data.table\nmy_dt &lt;- as.data.table(my_df)\n\n# Extract the last row using .N\nlast_row_dt &lt;- my_dt[.N]\nprint(last_row_dt)\n\n      Name   Age\n    &lt;char&gt; &lt;num&gt;\n1: Charlie    22\n\n\nExplanation: - We convert the data frame to a data.table using as.data.table(my_df). - The .N special variable in data.table represents the total number of rows. - We use my_dt[.N] to get the last row."
  },
  {
    "objectID": "posts/2024-04-18/index.html#method-1-using-base-r-1",
    "href": "posts/2024-04-18/index.html#method-1-using-base-r-1",
    "title": "Extracting the Last N’th Row in R Data Frames",
    "section": "Method 1: Using Base R",
    "text": "Method 1: Using Base R\n\n# Create a sample data frame\nmy_df &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  Age = c(25, 30, 22, 28, 24)\n)\n\n# Extract the second-to-last row using nrow() and indexing\nsecond_to_last_base &lt;- my_df[nrow(my_df) - 1, ]\nprint(second_to_last_base)\n\n   Name Age\n4 David  28\n\n\nExplanation: - We use nrow(my_df) to get the total number of rows in the data frame. - To extract the second-to-last row, we subtract 1 from the total number of rows."
  },
  {
    "objectID": "posts/2024-04-18/index.html#method-2-using-dplyr-1",
    "href": "posts/2024-04-18/index.html#method-2-using-dplyr-1",
    "title": "Extracting the Last N’th Row in R Data Frames",
    "section": "Method 2: Using dplyr",
    "text": "Method 2: Using dplyr\n\n# Extract the second-to-last row using slice()\nsecond_to_last_dplyr &lt;- my_df %&gt;% slice(n() - 1)\nprint(second_to_last_dplyr)\n\n   Name Age\n1 David  28\n\n\nExplanation: - The slice() function from dplyr allows us to select specific rows. - We use slice(my_df, n() - 1) to get the second-to-last row."
  },
  {
    "objectID": "posts/2024-04-18/index.html#method-3-using-data.table-1",
    "href": "posts/2024-04-18/index.html#method-3-using-data.table-1",
    "title": "Extracting the Last N’th Row in R Data Frames",
    "section": "Method 3: Using data.table",
    "text": "Method 3: Using data.table\n\n# Convert data frame to data.table\nmy_dt &lt;- as.data.table(my_df)\n\n# Extract the second-to-last row using .N\nsecond_to_last_dt &lt;- my_dt[.N - 1]\nprint(second_to_last_dt)\n\n     Name   Age\n   &lt;char&gt; &lt;num&gt;\n1:  David    28\n\n\nExplanation: - Similar to the previous method, we convert the data frame to a data.table. - The .N special variable in data.table represents the total number of rows. - We use my_dt[.N - 1] to get the second-to-last row."
  },
  {
    "objectID": "posts/2024-04-19/index.html",
    "href": "posts/2024-04-19/index.html",
    "title": "Checking Row Existence Across Data Frames in R",
    "section": "",
    "text": "Hello, fellow R users! Today, we’re going to explore a common scenario you might encounter when working with data frames: checking if a row from one data frame exists in another. This is a handy skill that can help you compare datasets and verify data integrity."
  },
  {
    "objectID": "posts/2024-04-19/index.html#example-1-using-merge-function",
    "href": "posts/2024-04-19/index.html#example-1-using-merge-function",
    "title": "Checking Row Existence Across Data Frames in R",
    "section": "Example 1: Using merge() Function",
    "text": "Example 1: Using merge() Function\nLet’s start with our first example. We have two data frames, df1 and df2. We want to check if the rows in df1 are also present in df2.\n\n# Sample data frames\ndf1 &lt;- data.frame(ID = c(1, 2, 3), Value = c(\"A\", \"B\", \"C\"))\ndf2 &lt;- data.frame(ID = c(2, 3, 4), Value = c(\"B\", \"C\", \"D\"))\n\n# Use merge() to find common rows\ncommon_rows &lt;- merge(df1, df2)\n\n# Display the result\nprint(common_rows)\n\n  ID Value\n1  2     B\n2  3     C"
  },
  {
    "objectID": "posts/2024-04-19/index.html#step-by-step-explanation",
    "href": "posts/2024-04-19/index.html#step-by-step-explanation",
    "title": "Checking Row Existence Across Data Frames in R",
    "section": "Step-by-Step Explanation:",
    "text": "Step-by-Step Explanation:\n\nWe create two data frames, df1 and df2, each with an ‘ID’ column and a ‘Value’ column.\nWe use the merge() function to find the common rows between df1 and df2.\nThe result, common_rows, will display rows that exist in both data frames."
  },
  {
    "objectID": "posts/2024-04-19/index.html#example-2-using-in-operator",
    "href": "posts/2024-04-19/index.html#example-2-using-in-operator",
    "title": "Checking Row Existence Across Data Frames in R",
    "section": "Example 2: Using %in% Operator",
    "text": "Example 2: Using %in% Operator\nFor our second example, we’ll use the %in% operator to check for the existence of specific values from one data frame in another.\n\n# Check if 'ID' from df1 exists in df2\ndf1$ExistsInDF2 &lt;- df1$ID %in% df2$ID\n\n# Display the updated df1 with the existence check\nprint(df1)\n\n  ID Value ExistsInDF2\n1  1     A       FALSE\n2  2     B        TRUE\n3  3     C        TRUE"
  },
  {
    "objectID": "posts/2024-04-19/index.html#step-by-step-explanation-1",
    "href": "posts/2024-04-19/index.html#step-by-step-explanation-1",
    "title": "Checking Row Existence Across Data Frames in R",
    "section": "Step-by-Step Explanation:",
    "text": "Step-by-Step Explanation:\n\nWe add a new column to df1 named ‘ExistsInDF2’.\nThe %in% operator checks each ‘ID’ in df1 against the ’ID’s in df2.\nThe new column in df1 will show TRUE if the ‘ID’ exists in df2 and FALSE otherwise."
  },
  {
    "objectID": "posts/2024-04-24/index.html",
    "href": "posts/2024-04-24/index.html",
    "title": "A Practical Guide to Selecting Top N Values by Group in R",
    "section": "",
    "text": "In data analysis, there often arises a need to extract the top N values within each group of a dataset. Whether you’re dealing with sales data, survey responses, or any other type of grouped data, identifying the top performers or outliers within each group can provide valuable insights. In this tutorial, we’ll explore how to accomplish this task using three popular R packages: dplyr, data.table, and base R. By the end of this guide, you’ll have a solid understanding of various approaches to selecting top N values by group in R."
  },
  {
    "objectID": "posts/2024-04-24/index.html#using-dplyr",
    "href": "posts/2024-04-24/index.html#using-dplyr",
    "title": "A Practical Guide to Selecting Top N Values by Group in R",
    "section": "Using dplyr",
    "text": "Using dplyr\ndplyr is a powerful package for data manipulation, providing intuitive functions for common data manipulation tasks. To select the top N values by group using dplyr, we’ll use the group_by() and top_n() functions.\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Example dataset\ndata &lt;- data.frame(\n  group = c(rep(\"A\", 5), rep(\"B\", 5)),\n  value = c(10, 15, 8, 12, 20, 25, 18, 22, 17, 30)\n)\n\n# Select top 2 values by group\ntop_n_values &lt;- data %&gt;%\n  group_by(group) %&gt;%\n  top_n(2, value)\n\n# View the result\nprint(top_n_values)\n\n# A tibble: 4 × 2\n# Groups:   group [2]\n  group value\n  &lt;chr&gt; &lt;dbl&gt;\n1 A        15\n2 A        20\n3 B        25\n4 B        30\n\n\n\nExplanation\n\nWe begin by loading the dplyr package.\nWe create a sample dataset with two columns: ‘group’ and ‘value’.\nUsing the %&gt;% (pipe) operator, we first group the data by the ‘group’ column using group_by().\nThen, we use the top_n() function to select the top 2 values within each group based on the ‘value’ column.\nFinally, we print the resulting dataset containing the top N values by group."
  },
  {
    "objectID": "posts/2024-04-24/index.html#using-data.table",
    "href": "posts/2024-04-24/index.html#using-data.table",
    "title": "A Practical Guide to Selecting Top N Values by Group in R",
    "section": "Using data.table",
    "text": "Using data.table\ndata.table is another popular package for efficient data manipulation, particularly with large datasets. To achieve the same task using data.table, we’ll use the by argument along with the .SD special symbol.\n\n# Load the data.table package\nlibrary(data.table)\n\n# Convert data frame to data.table\nsetDT(data)\n\n# Select top 2 values by group\ntop_n_values &lt;- data[, .SD[order(-value)][1:2], by = group]\n\n# View the result\nprint(top_n_values)\n\n    group value\n   &lt;char&gt; &lt;num&gt;\n1:      A    20\n2:      A    15\n3:      B    30\n4:      B    25\n\n\n\nExplanation\n\nAfter loading the data.table package, we convert our data frame to a data.table using setDT().\nWe then select the top 2 values within each group by ordering the data in descending order of ‘value’ and selecting the first 2 rows using [1:2].\nThe by argument is used to specify grouping by the ‘group’ column.\nFinally, we print the resulting dataset containing the top N values by group."
  },
  {
    "objectID": "posts/2024-04-24/index.html#using-base-r",
    "href": "posts/2024-04-24/index.html#using-base-r",
    "title": "A Practical Guide to Selecting Top N Values by Group in R",
    "section": "Using base R",
    "text": "Using base R\nWhile dplyr and data.table are powerful packages for data manipulation, base R also provides functionality to achieve this task using functions like split() and lapply().\n\n# Example dataset\ndata &lt;- data.frame(\n  group = c(rep(\"A\", 5), rep(\"B\", 5)),\n  value = c(10, 15, 8, 12, 20, 25, 18, 22, 17, 30)\n)\n\n# Select top 2 values by group using base R\ntop_n_values &lt;- do.call(rbind, lapply(split(data, data$group), function(x) head(x[order(-x$value), ], 2)))\n\n# Convert row names to a column\nrownames(top_n_values) &lt;- NULL\n\n# View the result\nprint(top_n_values)\n\n  group value\n1     A    20\n2     A    15\n3     B    30\n4     B    25\n\n\n\nExplanation\n\nWe start with our sample dataset.\nUsing split(), we split the dataset into subsets based on the ‘group’ column.\nThen, we apply a function using lapply() to each subset, which sorts the values in descending order and selects the top 2 rows using head().\nThe resulting subsets are combined into a single data frame using do.call(rbind, ...)."
  },
  {
    "objectID": "posts/2024-04-25/index.html",
    "href": "posts/2024-04-25/index.html",
    "title": "Simplifying Data Manipulation: How to Drop Columns from Data Frames in R",
    "section": "",
    "text": "As an R programmer, one of the fundamental tasks you’ll encounter is manipulating data frames. Whether you’re cleaning messy data or preparing it for analysis, knowing how to drop unnecessary columns is a valuable skill. In this guide, we’ll walk through the process of dropping columns from data frames in R, using simple examples to demystify the process."
  },
  {
    "objectID": "posts/2024-04-25/index.html#method-1-using-the-operator",
    "href": "posts/2024-04-25/index.html#method-1-using-the-operator",
    "title": "Simplifying Data Manipulation: How to Drop Columns from Data Frames in R",
    "section": "Method 1: Using the $ Operator",
    "text": "Method 1: Using the $ Operator\nOne straightforward way to drop columns from a data frame is by using the $ operator. This method is ideal when you know the exact name of the column you want to remove.\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  A = c(1, 2, 3),\n  B = c(4, 5, 6),\n  C = c(7, 8, 9)\n)\n\n# Drop column B\ndata &lt;- data[, -which(names(data) == \"B\")]\n\n# View the updated data frame\nprint(data)\n\n  A C\n1 1 7\n2 2 8\n3 3 9\n\n\nIn this example, we create a data frame data with columns A, B, and C. To drop column B, we use the which() function to find the index of column B in the names(data) vector and then remove it using negative indexing."
  },
  {
    "objectID": "posts/2024-04-25/index.html#method-2-using-the-subset-function",
    "href": "posts/2024-04-25/index.html#method-2-using-the-subset-function",
    "title": "Simplifying Data Manipulation: How to Drop Columns from Data Frames in R",
    "section": "Method 2: Using the subset() Function",
    "text": "Method 2: Using the subset() Function\nAnother approach to dropping columns is by using the subset() function. This method allows for more flexibility, as you can specify multiple columns to drop at once.\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  A = c(1, 2, 3),\n  B = c(4, 5, 6),\n  C = c(7, 8, 9)\n)\n\n# Drop columns B and C\ndata &lt;- subset(data, select = -c(B, C))\n\n# View the updated data frame\nprint(data)\n\n  A\n1 1\n2 2\n3 3\n\n\nIn this example, we use the select argument of the subset() function to specify the columns we want to keep. By prepending a minus sign to the column names we want to drop, we effectively remove them from the data frame."
  },
  {
    "objectID": "posts/2024-04-25/index.html#method-3-using-the-dplyr-package",
    "href": "posts/2024-04-25/index.html#method-3-using-the-dplyr-package",
    "title": "Simplifying Data Manipulation: How to Drop Columns from Data Frames in R",
    "section": "Method 3: Using the dplyr Package",
    "text": "Method 3: Using the dplyr Package\nFor more complex data manipulation tasks, the dplyr package provides a convenient set of functions. One such function is select(), which allows for intuitive column selection and dropping.\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  A = c(1, 2, 3),\n  B = c(4, 5, 6),\n  C = c(7, 8, 9)\n)\n\n# Drop column B\ndata &lt;- select(data, -B)\n\n# View the updated data frame\nprint(data)\n\n  A C\n1 1 7\n2 2 8\n3 3 9\n\n\nIn this example, we use the select() function from the dplyr package to drop column B from the data frame. The -B argument specifies that we want to exclude column B from the result."
  },
  {
    "objectID": "posts/2024-04-25/index.html#conclusion",
    "href": "posts/2024-04-25/index.html#conclusion",
    "title": "Simplifying Data Manipulation: How to Drop Columns from Data Frames in R",
    "section": "Conclusion",
    "text": "Conclusion\nDropping columns from data frames in R doesn’t have to be a daunting task. By familiarizing yourself with these simple techniques, you can efficiently clean and manipulate your data with ease. I encourage you to try these examples on your own datasets and experiment with different variations. Remember, the best way to learn is by doing!"
  },
  {
    "objectID": "posts/2023-12-06/index.html#empty-model",
    "href": "posts/2023-12-06/index.html#empty-model",
    "title": "A Complete Guide to Stepwise Regression in R",
    "section": "Empty Model:",
    "text": "Empty Model:\nLet’s start with an empty model, an intercept only model.\n\nintercept_model &lt;- lm(mpg ~ 1, data = mtcars)\nstep(intercept_model)\n\nStart:  AIC=115.94\nmpg ~ 1\n\n\n\nCall:\nlm(formula = mpg ~ 1, data = mtcars)\n\nCoefficients:\n(Intercept)  \n      20.09  \n\n\nIn simple terms, we start with a model containing no predictors (mpg ~ 1) and iteratively add the most statistically significant variables until no improvement is observed. Since there are no predictors there is nothing to run through."
  },
  {
    "objectID": "posts/2024-04-26/index.html",
    "href": "posts/2024-04-26/index.html",
    "title": "Exploring strsplit() with Multiple Delimiters in R",
    "section": "",
    "text": "In data preprocessing and text manipulation tasks, the strsplit() function in R is incredibly useful for splitting strings based on specific delimiters. However, what if you need to split a string using multiple delimiters? This is where strsplit() can really shine by allowing you to specify a regular expression that defines these delimiters. In this blog post, we’ll dive into how you can use strsplit() effectively with multiple delimiters to parse strings in your data."
  },
  {
    "objectID": "posts/2024-04-26/index.html#example-1-splitting-with-numbers-as-delimiters",
    "href": "posts/2024-04-26/index.html#example-1-splitting-with-numbers-as-delimiters",
    "title": "Exploring strsplit() with Multiple Delimiters in R",
    "section": "Example 1: Splitting with Numbers as Delimiters",
    "text": "Example 1: Splitting with Numbers as Delimiters\n\ntext &lt;- \"Hello123world456R789users\"\nresult &lt;- strsplit(text, \"[0-9]+\")\n\nIn this case, we use [0-9]+ to split the string wherever there are one or more consecutive digits. The result will be:\n\nresult\n\n[[1]]\n[1] \"Hello\" \"world\" \"R\"     \"users\""
  },
  {
    "objectID": "posts/2024-04-26/index.html#example-2-splitting-urls",
    "href": "posts/2024-04-26/index.html#example-2-splitting-urls",
    "title": "Exploring strsplit() with Multiple Delimiters in R",
    "section": "Example 2: Splitting URLs",
    "text": "Example 2: Splitting URLs\n\nurl &lt;- \"https://www.example.com/path/to/page.html\"\nresult &lt;- strsplit(url, \"[:/\\\\.]\")\n\nHere, we split the URL based on :, /, and . characters. The result will be:\n\nresult\n\n[[1]]\n [1] \"https\"   \"\"        \"\"        \"www\"     \"example\" \"com\"     \"path\"   \n [8] \"to\"      \"page\"    \"html\""
  },
  {
    "objectID": "posts/2024-04-29/index.html",
    "href": "posts/2024-04-29/index.html",
    "title": "Introducing TidyDensity Version 1.4.0: Enhancing Data Analysis in R",
    "section": "",
    "text": "I’m thrilled to announce the release of TidyDensity version 1.4.0, packed with exciting features and improvements to elevate your data analysis experience in R. Let’s dive into what this latest update has to offer."
  },
  {
    "objectID": "posts/2024-04-29/index.html#quantile-normalization",
    "href": "posts/2024-04-29/index.html#quantile-normalization",
    "title": "Introducing TidyDensity Version 1.4.0: Enhancing Data Analysis in R",
    "section": "Quantile Normalization",
    "text": "Quantile Normalization\nSay goodbye to skewed data distributions! With the new quantile_normalize() function, you can now easily normalize your data using quantiles, ensuring more accurate and reliable analysis results."
  },
  {
    "objectID": "posts/2024-04-29/index.html#duplicate-row-detection",
    "href": "posts/2024-04-29/index.html#duplicate-row-detection",
    "title": "Introducing TidyDensity Version 1.4.0: Enhancing Data Analysis in R",
    "section": "Duplicate Row Detection",
    "text": "Duplicate Row Detection\nData integrity matters, which is why we’ve introduced the check_duplicate_rows() function. Quickly identify and eliminate duplicate rows in your data frame, streamlining your workflow and improving data quality."
  },
  {
    "objectID": "posts/2024-04-29/index.html#chi-square-distribution-parameter-estimation",
    "href": "posts/2024-04-29/index.html#chi-square-distribution-parameter-estimation",
    "title": "Introducing TidyDensity Version 1.4.0: Enhancing Data Analysis in R",
    "section": "Chi-Square Distribution Parameter Estimation",
    "text": "Chi-Square Distribution Parameter Estimation\nEstimating parameters for the chi-square distribution is now a breeze with the util_chisquare_param_estimate() function. Empower your statistical analysis with precise parameter estimation capabilities."
  },
  {
    "objectID": "posts/2024-04-29/index.html#markov-chain-monte-carlo-mcmc-sampling",
    "href": "posts/2024-04-29/index.html#markov-chain-monte-carlo-mcmc-sampling",
    "title": "Introducing TidyDensity Version 1.4.0: Enhancing Data Analysis in R",
    "section": "Markov Chain Monte Carlo (MCMC) Sampling",
    "text": "Markov Chain Monte Carlo (MCMC) Sampling\nUnlock the power of Markov Chain Monte Carlo sampling with the new tidy_mcmc_sampling() function. Seamlessly sample from distributions using MCMC, and visualize the results with diagnostic plots for deeper insights into your data."
  },
  {
    "objectID": "posts/2024-04-29/index.html#aic-calculation-for-distributions",
    "href": "posts/2024-04-29/index.html#aic-calculation-for-distributions",
    "title": "Introducing TidyDensity Version 1.4.0: Enhancing Data Analysis in R",
    "section": "AIC Calculation for Distributions",
    "text": "AIC Calculation for Distributions\nMaking informed model selection decisions just got easier! TidyDensity now includes util_dist_aic() functions to calculate the Akaike Information Criterion (AIC) for various distributions, providing valuable metrics for model evaluation."
  },
  {
    "objectID": "posts/2024-04-30/index.html",
    "href": "posts/2024-04-30/index.html",
    "title": "Quantile Normalization in R with the {TidyDensity} Package",
    "section": "",
    "text": "Introduction\nIn data analysis, especially when dealing with multiple samples or distributions, ensuring comparability and removing biases is crucial. One powerful technique for achieving this is quantile normalization. This method aligns the distributions of values across different samples, making them more similar in terms of their statistical properties.\n\n\nWhat is Quantile Normalization?\nQuantile normalization is a statistical method used to adjust the distributions of values in different datasets so that they have similar quantiles. This technique is particularly valuable when working with high-dimensional data, such as gene expression data or other omics datasets, where ensuring comparability across samples is essential.\n\n\nIntroducing quantile_normalize() in TidyDensity\nThe quantile_normalize() function is a new addition to the TidyDensity package, designed to simplify the process of quantile normalization within R. Let’s delve into how this function works and how you can integrate it into your data analysis pipeline.\n\n\nFunction Usage\nThe quantile_normalize() function takes a numeric matrix as input, where each column represents a sample. Here’s a breakdown of its usage:\nquantile_normalize(.data, .return_tibble = FALSE)\n\n.data: A numeric matrix where each column corresponds to a sample that requires quantile normalization.\n.return_tibble: A logical value (default: FALSE) indicating whether the output should be returned as a tibble.\n\n\n\nUnderstanding the Output\nWhen you apply quantile_normalize() to your data, you receive a list object containing the following components:\n\nQuantile-Normalized Matrix: A numeric matrix where each column has been quantile-normalized.\nRow Means: The means of each row across the quantile-normalized matrix.\nSorted Data: The sorted values used during the quantile normalization process.\nRanked Indices: The indices of the sorted values.\n\n\n\nHow Quantile Normalization Works\nThe quantile_normalize() function performs quantile normalization through the following steps:\n\nSorting: Each column of the input matrix is sorted.\nRow Mean Calculation: The mean of each row across the sorted columns is computed.\nNormalization: Each column’s sorted values are replaced with the corresponding row means.\nUnsorting: The columns are restored to their original order, ensuring that the quantile-normalized matrix maintains the same structure as the input.\n\n\n\nExamples\nLet’s demonstrate the usage of quantile_normalize() with a simple example:\n\n# Load TidyDensity\nlibrary(TidyDensity)\n\n# Create a sample matrix\nset.seed(123)\ndata &lt;- matrix(rnorm(50), ncol = 4)\nhead(data, 5)\n\n            [,1]       [,2]       [,3]       [,4]\n[1,] -0.56047565  0.1106827  0.8377870 -0.3804710\n[2,] -0.23017749 -0.5558411  0.1533731 -0.6947070\n[3,]  1.55870831  1.7869131 -1.1381369 -0.2079173\n[4,]  0.07050839  0.4978505  1.2538149 -1.2653964\n[5,]  0.12928774 -1.9666172  0.4264642  2.1689560\n\n# Apply quantile normalization\nresult &lt;- quantile_normalize(data)\n\n# Access the quantile-normalized matrix\nnormalized_matrix &lt;- result[[\"normalized_data\"]]\n\n# View the normalized matrix\nhead(normalized_matrix, 5)\n\n            [,1]       [,2]        [,3]       [,4]\n[1,] -0.65451945 -0.3180877  0.84500772 -0.6545195\n[2,] -0.06327669  0.8450077  1.09078797 -0.9506544\n[3,] -1.40880292 -0.5235134  0.33150422  0.0863713\n[4,]  0.84500772  1.0907880  0.08637130  0.1991151\n[5,] -0.31808774 -0.6545195 -0.06327669  0.3315042\n\n\nLet’s now look at the rest of the output components:\n\nhead(result[[\"row_means\"]], 5)\n\n[1] -1.4088029 -0.9506544 -0.6545195 -0.5235134 -0.3180877\n\nhead(result[[\"duplicated_ranks\"]], 5)\n\n     [,1] [,2] [,3] [,4]\n[1,]    9   13   13    7\n[2,]   10   10   12   12\n[3,]    2   11    2    9\n[4,]   13    9    9    3\n[5,]    7    1    1   11\n\nhead(result[[\"duplicated_rank_row_indicies\"]], 5)\n\nNULL\n\nhead(result[[\"duplicated_rank_data\"]], 5)\n\n            [,1]       [,2]      [,3]       [,4]\n[1,] -0.23017749 -0.5558411 0.1533731 -0.6947070\n[2,]  0.07050839  0.4978505 1.2538149 -1.2653964\n[3,]  0.12928774 -1.9666172 0.4264642  2.1689560\n[4,] -0.68685285 -0.2179749 0.8215811 -0.4666554\n[5,] -0.44566197 -1.0260044 0.6886403  0.7799651\n\n\nNow, lets take a look at the before and after quantile normalization summary:\n\nas.data.frame(data) |&gt;\n  sapply(function(x) quantile(x, probs = seq(0, 1, 1/4)))\n\n             V1         V2          V3          V4\n0%   -1.2650612 -1.9666172 -1.13813694 -1.26539635\n25%  -0.4456620 -1.0260044 -0.06191171 -0.56047565\n50%   0.1292877 -0.5558411  0.55391765 -0.38047100\n75%   0.4609162  0.1106827  0.83778704 -0.08336907\n100%  1.7150650  1.7869131  1.25381492  2.16895597\n\nas.data.frame(normalized_matrix) |&gt;\n  sapply(function(x) quantile(x, probs = seq(0, 1, 1/4)))\n\n              V1          V2          V3          V4\n0%   -1.40880292 -1.40880292 -1.40880292 -1.40880292\n25%  -0.52351344 -0.52351344 -0.52351344 -0.52351344\n50%  -0.06327669 -0.06327669 -0.06327669 -0.06327669\n75%   0.33150422  0.33150422  0.33150422  0.33150422\n100%  1.73118725  1.73118725  1.73118725  1.73118725\n\n\nNow let’s use the .return_tibble argument to return the output as a tibble:\n\nquantile_normalize(data, .return_tibble = TRUE)\n\n$normalized_data\n# A tibble: 13 × 4\n        V1      V2      V3      V4\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.655  -0.318   0.845  -0.655 \n 2 -0.0633  0.845   1.09   -0.951 \n 3 -1.41   -0.524   0.332   0.0864\n 4  0.845   1.09    0.0864  0.199 \n 5 -0.318  -0.655  -0.0633  0.332 \n 6  1.73   -0.0633 -0.133  -0.133 \n 7 -0.524  -0.133  -0.524  -0.524 \n 8 -0.133   1.73    1.73    1.73  \n 9  0.332   0.0864  0.199   1.09  \n10  1.09   -0.951  -0.655  -0.318 \n11 -0.951  -1.41   -0.318  -1.41  \n12  0.199   0.199  -1.41    0.845 \n13  0.0864  0.332  -0.951  -0.0633\n\n$row_means\n# A tibble: 13 × 1\n     value\n     &lt;dbl&gt;\n 1 -1.41  \n 2 -0.951 \n 3 -0.655 \n 4 -0.524 \n 5 -0.318 \n 6 -0.133 \n 7 -0.0633\n 8  0.0864\n 9  0.199 \n10  0.332 \n11  0.845 \n12  1.09  \n13  1.73  \n\n$duplicated_ranks\n# A tibble: 6 × 4\n     V1    V2    V3    V4\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     9    13    13     7\n2    10    10    12    12\n3     2    11     2     9\n4    13     9     9     3\n5     7     1     1    11\n6     3     6     7     6\n\n$duplicated_rank_row_indices\n# A tibble: 6 × 1\n  row_index\n      &lt;int&gt;\n1         2\n2         4\n3         5\n4         9\n5        10\n6        12\n\n$duplicated_rank_data\n# A tibble: 6 × 4\n       V1     V2      V3     V4\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 -0.230  -0.556  0.153  -0.695\n2  0.0705  0.498  1.25   -1.27 \n3  0.129  -1.97   0.426   2.17 \n4 -0.687  -0.218  0.822  -0.467\n5 -0.446  -1.03   0.689   0.780\n6  0.360  -0.625 -0.0619 -0.560\n\n\n\nConclusion\nIn summary, the quantile_normalize() function from the TidyDensity package offers a convenient and efficient way to perform quantile normalization on numeric matrices in R. By leveraging this function, you can enhance the comparability and statistical integrity of your data across multiple samples or distributions. Incorporate quantile_normalize() into your data preprocessing workflow to unlock deeper insights and more robust analyses.\nTo explore more functionalities of TidyDensity and leverage its capabilities for advanced data analysis tasks, check out the package documentation and experiment with different parameters and options provided by the quantile_normalize() function."
  },
  {
    "objectID": "posts/2024-05-01/index.html",
    "href": "posts/2024-05-01/index.html",
    "title": "Introducing check_duplicate_rows() from TidyDensity",
    "section": "",
    "text": "Today, we’re diving into a useful new function from the TidyDensity R package: check_duplicate_rows(). This function is designed to efficiently identify duplicate rows within a data frame, providing a logical vector that flags each row as either a duplicate or unique. Let’s explore how this function works and see it in action with some illustrative examples."
  },
  {
    "objectID": "posts/2024-05-01/index.html#example-1-no-duplicates",
    "href": "posts/2024-05-01/index.html#example-1-no-duplicates",
    "title": "Introducing check_duplicate_rows() from TidyDensity",
    "section": "Example 1: No Duplicates",
    "text": "Example 1: No Duplicates\nFirst, let’s create a data frame where all rows are unique. We’ll use the iris dataset for this example:\n\n# Load required libraries\nlibrary(TidyDensity)\n\n# Create a data frame (iris dataset)\ndata_no_duplicates &lt;- iris\n\n# Check for duplicate rows\nduplicates &lt;- check_duplicate_rows(data_no_duplicates)\n\n# View the result\nany(duplicates)\n\n[1] FALSE\n\n\nIn this case, the duplicates vector will contain only FALSE values, indicating that no rows in iris are exact duplicates of each other."
  },
  {
    "objectID": "posts/2024-05-01/index.html#example-2-duplicate-rows",
    "href": "posts/2024-05-01/index.html#example-2-duplicate-rows",
    "title": "Introducing check_duplicate_rows() from TidyDensity",
    "section": "Example 2: Duplicate Rows",
    "text": "Example 2: Duplicate Rows\nNext, let’s create a scenario where some rows contain identical values in specific columns. We’ll manually construct a data frame for this purpose:\n\n# Create a data frame with duplicate rows\ndata_with_duplicates &lt;- data.frame(\n  Name = c(\"John\", \"Alice\", \"John\", \"Bob\", \"Alice\",\"David\"),\n  Age = c(25, 30, 25, 40, 30, 50),\n  Score = c(85, 90, 85, 75, 90, 50)\n)\n\n# Check for duplicate rows\nduplicates &lt;- check_duplicate_rows(data_with_duplicates)\n\n# View the result\nduplicates\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nIn this example, the duplicates vector will indicate which rows are duplicates (TRUE for duplicates, FALSE for unique rows). You’ll notice that the last row is flagged as a duplicate because there is the same value for the Age and Score columns."
  },
  {
    "objectID": "posts/2024-05-02/index.html",
    "href": "posts/2024-05-02/index.html",
    "title": "Estimating Chisquare Parameters with TidyDensity",
    "section": "",
    "text": "Introduction\nHello R users! Today, let’s explore the latest addition to the TidyDensity package: util_chisquare_param_estimate(). This function is designed to estimate parameters for a Chi-square distribution from your data, providing valuable insights into the underlying distribution characteristics.\n\n\nUnderstanding the Purpose\nThe util_chisquare_param_estimate() function is a powerful tool for analyzing data that conforms to a Chi-square distribution. It utilizes maximum likelihood estimation (MLE) to infer the degrees of freedom (dof) and non-centrality parameter (ncp) of the Chi-square distribution based on your input vector.\n\n\nGetting Started\nTo begin, let’s generate a dataset that conforms to a Chi-square distribution:\n\nlibrary(TidyDensity)\n\n# Generate Chi-square distributed data\nset.seed(123)\ndata &lt;- rchisq(250, 10, 2)\n\n# Call util_chisquare_param_estimate()\nresult &lt;- util_chisquare_param_estimate(data)\n\nBy default, the function will automatically generate empirical distribution data if .auto_gen_empirical is set to TRUE. This means you’ll not only get the Chi-square parameters but also a combined table of empirical and Chi-square distribution data.\n\n\nExploring the Output\nLet’s unpack what the function returns:\n\ndist_type: Identifies the type of distribution, which will be “Chisquare” for this analysis.\nsamp_size: Indicates the sample size, i.e., the number of data points in your vector .x.\nmin, max, mean: Basic statistics summarizing your data.\ndof: The estimated degrees of freedom for the Chi-square distribution.\nncp: The estimated non-centrality parameter for the Chi-square distribution.\n\nThis comprehensive output allows you to gain deeper insights into your data’s distribution characteristics, particularly when the Chi-square distribution is a potential model.\nLet’s now take a look at the output itself.\n\nlibrary(dplyr)\n\nresult$combined_data_tbl |&gt;\n  head(5) |&gt;\n  glimpse()\n\nRows: 5\nColumns: 8\n$ sim_number &lt;fct&gt; 1, 1, 1, 1, 1\n$ x          &lt;int&gt; 1, 2, 3, 4, 5\n$ y          &lt;dbl&gt; 12.716908, 17.334453, 11.913559, 15.252845, 7.208524\n$ dx         &lt;dbl&gt; -2.100590, -1.952295, -1.803999, -1.655704, -1.507408\n$ dy         &lt;dbl&gt; 2.741444e-05, 3.676673e-05, 4.930757e-05, 6.515313e-05, 8.6…\n$ p          &lt;dbl&gt; 0.640, 0.848, 0.576, 0.744, 0.204\n$ q          &lt;dbl&gt; 2.765968, 3.205658, 3.297085, 3.567437, 3.869764\n$ dist_type  &lt;fct&gt; \"Empirical\", \"Empirical\", \"Empirical\", \"Empirical\", \"Empiri…\n\nresult$combined_data_tbl |&gt;\n  tidy_distribution_summary_tbl(dist_type) |&gt;\n  glimpse()\n\nRows: 2\nColumns: 13\n$ dist_type  &lt;fct&gt; \"Empirical\", \"Chisquare c(9.961, 1.979)\"\n$ mean_val   &lt;dbl&gt; 11.95263, 12.04686\n$ median_val &lt;dbl&gt; 10.79615, 11.48777\n$ std_val    &lt;dbl&gt; 5.438087, 5.349567\n$ min_val    &lt;dbl&gt; 2.765968, 1.922223\n$ max_val    &lt;dbl&gt; 29.95844, 30.43480\n$ skewness   &lt;dbl&gt; 0.9344797, 0.6903444\n$ kurtosis   &lt;dbl&gt; 3.790972, 3.243122\n$ range      &lt;dbl&gt; 27.19248, 28.51258\n$ iqr        &lt;dbl&gt; 7.469292, 7.282262\n$ variance   &lt;dbl&gt; 29.57279, 28.61787\n$ ci_low     &lt;dbl&gt; 4.010739, 3.997601\n$ ci_high    &lt;dbl&gt; 26.33689, 23.60014\n\n\n\n\nBehind the Scenes: MLE Optimization\nUnder the hood, the function leverages MLE through the optim() function to estimate the Chi-square parameters. It minimizes the negative log-likelihood function to obtain the best-fitting degrees of freedom (dof) and non-centrality parameter (ncp) for your data.\nInitial values for the optimization are intelligently set based on your data’s sample variance and mean, ensuring a robust estimation process.\n\n\nVisualizing the Results\nOne of the strengths of TidyDensity is its seamless integration with visualization tools like ggplot2. With the combined output from util_chisquare_param_estimate(), you can easily create insightful plots that compare the empirical distribution with the estimated Chi-square distribution.\n\nresult$combined_data_tbl |&gt;\n  tidy_combined_autoplot()\n\n\n\n\n\n\n\n\nThis example demonstrates how you can visualize the empirical data overlaid with the fitted Chi-square distribution, providing a clear representation of your dataset’s fit to the model.\n\n\nConclusion\nIn summary, util_chisquare_param_estimate() from TidyDensity is a versatile tool for estimating Chi-square distribution parameters from your data. Whether you’re exploring the underlying distribution of your dataset or conducting statistical inference, this function equips you with the necessary tools to gain valuable insights.\nIf you haven’t already, give it a try and let us know how you’re using TidyDensity to enhance your data analysis workflows! Stay tuned for more updates and insights from the world of R programming. Happy coding!"
  },
  {
    "objectID": "posts/2024-05-03/index.html",
    "href": "posts/2024-05-03/index.html",
    "title": "Exploring Data with TidyDensity’s tidy_mcmc_sampling()",
    "section": "",
    "text": "Introduction\nIn the area of statistical modeling and Bayesian inference, Markov Chain Monte Carlo (MCMC) methods are indispensable tools for tackling complex problems. The new tidy_mcmc_sampling() function in the TidyDensity R package simplifies MCMC sampling and visualization, making it accessible to a broader audience of data enthusiasts and analysts.\n\n\nUnderstanding MCMC\nBefore we dive into the practical use of tidy_mcmc_sampling(), let’s briefly discuss why MCMC is valuable. MCMC methods are particularly useful when dealing with Bayesian statistics, where exact analytical solutions are challenging or impossible due to the complexity of the models involved.\nMCMC allows us to draw samples from a probability distribution, especially in cases where direct sampling is impractical. This is achieved by constructing a Markov chain that converges to the desired distribution after a sufficient number of iterations. Once converged, these samples can provide insights into the posterior distribution of parameters, allowing us to make probabilistic inferences.\n\n\nIntroducing tidy_mcmc_sampling()\nThe tidy_mcmc_sampling() function in TidyDensity harnesses the power of MCMC sampling and presents the results in a tidy format, facilitating further analysis and visualization. Let’s explore its usage and capabilities.\n\n\nUsage Example\nSuppose we have a dataset data that we want to analyze using MCMC sampling:\n\nlibrary(TidyDensity)\n\n# Generate MCMC samples\nset.seed(123)\ndata &lt;- rnorm(100)\nresult &lt;- tidy_mcmc_sampling(data, .fns = \"median\", .cum_fns = \"cmedian\")\nresult\n\n$mcmc_data\n# A tibble: 4,000 × 3\n   sim_number name                 value\n   &lt;fct&gt;      &lt;fct&gt;                &lt;dbl&gt;\n 1 1          .sample_median    -0.0285 \n 2 1          .cum_stat_cmedian -0.0285 \n 3 2          .sample_median     0.239  \n 4 2          .cum_stat_cmedian  0.105  \n 5 3          .sample_median     0.00576\n 6 3          .cum_stat_cmedian  0.00576\n 7 4          .sample_median    -0.0357 \n 8 4          .cum_stat_cmedian -0.0114 \n 9 5          .sample_median    -0.111  \n10 5          .cum_stat_cmedian -0.0285 \n# ℹ 3,990 more rows\n\n$plt\n\n\n\n\n\n\n\n\n\nIn this example: - We generate 100 random normal values using rnorm(100). - The tidy_mcmc_sampling() function is then applied to this data, specifying that we want to compute the median (\"median\") of each MCMC sample and the cumulative median (\"cmedian\") across all samples, here the default sample size is 2000.\n\n\nKey Arguments\n\n.x: The input data vector for MCMC sampling.\n.fns: A character vector specifying the function(s) to apply to each MCMC sample. By default, it computes the mean (\"mean\"), but you can customize this to any function that makes sense for your analysis.\n.cum_fns: A character vector specifying the function(s) to apply to the cumulative MCMC samples. The default is to compute the cumulative mean (\"cmean\"), but you can change this based on your requirements.\n.num_sims: The number of MCMC simulations to run. More simulations generally lead to more accurate results but can be computationally expensive. The default is 2000.\n\n\n\nVisualizing Results\nThe tidy_mcmc_sampling() function not only returns tidy data but also generates a plot to visualize the MCMC samples and cumulative statistics. This visualization is essential for understanding the distribution of samples and how they evolve over iterations.\n\n\nTry It Yourself!\nIf you’re intrigued by the capabilities of MCMC and want to explore it in your data analysis workflow, I encourage you to try out tidy_mcmc_sampling() with your own datasets and custom functions. Experiment with different parameters and visualize the results to gain deeper insights into your data.\nIn conclusion, tidy_mcmc_sampling() extends the functionality of TidyDensity by offering a user-friendly interface for conducting MCMC sampling and analysis. Whether you’re new to Bayesian statistics or a seasoned practitioner, this function can streamline your workflow and enhance your understanding of complex datasets. Give it a spin and unlock new possibilities in your data exploration journey!"
  },
  {
    "objectID": "posts/2024-05-06/index.html",
    "href": "posts/2024-05-06/index.html",
    "title": "Exploring Model Selection with TidyDensity: Understanding AIC for Statistical Distributions",
    "section": "",
    "text": "Introduction\nIn the world of data analysis and statistics, one of the key challenges is selecting the best model to describe and analyze your data. This decision is crucial because it impacts the accuracy and reliability of your results. Among the many tools available, the Akaike Information Criterion (AIC) stands out as a powerful method for comparing different models and choosing the most suitable one.\nToday we will go through an example of model selection using the AIC, specifically focusing on its application to various statistical distributions available in the TidyDensity package. TidyDensity, a part of the healthyverse ecosystem, offers a comprehensive suite of tools for data analysis in R, including functions to compute AIC scores for different probability distributions.\n\n\nWhat is AIC?\nThe Akaike Information Criterion (AIC) is a mathematical tool used for model selection. It balances the goodness of fit of a model with its complexity, penalizing overly complex models to prevent overfitting. In simpler terms, AIC helps us choose the most effective model that explains our data without being too complex.\n\n\nExploring TidyDensity’s Distribution Functions\nTidyDensity provides a range of utility functions prefixed with util_ that calculate the AIC for specific probability distributions. Let’s take a closer look at some of these functions:\n\nBeta Distribution (util_beta_aic()): Computes the AIC for a beta distribution, which is often used to model random variables constrained to the interval [0, 1].\nBinomial Distribution (util_binomial_aic()): Calculates the AIC for a binomial distribution, commonly used to model the number of successes in a fixed number of independent trials.\nCauchy Distribution (util_cauchy_aic()): Computes the AIC for a Cauchy distribution, known for its symmetric bell-shaped curve.\nExponential Distribution (util_exponential_aic()): Determines the AIC for an exponential distribution, frequently used to model the time between events in a Poisson process.\nNormal Distribution (util_normal_aic()): Computes the AIC for a normal distribution, which is ubiquitous in statistics due to the central limit theorem.\n\nThese are just a few examples of the distribution-specific AIC functions available in TidyDensity. Each function evaluates the goodness of fit of a particular distribution to your data and provides an AIC score, aiding in the selection of the most appropriate model.\n\n\nHow to Use AIC for Model Selection\nUsing these functions in TidyDensity is straightforward. Simply pass your data to the desired distribution function, and it will return the AIC score. Lower AIC values indicate a better fit, so the distribution with the lowest AIC is typically chosen as the optimal model.\nHere’s a simplified example of how you might use these functions:\n\n# Load TidyDensity library\nlibrary(TidyDensity)\n\n# Generate some sample data\ndata &lt;- rnorm(100, mean = 0, sd = 1)\n\n# Compute AIC for normal distribution\nnormal_aic &lt;- util_normal_aic(data)\n\n# Compute AIC for exponential distribution\ncauchy_aic &lt;- util_cauchy_aic(data)\n\n# Compare AIC scores\nif (normal_aic &lt; cauchy_aic) {\n  print(\"Normal distribution is a better fit.\")\n} else {\n  print(\"Cauchy distribution is a better fit.\")\n}\n\n[1] \"Normal distribution is a better fit.\"\n\ncat(\"Normal AIC: \", normal_aic, \"\\n\")\n\nNormal AIC:  285.9777 \n\ncat(\"Cauchy AIC: \", cauchy_aic)\n\nCauchy AIC:  317.1025\n\n\n\n\nConclusion\nIn conclusion, the Akaike Information Criterion (AIC) plays a crucial role in statistical modeling and model selection. The TidyDensity package enhances this capability by providing specialized functions to compute AIC scores for various probability distributions. By leveraging these functions, data analysts and researchers can make informed decisions about which distribution best describes their data, leading to more robust and accurate statistical analyses.\nIf you’re interested in harnessing the power of AIC and exploring different probability distributions in R, be sure to check out TidyDensity and incorporate these tools into your data analysis toolkit. Happy modeling!"
  },
  {
    "objectID": "posts/2024-05-07/index.html",
    "href": "posts/2024-05-07/index.html",
    "title": "Counting NA Values in Each Column: Comparing Methods in R",
    "section": "",
    "text": "Welcome back, R enthusiasts! Today, we’re going to explore a fundamental task in data analysis: counting the number of missing (NA) values in each column of a dataset. This might seem straightforward, but there are different ways to achieve this using different packages and methods in R.\nLet’s dive right in and compare how to accomplish this task using base R, dplyr, and data.table. Each method has its own strengths and can cater to different preferences and data handling scenarios."
  },
  {
    "objectID": "posts/2024-05-07/index.html#using-base-r",
    "href": "posts/2024-05-07/index.html#using-base-r",
    "title": "Counting NA Values in Each Column: Comparing Methods in R",
    "section": "Using Base R",
    "text": "Using Base R\nFirst up, let’s tackle this using base R functions. We’ll leverage the colSums() function along with is.na() to count NA values in each column of a dataframe.\n\n# Sample dataframe\ndf &lt;- data.frame(\n  A = c(1, 2, NA, 4),\n  B = c(NA, 2, 3, NA),\n  C = c(1, NA, NA, 4)\n)\n\n# Count NA values in each column using base R\nna_counts_base &lt;- colSums(is.na(df))\nprint(na_counts_base)\n\nA B C \n1 2 2 \n\n\nIn this code snippet, is.na(df) creates a logical matrix indicating NA positions in df. colSums() then sums up the TRUE values (which represent NA) across each column, giving us the count of NAs per column. Simple and effective!"
  },
  {
    "objectID": "posts/2024-05-07/index.html#using-base-r-with-lapply",
    "href": "posts/2024-05-07/index.html#using-base-r-with-lapply",
    "title": "Counting NA Values in Each Column: Comparing Methods in R",
    "section": "Using Base R (with lapply)",
    "text": "Using Base R (with lapply)\nTo adapt this method for base R, we can directly apply lapply() to the dataframe (df) to achieve the same result.\n\n# Count NA values in each column using base R and lapply\nna_counts_base &lt;- lapply(df, function(x) sum(is.na(x)))\n\nprint(na_counts_base)\n\n$A\n[1] 1\n\n$B\n[1] 2\n\n$C\n[1] 2\n\n\nIn this snippet, lapply(df, function(x) sum(is.na(x))) applies the function function(x) sum(is.na(x)) to each column of the dataframe (df), resulting in a list of NA counts per column."
  },
  {
    "objectID": "posts/2024-05-07/index.html#using-dplyr",
    "href": "posts/2024-05-07/index.html#using-dplyr",
    "title": "Counting NA Values in Each Column: Comparing Methods in R",
    "section": "Using dplyr",
    "text": "Using dplyr\nNow, let’s switch gears and utilize the popular dplyr package to achieve the same task in a more streamlined manner.\n\nlibrary(dplyr)\n\n# Count NA values in each column using dplyr\nna_counts_dplyr &lt;- df %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\nprint(na_counts_dplyr)\n\n  A B C\n1 1 2 2\n\n\nHere, summarise_all() from dplyr applies the sum(is.na(.)) function to each column (. represents each column in this context), providing us with the count of NA values in each. This approach is clean and fits well into a tidyverse workflow."
  },
  {
    "objectID": "posts/2024-05-07/index.html#using-data.table",
    "href": "posts/2024-05-07/index.html#using-data.table",
    "title": "Counting NA Values in Each Column: Comparing Methods in R",
    "section": "Using data.table",
    "text": "Using data.table\nLast but not least, let’s see how to accomplish this using data.table, a powerful package known for its efficiency with large datasets.\n\nlibrary(data.table)\n\n# Convert dataframe to data.table\ndt &lt;- as.data.table(df)\n\n# Count NA values in each column using data.table\nna_counts_data_table &lt;- dt[, lapply(.SD, function(x) sum(is.na(x)))]\n\nprint(na_counts_data_table)\n\n       A     B     C\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1:     1     2     2\n\n\nIn this snippet, lapply(.SD, function(x) sum(is.na(x))) within data.table allows us to apply the sum(is.na()) function to each column (.SD represents the Subset of Data for each group, which in this case is each column)."
  },
  {
    "objectID": "posts/2024-05-08/index.html",
    "href": "posts/2024-05-08/index.html",
    "title": "How to Select Columns by Index in R (Using Base R)",
    "section": "",
    "text": "When working with data frames in R, it’s common to need to select specific columns based on their index positions. This task is straightforward in R, especially with base functions. In this article, we’ll explore how to select columns by their index using simple and effective techniques in base R."
  },
  {
    "objectID": "posts/2024-05-08/index.html#example-1-selecting-single-column-by-index",
    "href": "posts/2024-05-08/index.html#example-1-selecting-single-column-by-index",
    "title": "How to Select Columns by Index in R (Using Base R)",
    "section": "Example 1: Selecting Single Column by Index",
    "text": "Example 1: Selecting Single Column by Index\nSuppose we have a data frame df with several columns, and we want to select the second column. Here’s how you can do it:\n\n# Create a sample data frame\ndf &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = c(25, 30, 28),\n  Score = c(88, 92, 75)\n)\n\n# Select the second column by index (Age)\nselected_column &lt;- df[, 2]\n\nprint(selected_column)\n\n[1] 25 30 28\n\n\nIn this code snippet:\n\ndf[, 2] specifies that we want to select all rows ([,]) from the second column (2) of the data frame df.\nThe result (selected_column) will be a vector containing the values from the “Age” column."
  },
  {
    "objectID": "posts/2024-05-08/index.html#example-2-selecting-multiple-columns-by-indices",
    "href": "posts/2024-05-08/index.html#example-2-selecting-multiple-columns-by-indices",
    "title": "How to Select Columns by Index in R (Using Base R)",
    "section": "Example 2: Selecting Multiple Columns by Indices",
    "text": "Example 2: Selecting Multiple Columns by Indices\nTo select multiple columns simultaneously, you can provide a vector of column indices within the square brackets. For instance, if we want to select the first and third columns from df:\n\n# Select the first and third columns by indices (Name and Score)\nselected_columns &lt;- df[, c(1, 3)]\n\nprint(selected_columns)\n\n     Name Score\n1   Alice    88\n2     Bob    92\n3 Charlie    75\n\n\nIn this example:\n\ndf[, c(1, 3)] selects all rows ([,]) from the first and third columns (c(1, 3)) of the data frame df.\nThe result (selected_columns) will be a subset of df containing only the “Name” and “Score” columns."
  },
  {
    "objectID": "posts/2024-05-08/index.html#example-3-selecting-all-columns-except-one",
    "href": "posts/2024-05-08/index.html#example-3-selecting-all-columns-except-one",
    "title": "How to Select Columns by Index in R (Using Base R)",
    "section": "Example 3: Selecting All Columns Except One",
    "text": "Example 3: Selecting All Columns Except One\nIf you want to exclude specific columns while selecting all others, you can use negative indexing. For instance, to select all columns except the second one:\n\n# Select all columns except the second one (Age)\nselected_columns &lt;- df[, -2]\n\nprint(selected_columns)\n\n     Name Score\n1   Alice    88\n2     Bob    92\n3 Charlie    75\n\n\nHere:\n\ndf[, -2] selects all rows ([,]) from df, excluding the second column (-2).\nThe result (selected_columns) will be a data frame containing columns “Name” and “Score”, excluding “Age”."
  },
  {
    "objectID": "posts/2024-05-09/index.html",
    "href": "posts/2024-05-09/index.html",
    "title": "How to Collapse Text by Group in a Data Frame Using R",
    "section": "",
    "text": "When working with data frames in R, you may often encounter scenarios where you need to collapse or concatenate text values based on groups within your dataset. This could involve combining text from multiple rows into a single row per group, which can be useful for summarizing data or preparing it for further analysis. In this post, we’ll explore how to achieve this task using different methods in R—specifically using base R, the dplyr package, and the data.table package."
  },
  {
    "objectID": "posts/2024-05-09/index.html#using-base-r",
    "href": "posts/2024-05-09/index.html#using-base-r",
    "title": "How to Collapse Text by Group in a Data Frame Using R",
    "section": "Using Base R",
    "text": "Using Base R\nIn base R, you can use aggregate() to collapse text values by group. Let’s say we want to collapse the Product column by CustomerID:\n\n# Collapse text by CustomerID using base R\ncollapsed_df &lt;- aggregate(Product ~ CustomerID, data = df, FUN = function(x) paste(x, collapse = \", \"))\n\n# Print the result\nprint(collapsed_df)\n\n  CustomerID       Product\n1          1 Apple, Orange\n2          2 Banana, Peach\n3          3        Grapes\n\n\nHere, we used aggregate() to group the Product column by CustomerID and applied a custom function to concatenate the text values separated by commas."
  },
  {
    "objectID": "posts/2024-05-09/index.html#using-dplyr",
    "href": "posts/2024-05-09/index.html#using-dplyr",
    "title": "How to Collapse Text by Group in a Data Frame Using R",
    "section": "Using dplyr",
    "text": "Using dplyr\nThe dplyr package provides a concise way to manipulate data frames. We can achieve the same result using dplyr’s group_by() and summarise() functions:\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Collapse text by CustomerID using dplyr\ncollapsed_df &lt;- df %&gt;%\n  group_by(CustomerID) %&gt;%\n  summarise(Product = paste(Product, collapse = \", \"))\n\n# Print the result\nprint(collapsed_df)\n\n# A tibble: 3 × 2\n  CustomerID Product      \n       &lt;dbl&gt; &lt;chr&gt;        \n1          1 Apple, Orange\n2          2 Banana, Peach\n3          3 Grapes"
  },
  {
    "objectID": "posts/2024-05-09/index.html#using-data.table",
    "href": "posts/2024-05-09/index.html#using-data.table",
    "title": "How to Collapse Text by Group in a Data Frame Using R",
    "section": "Using data.table",
    "text": "Using data.table\nFor larger datasets, the data.table package can offer efficient solutions. Here’s how you can collapse text by group using data.table:\n\n# Load the data.table package\nlibrary(data.table)\n\n# Convert data.frame to data.table\nsetDT(df)\n\n# Collapse text by CustomerID using data.table\ncollapsed_df &lt;- df[, .(Product = paste(Product, collapse = \", \")), by = CustomerID]\n\n# Print the result\nprint(collapsed_df)\n\n   CustomerID       Product\n        &lt;num&gt;        &lt;char&gt;\n1:          1 Apple, Orange\n2:          2 Banana, Peach\n3:          3        Grapes"
  },
  {
    "objectID": "posts/2024-05-10/index.html",
    "href": "posts/2024-05-10/index.html",
    "title": "How to Check if a Column Contains a String in R",
    "section": "",
    "text": "Whether you’re doing some data cleaning or exploring your dataset, checking if a column contains a specific string can be a crucial task. Today, I’ll show you how to do this using both str_detect() from the stringr package and base R methods. We’ll also tackle finding partial strings and counting occurrences. Let’s dive right in!"
  },
  {
    "objectID": "posts/2024-05-10/index.html#using-stringr",
    "href": "posts/2024-05-10/index.html#using-stringr",
    "title": "How to Check if a Column Contains a String in R",
    "section": "Using stringr",
    "text": "Using stringr\n\nCheck for Full String\nSuppose we want to check if any of the description column contains “Data analyst”:\n\n# Detect if 'description' contains 'Data analyst'\ndata$has_data_analyst &lt;- str_detect(data$description, \"Data analyst\")\nprint(data)\n\n   name        description has_data_analyst\n1 Alice Software developer            FALSE\n2   Bob       Data analyst             TRUE\n3 Carol        UX designer            FALSE\n4  Dave    Project manager            FALSE\n5   Eve     Data scientist            FALSE\n\n\nIn the output, the has_data_analyst column will be TRUE for “Bob” and FALSE for others.\n\n\nCheck for Partial String\nLet’s expand our search to any string containing “Data”:\n\n# Detect if 'description' contains any word with 'Data'\ndata$has_data &lt;- str_detect(data$description, \"Data\")\nprint(data)\n\n   name        description has_data_analyst has_data\n1 Alice Software developer            FALSE    FALSE\n2   Bob       Data analyst             TRUE     TRUE\n3 Carol        UX designer            FALSE    FALSE\n4  Dave    Project manager            FALSE    FALSE\n5   Eve     Data scientist            FALSE     TRUE\n\n\nThis will show TRUE for “Bob” and “Eve,” where both “Data analyst” and “Data scientist” are detected.\n\n\nCount Occurrences\nIf you need to count how many times “Data” appears, use str_count:\n\n# Count occurrences of 'Data'\ndata$data_count &lt;- str_count(data$description, \"Data\")\nprint(data)\n\n   name        description has_data_analyst has_data data_count\n1 Alice Software developer            FALSE    FALSE          0\n2   Bob       Data analyst             TRUE     TRUE          1\n3 Carol        UX designer            FALSE    FALSE          0\n4  Dave    Project manager            FALSE    FALSE          0\n5   Eve     Data scientist            FALSE     TRUE          1\n\n\nThis will add a column data_count with the exact count of occurrences per row."
  },
  {
    "objectID": "posts/2024-05-10/index.html#using-base-r",
    "href": "posts/2024-05-10/index.html#using-base-r",
    "title": "How to Check if a Column Contains a String in R",
    "section": "Using Base R",
    "text": "Using Base R\nFor those who prefer base R, the grepl and gregexpr functions can help.\n\nCheck for Full or Partial String\ngrepl is ideal for checking if a string is present:\n\n# Using grepl for full/partial string detection\ndata$has_data_grepl &lt;- grepl(\"Data\", data$description)\nprint(data)\n\n   name        description has_data_analyst has_data data_count has_data_grepl\n1 Alice Software developer            FALSE    FALSE          0          FALSE\n2   Bob       Data analyst             TRUE     TRUE          1           TRUE\n3 Carol        UX designer            FALSE    FALSE          0          FALSE\n4  Dave    Project manager            FALSE    FALSE          0          FALSE\n5   Eve     Data scientist            FALSE     TRUE          1           TRUE\n\n\nThis will yield the same output as str_detect.\n\n\nCount Occurrences\nFor counting occurrences, gregexpr is helpful:\n\n# Count occurrences using gregexpr\nmatches &lt;- gregexpr(\"Data\", data$description)\ndata$data_count_base &lt;- sapply(\n  matches, \n  function(x) ifelse(x[1] == -1, 0, length(x))\n  )\nprint(data)\n\n   name        description has_data_analyst has_data data_count has_data_grepl\n1 Alice Software developer            FALSE    FALSE          0          FALSE\n2   Bob       Data analyst             TRUE     TRUE          1           TRUE\n3 Carol        UX designer            FALSE    FALSE          0          FALSE\n4  Dave    Project manager            FALSE    FALSE          0          FALSE\n5   Eve     Data scientist            FALSE     TRUE          1           TRUE\n  data_count_base\n1               0\n2               1\n3               0\n4               0\n5               1\n\n\nThis will add a new data_count_base column containing the count of “Data” in each row."
  },
  {
    "objectID": "posts/2024-05-13/index.html",
    "href": "posts/2024-05-13/index.html",
    "title": "How to Check if a Column Exists in a Data Frame in R",
    "section": "",
    "text": "When working with data frames in R, it’s common to need to check whether a specific column exists. This is particularly useful in data cleaning and preprocessing, to ensure your scripts don’t throw errors if a column is missing. Today, we’ll explore several methods to perform this check efficiently in R, and I encourage you to try these methods out with your own data sets."
  },
  {
    "objectID": "posts/2024-05-13/index.html#example-1-using-the-in-operator",
    "href": "posts/2024-05-13/index.html#example-1-using-the-in-operator",
    "title": "How to Check if a Column Exists in a Data Frame in R",
    "section": "Example 1: Using the %in% Operator",
    "text": "Example 1: Using the %in% Operator\nThe %in% operator is one of the simplest ways to check if a column exists in a data frame. This operator checks for membership and returns TRUE if the specified item is found in the given vector or list.\n\nCode:\n\n# Sample data frame\ndf &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35)\n)\n\n# Check if 'age' column exists\n\"age\" %in% names(df)\n\n[1] TRUE\n\n\n\n\nExplanation:\nIn this code, names(df) retrieves a vector of the column names from the data frame df. The %in% operator then checks whether \"age\" is one of the elements in this vector. If \"age\" exists, it returns TRUE; otherwise, it returns FALSE."
  },
  {
    "objectID": "posts/2024-05-13/index.html#example-2-using-the-colnames-function",
    "href": "posts/2024-05-13/index.html#example-2-using-the-colnames-function",
    "title": "How to Check if a Column Exists in a Data Frame in R",
    "section": "Example 2: Using the colnames() Function",
    "text": "Example 2: Using the colnames() Function\nThe colnames() function is another straightforward approach to check for the presence of a column in a data frame. It is very similar to using names() but specifically designed to handle the column names.\n\nExample Code:\n\n# Check if 'salary' column exists\n\"salary\" %in% colnames(df)\n\n[1] FALSE\n\n\n\n\nExplanation:\nThis example checks if the \"salary\" column exists in df. colnames(df) gives us the column names, and \"salary\" %in% colnames(df) evaluates to FALSE since there is no salary column in our sample data frame."
  },
  {
    "objectID": "posts/2024-05-13/index.html#example-3-using-the-exists-function-with-within",
    "href": "posts/2024-05-13/index.html#example-3-using-the-exists-function-with-within",
    "title": "How to Check if a Column Exists in a Data Frame in R",
    "section": "Example 3: Using the exists() Function with within()",
    "text": "Example 3: Using the exists() Function with within()\nFor a more dynamic approach, especially when dealing with environments or complex expressions, exists() can be used in combination with within(). This is a bit more advanced but quite powerful.\n\nExample Code:\n\n# Check if 'age' column exists using exists() within df\nexists(\"age\", where = within(df, list()))\n\n[1] TRUE\n\n\n\n\nExplanation:\nHere, exists() checks if \"age\" exists within the local environment created by within(df, list()). This method is particularly useful when you want to evaluate the existence of a column dynamically within a certain scope or environment."
  },
  {
    "objectID": "posts/2024-05-13/index.html#example-4-using-the-grepl-function",
    "href": "posts/2024-05-13/index.html#example-4-using-the-grepl-function",
    "title": "How to Check if a Column Exists in a Data Frame in R",
    "section": "Example 4: Using the grepl() Function",
    "text": "Example 4: Using the grepl() Function\nThe grepl() function can be utilized for pattern matching, which can also serve to check column names if you’re looking for names that match a specific pattern.\n\nExample Code:\n\n# Check for partial matches, e.g., any column name containing 'ag'\nany(grepl(\"ag\", colnames(df)))\n\n[1] TRUE\n\n\n\n\nExplanation:\ngrepl(\"ag\", colnames(df)) returns a logical vector indicating which column names contain \"ag\". The any() function then checks if there is at least one TRUE in the vector, indicating at least one column name contains the pattern."
  },
  {
    "objectID": "posts/2024-05-13/index.html#your-turn",
    "href": "posts/2024-05-13/index.html#your-turn",
    "title": "How to Check if a Column Exists in a Data Frame in R",
    "section": "Your Turn!",
    "text": "Your Turn!\nThese methods provide robust ways to verify the presence of columns in your data frames in R. Whether you are a novice or more experienced with R, experimenting with these techniques on your own datasets can help solidify your understanding and potentially reveal more about your data’s structure.\nRemember, the more you practice, the more intuitive these checks will become, allowing you to handle data more efficiently and effectively. So, go ahead and try these methods out with different datasets and see how they work for you!"
  },
  {
    "objectID": "posts/2024-05-14/index.html",
    "href": "posts/2024-05-14/index.html",
    "title": "Checking if Multiple Columns are Equal in R",
    "section": "",
    "text": "When working with data in R, you might need to check if values across multiple columns are equal. This is a common task in data cleaning and preprocessing. In this blog, I’ll show you how to do this using base R, dplyr, and data.table. Let’s dive into some examples that demonstrate how to check if every column in a row is equal or if specific columns are equal."
  },
  {
    "objectID": "posts/2024-05-14/index.html#base-r",
    "href": "posts/2024-05-14/index.html#base-r",
    "title": "Checking if Multiple Columns are Equal in R",
    "section": "Base R",
    "text": "Base R\nLet’s start with a simple data frame:\n\ndf &lt;- data.frame(\n  A = c(1, 2, 3, 4),\n  B = c(1, 2, 3, 5),\n  C = c(1, 2, 3, 4)\n)\n\n\nCheck if All Columns in a Row are Equal\nTo check if all columns in a row are equal, you can use the apply function:\n\ndf$AllEqual &lt;- apply(df, 1, function(row) all(row == row[1]))\nprint(df)\n\n  A B C AllEqual\n1 1 1 1     TRUE\n2 2 2 2     TRUE\n3 3 3 3     TRUE\n4 4 5 4    FALSE\n\n\nHere’s what the code does: - apply(df, 1, ...) applies a function to each row of the data frame. - function(row) all(row == row[1]) checks if all elements in the row are equal to the first element of the row.\n\n\nCheck if Specific Columns are Equal\nTo check if specific columns are equal, you can do something similar:\n\ndf$ABEqual &lt;- df$A == df$B\nprint(df)\n\n  A B C AllEqual ABEqual\n1 1 1 1     TRUE    TRUE\n2 2 2 2     TRUE    TRUE\n3 3 3 3     TRUE    TRUE\n4 4 5 4    FALSE   FALSE\n\n\nThis code creates a new column ABEqual that is TRUE if columns A and B are equal, and FALSE otherwise."
  },
  {
    "objectID": "posts/2024-05-14/index.html#using-dplyr",
    "href": "posts/2024-05-14/index.html#using-dplyr",
    "title": "Checking if Multiple Columns are Equal in R",
    "section": "Using dplyr",
    "text": "Using dplyr\nNow let’s see how to do the same tasks using dplyr, a popular package for data manipulation.\nFirst, install and load the package if you haven’t already:\n\n#install.packages(\"dplyr\")\nlibrary(dplyr)\n\n\nCheck if All Columns in a Row are Equal\n\ndf &lt;- df %&gt;%\n  rowwise() %&gt;%\n  mutate(AllEqual = all(\n    c_across(\n      everything()) == first(c_across(everything()))\n    )\n  )\nprint(df)\n\n# A tibble: 4 × 5\n# Rowwise: \n      A     B     C AllEqual ABEqual\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;    &lt;lgl&gt;  \n1     1     1     1 TRUE     TRUE   \n2     2     2     2 FALSE    TRUE   \n3     3     3     3 FALSE    TRUE   \n4     4     5     4 FALSE    FALSE  \n\n\nHere’s a breakdown: - rowwise() groups the data frame by rows, allowing row-wise operations. - mutate(AllEqual = all(c_across(everything()) == first(c_across(everything())))) creates a new column AllEqual that checks if all values in the row are the same.\n\n\nCheck if Specific Columns are Equal\n\ndf &lt;- df %&gt;%\n  mutate(ABEqual = A == B)\nprint(df)\n\n# A tibble: 4 × 5\n# Rowwise: \n      A     B     C AllEqual ABEqual\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;    &lt;lgl&gt;  \n1     1     1     1 TRUE     TRUE   \n2     2     2     2 FALSE    TRUE   \n3     3     3     3 FALSE    TRUE   \n4     4     5     4 FALSE    FALSE  \n\n\nThis code creates a new column ABEqual in the same way as in base R."
  },
  {
    "objectID": "posts/2024-05-14/index.html#using-data.table",
    "href": "posts/2024-05-14/index.html#using-data.table",
    "title": "Checking if Multiple Columns are Equal in R",
    "section": "Using data.table",
    "text": "Using data.table\nFinally, let’s use data.table, another powerful package for data manipulation. Install and load the package if needed:\n\n#install.packages(\"data.table\")\nlibrary(data.table)\n\nConvert the data frame to a data table:\n\ndt &lt;- as.data.table(df)\n\n\nCheck if All Columns in a Row are Equal\n\ndt[, AllEqual := apply(.SD, 1, function(row) all(row == row[1]))]\nprint(dt)\n\n       A     B     C AllEqual ABEqual\n   &lt;num&gt; &lt;num&gt; &lt;num&gt;   &lt;lgcl&gt;  &lt;lgcl&gt;\n1:     1     1     1     TRUE    TRUE\n2:     2     2     2    FALSE    TRUE\n3:     3     3     3    FALSE    TRUE\n4:     4     5     4    FALSE   FALSE\n\n\n\n.SD refers to the subset of the data table.\napply(.SD, 1, function(row) all(row == row[1])) applies the function row-wise to check equality.\n\n\n\nCheck if Specific Columns are Equal\n\ndt[, ABEqual := A == B]\nprint(dt)\n\n       A     B     C AllEqual ABEqual\n   &lt;num&gt; &lt;num&gt; &lt;num&gt;   &lt;lgcl&gt;  &lt;lgcl&gt;\n1:     1     1     1     TRUE    TRUE\n2:     2     2     2    FALSE    TRUE\n3:     3     3     3    FALSE    TRUE\n4:     4     5     4    FALSE   FALSE\n\n\nThis creates a new column ABEqual just like in the previous examples."
  },
  {
    "objectID": "posts/2024-05-14/index.html#conclusion",
    "href": "posts/2024-05-14/index.html#conclusion",
    "title": "Checking if Multiple Columns are Equal in R",
    "section": "Conclusion",
    "text": "Conclusion\nChecking if multiple columns are equal is straightforward in R, whether you use base R, dplyr, or data.table. Each method has its advantages, and you can choose based on your preference or the specific needs of your project. I encourage you to try these examples on your own data and see how they work. Experimenting with different datasets can help you become more comfortable with these techniques. Happy coding!"
  },
  {
    "objectID": "posts/2024-05-15/index.html",
    "href": "posts/2024-05-15/index.html",
    "title": "How to Select Columns Containing a Specific String in R",
    "section": "",
    "text": "Today I want to discuss a common task in data manipulation: selecting columns containing a specific string. Whether you’re working with base R or popular packages like stringr, stringi, or dplyr, I’ll show you how to efficiently achieve this. We’ll cover various methods and provide clear examples to help you understand each approach. Let’s get started!"
  },
  {
    "objectID": "posts/2024-05-15/index.html#using-base-r",
    "href": "posts/2024-05-15/index.html#using-base-r",
    "title": "How to Select Columns Containing a Specific String in R",
    "section": "Using Base R",
    "text": "Using Base R\n\nExample 1: Using grep\nIn base R, the grep function is your friend. It searches for patterns in a character vector and returns the indices of the matching elements.\n\n# Sample data frame\ndf &lt;- data.frame(\n  apple_price = c(1, 2, 3),\n  orange_price = c(4, 5, 6),\n  banana_weight = c(7, 8, 9),\n  grape_weight = c(10, 11, 12)\n)\n\n# Select columns containing \"price\"\ncols &lt;- grep(\"price\", names(df))\nprint(cols)\n\n[1] 1 2\n\ndf_price &lt;- df[, cols]\nprint(df_price)\n\n  apple_price orange_price\n1           1            4\n2           2            5\n3           3            6\n\n# Using value = TRUE to return column names\ncols &lt;- grep(\"price\", names(df), value = TRUE)\nprint(cols)\n\n[1] \"apple_price\"  \"orange_price\"\n\ndf_price &lt;- df[, cols]\nprint(df_price)\n\n  apple_price orange_price\n1           1            4\n2           2            5\n3           3            6\n\n\nIn this example, we use grep to search for the string “price” in the column names. The value = TRUE argument returns the names of the matching columns instead of their indices. We then use these names to subset the data frame.\n\n\nExample 2: Using grepl\ngrepl is another useful function that returns a logical vector indicating whether the pattern was found.\n\n# Select columns containing \"weight\"\ncols &lt;- grepl(\"weight\", names(df))\ndf_weight &lt;- df[, cols]\n\nprint(df_weight)\n\n  banana_weight grape_weight\n1             7           10\n2             8           11\n3             9           12\n\n\nHere, grepl checks each column name for the string “weight” and returns a logical vector. We use this vector to subset the data frame."
  },
  {
    "objectID": "posts/2024-05-15/index.html#using-stringr",
    "href": "posts/2024-05-15/index.html#using-stringr",
    "title": "How to Select Columns Containing a Specific String in R",
    "section": "Using stringr",
    "text": "Using stringr\nThe stringr package provides a set of convenient functions for string manipulation. Let’s see how to use it for our task.\n\nExample 3: Using str_detect\n\nlibrary(stringr)\n\n# Select columns containing \"price\"\ncols &lt;- str_detect(names(df), \"price\")\ndf_price &lt;- df[, cols]\n\nprint(df_price)\n\n  apple_price orange_price\n1           1            4\n2           2            5\n3           3            6\n\n\nstr_detect checks each column name for the presence of the string “price” and returns a logical vector, which we use to subset the data frame."
  },
  {
    "objectID": "posts/2024-05-15/index.html#using-stringi",
    "href": "posts/2024-05-15/index.html#using-stringi",
    "title": "How to Select Columns Containing a Specific String in R",
    "section": "Using stringi",
    "text": "Using stringi\nstringi is another powerful package for string manipulation. It offers a variety of functions for pattern matching.\n\nExample 4: Using stri_detect_fixed\n\nlibrary(stringi)\n\n# Select columns containing \"weight\"\ncols &lt;- stri_detect_fixed(names(df), \"weight\")\ndf_weight &lt;- df[, cols]\n\nprint(df_weight)\n\n  banana_weight grape_weight\n1             7           10\n2             8           11\n3             9           12\n\n\nstri_detect_fixed is similar to str_detect but comes from the stringi package. It checks for the fixed pattern “weight” and returns a logical vector."
  },
  {
    "objectID": "posts/2024-05-15/index.html#using-dplyr",
    "href": "posts/2024-05-15/index.html#using-dplyr",
    "title": "How to Select Columns Containing a Specific String in R",
    "section": "Using dplyr",
    "text": "Using dplyr\ndplyr is a popular package for data manipulation. It provides a straightforward way to select columns based on their names.\n\nExample 5: Using select with contains\n\nlibrary(dplyr)\n\n# Select columns containing \"price\"\ndf_price &lt;- df %&gt;% select(contains(\"price\"))\n\nprint(df_price)\n\n  apple_price orange_price\n1           1            4\n2           2            5\n3           3            6\n\n\nThe select function combined with contains makes it easy to select columns that include the string “price”. This approach is highly readable and concise."
  },
  {
    "objectID": "posts/2024-05-15/index.html#conclusion",
    "href": "posts/2024-05-15/index.html#conclusion",
    "title": "How to Select Columns Containing a Specific String in R",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve covered several methods to select columns containing a specific string in R using base R, stringr, stringi, and dplyr. Each method has its strengths, so choose the one that best fits your needs and coding style.\nFeel free to experiment with these examples on your own data sets. Understanding these techniques will enhance your data manipulation skills and make your code more efficient and readable. Happy coding!"
  },
  {
    "objectID": "posts/2024-05-16/index.html",
    "href": "posts/2024-05-16/index.html",
    "title": "Counting Words in a String in R: A Comprehensive Guide",
    "section": "",
    "text": "Counting words in a string is a common task in data manipulation and text analysis. Whether you’re parsing tweets, analyzing survey responses, or processing any textual data, knowing how to count words is crucial. In this post, we’ll explore three ways to achieve this in R: using base R’s strsplit(), the stringr package, and the stringi package. We’ll provide clear examples and explanations to help you get started."
  },
  {
    "objectID": "posts/2024-05-16/index.html#counting-words-using-base-rs-strsplit",
    "href": "posts/2024-05-16/index.html#counting-words-using-base-rs-strsplit",
    "title": "Counting Words in a String in R: A Comprehensive Guide",
    "section": "Counting Words Using Base R’s strsplit()",
    "text": "Counting Words Using Base R’s strsplit()\nBase R provides a straightforward way to split strings and count words using the strsplit() function. Here’s a simple example:\n\n# Define a string\ntext &lt;- \"R is a powerful language for data analysis.\"\n\n# Split the string into words\nwords &lt;- strsplit(text, \"\\\\s+\")[[1]]\n\n# Count the words\nword_count &lt;- length(words)\n\n# Print the result\nword_count\n\n[1] 8\n\n\nExplanation:\n\nDefine a String: We start with a string, text.\nSplit the String: The strsplit() function splits the string into words based on whitespace (\\\\s+).\nCount the Words: We use length() to count the elements in the resulting vector, which represents the words.\n\nSyntax:\nstrsplit(x, split, fixed = FALSE, perl = FALSE, useBytes = FALSE)\n\nx: Character vector or string to be split.\nsplit: Regular expression or string to split by.\nfixed: Logical, if TRUE, split is a fixed string, not a regular expression.\nperl: Logical, if TRUE, perl = TRUE enables Perl-compatible regexps.\nuseBytes: Logical, if TRUE, use byte-wise splitting.\n\nTry modifying the text variable to see how the word count changes!"
  },
  {
    "objectID": "posts/2024-05-16/index.html#counting-words-using-stringr",
    "href": "posts/2024-05-16/index.html#counting-words-using-stringr",
    "title": "Counting Words in a String in R: A Comprehensive Guide",
    "section": "Counting Words Using stringr",
    "text": "Counting Words Using stringr\nThe stringr package provides a more readable and convenient approach to string manipulation. To use stringr, you’ll need to install and load the package:\n\n# Install stringr if you haven't already\n# install.packages(\"stringr\")\n\n# Load the stringr package\nlibrary(stringr)\n\n# Define a string\ntext &lt;- \"R makes text manipulation easy and fun.\"\n\n# Split the string into words\nwords &lt;- str_split(text, \"\\\\s+\")[[1]]\n\n# Count the words\nword_count &lt;- length(words)\n\n# Print the result\nword_count\n\n[1] 7\n\n\nExplanation:\n\nLoad the Package: After installing and loading stringr, we define our string, text.\nSplit the String: We use str_split() to split the string into words.\nCount the Words: The length() function counts the number of words.\n\nSyntax:\nstr_split(string, pattern, n = Inf, simplify = FALSE)\n\nstring: Input character vector.\npattern: Pattern to split by (regular expression).\nn: Maximum number of pieces to return.\nsimplify: Logical, if TRUE, return a matrix with elements.\n\nThe stringr package makes the code more intuitive and easier to read. Experiment with different strings to get comfortable with str_split()."
  },
  {
    "objectID": "posts/2024-05-16/index.html#counting-words-using-stringi",
    "href": "posts/2024-05-16/index.html#counting-words-using-stringi",
    "title": "Counting Words in a String in R: A Comprehensive Guide",
    "section": "Counting Words Using stringi",
    "text": "Counting Words Using stringi\nThe stringi package is known for its powerful and efficient string manipulation functions. Here’s how to use it to count words:\n\n# Install stringi if you haven't already\n# install.packages(\"stringi\")\n\n# Load the stringi package\nlibrary(stringi)\n\n# Define a string\ntext &lt;- \"Learning R can be a rewarding experience.\"\n\n# Split the string into words\nwords &lt;- stri_split_regex(text, \"\\\\s+\")[[1]]\n\n# Count the words\nword_count &lt;- length(words)\n\n# Print the result\nword_count\n\n[1] 7\n\n\nExplanation:\n\nLoad the Package: Install and load the stringi package.\nSplit the String: Use stri_split_regex() to split the string based on whitespace.\nCount the Words: Count the words using length().\n\nSyntax:\nstri_split_regex(str, pattern, n = -1, omit_empty = FALSE, \n                tokens_only = FALSE, simplify = FALSE)\n\nstr: Input character vector.\npattern: Regular expression pattern.\nn: Maximum number of pieces.\nomit_empty: Logical, if TRUE, remove empty strings from the output.\ntokens_only: Logical, if TRUE, return tokens.\nsimplify: Logical, if TRUE, return a matrix with elements.\n\nThe stringi package offers high performance and is great for handling large datasets or complex text manipulations. Give it a try with different text inputs to see its efficiency in action."
  },
  {
    "objectID": "posts/2024-05-17/index.html",
    "href": "posts/2024-05-17/index.html",
    "title": "Mastering gregexpr() in R: A Comprehensive Guide",
    "section": "",
    "text": "If you’ve ever worked with text data in R, you know how important it is to have powerful tools for pattern matching. One such tool is the gregexpr() function. This function is incredibly useful when you need to find all occurrences of a pattern within a string. Today, we’ll go into how gregexpr() works, explore its syntax, and go through several examples to make things clear."
  },
  {
    "objectID": "posts/2024-05-17/index.html#example-1-basic-usage",
    "href": "posts/2024-05-17/index.html#example-1-basic-usage",
    "title": "Mastering gregexpr() in R: A Comprehensive Guide",
    "section": "Example 1: Basic Usage",
    "text": "Example 1: Basic Usage\nLet’s start with a simple example. Suppose we want to find all occurrences of the letter “a” in the string “banana”.\n\ntext &lt;- \"banana\"\npattern &lt;- \"a\"\nmatches &lt;- gregexpr(pattern, text)\nprint(matches)\n\n[[1]]\n[1] 2 4 6\nattr(,\"match.length\")\n[1] 1 1 1\nattr(,\"index.type\")\n[1] \"chars\"\nattr(,\"useBytes\")\n[1] TRUE\n\n\nThis will return a list with the starting positions of each match. Here, the numbers 2, 4, and 6 indicate the positions of “a” in the string “banana”."
  },
  {
    "objectID": "posts/2024-05-17/index.html#example-2-ignoring-case",
    "href": "posts/2024-05-17/index.html#example-2-ignoring-case",
    "title": "Mastering gregexpr() in R: A Comprehensive Guide",
    "section": "Example 2: Ignoring Case",
    "text": "Example 2: Ignoring Case\nWhat if we want to search for the pattern without considering case? We can set ignore.case = TRUE.\n\ntext &lt;- \"BaNaNa\"\npattern &lt;- \"a\"\nmatches &lt;- gregexpr(pattern, text, ignore.case = TRUE)\nprint(matches)\n\n[[1]]\n[1] 2 4 6\nattr(,\"match.length\")\n[1] 1 1 1\nattr(,\"index.type\")\n[1] \"chars\"\nattr(,\"useBytes\")\n[1] TRUE\n\n\nEven though our string has uppercase “A” and lowercase “a”, the function treats them the same because we set ignore.case = TRUE."
  },
  {
    "objectID": "posts/2024-05-17/index.html#example-3-using-perl-compatible-regex",
    "href": "posts/2024-05-17/index.html#example-3-using-perl-compatible-regex",
    "title": "Mastering gregexpr() in R: A Comprehensive Guide",
    "section": "Example 3: Using Perl-Compatible Regex",
    "text": "Example 3: Using Perl-Compatible Regex\nSometimes, we need more advanced pattern matching. By setting perl = TRUE, we can use Perl-compatible regular expressions.\n\ntext &lt;- \"cat, bat, rat\"\npattern &lt;- \"[bcr]at\"\nmatches &lt;- gregexpr(pattern, text, perl = TRUE)\nprint(matches)\n\n[[1]]\n[1]  1  6 11\nattr(,\"match.length\")\n[1] 3 3 3\nattr(,\"index.type\")\n[1] \"chars\"\nattr(,\"useBytes\")\n[1] TRUE\n\n\nThis will find all occurrences of “bat”, “cat”, and “rat”. The positions 1, 6, and 11 correspond to the starting positions of “cat”, “bat”, and “rat” respectively."
  },
  {
    "objectID": "posts/2024-05-17/index.html#example-4-fixed-string-matching",
    "href": "posts/2024-05-17/index.html#example-4-fixed-string-matching",
    "title": "Mastering gregexpr() in R: A Comprehensive Guide",
    "section": "Example 4: Fixed String Matching",
    "text": "Example 4: Fixed String Matching\nIf you want to search for a fixed substring rather than a regex pattern, set fixed = TRUE.\n\ntext &lt;- \"batman and catwoman\"\npattern &lt;- \"man\"\nmatches &lt;- gregexpr(pattern, text, fixed = TRUE)\nprint(matches)\n\n[[1]]\n[1]  4 17\nattr(,\"match.length\")\n[1] 3 3\nattr(,\"index.type\")\n[1] \"chars\"\nattr(,\"useBytes\")\n[1] TRUE\n\n\nThis will match the substring “man” exactly. The output will show the starting positions of each match along with the length of the match."
  },
  {
    "objectID": "posts/2024-05-17/index.html#example-5-extracting-matches",
    "href": "posts/2024-05-17/index.html#example-5-extracting-matches",
    "title": "Mastering gregexpr() in R: A Comprehensive Guide",
    "section": "Example 5: Extracting Matches",
    "text": "Example 5: Extracting Matches\nYou can extract the matched substrings using the regmatches() function.\n\ntext &lt;- \"apple, banana, cherry\"\npattern &lt;- \"[a-z]{5}\"\nmatches &lt;- gregexpr(pattern, text)\nextracted &lt;- regmatches(text, matches)\nprint(extracted)\n\n[[1]]\n[1] \"apple\" \"banan\" \"cherr\"\n\n\nThis will extract all substrings of length 5 from the text. The output will be a list of the matched substrings."
  },
  {
    "objectID": "posts/2024-05-20/index.html",
    "href": "posts/2024-05-20/index.html",
    "title": "How to Remove Specific Elements from a Vector in R",
    "section": "",
    "text": "Working with vectors is one of the fundamental aspects of R programming. Sometimes, you need to remove specific elements from a vector to clean your data or prepare it for analysis. This post will guide you through several methods to achieve this, using base R, dplyr, and data.table. We’ll look at examples for both numeric and character vectors and explain the code in a straightforward manner. By the end, you’ll have a clear understanding of how to manipulate your vectors efficiently. Let’s dive in!"
  },
  {
    "objectID": "posts/2024-05-20/index.html#using-base-r",
    "href": "posts/2024-05-20/index.html#using-base-r",
    "title": "How to Remove Specific Elements from a Vector in R",
    "section": "Using Base R",
    "text": "Using Base R\nBase R provides straightforward methods to remove elements from vectors. Let’s start with some examples.\n\nNumeric Vector\nSuppose you have a numeric vector and you want to remove specific numbers.\n\n# Create a numeric vector\nnumeric_vec &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n# Remove the numbers 3 and 7\nnumeric_vec &lt;- numeric_vec[!numeric_vec %in% c(3, 7)]\n\n# Print the updated vector\nprint(numeric_vec)\n\n[1] 1 2 4 5 6 8 9\n\n\nExplanation: - numeric_vec %in% c(3, 7) checks if each element in numeric_vec is in the set of numbers {3, 7}. - !numeric_vec %in% c(3, 7) negates the condition, giving TRUE for elements not in {3, 7}. - numeric_vec[!] selects the elements that meet the condition.\n\n\nCharacter Vector\nNow let’s work with a character vector.\n\n# Create a character vector\nchar_vec &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\")\n\n# Remove \"banana\" and \"date\"\nchar_vec &lt;- char_vec[!char_vec %in% c(\"banana\", \"date\")]\n\n# Print the updated vector\nprint(char_vec)\n\n[1] \"apple\"      \"cherry\"     \"elderberry\"\n\n\nThe process is similar: we use logical indexing to exclude the unwanted elements."
  },
  {
    "objectID": "posts/2024-05-20/index.html#using-dplyr",
    "href": "posts/2024-05-20/index.html#using-dplyr",
    "title": "How to Remove Specific Elements from a Vector in R",
    "section": "Using dplyr",
    "text": "Using dplyr\nThe dplyr package is part of the tidyverse and provides powerful tools for data manipulation. While it is often used with data frames, we can also use it to work with vectors by converting them to tibbles.\n\nNumeric Vector\n\nlibrary(dplyr)\n\n# Create a numeric vector\nnumeric_vec &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n# Convert to tibble\nnumeric_tibble &lt;- tibble(value = numeric_vec)\n\n# Remove the numbers 3 and 7\nnumeric_tibble &lt;- numeric_tibble %&gt;%\n  filter(!value %in% c(3, 7))\n\n# Extract the updated vector\nnumeric_vec &lt;- pull(numeric_tibble, value)\n\n# Print the updated vector\nprint(numeric_vec)\n\n[1] 1 2 4 5 6 8 9\n\n\nExplanation: - Convert the vector to a tibble. - Use filter(!value %in% c(3, 7)) to remove rows where the value is in {3, 7}. - Use pull to convert the tibble back to a vector.\n\n\nCharacter Vector\n\n# Create a character vector\nchar_vec &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\")\n\n# Convert to tibble\nchar_tibble &lt;- tibble(value = char_vec)\n\n# Remove \"banana\" and \"date\"\nchar_tibble &lt;- char_tibble %&gt;%\n  filter(!value %in% c(\"banana\", \"date\"))\n\n# Extract the updated vector\nchar_vec &lt;- pull(char_tibble, value)\n\n# Print the updated vector\nprint(char_vec)\n\n[1] \"apple\"      \"cherry\"     \"elderberry\"\n\n\nThe filter function from dplyr allows for efficient removal of unwanted elements."
  },
  {
    "objectID": "posts/2024-05-20/index.html#using-data.table",
    "href": "posts/2024-05-20/index.html#using-data.table",
    "title": "How to Remove Specific Elements from a Vector in R",
    "section": "Using data.table",
    "text": "Using data.table\nThe data.table package is known for its speed and efficiency, especially with large datasets. Let’s see how we can use it to remove elements from vectors.\n\nNumeric Vector\n\nlibrary(data.table)\n\n# Create a numeric vector\nnumeric_vec &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n# Convert to data.table\ndt &lt;- data.table(value = numeric_vec)\n\n# Remove the numbers 3 and 7\ndt &lt;- dt[!value %in% c(3, 7)]\n\n# Extract the updated vector\nnumeric_vec &lt;- dt$value\n\n# Print the updated vector\nprint(numeric_vec)\n\n[1] 1 2 4 5 6 8 9\n\n\nExplanation: - We convert the vector to a data.table object. - Use the !value %in% c(3, 7) condition within the [] to filter the table. - Extract the updated vector using dt$value.\n\n\nCharacter Vector\n\n# Create a character vector\nchar_vec &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\")\n\n# Convert to data.table\ndt &lt;- data.table(value = char_vec)\n\n# Remove \"banana\" and \"date\"\ndt &lt;- dt[!value %in% c(\"banana\", \"date\")]\n\n# Extract the updated vector\nchar_vec &lt;- dt$value\n\n# Print the updated vector\nprint(char_vec)\n\n[1] \"apple\"      \"cherry\"     \"elderberry\"\n\n\nUsing data.table involves a few more steps, but it is very efficient, especially with large vectors."
  },
  {
    "objectID": "posts/2024-05-21/index.html",
    "href": "posts/2024-05-21/index.html",
    "title": "How to Split a Vector into Chunks in R",
    "section": "",
    "text": "In data analysis, there are times when you need to split a vector into smaller chunks. Whether you’re managing large datasets or preparing data for parallel processing, breaking down vectors can be incredibly useful. In this post, we’ll explore how to achieve this in R using base R, dplyr, and data.table."
  },
  {
    "objectID": "posts/2024-05-21/index.html#using-base-r",
    "href": "posts/2024-05-21/index.html#using-base-r",
    "title": "How to Split a Vector into Chunks in R",
    "section": "Using Base R",
    "text": "Using Base R\nBase R provides a straightforward way to split a vector into chunks using the split function and a combination of other basic functions.\n\nExample 1: Splitting a Vector into Chunks\nLet’s say we have a vector x and we want to split it into chunks of size 3.\n\nx &lt;- 1:10\nchunk_size &lt;- 3\nsplit_vector &lt;- split(x, ceiling(seq_along(x) / chunk_size))\nprint(split_vector)\n\n$`1`\n[1] 1 2 3\n\n$`2`\n[1] 4 5 6\n\n$`3`\n[1] 7 8 9\n\n$`4`\n[1] 10\n\n\nExplanation:\n\nx &lt;- 1:10: Creates a vector x with values from 1 to 10.\nchunk_size &lt;- 3: Defines the size of each chunk.\nseq_along(x): Generates a sequence of the same length as x.\nceiling(seq_along(x) / chunk_size): Divides the sequence by the chunk size and uses ceiling to round up to the nearest integer, creating a grouping factor.\nsplit(x, ...): Splits the vector based on the grouping factor."
  },
  {
    "objectID": "posts/2024-05-21/index.html#using-dplyr",
    "href": "posts/2024-05-21/index.html#using-dplyr",
    "title": "How to Split a Vector into Chunks in R",
    "section": "Using dplyr",
    "text": "Using dplyr\nThe dplyr package, part of the tidyverse, offers a more readable and pipe-friendly approach to splitting vectors.\n\nExample 2: Splitting a Vector into Chunks\nHere’s how you can do it with dplyr.\n\nlibrary(dplyr)\n\nx &lt;- 1:10\nchunk_size &lt;- 3\nsplit_vector &lt;- x %&gt;%\n  as.data.frame() %&gt;%\n  mutate(group = ceiling(row_number() / chunk_size)) %&gt;%\n  group_by(group) %&gt;%\n  summarise(chunk = list(.)) %&gt;%\n  pull(chunk)\nprint(split_vector)\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 4 5 6\n\n[[3]]\n[1] 7 8 9\n\n[[4]]\n[1] 10\n\n\nExplanation:\n\nas.data.frame(): Converts the vector to a data frame.\nmutate(group = ceiling(row_number() / chunk_size)): Adds a grouping column.\ngroup_by(group): Groups the data by the newly created group column.\nsummarise(chunk = list(.)): Summarizes the groups into list columns using the . placeholder.\npull(chunk): Extracts the list column as a vector of chunks.\n\n\n\nExample 3: Splitting a Vector using group_split()\ngroup_split() is another handy function from dplyr to split data into groups.\n\nx &lt;- 1:10\nchunk_size &lt;- 3\nsplit_vector &lt;- x %&gt;%\n  as.data.frame() %&gt;%\n  mutate(group = ceiling(row_number() / chunk_size)) %&gt;%\n  group_split(group)\nprint(split_vector)\n\n&lt;list_of&lt;\n  tbl_df&lt;\n    .    : integer\n    group: double\n  &gt;\n&gt;[4]&gt;\n[[1]]\n# A tibble: 3 × 2\n      . group\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n2     2     1\n3     3     1\n\n[[2]]\n# A tibble: 3 × 2\n      . group\n  &lt;int&gt; &lt;dbl&gt;\n1     4     2\n2     5     2\n3     6     2\n\n[[3]]\n# A tibble: 3 × 2\n      . group\n  &lt;int&gt; &lt;dbl&gt;\n1     7     3\n2     8     3\n3     9     3\n\n[[4]]\n# A tibble: 1 × 2\n      . group\n  &lt;int&gt; &lt;dbl&gt;\n1    10     4\n\n\nExplanation:\n\nas.data.frame(): Converts the vector to a data frame.\nmutate(group = ceiling(row_number() / chunk_size)): Adds a grouping column.\ngroup_split(group): Splits the data frame into a list of data frames based on the group column."
  },
  {
    "objectID": "posts/2024-05-21/index.html#using-data.table",
    "href": "posts/2024-05-21/index.html#using-data.table",
    "title": "How to Split a Vector into Chunks in R",
    "section": "Using data.table",
    "text": "Using data.table\ndata.table is known for its efficiency with large datasets. Here’s how you can split a vector using data.table.\n\nExample 4: Splitting a Vector into Chunks\n\nlibrary(data.table)\n\nx &lt;- 1:10\nchunk_size &lt;- 3\ndt &lt;- data.table(x = x)\ndt[, group := ceiling(.I / chunk_size)]\nsplit_vector &lt;- dt[, .(chunk = list(x)), by = group]$chunk\nprint(split_vector)\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 4 5 6\n\n[[3]]\n[1] 7 8 9\n\n[[4]]\n[1] 10\n\n\nExplanation:\n\ndata.table(x = x): Converts the vector to a data.table.\ngroup := ceiling(.I / chunk_size): Creates a group column using the row index .I.\n.(chunk = list(x)), by = group: Groups by the group column and creates list columns.\n$chunk: Extracts the list column."
  },
  {
    "objectID": "posts/2024-05-22/index.html",
    "href": "posts/2024-05-22/index.html",
    "title": "How to Split a Number into Digits in R Using gsub() and strsplit()",
    "section": "",
    "text": "Splitting numbers into individual digits can be a handy trick in data analysis and manipulation. Today, we’ll explore how to achieve this using base R functions, specifically gsub() and strsplit(). Let’s walk through the process step by step, explain the syntax of each function, and provide some examples for clarity."
  },
  {
    "objectID": "posts/2024-05-22/index.html#understanding-gsub-and-strsplit",
    "href": "posts/2024-05-22/index.html#understanding-gsub-and-strsplit",
    "title": "How to Split a Number into Digits in R Using gsub() and strsplit()",
    "section": "Understanding gsub() and strsplit()",
    "text": "Understanding gsub() and strsplit()\nFirst, let’s get familiar with the two main functions we’ll be using:\n\ngsub(pattern, replacement, x):\n\npattern: A regular expression describing the pattern to be matched.\nreplacement: The string to replace the matched pattern.\nx: The input vector, which is usually a character string.\n\n\nThe gsub() function replaces all occurrences of the pattern in x with the replacement.\n\nstrsplit(x, split):\n\nx: The input vector, which is usually a character string.\nsplit: The delimiter on which to split the input string.\n\n\nThe strsplit() function splits the elements of a character vector x into substrings based on the delimiter specified in split."
  },
  {
    "objectID": "posts/2024-05-22/index.html#splitting-a-number-into-digits",
    "href": "posts/2024-05-22/index.html#splitting-a-number-into-digits",
    "title": "How to Split a Number into Digits in R Using gsub() and strsplit()",
    "section": "Splitting a Number into Digits",
    "text": "Splitting a Number into Digits\nLet’s go through a few examples to see how we can split numbers into digits using these functions.\n\nExample 1: Basic Splitting of a Single Number\n\n# Step 1: Convert the number to a character string\nnumber &lt;- 12345\nnumber_str &lt;- as.character(number)\nnumber_str\n\n[1] \"12345\"\n\n# Step 2: Use gsub() to insert a delimiter (space) between each digit\nnumber_with_spaces &lt;- gsub(\"(.)\", \"\\\\1 \", number_str)\nnumber_with_spaces\n\n[1] \"1 2 3 4 5 \"\n\n# Step 3: Use strsplit() to split the string on the delimiter\ndigits &lt;- strsplit(number_with_spaces, \" \")[[1]]\n\n# Step 4: Convert the result back to numeric\ndigits_numeric &lt;- as.numeric(digits)\n\n# Print the result\nprint(digits_numeric)\n\n[1] 1 2 3 4 5\n\n\nExplanation:\n\nWe convert the number to a character string using as.character().\nWe use gsub(\"(.)\", \"\\\\1 \", number_str) to insert a space between each digit. The pattern (.) matches any character, and \\\\1 refers to the matched character followed by a space.\nWe split the string on spaces using strsplit(number_with_spaces, \" \").\nFinally, we convert the resulting character vector back to numeric using as.numeric().\n\n\n\nExample 2: Splitting Multiple Numbers in a Vector\n\n# Vector of numbers\nnumbers &lt;- c(6789, 5432)\n\n# Function to split a single number into digits\nsplit_number &lt;- function(number) {\n  number_str &lt;- as.character(number)\n  number_with_spaces &lt;- gsub(\"(.)\", \"\\\\1 \", number_str)\n  digits &lt;- strsplit(number_with_spaces, \" \")[[1]]\n  as.numeric(digits)\n}\n\n# Apply the function to each number in the vector\nsplit_digits &lt;- lapply(numbers, split_number)\n\n# Print the result\nprint(split_digits)\n\n[[1]]\n[1] 6 7 8 9\n\n[[2]]\n[1] 5 4 3 2\n\n\nExplanation:\n\nWe define a vector of numbers.\nWe create a function split_number that takes a number and splits it into digits using the same steps as in Example 1.\nWe apply this function to each number in the vector using lapply().\nThe result is a list where each element is a vector of digits for each number in the original vector."
  },
  {
    "objectID": "posts/2024-05-23/index.html",
    "href": "posts/2024-05-23/index.html",
    "title": "How to Drop or Select Rows with a Specific String in R",
    "section": "",
    "text": "Good morning, everyone!\nToday, we’re going to talk about how to handle rows in your dataset that contain a specific string. This is a common task in data cleaning and can be easily accomplished using both base R and the dplyr package. We’ll go through examples for each method and break down the code so you can understand and apply it to your own data."
  },
  {
    "objectID": "posts/2024-05-23/index.html#using-base-r",
    "href": "posts/2024-05-23/index.html#using-base-r",
    "title": "How to Drop or Select Rows with a Specific String in R",
    "section": "Using Base R",
    "text": "Using Base R\nFirst, let’s see how to select and drop rows containing a specific string using base R. We’ll use the grep() function for this.\n\nExample Data\nLet’s create a simple data frame to work with:\n\ndata &lt;- data.frame(\n  id = 1:5,\n  name = c(\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"),\n  stringsAsFactors = FALSE\n)\nprint(data)\n\n  id       name\n1  1      apple\n2  2     banana\n3  3     cherry\n4  4       date\n5  5 elderberry\n\n\n\n\nSelecting Rows with a Specific String\nSuppose we want to select rows where the name contains the letter “a”. We can use grep():\n\nselected_rows &lt;- data[grep(\"a\", data$name), ]\nprint(selected_rows)\n\n  id   name\n1  1  apple\n2  2 banana\n4  4   date\n\n\nExplanation:\n\ngrep(\"a\", data$name) searches for the letter “a” in the name column and returns the indices of the rows that match.\ndata[grep(\"a\", data$name), ] uses these indices to subset the original data frame.\n\n\n\nDropping Rows with a Specific String\nTo drop rows that contain the letter “a”, we can use the -grep() notation:\n\ndropped_rows &lt;- data[-grep(\"a\", data$name), ]\nprint(dropped_rows)\n\n  id       name\n3  3     cherry\n5  5 elderberry\n\n\nExplanation:\n\n-grep(\"a\", data$name) returns the indices of the rows that do not match the search term.\ndata[-grep(\"a\", data$name), ] subsets the original data frame by excluding these rows."
  },
  {
    "objectID": "posts/2024-05-23/index.html#using-dplyr",
    "href": "posts/2024-05-23/index.html#using-dplyr",
    "title": "How to Drop or Select Rows with a Specific String in R",
    "section": "Using dplyr",
    "text": "Using dplyr\nThe dplyr package makes these tasks even more straightforward with its intuitive functions.\n\nExample Data\nWe’ll use the same data frame as before. First, make sure you have dplyr installed and loaded:\n\n#install.packages(\"dplyr\")\nlibrary(dplyr)\n\n\n\nSelecting Rows with a Specific String\nUsing dplyr, we can select rows containing “a” with the filter() function combined with str_detect() from the stringr package:\n\nlibrary(stringr)\n\nselected_rows_dplyr &lt;- data %&gt;%\n  filter(str_detect(name, \"a\"))\nprint(selected_rows_dplyr)\n\n  id   name\n1  1  apple\n2  2 banana\n3  4   date\n\n\nExplanation:\n\n%&gt;% is the pipe operator, allowing us to chain functions together.\nfilter(str_detect(name, \"a\")) filters rows where the name column contains the letter “a”.\n\n\n\nDropping Rows with a Specific String\nTo drop rows containing “a” using dplyr, we use filter() with the negation operator !:\n\ndropped_rows_dplyr &lt;- data %&gt;%\n  filter(!str_detect(name, \"a\"))\nprint(dropped_rows_dplyr)\n\n  id       name\n1  3     cherry\n2  5 elderberry\n\n\nExplanation:\n\n!str_detect(name, \"a\") negates the condition, filtering out rows where the name column contains the letter “a”."
  },
  {
    "objectID": "posts/2024-05-24/index.html",
    "href": "posts/2024-05-24/index.html",
    "title": "Update to healthyR.data 1.1.0",
    "section": "",
    "text": "I’m excited to share the latest updates to the healthyR.data R package! This release brings new functionality and minor improvements, all aimed at making your data management tasks easier and more efficient. Here’s a breakdown of what’s new:\n\n\n\n\nThis new function is designed to retrieve metadata from the Centers for Medicare & Medicaid Services (CMS). Whether you’re working on health research, policy analysis, or clinical studies, this function provides a straightforward way to access essential CMS data.\nLearn more about get_cms_meta_data()\nSyntax:\nget_cms_meta_data(\n  .title = NULL,\n  .modified_date = NULL,\n  .keyword = NULL,\n  .identifier = NULL,\n  .data_version = \"current\",\n  .media_type = \"all\"\n)\n\n\n\nSimilarly, the get_provider_meta_data() function allows you to fetch metadata related to healthcare providers. This can be particularly useful for projects that require comprehensive information about provider attributes and characteristics.\nLearn more about get_provider_meta_data()\nSyntax:\nget_provider_meta_data(\n  .identifier = NULL,\n  .title = NULL,\n  .description = NULL,\n  .keyword = NULL,\n  .issued = NULL,\n  .modified = NULL,\n  .released = NULL,\n  .theme = NULL,\n  .media_type = NULL\n)\n\n\n\nWe’ve also added fetch_cms_data() and fetch_provider_data(), two powerful functions for fetching actual data from CMS and healthcare providers, respectively. These functions are perfect for those who need to integrate large datasets into their workflows seamlessly.\nLearn more about fetch_cms_data()\nLearn more about fetch_provider_data()\nSyntax:\nfetch_cms_data(.data_link)\nfetch_provider_data(.data_link)\n\n\n\n\n\n\nWe’ve addressed a bug related to directory file paths in the current_hosp_data() function. This fix ensures smoother operation and better reliability when managing hospital data.\nLearn more about current_hosp_data()\n\n\n\n\nI’m pleased to report that this update does not include any breaking changes. You can upgrade to the latest version without worrying about compatibility issues with your existing code."
  },
  {
    "objectID": "posts/2024-05-24/index.html#new-functions",
    "href": "posts/2024-05-24/index.html#new-functions",
    "title": "Update to healthyR.data 1.1.0",
    "section": "",
    "text": "This new function is designed to retrieve metadata from the Centers for Medicare & Medicaid Services (CMS). Whether you’re working on health research, policy analysis, or clinical studies, this function provides a straightforward way to access essential CMS data.\nLearn more about get_cms_meta_data()\nSyntax:\nget_cms_meta_data(\n  .title = NULL,\n  .modified_date = NULL,\n  .keyword = NULL,\n  .identifier = NULL,\n  .data_version = \"current\",\n  .media_type = \"all\"\n)\n\n\n\nSimilarly, the get_provider_meta_data() function allows you to fetch metadata related to healthcare providers. This can be particularly useful for projects that require comprehensive information about provider attributes and characteristics.\nLearn more about get_provider_meta_data()\nSyntax:\nget_provider_meta_data(\n  .identifier = NULL,\n  .title = NULL,\n  .description = NULL,\n  .keyword = NULL,\n  .issued = NULL,\n  .modified = NULL,\n  .released = NULL,\n  .theme = NULL,\n  .media_type = NULL\n)\n\n\n\nWe’ve also added fetch_cms_data() and fetch_provider_data(), two powerful functions for fetching actual data from CMS and healthcare providers, respectively. These functions are perfect for those who need to integrate large datasets into their workflows seamlessly.\nLearn more about fetch_cms_data()\nLearn more about fetch_provider_data()\nSyntax:\nfetch_cms_data(.data_link)\nfetch_provider_data(.data_link)"
  },
  {
    "objectID": "posts/2024-05-24/index.html#minor-fixes-and-improvements",
    "href": "posts/2024-05-24/index.html#minor-fixes-and-improvements",
    "title": "Update to healthyR.data 1.1.0",
    "section": "",
    "text": "We’ve addressed a bug related to directory file paths in the current_hosp_data() function. This fix ensures smoother operation and better reliability when managing hospital data.\nLearn more about current_hosp_data()"
  },
  {
    "objectID": "posts/2024-05-24/index.html#no-breaking-changes",
    "href": "posts/2024-05-24/index.html#no-breaking-changes",
    "title": "Update to healthyR.data 1.1.0",
    "section": "",
    "text": "I’m pleased to report that this update does not include any breaking changes. You can upgrade to the latest version without worrying about compatibility issues with your existing code."
  },
  {
    "objectID": "posts/2024-05-28/index.html",
    "href": "posts/2024-05-28/index.html",
    "title": "Unveiling the Power of get_cms_meta_data() in healthyR.data",
    "section": "",
    "text": "Hey, R users! 🌟 Today, we’re going to look at a great new addition to the healthyR.data package—the get_cms_meta_data() function! This function is a helpful tool for retrieving and analyzing metadata from CMS (Centers for Medicare & Medicaid Services) datasets. Whether you’re a healthcare analyst, data scientist, or R programming fan, you’ll find this function very useful. Let’s break it down and explore how it works.\n\n\nThe get_cms_meta_data() function lets you retrieve metadata from CMS datasets easily. You can customize your search using various parameters, ensuring you get precisely the data you need. Here’s the syntax:\nget_cms_meta_data(\n  .title = NULL,\n  .modified_date = NULL,\n  .keyword = NULL,\n  .identifier = NULL,\n  .data_version = \"current\",\n  .media_type = \"all\"\n)\n\n\n\n.title: Search by title.\n.modified_date: Search by modified date (format: “YYYY-MM-DD”).\n.keyword: Search by keyword.\n.identifier: Search by identifier.\n.data_version: Choose between “current”, “archive”, or “all”. Default is “current”.\n.media_type: Filter by media type (“all”, “csv”, “API”, “other”). Default is “all”.\n\n\n\n\nA tibble containing data links and relevant metadata about the datasets.\n\n\n\nThe function fetches JSON data from the CMS data URL and extracts relevant fields to create a tidy tibble. It selects specific columns, handles nested lists by unnesting them, cleans column names, and processes dates and media types to make the data more useful for analysis. The columns in the returned tibble include:\n\ntitle\ndescription\nlanding_page\nmodified\nkeyword\ndescribed_by\nfn\nhas_email\nidentifier\nstart\nend\nreferences\ndistribution_description\ndistribution_title\ndistribution_modified\ndistribution_start\ndistribution_end\nmedia_type\ndata_link\n\n\n\n\n\nLet’s see the get_cms_meta_data() function in action with a couple of examples.\n\n\nFirst, we’ll load the necessary libraries and fetch some metadata:\n\n# Library Loads\nlibrary(healthyR.data)\nlibrary(dplyr)\n\n# Get data\ncms_data &lt;- get_cms_meta_data()\nglimpse(cms_data)\n\nRows: 107\nColumns: 19\n$ title                    &lt;chr&gt; \"Accountable Care Organization Participants\",…\n$ description              &lt;chr&gt; \"The Accountable Care Organization Participan…\n$ landing_page             &lt;chr&gt; \"https://data.cms.gov/medicare-shared-savings…\n$ modified                 &lt;date&gt; 2024-01-29, 2024-04-23, 2024-01-12, 2024-01-…\n$ keyword                  &lt;list&gt; &lt;\"Medicare\", \"Value-Based Care\", \"Coordinate…\n$ described_by             &lt;chr&gt; \"https://data.cms.gov/resources/accountable-c…\n$ fn                       &lt;chr&gt; \"Shared Savings Program - CM\", \"Shared Saving…\n$ has_email                &lt;chr&gt; \"SharedSavingsProgram@cms.hhs.gov\", \"SharedSa…\n$ identifier               &lt;chr&gt; \"https://data.cms.gov/data-api/v1/dataset/976…\n$ start                    &lt;date&gt; 2014-01-01, 2017-01-01, 2021-01-01, 2021-01-…\n$ end                      &lt;date&gt; 2024-12-31, 2024-12-31, 2021-12-31, 2021-12-…\n$ references               &lt;chr&gt; \"https://data.cms.gov/resources/acos-aco-part…\n$ distribution_description &lt;chr&gt; \"latest\", \"latest\", \"latest\", \"latest\", \"late…\n$ distribution_title       &lt;chr&gt; \"Accountable Care Organization Participants\",…\n$ distribution_modified    &lt;date&gt; 2024-01-29, 2024-04-23, 2024-01-12, 2024-01-…\n$ distribution_start       &lt;date&gt; 2024-01-01, 2024-01-01, 2021-01-01, 2021-01-…\n$ distribution_end         &lt;date&gt; 2024-12-31, 2024-12-31, 2021-12-31, 2021-12-…\n$ media_type               &lt;chr&gt; \"API\", \"API\", \"API\", \"API\", \"API\", \"API\", \"AP…\n$ data_link                &lt;chr&gt; \"https://data.cms.gov/data-api/v1/dataset/976…\n\n# Attributes\natb &lt;- attributes(cms_data)\natb$names\n\n [1] \"title\"                    \"description\"             \n [3] \"landing_page\"             \"modified\"                \n [5] \"keyword\"                  \"described_by\"            \n [7] \"fn\"                       \"has_email\"               \n [9] \"identifier\"               \"start\"                   \n[11] \"end\"                      \"references\"              \n[13] \"distribution_description\" \"distribution_title\"      \n[15] \"distribution_modified\"    \"distribution_start\"      \n[17] \"distribution_end\"         \"media_type\"              \n[19] \"data_link\"               \n\natb$class\n\n[1] \"cms_meta_data\" \"tbl_df\"        \"tbl\"           \"data.frame\"   \n\natb$url\n\n[1] \"https://data.cms.gov/data.json\"\n\natb$date_retrieved\n\n[1] \"2024-05-28 10:20:18 EDT\"\n\natb$parameters\n\n$.data_version\n[1] \"current\"\n\n$.media_type\n[1] \"all\"\n\n$.title\nNULL\n\n$.modified_date\nNULL\n\n$.keyword\nNULL\n\n$.identifier\nNULL\n\n\nIn this example, we’re simply calling get_cms_meta_data() without any parameters. This fetches the default dataset metadata. The glimpse() function from the dplyr package provides a quick overview of the data structure.\n\n\n\nNow, let’s refine our search by specifying a keyword and title:\n\nget_cms_meta_data(\n  .keyword = \"nation\",\n  .title = \"Market Saturation & Utilization State-County\"\n) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 19\n$ title                    &lt;chr&gt; \"Market Saturation & Utilization State-County\"\n$ description              &lt;chr&gt; \"The Market Saturation and Utilization State-…\n$ landing_page             &lt;chr&gt; \"https://data.cms.gov/summary-statistics-on-u…\n$ modified                 &lt;date&gt; 2024-04-02\n$ keyword                  &lt;list&gt; &lt;\"National\", \"States & Territories\", \"Countie…\n$ described_by             &lt;chr&gt; \"https://data.cms.gov/resources/market-satur…\n$ fn                       &lt;chr&gt; \"Market Saturation - CPI\"\n$ has_email                &lt;chr&gt; \"MarketSaturation@cms.hhs.gov\"\n$ identifier               &lt;chr&gt; \"https://data.cms.gov/data-api/v1/dataset/89…\n$ start                    &lt;date&gt; 2023-10-01\n$ end                      &lt;date&gt; 2023-12-31\n$ references               &lt;chr&gt; \"https://data.cms.gov/resources/market-satura…\n$ distribution_description &lt;chr&gt; \"latest\"\n$ distribution_title       &lt;chr&gt; \"Market Saturation & Utilization StateCounty\"\n$ distribution_modified    &lt;date&gt; 2024-04-02\n$ distribution_start       &lt;date&gt; 2023-10-01\n$ distribution_end         &lt;date&gt; 2023-12-31\n$ media_type               &lt;chr&gt; \"API\"\n$ data_link                &lt;chr&gt; \"https://data.cms.gov/data-api/v1/dataset/890…\n\n\nIn this example, we filter the metadata by the keyword “nation” and the title “Market Saturation & Utilization State-County”. The pipe operator (|&gt;) is used to pass the result directly into the glimpse() function for a quick preview.\n\n\n\n\nLet’s break down the code blocks to understand what they’re doing:\n\n\n\nLoad Libraries:\nlibrary(healthyR.data)\nlibrary(dplyr)\nWe load the healthyR.data package to access the get_cms_meta_data() function and the dplyr package for data manipulation.\nFetch Metadata:\ncms_data &lt;- get_cms_meta_data()\nWe call get_cms_meta_data() without any parameters to get the default dataset metadata.\nPreview Data:\nglimpse(cms_data)\nThe glimpse() function gives us a quick look at the structure and contents of the fetched metadata.\n\n\n\n\n\nCustom Search Call:\nget_cms_meta_data(\n  .keyword = \"nation\",\n  .title = \"Market Saturation & Utilization State-County\"\n) |&gt;\nglimpse()\nHere, we call get_cms_meta_data() with specific parameters for keyword and title to narrow down our search. The result is passed to glimpse() using the pipe operator for an immediate preview.\n\n\n\n\n\nThe get_cms_meta_data() function is a versatile and flexible tool for accessing CMS metadata, making your data analysis tasks more efficient and effective. Whether you’re looking for specific datasets or just exploring the available metadata, this function has got you covered.\nTry out get_cms_meta_data() in your next R project and explore the potential of CMS data with ease! Happy coding! 🚀"
  },
  {
    "objectID": "posts/2024-05-28/index.html#overview-of-get_cms_meta_data",
    "href": "posts/2024-05-28/index.html#overview-of-get_cms_meta_data",
    "title": "Unveiling the Power of get_cms_meta_data() in healthyR.data",
    "section": "",
    "text": "The get_cms_meta_data() function lets you retrieve metadata from CMS datasets easily. You can customize your search using various parameters, ensuring you get precisely the data you need. Here’s the syntax:\nget_cms_meta_data(\n  .title = NULL,\n  .modified_date = NULL,\n  .keyword = NULL,\n  .identifier = NULL,\n  .data_version = \"current\",\n  .media_type = \"all\"\n)\n\n\n\n.title: Search by title.\n.modified_date: Search by modified date (format: “YYYY-MM-DD”).\n.keyword: Search by keyword.\n.identifier: Search by identifier.\n.data_version: Choose between “current”, “archive”, or “all”. Default is “current”.\n.media_type: Filter by media type (“all”, “csv”, “API”, “other”). Default is “all”.\n\n\n\n\nA tibble containing data links and relevant metadata about the datasets.\n\n\n\nThe function fetches JSON data from the CMS data URL and extracts relevant fields to create a tidy tibble. It selects specific columns, handles nested lists by unnesting them, cleans column names, and processes dates and media types to make the data more useful for analysis. The columns in the returned tibble include:\n\ntitle\ndescription\nlanding_page\nmodified\nkeyword\ndescribed_by\nfn\nhas_email\nidentifier\nstart\nend\nreferences\ndistribution_description\ndistribution_title\ndistribution_modified\ndistribution_start\ndistribution_end\nmedia_type\ndata_link"
  },
  {
    "objectID": "posts/2024-05-28/index.html#practical-examples",
    "href": "posts/2024-05-28/index.html#practical-examples",
    "title": "Unveiling the Power of get_cms_meta_data() in healthyR.data",
    "section": "",
    "text": "Let’s see the get_cms_meta_data() function in action with a couple of examples.\n\n\nFirst, we’ll load the necessary libraries and fetch some metadata:\n\n# Library Loads\nlibrary(healthyR.data)\nlibrary(dplyr)\n\n# Get data\ncms_data &lt;- get_cms_meta_data()\nglimpse(cms_data)\n\nRows: 107\nColumns: 19\n$ title                    &lt;chr&gt; \"Accountable Care Organization Participants\",…\n$ description              &lt;chr&gt; \"The Accountable Care Organization Participan…\n$ landing_page             &lt;chr&gt; \"https://data.cms.gov/medicare-shared-savings…\n$ modified                 &lt;date&gt; 2024-01-29, 2024-04-23, 2024-01-12, 2024-01-…\n$ keyword                  &lt;list&gt; &lt;\"Medicare\", \"Value-Based Care\", \"Coordinate…\n$ described_by             &lt;chr&gt; \"https://data.cms.gov/resources/accountable-c…\n$ fn                       &lt;chr&gt; \"Shared Savings Program - CM\", \"Shared Saving…\n$ has_email                &lt;chr&gt; \"SharedSavingsProgram@cms.hhs.gov\", \"SharedSa…\n$ identifier               &lt;chr&gt; \"https://data.cms.gov/data-api/v1/dataset/976…\n$ start                    &lt;date&gt; 2014-01-01, 2017-01-01, 2021-01-01, 2021-01-…\n$ end                      &lt;date&gt; 2024-12-31, 2024-12-31, 2021-12-31, 2021-12-…\n$ references               &lt;chr&gt; \"https://data.cms.gov/resources/acos-aco-part…\n$ distribution_description &lt;chr&gt; \"latest\", \"latest\", \"latest\", \"latest\", \"late…\n$ distribution_title       &lt;chr&gt; \"Accountable Care Organization Participants\",…\n$ distribution_modified    &lt;date&gt; 2024-01-29, 2024-04-23, 2024-01-12, 2024-01-…\n$ distribution_start       &lt;date&gt; 2024-01-01, 2024-01-01, 2021-01-01, 2021-01-…\n$ distribution_end         &lt;date&gt; 2024-12-31, 2024-12-31, 2021-12-31, 2021-12-…\n$ media_type               &lt;chr&gt; \"API\", \"API\", \"API\", \"API\", \"API\", \"API\", \"AP…\n$ data_link                &lt;chr&gt; \"https://data.cms.gov/data-api/v1/dataset/976…\n\n# Attributes\natb &lt;- attributes(cms_data)\natb$names\n\n [1] \"title\"                    \"description\"             \n [3] \"landing_page\"             \"modified\"                \n [5] \"keyword\"                  \"described_by\"            \n [7] \"fn\"                       \"has_email\"               \n [9] \"identifier\"               \"start\"                   \n[11] \"end\"                      \"references\"              \n[13] \"distribution_description\" \"distribution_title\"      \n[15] \"distribution_modified\"    \"distribution_start\"      \n[17] \"distribution_end\"         \"media_type\"              \n[19] \"data_link\"               \n\natb$class\n\n[1] \"cms_meta_data\" \"tbl_df\"        \"tbl\"           \"data.frame\"   \n\natb$url\n\n[1] \"https://data.cms.gov/data.json\"\n\natb$date_retrieved\n\n[1] \"2024-05-28 10:20:18 EDT\"\n\natb$parameters\n\n$.data_version\n[1] \"current\"\n\n$.media_type\n[1] \"all\"\n\n$.title\nNULL\n\n$.modified_date\nNULL\n\n$.keyword\nNULL\n\n$.identifier\nNULL\n\n\nIn this example, we’re simply calling get_cms_meta_data() without any parameters. This fetches the default dataset metadata. The glimpse() function from the dplyr package provides a quick overview of the data structure.\n\n\n\nNow, let’s refine our search by specifying a keyword and title:\n\nget_cms_meta_data(\n  .keyword = \"nation\",\n  .title = \"Market Saturation & Utilization State-County\"\n) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 19\n$ title                    &lt;chr&gt; \"Market Saturation & Utilization State-County\"\n$ description              &lt;chr&gt; \"The Market Saturation and Utilization State-…\n$ landing_page             &lt;chr&gt; \"https://data.cms.gov/summary-statistics-on-u…\n$ modified                 &lt;date&gt; 2024-04-02\n$ keyword                  &lt;list&gt; &lt;\"National\", \"States & Territories\", \"Countie…\n$ described_by             &lt;chr&gt; \"https://data.cms.gov/resources/market-satur…\n$ fn                       &lt;chr&gt; \"Market Saturation - CPI\"\n$ has_email                &lt;chr&gt; \"MarketSaturation@cms.hhs.gov\"\n$ identifier               &lt;chr&gt; \"https://data.cms.gov/data-api/v1/dataset/89…\n$ start                    &lt;date&gt; 2023-10-01\n$ end                      &lt;date&gt; 2023-12-31\n$ references               &lt;chr&gt; \"https://data.cms.gov/resources/market-satura…\n$ distribution_description &lt;chr&gt; \"latest\"\n$ distribution_title       &lt;chr&gt; \"Market Saturation & Utilization StateCounty\"\n$ distribution_modified    &lt;date&gt; 2024-04-02\n$ distribution_start       &lt;date&gt; 2023-10-01\n$ distribution_end         &lt;date&gt; 2023-12-31\n$ media_type               &lt;chr&gt; \"API\"\n$ data_link                &lt;chr&gt; \"https://data.cms.gov/data-api/v1/dataset/890…\n\n\nIn this example, we filter the metadata by the keyword “nation” and the title “Market Saturation & Utilization State-County”. The pipe operator (|&gt;) is used to pass the result directly into the glimpse() function for a quick preview."
  },
  {
    "objectID": "posts/2024-05-28/index.html#breaking-down-the-code",
    "href": "posts/2024-05-28/index.html#breaking-down-the-code",
    "title": "Unveiling the Power of get_cms_meta_data() in healthyR.data",
    "section": "",
    "text": "Let’s break down the code blocks to understand what they’re doing:\n\n\n\nLoad Libraries:\nlibrary(healthyR.data)\nlibrary(dplyr)\nWe load the healthyR.data package to access the get_cms_meta_data() function and the dplyr package for data manipulation.\nFetch Metadata:\ncms_data &lt;- get_cms_meta_data()\nWe call get_cms_meta_data() without any parameters to get the default dataset metadata.\nPreview Data:\nglimpse(cms_data)\nThe glimpse() function gives us a quick look at the structure and contents of the fetched metadata.\n\n\n\n\n\nCustom Search Call:\nget_cms_meta_data(\n  .keyword = \"nation\",\n  .title = \"Market Saturation & Utilization State-County\"\n) |&gt;\nglimpse()\nHere, we call get_cms_meta_data() with specific parameters for keyword and title to narrow down our search. The result is passed to glimpse() using the pipe operator for an immediate preview."
  },
  {
    "objectID": "posts/2024-05-28/index.html#conclusion",
    "href": "posts/2024-05-28/index.html#conclusion",
    "title": "Unveiling the Power of get_cms_meta_data() in healthyR.data",
    "section": "",
    "text": "The get_cms_meta_data() function is a versatile and flexible tool for accessing CMS metadata, making your data analysis tasks more efficient and effective. Whether you’re looking for specific datasets or just exploring the available metadata, this function has got you covered.\nTry out get_cms_meta_data() in your next R project and explore the potential of CMS data with ease! Happy coding! 🚀"
  },
  {
    "objectID": "posts/2024-05-29/index.html",
    "href": "posts/2024-05-29/index.html",
    "title": "Introducing get_provider_meta_data() in healthyR.data",
    "section": "",
    "text": "Introduction\nHello, R enthusiasts!\nToday, I’m excited to introduce a new function in the healthyR.data package: get_provider_meta_data(). This function is excellent for anyone working with healthcare datasets, making it easy to fetch and filter metadata from the Centers for Medicare & Medicaid Services (CMS) repository.\n\n\nOverview\nThe get_provider_meta_data() function simplifies the process of retrieving and managing metadata for healthcare datasets. By allowing users to filter data based on various criteria, it streamlines data management and enhances analytical capabilities.\n\n\nSyntax and Arguments\nThe function syntax is straightforward and highly customizable:\nget_provider_meta_data(\n  .identifier = NULL,\n  .title = NULL,\n  .description = NULL,\n  .keyword = NULL,\n  .issued = NULL,\n  .modified = NULL,\n  .released = NULL,\n  .theme = NULL,\n  .media_type = NULL\n)\nHere’s a breakdown of the arguments:\n\n.identifier: A dataset identifier to filter the data.\n.title: A title to filter the data.\n.description: A description to filter the data.\n.keyword: A keyword to filter the data.\n.issued: A date when the dataset was issued to filter the data.\n.modified: A date when the dataset was modified to filter the data.\n.released: A date when the dataset was released to filter the data.\n.theme: A theme to filter the data.\n.media_type: A media type to filter the data.\n\n\n\nWhat It Returns\nThe function returns a tidy tibble containing metadata about the datasets. This tibble includes the following columns:\n\nidentifier\ntitle\ndescription\nkeyword\nissued\nmodified\nreleased\ntheme\nmedia_type\ndownload_url\ncontact_fn\ncontact_email\npublisher_name\n\n\n\nDetails\nWhen you call get_provider_meta_data(), it fetches JSON data from the CMS metadata URL. The function then processes this data by: 1. Selecting relevant columns. 2. Unnesting nested lists. 3. Cleaning column names. 4. Processing dates and media types for enhanced usability.\n\n\nPractical Example\nLet’s walk through an example to see how get_provider_meta_data() works in action.\nSuppose we want to retrieve metadata for a dataset based upong a specific data identifier? Here’s how we can do it:\n\nlibrary(healthyR.data)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Retrieve metadata for a datset with identifier \"3614-1eef\"\nget_provider_meta_data(.identifier = \"3614-1eef\") |&gt;\n  glimpse()\n\nRows: 1\nColumns: 16\n$ identifier      &lt;chr&gt; \"3614-1eef\"\n$ title           &lt;chr&gt; \"Addiction Medicine Office Visit Costs\"\n$ description     &lt;chr&gt; \"Returns addiction medicine office visit costs per zip…\n$ keyword         &lt;list&gt; \"Addiction Medicine\"\n$ issued          &lt;date&gt; 2022-07-11\n$ modified        &lt;date&gt; 2022-07-11\n$ released        &lt;date&gt; 2023-09-28\n$ landing_page    &lt;chr&gt; \"https://data.medicare.gov/provider-data/dataset/3614-…\n$ theme           &lt;list&gt; \"Physician office visit costs\"\n$ access_level    &lt;chr&gt; \"public\"\n$ archive_exclude &lt;lgl&gt; NA\n$ contact_fn      &lt;chr&gt; \"PPL Dataset\"\n$ contact_email   &lt;chr&gt; \"PPL_Dataset@cms.hhs.gov\"\n$ publisher_name  &lt;chr&gt; \"Centers for Medicare & Medicaid Services (CMS)\"\n$ download_url    &lt;chr&gt; \"https://data.cms.gov/provider-data/sites/default/file…\n$ media_type      &lt;chr&gt; \"text/csv\"\n\n\nIn this example, we are filtering the metadata based on the dataset identifier “3614-1eef”. The glimpse() function allows us to view the structure of the resulting tibble.\nNow, what if we want to filter data that meets a certain keyword? Here’s how we can do that:\n\nprovider_data_tbl &lt;- get_provider_meta_data(.keyword = \"medic\")\n\n# Let's see all the titles that contain the keyword \"medic\"\nprovider_data_tbl[[\"title\"]]\n\n [1] \"Addiction Medicine Office Visit Costs\"                                     \n [2] \"Emergency Medicine Office Visit Costs\"                                     \n [3] \"Geriatric Medicine Office Visit Costs\"                                     \n [4] \"Internal Medicine Office Visit Costs\"                                      \n [5] \"Medical Genetics and Genomics Office Visit Costs\"                          \n [6] \"Medical Oncology Office Visit Costs\"                                       \n [7] \"Medical Toxicology Office Visit Costs\"                                     \n [8] \"Nuclear Medicine Office Visit Costs\"                                       \n [9] \"Osteopathic Manipulative Medicine Office Visit Costs\"                      \n[10] \"Pediatric Medicine Office Visit Costs\"                                     \n[11] \"Physical Medicine and Rehabilitation Office Visit Costs\"                   \n[12] \"Preventive Medicine Office Visit Costs\"                                    \n[13] \"Sleep Medicine Office Visit Costs\"                                         \n[14] \"Sports Medicine Office Visit Costs\"                                        \n[15] \"Undersea and Hyperbaric Medicine Office Visit Costs\"                       \n[16] \"Medical Equipment Suppliers\"                                               \n[17] \"Home Health Care - Patient Survey (HHCAHPS) 2022Q4 to 2023Q3\"              \n[18] \"Home Health Care - Patient Survey (HHCAHPS) National Data 2022Q4 to 2023Q3\"\n[19] \"Home Health Care - Patient Survey (HHCAHPS) State Data 2022Q4 to 2023Q3\"   \n[20] \"Home Health Care - Patient Survey (HHCAHPS) Measure Dates 2022Q4 to 2023Q3\"\n[21] \"Medicare Spending Per Beneficiary - Hospital Additional Decimal Places\"    \n[22] \"Hospital Value-Based Purchasing (HVBP) - Efficiency Scores\"                \n[23] \"Medicare Hospital Spending by Claim\"                                       \n[24] \"Medicare Spending Per Beneficiary - Hospital\"                              \n[25] \"Medicare Spending Per Beneficiary - National\"                              \n[26] \"Medicare Spending Per Beneficiary - State\"                                 \n\n# Now let's group them by theme\nprovider_data_tbl |&gt;\n  count(theme, sort = TRUE) |&gt;\n  unnest(cols = c(theme))\n\n# A tibble: 4 × 2\n  theme                            n\n  &lt;chr&gt;                        &lt;int&gt;\n1 Physician office visit costs    15\n2 Hospitals                        6\n3 Home health services             4\n4 Supplier directory               1\n\n\nIn this example, the metadata is filtered based on the keyword “medic”. We then extract the titles containing the keyword and group them by theme to see the distribution of themes in the filtered data. Notice that we filtered the keyword not on a full word but on a partial match, which can be useful for broad searches.\n\n\nBenefits of Using get_provider_meta_data()\nThis function is particularly useful for:\n\nData Scientists and Analysts: Quickly finding relevant datasets without manually searching through large repositories.\nHealthcare Researchers: Accessing comprehensive metadata to support research and analysis.\nDevelopers: Integrating CMS metadata retrieval into applications or workflows with minimal effort.\n\n\n\nConclusion\nThe get_provider_meta_data() function is a robust tool for anyone working with healthcare data. It not only saves time but also provides a cleaner, more efficient way to manage and analyze dataset metadata.\nGive it a try and see how it can enhance your data workflows. Happy coding!\nFeel free to share your experiences and any creative ways you’re using this function in the comments below. Until next time, keep exploring and innovating with R!\n\nSteve"
  },
  {
    "objectID": "posts/2024-05-30/index.html",
    "href": "posts/2024-05-30/index.html",
    "title": "Exciting New Updates to TidyDensity: Enhancing Distribution Analysis!",
    "section": "",
    "text": "Hello, fellow R enthusiasts! I’m thrilled to share some fantastic updates to the TidyDensity package. These updates bring a wealth of new features, functions, and enhancements, making distribution analysis more comprehensive and efficient. Let’s dive into the details!\n\n\n\n\n\nutil_negative_binomial_aic(): Calculate the Akaike Information Criterion (AIC) for the negative binomial distribution. This function aids in model selection, helping you determine the best-fitting model for your data.\n\n\n\n\n\nutil_zero_truncated_negative_binomial_param_estimate(): Estimate the parameters of the zero-truncated negative binomial distribution.\nutil_zero_truncated_negative_binomial_aic(): Calculate the AIC for the zero-truncated negative binomial distribution.\nutil_zero_truncated_negative_binomial_stats_tbl(): Create a summary table for the zero-truncated negative binomial distribution.\n\n\n\n\n\nutil_zero_truncated_poisson_param_estimate(): Estimate the parameters of the zero-truncated Poisson distribution.\nutil_zero_truncated_poisson_aic(): Calculate the AIC for the zero-truncated Poisson distribution.\nutil_zero_truncated_poisson_stats_tbl(): Create a summary table for the zero-truncated Poisson distribution.\n\n\n\n\n\nutil_f_param_estimate(): Estimate the parameters for the F distribution.\nutil_f_aic(): Calculate the AIC for the F distribution.\n\n\n\n\n\nutil_zero_truncated_geometric_param_estimate(): Estimate the parameters of the zero-truncated geometric distribution.\nutil_zero_truncated_geometric_aic(): Calculate the AIC for the zero-truncated geometric distribution.\nutil_zero_truncated_geometric_stats_tbl(): Create a summary table for the zero-truncated geometric distribution.\n\n\n\n\n\nutil_triangular_aic(): Calculate the AIC for the triangular distribution.\n\n\n\n\n\nutil_t_param_estimate(): Estimate the parameters of the T distribution.\nutil_t_aic(): Calculate the AIC for the T distribution.\n\n\n\n\n\nutil_pareto1_param_estimate(): Estimate the parameters of the Pareto Type I distribution.\nutil_pareto1_aic(): Calculate the AIC for the Pareto Type I distribution.\nutil_pareto1_stats_tbl(): Create a summary table for the Pareto Type I distribution.\n\n\n\n\n\nutil_paralogistic_param_estimate(): Estimate the parameters of the paralogistic distribution.\nutil_paralogistic_aic(): Calculate the AIC for the paralogistic distribution.\nutil_paralogistic_stats_tbl(): Create a summary table for the paralogistic distribution.\n\n\n\n\n\nutil_inverse_weibull_param_estimate(): Estimate the parameters of the Inverse Weibull distribution.\nutil_inverse_weibull_aic(): Calculate the AIC for the Inverse Weibull distribution.\nutil_inverse_weibull_stats_tbl(): Create a summary table for the Inverse Weibull distribution.\n\n\n\n\n\nutil_inverse_pareto_param_estimate(): Estimate the parameters of the Inverse Pareto distribution.\nutil_inverse_pareto_aic(): Calculate the AIC for the Inverse Pareto distribution.\nutil_inverse_pareto_stats_tbl(): Create a summary table for the Inverse Pareto distribution.\n\n\n\n\n\nutil_inverse_burr_param_estimate(): Estimate the parameters of the Inverse Gamma distribution.\nutil_inverse_burr_aic(): Calculate the AIC for the Inverse Gamma distribution.\nutil_inverse_burr_stats_tbl(): Create a summary table for the Inverse Gamma distribution.\n\n\n\n\n\nutil_generalized_pareto_param_estimate(): Estimate the parameters of the Generalized Pareto distribution.\nutil_generalized_pareto_aic(): Calculate the AIC for the Generalized Pareto distribution.\nutil_generalized_pareto_stats_tbl(): Create a summary table for the Generalized Pareto distribution.\n\n\n\n\n\nutil_generalized_beta_param_estimate(): Estimate the parameters of the Generalized Gamma distribution.\nutil_generalized_beta_aic(): Calculate the AIC for the Generalized Gamma distribution.\nutil_generalized_beta_stats_tbl(): Create a summary table for the Generalized Gamma distribution.\n\n\n\n\n\nutil_zero_truncated_binomial_stats_tbl(): Create a summary table for the Zero Truncated binomial distribution.\nutil_zero_truncated_binomial_param_estimate(): Estimate the parameters of the Zero Truncated binomial distribution.\nutil_zero_truncated_binomial_aic(): Calculate the AIC for the Zero Truncated binomial distribution.\n\n\n\n\n\n\nutil_negative_binomial_param_estimate(): Updated to use optim() for parameter estimation, enhancing accuracy and efficiency.\nquantile_normalize(): Added names to columns when .return_tibble = TRUE for better readability and usability.\n\n\n\n\nThese updates significantly expand the functionality of TidyDensity, providing more tools for robust distribution analysis. Whether you’re working with standard or specialized distributions, these new functions and improvements will streamline your workflow and enhance your analytical capabilities.\nI encourage you to explore these new features and see how they can benefit your projects. As always, your feedback is invaluable, so please share your thoughts and experiences with these updates. Happy coding!"
  },
  {
    "objectID": "posts/2024-05-30/index.html#new-features",
    "href": "posts/2024-05-30/index.html#new-features",
    "title": "Exciting New Updates to TidyDensity: Enhancing Distribution Analysis!",
    "section": "",
    "text": "util_negative_binomial_aic(): Calculate the Akaike Information Criterion (AIC) for the negative binomial distribution. This function aids in model selection, helping you determine the best-fitting model for your data.\n\n\n\n\n\nutil_zero_truncated_negative_binomial_param_estimate(): Estimate the parameters of the zero-truncated negative binomial distribution.\nutil_zero_truncated_negative_binomial_aic(): Calculate the AIC for the zero-truncated negative binomial distribution.\nutil_zero_truncated_negative_binomial_stats_tbl(): Create a summary table for the zero-truncated negative binomial distribution.\n\n\n\n\n\nutil_zero_truncated_poisson_param_estimate(): Estimate the parameters of the zero-truncated Poisson distribution.\nutil_zero_truncated_poisson_aic(): Calculate the AIC for the zero-truncated Poisson distribution.\nutil_zero_truncated_poisson_stats_tbl(): Create a summary table for the zero-truncated Poisson distribution.\n\n\n\n\n\nutil_f_param_estimate(): Estimate the parameters for the F distribution.\nutil_f_aic(): Calculate the AIC for the F distribution.\n\n\n\n\n\nutil_zero_truncated_geometric_param_estimate(): Estimate the parameters of the zero-truncated geometric distribution.\nutil_zero_truncated_geometric_aic(): Calculate the AIC for the zero-truncated geometric distribution.\nutil_zero_truncated_geometric_stats_tbl(): Create a summary table for the zero-truncated geometric distribution.\n\n\n\n\n\nutil_triangular_aic(): Calculate the AIC for the triangular distribution.\n\n\n\n\n\nutil_t_param_estimate(): Estimate the parameters of the T distribution.\nutil_t_aic(): Calculate the AIC for the T distribution.\n\n\n\n\n\nutil_pareto1_param_estimate(): Estimate the parameters of the Pareto Type I distribution.\nutil_pareto1_aic(): Calculate the AIC for the Pareto Type I distribution.\nutil_pareto1_stats_tbl(): Create a summary table for the Pareto Type I distribution.\n\n\n\n\n\nutil_paralogistic_param_estimate(): Estimate the parameters of the paralogistic distribution.\nutil_paralogistic_aic(): Calculate the AIC for the paralogistic distribution.\nutil_paralogistic_stats_tbl(): Create a summary table for the paralogistic distribution.\n\n\n\n\n\nutil_inverse_weibull_param_estimate(): Estimate the parameters of the Inverse Weibull distribution.\nutil_inverse_weibull_aic(): Calculate the AIC for the Inverse Weibull distribution.\nutil_inverse_weibull_stats_tbl(): Create a summary table for the Inverse Weibull distribution.\n\n\n\n\n\nutil_inverse_pareto_param_estimate(): Estimate the parameters of the Inverse Pareto distribution.\nutil_inverse_pareto_aic(): Calculate the AIC for the Inverse Pareto distribution.\nutil_inverse_pareto_stats_tbl(): Create a summary table for the Inverse Pareto distribution.\n\n\n\n\n\nutil_inverse_burr_param_estimate(): Estimate the parameters of the Inverse Gamma distribution.\nutil_inverse_burr_aic(): Calculate the AIC for the Inverse Gamma distribution.\nutil_inverse_burr_stats_tbl(): Create a summary table for the Inverse Gamma distribution.\n\n\n\n\n\nutil_generalized_pareto_param_estimate(): Estimate the parameters of the Generalized Pareto distribution.\nutil_generalized_pareto_aic(): Calculate the AIC for the Generalized Pareto distribution.\nutil_generalized_pareto_stats_tbl(): Create a summary table for the Generalized Pareto distribution.\n\n\n\n\n\nutil_generalized_beta_param_estimate(): Estimate the parameters of the Generalized Gamma distribution.\nutil_generalized_beta_aic(): Calculate the AIC for the Generalized Gamma distribution.\nutil_generalized_beta_stats_tbl(): Create a summary table for the Generalized Gamma distribution.\n\n\n\n\n\nutil_zero_truncated_binomial_stats_tbl(): Create a summary table for the Zero Truncated binomial distribution.\nutil_zero_truncated_binomial_param_estimate(): Estimate the parameters of the Zero Truncated binomial distribution.\nutil_zero_truncated_binomial_aic(): Calculate the AIC for the Zero Truncated binomial distribution."
  },
  {
    "objectID": "posts/2024-05-30/index.html#minor-improvements-and-fixes",
    "href": "posts/2024-05-30/index.html#minor-improvements-and-fixes",
    "title": "Exciting New Updates to TidyDensity: Enhancing Distribution Analysis!",
    "section": "",
    "text": "util_negative_binomial_param_estimate(): Updated to use optim() for parameter estimation, enhancing accuracy and efficiency.\nquantile_normalize(): Added names to columns when .return_tibble = TRUE for better readability and usability."
  },
  {
    "objectID": "posts/2024-05-30/index.html#conclusion",
    "href": "posts/2024-05-30/index.html#conclusion",
    "title": "Exciting New Updates to TidyDensity: Enhancing Distribution Analysis!",
    "section": "",
    "text": "These updates significantly expand the functionality of TidyDensity, providing more tools for robust distribution analysis. Whether you’re working with standard or specialized distributions, these new functions and improvements will streamline your workflow and enhance your analytical capabilities.\nI encourage you to explore these new features and see how they can benefit your projects. As always, your feedback is invaluable, so please share your thoughts and experiences with these updates. Happy coding!"
  },
  {
    "objectID": "posts/2024-05-31/index.html",
    "href": "posts/2024-05-31/index.html",
    "title": "An Overview of the New AIC Functions in the TidyDensity Package",
    "section": "",
    "text": "The latest update the the TidyDensity package introduces several new functions that make it easier to work with data in R. In this article, we’ll take a look at the new AIC functions and how they work."
  },
  {
    "objectID": "posts/2024-05-31/index.html#usage",
    "href": "posts/2024-05-31/index.html#usage",
    "title": "An Overview of the New AIC Functions in the TidyDensity Package",
    "section": "Usage",
    "text": "Usage\nutil_negative_binomial_aic()"
  },
  {
    "objectID": "posts/2024-05-31/index.html#arguments",
    "href": "posts/2024-05-31/index.html#arguments",
    "title": "An Overview of the New AIC Functions in the TidyDensity Package",
    "section": "Arguments",
    "text": "Arguments\n\n.x: A numeric vector of data values."
  },
  {
    "objectID": "posts/2024-05-31/index.html#value",
    "href": "posts/2024-05-31/index.html#value",
    "title": "An Overview of the New AIC Functions in the TidyDensity Package",
    "section": "Value",
    "text": "Value\nA numeric value representing the AIC for the given data and distribution."
  },
  {
    "objectID": "posts/2024-05-31/index.html#details",
    "href": "posts/2024-05-31/index.html#details",
    "title": "An Overview of the New AIC Functions in the TidyDensity Package",
    "section": "Details",
    "text": "Details\nThis function calculates the Akaike Information Criterion (AIC) for a distribution fitted to the provided data.\nThis function fits a distribution to the provided data. It estimates the parameters of the distribution from the data. Then, it calculates the AIC value based on the fitted distribution.\nInitial parameter estimates: The function uses the param estimate family of functions in order to estimate the starting point of the parameters. For example util_negative_binomial_param_estimate().\nOptimization method: Since the parameters are directly calculated from the data, no optimization is needed.\nGoodness-of-fit: While AIC is a useful metric for model comparison, it’s recommended to also assess the goodness-of-fit of the chosen model using visualization and other statistical tests."
  },
  {
    "objectID": "posts/2024-05-31/index.html#examples",
    "href": "posts/2024-05-31/index.html#examples",
    "title": "An Overview of the New AIC Functions in the TidyDensity Package",
    "section": "Examples",
    "text": "Examples\n\nlibrary(TidyDensity)\n\nset.seed(123)\n# Generate some data\nx &lt;- rnorm(100)\n\n# Calculate the AIC for a negative binomial distribution\ncat(\n  \" AIC of rnorm() using TidyDensity: \", util_normal_aic(x), \"\\n\",\n  \"AIC of rnorm() using fitdistrplus: \", \n  fitdistrplus::fitdist(x, \"norm\")$aic\n)\n\n AIC of rnorm() using TidyDensity:  268.5385 \n AIC of rnorm() using fitdistrplus:  268.5385"
  },
  {
    "objectID": "posts/2024-05-31/index.html#new-aic-functions",
    "href": "posts/2024-05-31/index.html#new-aic-functions",
    "title": "An Overview of the New AIC Functions in the TidyDensity Package",
    "section": "New AIC Functions",
    "text": "New AIC Functions\nHere is a listing of all of the new AIC functions:\n\nutil_negative_binomial_aic()\nutil_zero_truncated_negative_binomial_aic()\nutil_zero_truncated_poisson_aic()\nutil_f_aic()\nutil_zero_truncated_geometric_aic()\nutil_t_aic()\nutil_pareto1_aic()\nutil_paralogistic_aic()\nutil_inverse_weibull_aic()\nutil_pareto_aic()\nutil_inverse_burr_aic()\nutil_generalized_pareto_aic()\nutil_generalized_beta_aic()\nutil_zero_truncated_binomial_aic()"
  },
  {
    "objectID": "posts/2024-06-03/index.html",
    "href": "posts/2024-06-03/index.html",
    "title": "An Overview of the New Parameter Estimate Functions in the TidyDensity Package",
    "section": "",
    "text": "Hello, R enthusiasts! I’m excited to share some fantastic updates to the TidyDensity package. These updates introduce a suite of parameter estimate functions designed to make your data analysis more efficient and insightful. Whether you’re dealing with common distributions or more specialized ones, these functions have got you covered."
  },
  {
    "objectID": "posts/2024-06-03/index.html#exploring-the-new-parameter-estimate-functions-in-tidydensity",
    "href": "posts/2024-06-03/index.html#exploring-the-new-parameter-estimate-functions-in-tidydensity",
    "title": "An Overview of the New Parameter Estimate Functions in the TidyDensity Package",
    "section": "",
    "text": "Hello, R enthusiasts! I’m excited to share some fantastic updates to the TidyDensity package. These updates introduce a suite of parameter estimate functions designed to make your data analysis more efficient and insightful. Whether you’re dealing with common distributions or more specialized ones, these functions have got you covered.\n\n\nParameter estimation is crucial when working with statistical distributions. It allows you to infer the parameters of a distribution from your data, providing insights into its underlying structure. This is particularly useful when you want to model real-world phenomena accurately.\n\n\n\nHere’s a quick rundown of the newly introduced functions in TidyDensity:\n\nutil_zero_truncated_negative_binomial_param_estimate()\nutil_zero_truncated_poisson_param_estimate()\nutil_f_param_estimate()\nutil_zero_truncated_geometric_param_estimate()\nutil_t_param_estimate()\nutil_pareto1_param_estimate()\nutil_paralogistic_param_estimate()\nutil_inverse_weibull_param_estimate()\nutil_inverse_pareto_param_estimate()\nutil_inverse_burr_param_estimate()\nutil_generalized_pareto_param_estimate()\nutil_generalized_beta_param_estimate()\nutil_zero_truncated_binomial_param_estimate()\n\nEach function is tailored to a specific distribution, providing a streamlined way to estimate its parameters.\n\n\n\nLet’s dive into an example using the util_t_param_estimate() function. Suppose you have data that you believe follows a t distribution. Here’s how you can estimate its parameters:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(TidyDensity)\n\nset.seed(123)\nx &lt;- rt(100, df = 10, ncp = 0.5)\noutput &lt;- util_t_param_estimate(x)\n\n# Display the estimated parameters\nprint(output$parameter_tbl)\n\n# A tibble: 2 × 7\n  dist_type      samp_size  mean variance method df_est ncp_est\n  &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 T Distribution       100 0.612    0.949 MME     0.959   0.612\n2 T Distribution       100 0.612    0.949 MLE     8.32    0.571\n\n\nIn this example, we generated some data from a t distribution with degrees of freedom (df) of 10 and a non-centrality parameter (ncp) of 0.5. Using the util_t_param_estimate() function, we estimated these parameters from the data.\nThe parameter_tbl in the output contains the estimated values, while combined_data_tbl can be used for visualization. The tidy_combined_autoplot() function provides a convenient way to plot the data and see how well the estimated distribution fits.\n\n\n\nHere’s what the output might look like:\n\n# Visualize the combined data\noutput$combined_data_tbl |&gt;\n  tidy_combined_autoplot()\n\n\n\n\n\n\n\n\nIn the above plot, we visualize the output of the util_t_param_estimate() function from the TidyDensity package. The visualization shows how well the estimated t distribution fits our sample data. The x-axis represents the data values, while the y-axis shows the density. The different colors represent the data and the estimated density functions.\n\n\n\nEach of the new parameter estimate functions follows a similar approach. Here’s a step-by-step guide to get you started:\n\nLoad your data: Ensure your data is properly formatted and loaded into R.\nSelect the appropriate function: Choose the function that matches the distribution you believe your data follows.\nEstimate the parameters: Use the selected function to estimate the parameters.\nAnalyze and visualize: Review the estimated parameters and use the visualization functions to see how well the estimated distribution fits your data.\n\n\n\n\nI highly encourage you to try these new functions on your own datasets. Whether you’re working with common distributions or tackling more specialized ones, these tools can help you gain deeper insights into your data.\nFeel free to experiment and see how these functions perform with different types of data. The more you explore, the better you’ll understand the strengths and applications of each distribution.\n\n\n\nThe new parameter estimate functions in TidyDensity open up exciting possibilities for data analysis. By simplifying the process of parameter estimation, they allow you to focus more on interpreting results and making informed decisions.\nGive these functions a try and see how they can enhance your analysis workflow. Happy coding!"
  },
  {
    "objectID": "posts/2024-06-03/index.html#visualizing-the-results",
    "href": "posts/2024-06-03/index.html#visualizing-the-results",
    "title": "An Overview of the New Parameter Estimate Functions in the TidyDensity Package",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\nHere’s what the output might look like:\n\n# Visualize the combined data\noutput$combined_data_tbl |&gt;\n  tidy_combined_autoplot(.interactive = TRUE)\n\n\n\n\n\nIn the above plot, we visualize the output of the util_t_param_estimate() function from the TidyDensity package. The visualization shows how well the estimated t distribution fits our sample data. The x-axis represents the data values, while the y-axis shows the density. The different colors represent the data and the estimated density functions."
  },
  {
    "objectID": "posts/2024-06-04/index.html",
    "href": "posts/2024-06-04/index.html",
    "title": "Unveiling New Tools in the TidyDensity Arsenal: Distribution Parameter Wrangling",
    "section": "",
    "text": "Introduction\nGreetings, fellow data enthusiasts! Today, we’re thrilled to unveil a fresh wave of functionalities in the ever-evolving TidyDensity package. Buckle up, as we delve into the realm of distribution statistics!\nThis update brings a bounty of new functions that streamline the process of extracting key parameters from various probability distributions. These functions adhere to the familiar naming convention util_distribution_name_stats_tbl(), making them easily discoverable within your R workflow.\nLet’s meet the newcomers:\n\nutil_zero_truncated_negative_binomial_stats_tbl(): Uncovers the secrets of the zero-truncated negative binomial distribution.\nutil_zero_truncated_poisson_stats_tbl(): Demystifies the zero-truncated Poisson distribution.\nutil_zero_truncated_geometric_stats_tbl(): Unveils the hidden characteristics of the zero-truncated geometric distribution.\nutil_pareto1_stats_tbl(): Extracts the essence of the Pareto Type I distribution.\nutil_paralogistic_stats_tbl(): Unlocks the mysteries of the paralogistic distribution.\nutil_inverse_weibull_stats_tbl(): Illuminates the parameters of the inverse Weibull distribution.\nutil_inverse_pareto_stats_tbl(): Provides insights into the inverse Pareto distribution.\nutil_inverse_burr_stats_tbl(): Offers a glimpse into the world of the inverse Burr distribution.\nutil_generalized_pareto_stats_tbl(): Simplifies extracting parameters from the generalized Pareto distribution.\n\nNow, you might be wondering, “How do I put these new functions to use?” Fear not, for the answer is as easy as pie!\n\n\nExamples\nLet’s explore the zero-truncated binomial distribution. Suppose we’re simulating the number of successes in 10 trials with a success probability of 0.1 (but hey, successes of zero aren’t possible in this scenario!).\n\nlibrary(dplyr)\nlibrary(TidyDensity)  # Assuming you've installed TidyDensity\n\nset.seed(123)\ntidy_zero_truncated_binomial(.size = 10, .prob = 0.1) |&gt;\n  util_zero_truncated_binomial_stats_tbl() |&gt;\n  glimpse()\n\nRows: 1\nColumns: 15\n$ tidy_function     &lt;chr&gt; \"tidy_zero_truncated_binomial\"\n$ function_call     &lt;chr&gt; \"Zero Truncated Binomial c(10, 0.1)\"\n$ distribution      &lt;chr&gt; \"Zero Truncated Binomial\"\n$ distribution_type &lt;chr&gt; \"discrete\"\n$ points            &lt;dbl&gt; 50\n$ simulations       &lt;dbl&gt; 1\n$ mean              &lt;dbl&gt; 1.58\n$ mode              &lt;dbl&gt; 1\n$ range             &lt;chr&gt; \"1 to 4\"\n$ std_dv            &lt;dbl&gt; 0.8103917\n$ coeff_var         &lt;dbl&gt; 0.5129061\n$ computed_std_skew &lt;dbl&gt; 1.133051\n$ computed_std_kurt &lt;dbl&gt; 3.212143\n$ ci_lo             &lt;dbl&gt; 1\n$ ci_hi             &lt;dbl&gt; 3\n\n\nThis code snippet generates a dataset of zero-truncated binomial values and then utilizes the util_zero_truncated_binomial_stats_tbl() function to extract a summary table containing key parameters like the mean, variance, and quantiles.\n\n\nYour Turn to Explore!\nWe encourage you to jump in and experiment with these new additions. Explore the documentation for each function (accessible through ?util_distribution_name_stats_tbl) to discover their specific functionalities and supported distributions.\nWith these new tools at your disposal, you’ll be well-equipped to gain deeper insights into your data and unlock the power of various probability distributions in your R adventures!"
  },
  {
    "objectID": "posts/2024-06-05/index.html",
    "href": "posts/2024-06-05/index.html",
    "title": "How to Split a Character String and Get the First Element in R",
    "section": "",
    "text": "Hello, R community!\nToday, we’re jumping into a common yet powerful task in data manipulation: splitting character strings and extracting the first element. We’ll explore how to accomplish this in base R, as well as using the stringi and stringr packages.\nLet’s get started!"
  },
  {
    "objectID": "posts/2024-06-05/index.html#using-strsplit-in-base-r",
    "href": "posts/2024-06-05/index.html#using-strsplit-in-base-r",
    "title": "How to Split a Character String and Get the First Element in R",
    "section": "Using strsplit() in Base R",
    "text": "Using strsplit() in Base R\nBase R provides the strsplit() function for splitting strings. Here’s a quick look at the syntax:\nstrsplit(x, split, fixed = FALSE, perl = FALSE, useBytes = FALSE)\n\nx: Character vector to be split.\nsplit: The delimiter (separator) to use for splitting.\nfixed: If TRUE, split is interpreted as a string, not a regular expression.\nperl: If TRUE, perl-compatible regular expressions can be used.\nuseBytes: If TRUE, the operation is performed byte-wise rather than character-wise.\n\n\nExample 1: Splitting a single string\n\nstring &lt;- \"apple,orange,banana\"\nsplit_result &lt;- strsplit(string, \",\")\nfirst_element &lt;- sapply(split_result, `[`, 1)\nprint(first_element)\n\n[1] \"apple\"\n\n\n\n\nExample 2: Splitting a vector of strings\n\nstrings &lt;- c(\"apple,orange,banana\", \"cat,dog,mouse\")\nsplit_results &lt;- strsplit(strings, \",\")\nfirst_elements &lt;- sapply(split_results, `[`, 1)\nprint(first_elements)\n\n[1] \"apple\" \"cat\""
  },
  {
    "objectID": "posts/2024-06-05/index.html#using-stringi-package",
    "href": "posts/2024-06-05/index.html#using-stringi-package",
    "title": "How to Split a Character String and Get the First Element in R",
    "section": "Using stringi Package",
    "text": "Using stringi Package\nThe stringi package offers a powerful function stri_split_fixed() for splitting strings. Let’s look at its syntax:\nstri_split_fixed(str, pattern, n = -1, simplify = FALSE)\n\nstr: Character vector to be split.\npattern: The delimiter for splitting.\nn: Maximum number of pieces to return.\nsimplify: If TRUE, returns a matrix.\n\n\nExample 1: Splitting a single string\n\nlibrary(stringi)\nstring &lt;- \"apple,orange,banana\"\nsplit_result &lt;- stri_split_fixed(string, \",\")\nfirst_element &lt;- sapply(split_result, `[`, 1)\nprint(first_element)\n\n[1] \"apple\"\n\n\n\n\nExample 2: Splitting a vector of strings\n\nstrings &lt;- c(\"apple,orange,banana\", \"cat,dog,mouse\")\nsplit_results &lt;- stri_split_fixed(strings, \",\")\nfirst_elements &lt;- sapply(split_results, `[`, 1)\nprint(first_elements)\n\n[1] \"apple\" \"cat\""
  },
  {
    "objectID": "posts/2024-06-05/index.html#using-stringr-package",
    "href": "posts/2024-06-05/index.html#using-stringr-package",
    "title": "How to Split a Character String and Get the First Element in R",
    "section": "Using stringr Package",
    "text": "Using stringr Package\nThe stringr package provides str_split_fixed() and str_split() functions. Here’s the syntax for str_split():\nstr_split(string, pattern, n = Inf, simplify = FALSE)\n\nstring: Character vector to be split.\npattern: The delimiter for splitting.\nn: Maximum number of pieces to return.\nsimplify: If TRUE, returns a matrix.\n\n\nExample 1: Splitting a single string\n\nlibrary(stringr)\nstring &lt;- \"apple,orange,banana\"\nsplit_result &lt;- str_split(string, \",\")\nfirst_element &lt;- sapply(split_result, `[`, 1)\nprint(first_element)\n\n[1] \"apple\"\n\n\n\n\nExample 2: Splitting a vector of strings\n\nstrings &lt;- c(\"apple,orange,banana\", \"cat,dog,mouse\")\nsplit_results &lt;- str_split(strings, \",\")\nfirst_elements &lt;- sapply(split_results, `[`, 1)\nprint(first_elements)\n\n[1] \"apple\" \"cat\""
  },
  {
    "objectID": "posts/2024-06-06/index.html",
    "href": "posts/2024-06-06/index.html",
    "title": "Introduction of My Content Series",
    "section": "",
    "text": "Hello, everyone!\nI’m excited to introduce a new content series that will be shared across multiple platforms, including my blog, LinkedIn, Mastodon, and Telegram. This series is designed to provide you with valuable insights, practical tips, and deep dives into the tools and techniques I’ve developed and co-authored. Whether you’re a data scientist, analyst, or someone looking to enhance your data manipulation skills, there will be something for everyone.\nHere’s what you can expect each week:\n\n\nOn Mondays, I’ll introduce one of the packages I have written. These introductions will cover the package’s purpose, its main features, and how it can help you in your data analysis and visualization tasks. Whether you’re familiar with my work or new to it, these posts will provide a comprehensive overview of each package’s capabilities.\n\n\n\nTuesdays will be dedicated to exploring a specific R function. I’ll provide a detailed explanation of how the function works, its applications, and some examples to help you understand how to use it effectively in your projects. This will be a great way to expand your R programming skills and learn new techniques.\n\n\n\nOn Wednesdays, we’ll explore the integration of VBA and R. These posts will show you how to leverage the power of both tools to automate tasks, enhance your Excel capabilities, and streamline your workflows. If you’re looking to bridge the gap between Excel and R, these sessions will be invaluable.\n\n\n\nThursdays will feature practical examples from the package introduced on Monday. I’ll walk you through real-world scenarios and show you how to apply the package to solve specific problems. These examples will help you see the practical applications of the tools and give you ideas for your own projects.\n\n\n\nFinally, Fridays will be dedicated to insights and snippets from my book, co-authored with David Kun, titled “Extending Excel with Python and R.” We’ll cover various topics from the book, providing you with a sneak peek into its contents and practical tips for extending Excel’s functionality using Python and R.\nI am looking forward to sharing this journey with you and hearing your feedback. Make sure to follow along, and don’t hesitate to ask questions or share your thoughts in the comments. Let’s learn and grow together!\nStay tuned for the first post of this series coming next Monday!\nBest regards,\nSteve\nConnect with me:\n\nWebsite: www.spsanderson.com\nBlog: www.spsanderson.com/steveondata/\nLinkedIn: www.linkedin.com/in/spsanderson\nMastodon: mstdn.social/@stevensanderson\nTelegram: t.me/steveondata"
  },
  {
    "objectID": "posts/2024-06-06/index.html#monday-introduction-to-a-package",
    "href": "posts/2024-06-06/index.html#monday-introduction-to-a-package",
    "title": "Introduction of My Content Series",
    "section": "",
    "text": "On Mondays, I’ll introduce one of the packages I have written. These introductions will cover the package’s purpose, its main features, and how it can help you in your data analysis and visualization tasks. Whether you’re familiar with my work or new to it, these posts will provide a comprehensive overview of each package’s capabilities."
  },
  {
    "objectID": "posts/2024-06-06/index.html#tuesday-an-r-function",
    "href": "posts/2024-06-06/index.html#tuesday-an-r-function",
    "title": "Introduction of My Content Series",
    "section": "",
    "text": "Tuesdays will be dedicated to exploring a specific R function. I’ll provide a detailed explanation of how the function works, its applications, and some examples to help you understand how to use it effectively in your projects. This will be a great way to expand your R programming skills and learn new techniques."
  },
  {
    "objectID": "posts/2024-06-06/index.html#wednesday-vba-and-r",
    "href": "posts/2024-06-06/index.html#wednesday-vba-and-r",
    "title": "Introduction of My Content Series",
    "section": "",
    "text": "On Wednesdays, we’ll explore the integration of VBA and R. These posts will show you how to leverage the power of both tools to automate tasks, enhance your Excel capabilities, and streamline your workflows. If you’re looking to bridge the gap between Excel and R, these sessions will be invaluable."
  },
  {
    "objectID": "posts/2024-06-06/index.html#thursday-practical-example",
    "href": "posts/2024-06-06/index.html#thursday-practical-example",
    "title": "Introduction of My Content Series",
    "section": "",
    "text": "Thursdays will feature practical examples from the package introduced on Monday. I’ll walk you through real-world scenarios and show you how to apply the package to solve specific problems. These examples will help you see the practical applications of the tools and give you ideas for your own projects."
  },
  {
    "objectID": "posts/2024-06-06/index.html#friday-insights-from-extending-excel-with-python-and-r",
    "href": "posts/2024-06-06/index.html#friday-insights-from-extending-excel-with-python-and-r",
    "title": "Introduction of My Content Series",
    "section": "",
    "text": "Finally, Fridays will be dedicated to insights and snippets from my book, co-authored with David Kun, titled “Extending Excel with Python and R.” We’ll cover various topics from the book, providing you with a sneak peek into its contents and practical tips for extending Excel’s functionality using Python and R.\nI am looking forward to sharing this journey with you and hearing your feedback. Make sure to follow along, and don’t hesitate to ask questions or share your thoughts in the comments. Let’s learn and grow together!\nStay tuned for the first post of this series coming next Monday!\nBest regards,\nSteve\nConnect with me:\n\nWebsite: www.spsanderson.com\nBlog: www.spsanderson.com/steveondata/\nLinkedIn: www.linkedin.com/in/spsanderson\nMastodon: mstdn.social/@stevensanderson\nTelegram: t.me/steveondata"
  },
  {
    "objectID": "posts/2024-06-07/index.html",
    "href": "posts/2024-06-07/index.html",
    "title": "How to Check if a Character is in a String in R",
    "section": "",
    "text": "When working with text data in R, one common task is to check if a character or substring is present within a larger string. R offers multiple ways to accomplish this, ranging from base R functions to packages like stringr and stringi. In this post, we’ll explore how to use grepl() from base R, str_detect() from stringr, and stri_detect_fixed() from stringi to achieve this."
  },
  {
    "objectID": "posts/2024-06-07/index.html#using-grepl-in-base-r",
    "href": "posts/2024-06-07/index.html#using-grepl-in-base-r",
    "title": "How to Check if a Character is in a String in R",
    "section": "Using grepl() in Base R",
    "text": "Using grepl() in Base R\nThe grepl() function in base R is a handy tool for detecting patterns within strings. It returns TRUE if the pattern is found and FALSE otherwise.\nSyntax:\ngrepl(pattern, x, ignore.case = FALSE, fixed = FALSE)\n\npattern: The character string to search for.\nx: The character vector where the search is performed.\nignore.case: Logical value indicating whether the search should be case-insensitive.\nfixed: Logical value indicating whether to treat the pattern as a fixed string.\n\n\nExample:\n\ntext &lt;- \"Hello, World!\"\n# Check if 'World' is in the text\nresult &lt;- grepl(\"World\", text)\nprint(result)\n\n[1] TRUE\n\n# Check if 'world' is in the text, ignoring case\nresult &lt;- grepl(\"world\", text, ignore.case = TRUE)\nprint(result)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/2024-06-07/index.html#using-str_detect-from-stringr",
    "href": "posts/2024-06-07/index.html#using-str_detect-from-stringr",
    "title": "How to Check if a Character is in a String in R",
    "section": "Using str_detect() from stringr",
    "text": "Using str_detect() from stringr\nThe stringr package simplifies string operations with a consistent and user-friendly interface. The str_detect() function checks for the presence of a pattern in a string.\nSyntax:\nlibrary(stringr)\nstr_detect(string, pattern)\n\nstring: The character vector to search in.\npattern: The pattern to search for.\n\n\nExample:\n\nlibrary(stringr)\n\ntext &lt;- \"Hello, World!\"\n# Check if 'World' is in the text\nresult &lt;- str_detect(text, \"World\")\nprint(result)\n\n[1] TRUE\n\n# Check if 'world' is in the text, ignoring case\nresult &lt;- str_detect(text, regex(\"world\", ignore_case = TRUE))\nprint(result)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/2024-06-07/index.html#using-stri_detect_fixed-from-stringi",
    "href": "posts/2024-06-07/index.html#using-stri_detect_fixed-from-stringi",
    "title": "How to Check if a Character is in a String in R",
    "section": "Using stri_detect_fixed() from stringi",
    "text": "Using stri_detect_fixed() from stringi\nThe stringi package is a comprehensive suite for string manipulation. The stri_detect_fixed() function is used for detecting fixed patterns within strings.\nSyntax:\nlibrary(stringi)\nstri_detect_fixed(str, pattern, case_insensitive = FALSE)\n\nstr: The character vector to search in.\npattern: The pattern to search for.\ncase_insensitive: Logical value indicating whether the search should be case-insensitive.\n\n\nExample:\n\nlibrary(stringi)\n\ntext &lt;- \"Hello, World!\"\n# Check if 'World' is in the text\nresult &lt;- stri_detect_fixed(text, \"World\")\nprint(result)\n\n[1] TRUE\n\n# Check if 'world' is in the text, ignoring case\nresult &lt;- stri_detect_fixed(text, \"world\", case_insensitive = TRUE)\nprint(result)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/2024-06-10/index.html",
    "href": "posts/2024-06-10/index.html",
    "title": "Introduction to My Content Series",
    "section": "",
    "text": "Introduction\n\nHello Everyone,\nI’m excited to kick off a new content series dedicated to reviewing and exploring the R packages I’ve developed. Over the coming weeks, I’ll be diving into the details, features, and practical applications of each package, providing you with a comprehensive understanding of how they can enhance your data analysis and machine learning projects.\n\n\nWhat to Expect\nEach Monday, I’ll introduce a new package from my suite of R tools. You’ll get an overview of the package’s purpose, its key functions, and the problems it aims to solve. Throughout the week, I’ll provide detailed insights and examples to help you get the most out of these tools.\n\n\nWhy This Series?\nThe goal of this series is to share the knowledge and utility of the packages I’ve created, helping you streamline your data workflows, improve your analytical capabilities, and leverage advanced techniques with ease. Whether you’re a seasoned data scientist or just getting started, there’s something here for everyone.\n\n\nOverview of the Packages\n\nhealthyR: Designed to simplify health analytics, this package offers a range of functions for common tasks in healthcare data analysis.\nhealthyR.data: A companion to healthyR, this package provides datasets specifically tailored for health analytics.\nhealthyR.ts: Focused on time series analysis in healthcare, healthyR.ts offers tools for modeling, forecasting, and visualizing time-dependent data.\nhealthyR.ai: Bringing AI to health analytics, this package integrates machine learning algorithms for predictive analytics and decision support.\nTidyDensity: A tool for density estimation and probabilistic modeling, TidyDensity makes it easy to work with distributions and perform simulations.\ntidyAML: An approachable package for automated machine learning, tidyAML simplifies the process of building and evaluating machine learning models.\n\n\n\nFirst Up: healthyR\nThis Thursday, we’ll start with an in-depth look at healthyR. I’ll share its core functionalities, how it can be used for health analytics, and practical examples to get you started. Each Thursday, I’ll provide practical examples to help you apply the package being discussed that week.\n\n\nGet Ready\nTo make the most of this series, I encourage you to install the healthyverse suite of packages. This will ensure you have all the tools at your fingertips as we explore their capabilities together.\ninstall.packages(\"healthyverse\")\nStay tuned for more updates, and let’s embark on this journey of enhancing our R skills together!\nBest, Steve"
  },
  {
    "objectID": "posts/2024-06-11/index.html",
    "href": "posts/2024-06-11/index.html",
    "title": "Extracting Numbers from Strings in R",
    "section": "",
    "text": "Hello! Today, we’ll jump into something I think is a pretty neat task in data processing: extracting numbers from strings. We’ll explore three different methods using base R, the stringr package, and the stringi package. Each method has its own strengths, so let’s get started!"
  },
  {
    "objectID": "posts/2024-06-11/index.html#extracting-numbers-with-base-r",
    "href": "posts/2024-06-11/index.html#extracting-numbers-with-base-r",
    "title": "Extracting Numbers from Strings in R",
    "section": "Extracting Numbers with Base R",
    "text": "Extracting Numbers with Base R\nBase R provides powerful tools to manipulate strings, and you can use regular expressions to extract numbers. Here’s a simple example:\n\n# Sample string\ntext &lt;- \"The price is 45 dollars and 50 cents.\"\n\n# Extract numbers using regular expressions\nnumbers &lt;- gregexpr(\"[0-9]+\", text)\nresult &lt;- regmatches(text, numbers)\n\n# Convert to numeric\nnumeric_result &lt;- as.numeric(unlist(result))\n\nprint(numeric_result)\n\n[1] 45 50\n\n\nExplanation:\n\ngregexpr(\"[0-9]+\", text) finds all sequences of digits in the text.\nregmatches(text, numbers) extracts these sequences from the text.\nunlist(result) flattens the list of matches.\nas.numeric() converts the character strings to numeric values."
  },
  {
    "objectID": "posts/2024-06-11/index.html#extracting-numbers-with-stringr",
    "href": "posts/2024-06-11/index.html#extracting-numbers-with-stringr",
    "title": "Extracting Numbers from Strings in R",
    "section": "Extracting Numbers with stringr",
    "text": "Extracting Numbers with stringr\nThe stringr package offers a more user-friendly approach to string manipulation. Here’s how you can extract numbers:\n\nlibrary(stringr)\n\n# Sample string\ntext &lt;- \"The price is 45 dollars and 50 cents.\"\n\n# Extract numbers using stringr\nnumbers &lt;- str_extract_all(text, \"\\\\d+\")\n\n# Convert to numeric\nnumeric_result &lt;- as.numeric(unlist(numbers))\n\nprint(numeric_result)\n\n[1] 45 50\n\n\nExplanation:\n\nstr_extract_all(text, \"\\\\d+\") extracts all sequences of digits from the text. \\\\d+ is a regular expression that matches one or more digits.\nunlist(numbers) and as.numeric() convert the result to numeric, as explained in the base R method."
  },
  {
    "objectID": "posts/2024-06-11/index.html#extracting-numbers-with-stringi",
    "href": "posts/2024-06-11/index.html#extracting-numbers-with-stringi",
    "title": "Extracting Numbers from Strings in R",
    "section": "Extracting Numbers with stringi",
    "text": "Extracting Numbers with stringi\nThe stringi package is another excellent tool for string manipulation, providing robust and efficient functions. Here’s an example:\n\nlibrary(stringi)\n\n# Sample string\ntext &lt;- \"The price is 45 dollars and 50 cents.\"\n\n# Extract numbers using stringi\nnumbers &lt;- stri_extract_all_regex(text, \"\\\\d+\")\n\n# Convert to numeric\nnumeric_result &lt;- as.numeric(unlist(numbers))\n\nprint(numeric_result)\n\n[1] 45 50\n\n\nExplanation:\n\nstri_extract_all_regex(text, \"\\\\d+\") extracts all sequences of digits from the text using regular expressions.\nAs before, unlist(numbers) and as.numeric() are used to convert the result to numeric values."
  },
  {
    "objectID": "posts/2024-06-12/index.html",
    "href": "posts/2024-06-12/index.html",
    "title": "VBA Code to Check if a Sheet Exists",
    "section": "",
    "text": "In today’s post we are going to go over VBA code to check if a sheet exists and then we are going to call that function from R using the RDCOMClient package. This can be useful when you need to perform certain actions based on the existence of a sheet in an Excel workbook.\nLet’s break this down step by step. We’ll start by writing a VBA function to check if a sheet exists, then we’ll show how to call this function from R using the RDCOMClient package."
  },
  {
    "objectID": "posts/2024-06-12/index.html#vba-code-to-check-if-a-sheet-exists",
    "href": "posts/2024-06-12/index.html#vba-code-to-check-if-a-sheet-exists",
    "title": "VBA Code to Check if a Sheet Exists",
    "section": "VBA Code to Check if a Sheet Exists",
    "text": "VBA Code to Check if a Sheet Exists\n\nVBA Function\nFirst, let’s create a simple VBA function to check if a sheet exists in the workbook.\nFunction SheetExists(sheetName As String) As Boolean\n    Dim ws As Worksheet\n    SheetExists = False\n    For Each ws In ThisWorkbook.Sheets\n        If ws.Name = sheetName Then\n            SheetExists = True\n            Exit Function\n        End If\n    Next ws\nEnd Function\nLet’s see it in action:\n\n\n\nUsing VBA Function to Check if a Sheet Exists\n\n\n\nExplanation:\n\nFunction SheetExists(sheetName As String) As Boolean: Defines a function named SheetExists that takes a sheet name as a string and returns a boolean.\nDim ws As Worksheet: Declares a variable ws as a worksheet.\nSheetExists = False: Initializes the function to return False by default.\nFor Each ws In ThisWorkbook.Sheets: Loops through each worksheet in the workbook.\nIf ws.Name = sheetName Then: Checks if the current worksheet’s name matches the provided sheet name.\nSheetExists = True: Sets the function to return True if a match is found.\nExit Function: Exits the function as soon as a match is found.\nNext ws: Continues to the next worksheet.\n\nThis VBA function SheetExists takes a sheet name as an argument and returns True if the sheet exists, and False otherwise."
  },
  {
    "objectID": "posts/2024-06-12/index.html#explanation",
    "href": "posts/2024-06-12/index.html#explanation",
    "title": "VBA Code to Check if a Sheet Exists",
    "section": "Explanation:",
    "text": "Explanation:\n\nFunction SheetExists(sheetName As String) As Boolean: Defines a function named SheetExists that takes a sheet name as a string and returns a boolean.\nDim ws As Worksheet: Declares a variable ws as a worksheet.\nSheetExists = False: Initializes the function to return False by default.\nFor Each ws In ThisWorkbook.Sheets: Loops through each worksheet in the workbook.\nIf ws.Name = sheetName Then: Checks if the current worksheet’s name matches the provided sheet name.\nSheetExists = True: Sets the function to return True if a match is found.\nExit Function: Exits the function as soon as a match is found.\nNext ws: Continues to the next worksheet.\n\nThis VBA function SheetExists takes a sheet name as an argument and returns True if the sheet exists, and False otherwise.\n\nR Code to Execute the VBA Macro and Return a Boolean Value\nTo run this VBA macro from R, you can use the RDCOMClient package. Here’s how you can do it:\n\nFirst, you’ll need to create an Excel workbook with the VBA macro.\nThen, use the following R code to execute the macro."
  },
  {
    "objectID": "posts/2024-06-12/index.html#r-code-using-rdcomclient-to-execute-the-vba-macro",
    "href": "posts/2024-06-12/index.html#r-code-using-rdcomclient-to-execute-the-vba-macro",
    "title": "VBA Code to Check if a Sheet Exists",
    "section": "R Code using RDCOMClient to Execute the VBA Macro",
    "text": "R Code using RDCOMClient to Execute the VBA Macro\n\n# Load RDCOMClient package\nlibrary(RDCOMClient)\n\n# Create a connection to Excel\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# Open your workbook\nwb_path &lt;- \"U:/R_VBA/sheet_exists.xlsm\"\nworkbook &lt;- excel_app$Workbooks()$Open(wb_path)\n\n# Ensure Excel is visible (optional)\nexcel_app[[\"Visible\"]] &lt;- TRUE\n\n# Run the VBA function and get the result\nsheet_name &lt;- \"Sheet1\" # Replace with the sheet name you want to check\nresult &lt;- excel_app$Run(\"SheetExists\", sheet_name)\n\n# Close the workbook without saving\nworkbook$Close(FALSE)\n\n[1] TRUE\n\n# Quit the Excel application\nexcel_app$Quit()\n\nNULL\n\n# Release the COM objects\nrm(excel_app, workbook)\n\n# Output the result\nresult\n\n[1] TRUE\n\n\nReplace \"path_to_your_excel_file.xlsm\" with the actual path to your Excel file containing the VBA macro.\n\nR Code using RDCOMClient to Achieve the Same Goal Without VBA\nIf you prefer to check if a sheet exists directly using R without invoking VBA, you can do it with the RDCOMClient package as well:\n\n# Load RDCOMClient package\nlibrary(RDCOMClient)\n\n# Create a connection to Excel\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# Open your workbook\nwb_path &lt;- \"U:/R_VBA/sheet_exists.xlsm\"\nworkbook &lt;- excel_app$Workbooks()$Open(wb_path)\n\n# Ensure Excel is visible (optional)\nexcel_app[[\"Visible\"]] &lt;- TRUE\n\n# Function to check if a sheet exists\nsheet_exists &lt;- function(workbook, sheet_name) {\n  sheets &lt;- workbook$Sheets()\n  for (i in 1:sheets$Count()) {\n    if (sheets$Item(i)$Name() == sheet_name) {\n      return(TRUE)\n    }\n  }\n  return(FALSE)\n}\n\n# Check if the sheet exists\nsheet_name &lt;- \"Sheet1\" # Replace with the sheet name you want to check\nresult &lt;- sheet_exists(workbook, sheet_name)\n\n# Close the workbook without saving\nworkbook$Close(FALSE)\n\n[1] TRUE\n\n# Quit the Excel application\nexcel_app$Quit()\n\nNULL\n\n# Release the COM objects\nrm(excel_app, workbook)\n\n# Output the result\nresult\n\n[1] TRUE\n\n\nIn this code, we directly check the existence of a sheet using the RDCOMClient package without invoking a VBA macro.\n\n\nSummary\n\nThe VBA code checks if a sheet exists in an Excel workbook.\nThe first R code executes the VBA macro using the RDCOMClient package.\nThe second R code achieves the same goal directly using the RDCOMClient package without invoking VBA.\n\nFeel free to modify the paths and sheet names as needed for your specific use case. If you need any more help or further customization, just let me know!"
  },
  {
    "objectID": "posts/2024-06-12/index.html#r-code-to-execute-the-vba-macro-and-return-a-boolean-value",
    "href": "posts/2024-06-12/index.html#r-code-to-execute-the-vba-macro-and-return-a-boolean-value",
    "title": "VBA Code to Check if a Sheet Exists",
    "section": "R Code to Execute the VBA Macro and Return a Boolean Value",
    "text": "R Code to Execute the VBA Macro and Return a Boolean Value\nTo run this VBA macro from R, you can use the RDCOMClient package. Here’s how you can do it:\n\nFirst, you’ll need to create an Excel workbook with the VBA macro.\nThen, use the following R code to execute the macro.\n\n\nR Code using RDCOMClient to Execute the VBA Macro\nFirst you need to install the package which can be slightly cumbersome:\n# Install RDCOMClient if not already installed\nif (!requireNamespace(\"RDCOMClient\", quietly = TRUE)) {\n  install.packages(\"RDCOMClient\", repos = \"http://www.omegahat.net/R\")\n}\n\n# Load RDCOMClient package\nlibrary(RDCOMClient)\n\n# Create a connection to Excel\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# Open your workbook\nwb_path &lt;- \"C:/Users/ssanders/Documents/GitHub/steveondata/posts/2024-06-12/sheet_exists.xlsm\"\nworkbook &lt;- excel_app$Workbooks()$Open(wb_path)\n\n# Ensure Excel is visible (optional)\nexcel_app[[\"Visible\"]] &lt;- TRUE\n\n# Run the VBA function and get the result\nsheet_name &lt;- \"Sheet1\" # Replace with the sheet name you want to check\nresult &lt;- excel_app$Run(\"SheetExists\", sheet_name)\n\n# Close the workbook without saving\nworkbook$Close(FALSE)\n\n[1] TRUE\n\n# Quit the Excel application\nexcel_app$Quit()\n\nNULL\n\n# Release the COM objects\nrm(excel_app, workbook)\n\n# Output the result\nresult\n\n[1] TRUE\n\n\nReplace wb_path with the actual path to your Excel file containing the VBA macro.\n\nExplanation:\n\nRDCOMClient::COMCreate(“Excel.Application”): Creates a COM object for Excel.\nexcel_app[[“Visible”]] &lt;- TRUE: Makes Excel visible (optional, can be removed).\nexcel_app[[“Workbooks”]]$Open(“C:\\path\\to\\your\\workbook.xlsx”): Opens the specified workbook. Adjust the path as needed.\nexcel_app$Run(“SheetExists”, sheet_name): Runs the SheetExists VBA function with the provided sheet name and stores the result.\nworkbook$Close(FALSE): Closes the workbook without saving changes.\nexcel_app$Quit(): Quits the Excel application.\nexcel_app &lt;- NULL: Releases the COM object resources.\n\n\n\n\nR Code using RDCOMClient to Achieve the Same Goal Without VBA\nIf you prefer to check if a sheet exists directly using R without invoking VBA, you can do it with the RDCOMClient package as well:\n\n# Load RDCOMClient package\nlibrary(RDCOMClient)\n\n# Create a connection to Excel\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# Open your workbook\nwb_path &lt;- \"C:/Users/ssanders/Documents/GitHub/steveondata/posts/2024-06-12/sheet_exists.xlsm\"\nworkbook &lt;- excel_app$Workbooks()$Open(wb_path)\n\n# Ensure Excel is visible (optional)\nexcel_app[[\"Visible\"]] &lt;- TRUE\n\n# Function to check if a sheet exists\nsheet_exists &lt;- function(workbook, sheet_name) {\n  sheets &lt;- workbook$Sheets()\n  for (i in 1:sheets$Count()) {\n    if (sheets$Item(i)$Name() == sheet_name) {\n      return(TRUE)\n    }\n  }\n  return(FALSE)\n}\n\n# Check if the sheet exists\nsheet_name &lt;- \"Sheet1\" # Replace with the sheet name you want to check\nresult &lt;- sheet_exists(workbook, sheet_name)\n\n# Close the workbook without saving\nworkbook$Close(FALSE)\n\n[1] TRUE\n\n# Quit the Excel application\nexcel_app$Quit()\n\nNULL\n\n# Release the COM objects\nrm(excel_app, workbook)\n\n# Output the result\nresult\n\n[1] TRUE\n\n\nIn this code, we directly check the existence of a sheet using the RDCOMClient package without invoking a VBA macro.\n\nExplanation:\n\nSimilar steps to the previous R code, but instead of running a VBA macro, it directly interacts with the Excel object model.\nLoops through the sheets in the workbook to check if the specified sheet exists."
  },
  {
    "objectID": "posts/2024-06-13/index.html",
    "href": "posts/2024-06-13/index.html",
    "title": "An Introduction to healthyR",
    "section": "",
    "text": "This article will introduce you to the healthyR package. healthyR is a package that provides functions for analyzing and visualizing health-related data. It is designed to make it easier for health professionals and researchers to work with health data in R. It is an experimental package that is still under active development, so some functions may change in the future along with the package structure and scope.\nUnfortunately, the package needs some love and attention. Which I am trying to give it. Given that information, I will be updating the package to include more functions and improve the existing ones. I will also be updating the documentation and adding more examples to help users get started with the package.\nSo let’s get started!"
  },
  {
    "objectID": "posts/2024-06-13/index.html#the-goal",
    "href": "posts/2024-06-13/index.html#the-goal",
    "title": "An Introduction to healthyR",
    "section": "The Goal",
    "text": "The Goal\nThe ultimate goal really of this package is to provide a set of functions that are easy to understand and follow. The functions should be able to take in data, process it, and output results in a way that is easy to understand and interpret. The package should also provide functions for visualizing the data in a way that is easy to understand and interpret. In healthycare, at least in my experience there are a great many small rural hospitals that do not have the resources to hire a data scientist or a statistician. This package and in fact the entire healthyverse suite are being designed to help those hospitals and other health organizations that may not have the resources to hire a data scientist or statistician.\nThe only way anyone can improve is if they have their data and can then in turn analyze and interpret that data. This package is designed to help with that in some short way for now.\nLet’s go through some examples. To do this we will also load in my healthyR.data package as it comes with a standard dataset that we can use to demonstrate the functions in healthyR and a host of other issues.\n\n# install.packages(healthyR.data)\nlibrary(healthyR)\nlibrary(healthyR.data)\nlibrary(tidyverse)\nlibrary(DT)\n\nNow let’s get a list of all of the functions that are exposed via the healthyR package.\n\n# Functions and their arguments for healthyR\n\npat &lt;- c(\"%&gt;%\",\":=\",\"as_label\",\"as_name\",\"enquo\",\"enquos\",\"expr\",\n         \"dx_cc_mapping\",\"px_cc_mapping\",\"sym\",\"syms\")\n\ntibble(fns = ls.str(\"package:healthyR\")) |&gt;\n  filter(!fns %in% pat) |&gt;\n  mutate(params = purrr::map(fns, formalArgs)) |&gt; \n  group_by(fns) |&gt; \n  mutate(func_with_params = toString(params)) |&gt;\n  mutate(\n    func_with_params = ifelse(\n      str_detect(\n        func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |&gt;\n  select(fns, func_with_params) |&gt;\n  mutate(fns = as.factor(fns)) |&gt;\n  datatable(\n    #class = 'cell-boarder-stripe',\n    colnames = c(\"Function\", \"Full Call\"),\n    options = list(\n      autowidth = TRUE,\n      pageLength = 10\n    )\n  )\n\n\n\n\n\nYou can see the reference page for all of the available functions here: healthyR Reference\nLet’s get started with a first example."
  },
  {
    "objectID": "posts/2024-06-13/index.html#example-1-median-excess-lenght-of-stay",
    "href": "posts/2024-06-13/index.html#example-1-median-excess-lenght-of-stay",
    "title": "An Introduction to healthyR",
    "section": "Example 1: Median Excess Lenght of Stay",
    "text": "Example 1: Median Excess Lenght of Stay\nIn this example, we will calculate the median excess length of stay for patients in the inpatient dataset. The excess length of stay is the difference between the actual length of stay and the expected length of stay for a patient. The expected length of stay is calculated based on the patient’s diagnosis-related group (DRG) and other factors.\nFor providers and hospitals, the excess length of stay is an important metric because it can help identify patients who are at risk of complications or other adverse outcomes. By identifying these patients early, providers can take steps to prevent complications and improve patient outcomes. Afterall, hospitals are full of sick people, as hard as they work to keep environments sterile one must remember that when your at your worst, you go to a hospital, so it is natural that complications can arise.\n\n# Load the inpatient dataset\ndf &lt;- healthyR_data |&gt;\n  filter(ip_op_flag == \"I\") |&gt;\n  select(visit_id, visit_end_date_time, length_of_stay)  |&gt;\n  mutate(visit_end_date = as.Date(\n    visit_end_date_time, format = \"%Y-%m-%d\"\n    )) |&gt;\n  select(-visit_end_date_time, visit_id, visit_end_date, length_of_stay) |&gt;\n  filter(visit_end_date &gt;= \"2012-01-01\",\n         visit_end_date &lt; \"2020-01-01\") |&gt;\n  arrange(visit_end_date)\n\nglimpse(df)\n\nRows: 105,577\nColumns: 3\n$ visit_id       &lt;chr&gt; \"1283065398\", \"1171004549\", \"1331016562\", \"1970894633\",…\n$ length_of_stay &lt;dbl&gt; 6, 1, 3, 2, 3, 5, 21, 4, 2, 4, 1, 9, 1, 2, 2, 1, 9, 1, …\n$ visit_end_date &lt;date&gt; 2012-01-01, 2012-01-01, 2012-01-01, 2012-01-01, 2012-0…\n\n\nNow let’s use the ts_alos_plt() function to see what the average length of stay (ALOS) looks like:\n\nts_alos_plt(\n  df, \n  .date_col = visit_end_date, \n  .value_col = length_of_stay, \n  .by_grouping = \"month\",\n  .interactive = TRUE\n  )\n\n\n\n\n\nFrom here, we see that the alos is increasing over time. So, is this a bad sign? This could be due to a number of factors, such as an increase in the number of patients with complex conditions or an increase in the number of patients with complications. It could also be due to a lack of resources or staff, which can lead to delays in care and longer lengths of stay.\nThis is why we want to see the median excess length of stay. Let’s calculate that now.\n\ndf_tbl &lt;- ts_signature_tbl(df, .date_col = visit_end_date)\n\npad applied on the interval: day\n\nglimpse(df_tbl)\n\nRows: 105,577\nColumns: 31\n$ visit_id       &lt;chr&gt; \"1283065398\", \"1171004549\", \"1331016562\", \"1970894633\",…\n$ length_of_stay &lt;dbl&gt; 6, 1, 3, 2, 3, 5, 21, 4, 2, 4, 1, 9, 1, 2, 2, 1, 9, 1, …\n$ visit_end_date &lt;date&gt; 2012-01-01, 2012-01-01, 2012-01-01, 2012-01-01, 2012-0…\n$ index.num      &lt;dbl&gt; 1325376000, 1325376000, 1325376000, 1325376000, 1325376…\n$ diff           &lt;dbl&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 86400,…\n$ year           &lt;int&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2…\n$ year.iso       &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2…\n$ half           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ quarter        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ month.xts      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ month.lbl      &lt;ord&gt; January, January, January, January, January, January, J…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ hour           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ minute         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ second         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hour12         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ am.pm          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ wday           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ wday.xts       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1…\n$ wday.lbl       &lt;ord&gt; Sunday, Sunday, Sunday, Sunday, Sunday, Sunday, Sunday,…\n$ mday           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ qday           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ yday           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ mweek          &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1…\n$ week           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ week.iso       &lt;int&gt; 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52,…\n$ week2          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ week3          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ week4          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ mday7          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\nnames(df_tbl)\n\n [1] \"visit_id\"       \"length_of_stay\" \"visit_end_date\" \"index.num\"     \n [5] \"diff\"           \"year\"           \"year.iso\"       \"half\"          \n [9] \"quarter\"        \"month\"          \"month.xts\"      \"month.lbl\"     \n[13] \"day\"            \"hour\"           \"minute\"         \"second\"        \n[17] \"hour12\"         \"am.pm\"          \"wday\"           \"wday.xts\"      \n[21] \"wday.lbl\"       \"mday\"           \"qday\"           \"yday\"          \n[25] \"mweek\"          \"week\"           \"week.iso\"       \"week2\"         \n[29] \"week3\"          \"week4\"          \"mday7\"         \n\n\nNow that we have our table ready for calculation, let’s get it done!\n\nts_median_excess_plt(\n  .data = df_tbl, \n  .date_col = visit_end_date,\n  .value_col = length_of_stay,\n  .x_axis = week,\n  .ggplot_group_var = year,\n  .years_back = 1\n) +\n  labs(\n    x = \"Week of the Year\",\n    y = \"Median Excess Length of Stay (Days)\",\n    title = \"Median Excess Length of Stay by Week\",\n    caption = \"Data Source: Inpatient Dataset; Red Line Indicates Latest Year\"\n  )\n\n\n\n\n\n\n\n\nSo we can see from here that even though the ALOS is increasing, the median excess length of stay is decreasing. This is a good sign as it indicates that the hospital is improving its efficiency and reducing the number of patients who are staying longer than expected.\nLet’s move onto another example."
  },
  {
    "objectID": "posts/2024-06-13/index.html#example-2-gartner-magic-chart",
    "href": "posts/2024-06-13/index.html#example-2-gartner-magic-chart",
    "title": "An Introduction to healthyR",
    "section": "Example 2: Gartner Magic Chart",
    "text": "Example 2: Gartner Magic Chart\nIn this example, we will create a Gartner Magic Chart to visualize the performance of different hospitals in terms of their length of stay and readmission rates. The Gartner Magic Chart is a popular tool used by healthcare organizations to compare the performance of different hospitals and identify areas for improvement.\nWe will create a simulated dataset of 100 hospitals to achieve this and we will want it scaled, think of this like taking a look at the performance of the excess alos and excess readmit rates:\n\nset.seed(123)\ngartner_tbl &lt;- tibble(\n  hospital_id = 1:100,\n  x = scale(rnorm(100, mean = 5, sd = 2)),\n  y = scale(rnorm(100, mean = 0.1, sd = 0.05))\n)\n\nsummary(gartner_tbl[,-1])\n\n         x.V1                y.V1        \n Min.   :-2.6287610   Min.   :-2.012128  \n 1st Qu.:-0.6400635   1st Qu.:-0.717236  \n Median :-0.0313860   Median :-0.122321  \n Mean   : 0.0000000   Mean   : 0.000000  \n 3rd Qu.: 0.6588549   3rd Qu.: 0.595034  \n Max.   : 2.2972071   Max.   : 3.462909  \n\n\nNow that we have our simulated dataset, let’s create the Gartner Magic Chart. Unfortunately at this point the columns must be named x and y, but this will be updated in the future to pass whatever column you like.\n\ngartner_magic_chart_plt(\n  .data = gartner_tbl,\n  .x_col = x,\n  .y_col = y,\n  .x_lab = \"ALOS\",\n  .y_lab = \"ARR\",\n  .point_size = NULL,\n  .plt_title = \"Gartner Magic Chart - Scaled Data\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)"
  },
  {
    "objectID": "posts/2024-06-13/index.html#example-3-length-of-stay-and-readmit-index-with-variance",
    "href": "posts/2024-06-13/index.html#example-3-length-of-stay-and-readmit-index-with-variance",
    "title": "An Introduction to healthyR",
    "section": "Example 3: Length Of Stay and Readmit Index with Variance",
    "text": "Example 3: Length Of Stay and Readmit Index with Variance\nSometimes we want to see how the variance of the length of stay and readmission rates are changing over the length of stay of a patient visit. This can help us identify trends and patterns that may be affecting the performance of hospitals. What this means is that maybe we would rather have a longer length stay (a variance, longer than expected) if it helps to keep the reamission rate down. A provider/hospital would rather have someone in the hospital longer than see them get readmitted because they were discharged prematurely.\nLet’s make our data:\n\ndata_tbl &lt;- tibble(\n  \"alos\"                 = runif(186, 1, 20)\n  , \"elos\"               = runif(186, 1, 17)\n  , \"readmit_rate\"       = runif(186, 0, .25)\n  , \"readmit_rate_bench\" = runif(186, 0, .2)\n)\n\nsummary(data_tbl)\n\n      alos             elos         readmit_rate       readmit_rate_bench\n Min.   : 1.009   Min.   : 1.019   Min.   :0.0009741   Min.   :0.00140   \n 1st Qu.: 6.224   1st Qu.: 4.785   1st Qu.:0.0647280   1st Qu.:0.05446   \n Median :10.518   Median : 9.277   Median :0.1273747   Median :0.09597   \n Mean   :10.553   Mean   : 8.979   Mean   :0.1230823   Mean   :0.09830   \n 3rd Qu.:15.349   3rd Qu.:13.094   3rd Qu.:0.1853718   3rd Qu.:0.14387   \n Max.   :19.936   Max.   :16.919   Max.   :0.2479363   Max.   :0.19852   \n\n\nLet’s take a look at the data quickly:\n\ndata_tbl |&gt;\n  pivot_longer(\n    cols = c(alos, elos, readmit_rate, readmit_rate_bench), \n    names_to = \"metric\", values_to = \"value\"\n    ) |&gt;\n  mutate(metric_group = ifelse(\n    metric %in% c(\"alos\", \"elos\"), \"Length of Stay\", \"Readmit Rate\"\n    )\n  ) |&gt;\n  ggplot(aes(x = value, color = metric)) +\n  facet_wrap(~ metric_group, scales = \"free\") +\n  geom_density() +\n  theme_minimal() +\n  labs(\n    x = \"Value\",\n    y = \"Density\",\n    title = \"Density Plot of Length of Stay and Readmit Rate\",\n    color = \"Metric\"\n  )\n\n\n\n\n\n\n\n\nNow let’s see how the variance of the length of stay and readmission rates are changing over the length of stay of a patient visit:\n\nlos_ra_index_summary_tbl(\n  .data = data_tbl\n  , .max_los       = 15\n  , .alos_col      = alos\n  , .elos_col      = elos\n  , .readmit_rate  = readmit_rate\n  , .readmit_bench = readmit_rate_bench\n) %&gt;%\n  los_ra_index_plt()\n\n\n\n\n\n\n\n\nFrom here we can see that the variance of the length of stay and readmission rates are decreasing as the length of stay increases. This is a good sign as it indicates that the hospital is able to provide more consistent care to patients with longer stays, which may help to reduce the risk of readmission. Even though resource utilization may increase, the hospital is able to provide better care to patients with longer stays, which may help to reduce the risk of readmission in this example."
  },
  {
    "objectID": "posts/2024-06-13/index.html#example-4-service-line-augmentation",
    "href": "posts/2024-06-13/index.html#example-4-service-line-augmentation",
    "title": "An Introduction to healthyR",
    "section": "Example 4: Service Line Augmentation",
    "text": "Example 4: Service Line Augmentation\nIn this example, we will agument a service line to a patient visit. This can help us create groups of patient visits in a manner that is more managable then say at the DX, DRG or MDC levels. This can help us identify trends and patterns that may be affecting the performance of hospitals.\nLet’s see how it works:\n\ndf &lt;- data.frame(\n  dx_col = \"F10.10\",\n  px_col = NA,\n  drg_col = \"896\"\n)\n\nservice_line_augment(\n  .data = df,\n  .dx_col = dx_col,\n  .px_col = px_col,\n  .drg_col = drg_col\n)\n\n# A tibble: 1 × 4\n  dx_col px_col drg_col service_line \n  &lt;chr&gt;  &lt;lgl&gt;  &lt;chr&gt;   &lt;chr&gt;        \n1 F10.10 NA     896     alcohol_abuse\n\n\nWe see here that a patient discharged with a diagnosis of F10.10 and DRG 896 would be classified as a patient visit for the service line of “alcohol_abuse”. A term that is more generic then F10.10 which is “Alcohol Abuse, uncomplicated”. This can help us identify trends and patterns that may be affecting the performance of hospitals."
  },
  {
    "objectID": "posts/2024-06-14/index.html",
    "href": "posts/2024-06-14/index.html",
    "title": "Working with Excel Files in R and Python",
    "section": "",
    "text": "If you often work with Excel files and are looking to streamline your data import and export processes, R and Python offer some powerful packages to help you. Here, I’ll introduce you to some essential tools in both R and Python that will make handling Excel files a breeze."
  },
  {
    "objectID": "posts/2024-06-14/index.html#readxl",
    "href": "posts/2024-06-14/index.html#readxl",
    "title": "Working with Excel Files in R and Python",
    "section": "readxl",
    "text": "readxl\nThe readxl package is one of the most straightforward options for reading Excel files into R. It supports both .xls and .xlsx formats and is particularly appreciated for its simplicity and speed.\nHere’s a quick example:\n# Load the readxl package\nlibrary(readxl)\n\n# Read the Excel file\ndata &lt;- read_excel(\"path_to_your_file.xlsx\")\n\n# View the first few rows of the data\nhead(data)"
  },
  {
    "objectID": "posts/2024-06-14/index.html#openxlsx",
    "href": "posts/2024-06-14/index.html#openxlsx",
    "title": "Working with Excel Files in R and Python",
    "section": "openxlsx",
    "text": "openxlsx\nIf you need to do more than just read Excel files, openxlsx is a fantastic choice. This package allows you to read, write, and format Excel files, providing greater flexibility for data manipulation and presentation.\nExample:\n# Load the openxlsx package\nlibrary(openxlsx)\n\n# Read the Excel file\ndata &lt;- read.xlsx(\"path_to_your_file.xlsx\")\n\n# Write data to a new Excel file\nwrite.xlsx(data, \"path_to_new_file.xlsx\")"
  },
  {
    "objectID": "posts/2024-06-14/index.html#xlsx",
    "href": "posts/2024-06-14/index.html#xlsx",
    "title": "Working with Excel Files in R and Python",
    "section": "xlsx",
    "text": "xlsx\nThe xlsx package is another versatile tool for handling Excel files in R. It supports reading, writing, and formatting Excel files, and works well for both .xls and .xlsx formats.\nExample:\n# Load the xlsx package\nlibrary(xlsx)\n\n# Read the Excel file\ndata &lt;- read.xlsx(\"path_to_your_file.xlsx\", sheetIndex = 1)\n\n# Write data to a new Excel file\nwrite.xlsx(data, \"path_to_new_file.xlsx\")"
  },
  {
    "objectID": "posts/2024-06-14/index.html#pandas",
    "href": "posts/2024-06-14/index.html#pandas",
    "title": "Working with Excel Files in R and Python",
    "section": "pandas",
    "text": "pandas\nThe pandas library is a cornerstone of data analysis in Python, and it includes the read_excel() function for reading Excel files. This function is highly versatile and integrates seamlessly with other pandas functionalities.\nExample:\n# Import the pandas package\nimport pandas as pd\n\n# Read the Excel file\ndata = pd.read_excel(\"path_to_your_file.xlsx\", sheet_name=\"Sheet1\")\n\n# Display the first few rows of the data\nprint(data.head())"
  },
  {
    "objectID": "posts/2024-06-14/index.html#openpyxl",
    "href": "posts/2024-06-14/index.html#openpyxl",
    "title": "Working with Excel Files in R and Python",
    "section": "openpyxl",
    "text": "openpyxl\nFor more advanced Excel operations in Python, openpyxl is an excellent choice. It allows you to read and write Excel 2010 xlsx/xlsm/xltx/xltm files and offers extensive formatting capabilities.\nExample:\n# Import the openpyxl package\nfrom openpyxl import load_workbook\nimport pandas as pd\n\n# Load the workbook\nwb = load_workbook(\"path_to_your_file.xlsx\")\n\n# Select a sheet by name\nsheet = wb['Sheet1']\n\n# Print the value of cell A1\nprint(sheet['A1'].value)"
  },
  {
    "objectID": "posts/2024-06-17/index.html",
    "href": "posts/2024-06-17/index.html",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "",
    "text": "Hello everyone,\nI’m excited to give you an overview of healthyR.ts, an R package designed to simplify and enhance your time series analysis experience. Just like my healthyR package, it is designed to be user friendly."
  },
  {
    "objectID": "posts/2024-06-17/index.html#versatile-functionality",
    "href": "posts/2024-06-17/index.html#versatile-functionality",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "1. Versatile Functionality",
    "text": "1. Versatile Functionality\nhealthyR.ts comes packed with functions to handle various aspects of time series analysis, from basic preprocessing to advanced modeling and forecasting. Whether you need to decompose your series, detect anomalies, or fit complex models, healthyR.ts has got you covered."
  },
  {
    "objectID": "posts/2024-06-17/index.html#user-friendly-interface",
    "href": "posts/2024-06-17/index.html#user-friendly-interface",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "2. User-Friendly Interface",
    "text": "2. User-Friendly Interface\nThe package is designed with usability in mind. Functions are well-documented and intuitive, making it easier for users at all levels to implement sophisticated time series techniques. You can find a comprehensive list of functions and their detailed descriptions in the Reference Section."
  },
  {
    "objectID": "posts/2024-06-17/index.html#seamless-integration",
    "href": "posts/2024-06-17/index.html#seamless-integration",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "3. Seamless Integration",
    "text": "3. Seamless Integration\nhealthyR.ts integrates smoothly with other popular R packages, enhancing its utility and flexibility. This allows you to leverage the strengths of multiple tools within a single workflow, optimizing your analysis process."
  },
  {
    "objectID": "posts/2024-06-17/index.html#syntax",
    "href": "posts/2024-06-17/index.html#syntax",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Syntax:",
    "text": "Syntax:\nts_random_walk(\n  .mean = 0,\n  .sd = 0.1,\n  .num_walks = 100,\n  .periods = 100,\n  .initial_value = 1000\n)"
  },
  {
    "objectID": "posts/2024-06-17/index.html#arguments",
    "href": "posts/2024-06-17/index.html#arguments",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Arguments:",
    "text": "Arguments:\n\n.mean: The desired mean of the random walks\n.sd: The standard deviation of the random walks\n.num_walks: The number of random walks you want generated\n.periods: The length of the random walk(s) you want generated\n.initial_value: The initial value where the random walks should start"
  },
  {
    "objectID": "posts/2024-06-17/index.html#visualize",
    "href": "posts/2024-06-17/index.html#visualize",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Visualize",
    "text": "Visualize\nNow, let’s visualize our data:\n\ndf |&gt;\n   ggplot(\n       mapping = aes(\n           x = x\n           , y = cum_y\n           , color = factor(run)\n           , group = factor(run)\n        )\n    ) +\n    geom_line(alpha = 0.8) +\n    ts_random_walk_ggplot_layers(df)\n\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf |&gt;\n    group_by(x) |&gt;\n    summarise(\n        min_y = min(cum_y),\n        max_y = max(cum_y)\n    ) |&gt;\n    ggplot(\n        aes(x = x)\n    ) +\n    geom_line(aes(y = max_y), color = \"steelblue\") +\n    geom_line(aes(y = min_y), color = \"firebrick\") +\n    geom_ribbon(aes(ymin = min_y, ymax = max_y), alpha = 0.2) +\n    ts_random_walk_ggplot_layers(df)\n\n\n\n\n\n\n\n\nNow we have just gone over how to use a function to generate a simple random walk, this is only scratching the surface of what this package can do. I am going to go over a few more examples and try to break things up into sections."
  },
  {
    "objectID": "posts/2024-06-17/index.html#generating-functions",
    "href": "posts/2024-06-17/index.html#generating-functions",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Generating Functions",
    "text": "Generating Functions\nWe have already gone over how to generate a simple random walk, but there are other functions that can be used to generate data. Here are examples:\n\nts_brownian_motion\n\n# Generate\nset.seed(123)\nbm &lt;- ts_brownian_motion()\nglimpse(bm)\n\nRows: 1,010\nColumns: 3\n$ sim_number &lt;fct&gt; sim_number 1, sim_number 2, sim_number 3, sim_number 4, sim…\n$ t          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ y          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000,…\n\nbm |&gt;\n  ts_brownian_motion_plot(\n    .date_col = t,\n    .value_col = y,\n    .interactive = TRUE\n    )\n\n\n\n\n\n\n\nts_geometric_brownian_motion\n\ngm &lt;- ts_geometric_brownian_motion()\nglimpse(gm)\n\nRows: 2,600\nColumns: 3\n$ sim_number &lt;fct&gt; sim_number 1, sim_number 2, sim_number 3, sim_number 4, sim…\n$ t          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y          &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n\ngm |&gt;\n  ts_brownian_motion_plot(\n    .date_col = t,\n    .value_col = y,\n    .interactive = TRUE\n    )"
  },
  {
    "objectID": "posts/2024-06-17/index.html#plotting-functions",
    "href": "posts/2024-06-17/index.html#plotting-functions",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Plotting Functions",
    "text": "Plotting Functions\nThe package also includes a variety of plotting functions to help you visualize your data. Here are a few examples:\n\nts_vva_plot\n\n# Generate\nset.seed(123)\ndf &lt;- ts_random_walk(.num_walks = 1, .periods = 100) |&gt;\n  filter(run == 1)\nglimpse(df)\n\nRows: 200\nColumns: 4\n$ run   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ x     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ y     &lt;dbl&gt; -0.056047565, -0.023017749, 0.155870831, 0.007050839, 0.01292877…\n$ cum_y &lt;dbl&gt; 943.9524, 922.2248, 1065.9727, 1073.4887, 1087.3676, 1273.8582, …\n\nts_vva_plot(\n  .data = df,\n  .date_col = x,\n  .value_col = cum_y\n)\n\n$data\n$data$augmented_data_tbl\n# A tibble: 600 × 3\n       x name          value\n   &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1     1 Cum_y         944. \n 2     1 Velocity       NA  \n 3     1 Acceleration   NA  \n 4     2 Cum_y         922. \n 5     2 Velocity      -21.7\n 6     2 Acceleration   NA  \n 7     3 Cum_y        1066. \n 8     3 Velocity      144. \n 9     3 Acceleration  165. \n10     4 Cum_y        1073. \n# ℹ 590 more rows\n\n\n$plots\n$plots$static_plot\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n$plots$interactive_plot"
  },
  {
    "objectID": "posts/2024-06-17/index.html#filtering-functions",
    "href": "posts/2024-06-17/index.html#filtering-functions",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Filtering Functions",
    "text": "Filtering Functions\n\nts_compare_data\nCompare data over time periods:\n\ndata_tbl &lt;- ts_to_tbl(AirPassengers) |&gt;\n  select(-index)\n\nts_compare_data(\n  .data           = data_tbl\n  , .date_col     = date_col\n  , .start_date   = \"1955-01-01\"\n  , .end_date     = \"1955-12-31\"\n  , .periods_back = \"2 years\"\n  ) |&gt;\n  summarise_by_time(\n    .date_var = date_col\n    , .by     = \"year\"\n    , visits  = sum(value)\n  )\n\n# A tibble: 2 × 2\n  date_col   visits\n  &lt;date&gt;      &lt;dbl&gt;\n1 1953-01-01   2700\n2 1955-01-01   3408\n\n\n\n\nts_time_event_analysis_tbl\n\ntst &lt;- ts_time_event_analysis_tbl(\n  data_tbl, \n  date_col, \n  value, \n  .direction = \"both\",\n  .horizon = 6\n)\n\ntst |&gt;\n  ts_event_analysis_plot(\n  .plot_type = \"mean\",\n  .plot_ci = TRUE,\n  .interactive = FALSE\n)"
  },
  {
    "objectID": "posts/2024-06-17/index.html#simulator",
    "href": "posts/2024-06-17/index.html#simulator",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Simulator",
    "text": "Simulator\n\nts_arima_simiulator\nSimulate an arima model and visualize the results:\n\noutput &lt;- ts_arima_simulator()\noutput$plots$static_plot"
  },
  {
    "objectID": "posts/2024-06-17/index.html#auto-workflowset-generators",
    "href": "posts/2024-06-17/index.html#auto-workflowset-generators",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Auto Workflowset Generators",
    "text": "Auto Workflowset Generators\nWant to create an automatic workflow set of data? Got you covered\n\nts_wfs_\n\nsplits &lt;- time_series_split(\n   data_tbl\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\nrec_objs &lt;- ts_auto_recipe(\n .data = training(splits)\n , .date_col = date_col\n , .pred_col = value\n)\n\nwf_sets &lt;- ts_wfs_arima_boost(\"all_engines\", rec_objs)\nwf_sets\n\n# A workflow set/tibble: 8 × 4\n  wflow_id                           info             option    result    \n  &lt;chr&gt;                              &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 rec_base_arima_boost_1             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 rec_base_arima_boost_2             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 rec_date_arima_boost_1             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 rec_date_arima_boost_2             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 rec_date_fourier_arima_boost_1     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 rec_date_fourier_arima_boost_2     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n7 rec_date_fourier_nzv_arima_boost_1 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n8 rec_date_fourier_nzv_arima_boost_2 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "posts/2024-06-17/index.html#boilerplate-functions",
    "href": "posts/2024-06-17/index.html#boilerplate-functions",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Boilerplate Functions",
    "text": "Boilerplate Functions\n\nts_auto_\nAutomatic functions to help you with your time series analysis:\n\nautomatic_mars &lt;- ts_auto_mars(\n  .data = data_tbl,\n  .num_cores = 2,\n  .date_col = date_col,\n  .value_col = value,\n  .rsamp_obj = splits,\n  .formula = value ~ .,\n  .grid_size = 20,\n  .tune = FALSE\n)\n\nautomatic_mars\n\n$recipe_info\n$recipe_info$recipe_call\nrecipe(.data = data_tbl, .date_col = date_col, .value_col = value, \n    .formula = value ~ ., .rsamp_obj = splits, .tune = FALSE, \n    .grid_size = 20, .num_cores = 2)\n\n$recipe_info$recipe_syntax\n[1] \"ts_mars_recipe &lt;-\"                                                                                                                                                      \n[2] \"\\n  recipe(.data = data_tbl, .date_col = date_col, .value_col = value, .formula = value ~ \\n    ., .rsamp_obj = splits, .tune = FALSE, .grid_size = 20, .num_cores = 2)\"\n\n$recipe_info$rec_obj\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Timeseries signature features from: date_col\n\n\n• Holiday signature features from: date_col\n\n\n• Novel factor level assignment for: recipes::all_nominal_predictors()\n\n\n• Variable mutation for: tidyselect::vars_select_helpers$where(is.character)\n\n\n• Dummy variables from: recipes::all_nominal()\n\n\n• Zero variance filter on: recipes::all_predictors() and -date_col_index.num\n\n\n• Centering and scaling for: recipes::all_numeric_predictors()\n\n\n\n\n$model_info\n$model_info$model_spec\nMARS Model Specification (regression)\n\nComputational engine: earth \n\n\n$model_info$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mars()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_timeseries_signature()\n• step_holiday_signature()\n• step_novel()\n• step_mutate_at()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMARS Model Specification (regression)\n\nComputational engine: earth \n\n\n$model_info$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mars()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_timeseries_signature()\n• step_holiday_signature()\n• step_novel()\n• step_mutate_at()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSelected 9 of 14 terms, and 6 of 72 predictors\nTermination condition: RSq changed by less than 0.001 at 14 terms\nImportance: date_col_index.num, date_col_week, date_col_yday, ...\nNumber of terms at each degree of interaction: 1 8 (additive model)\nGCV 586.2527    RSS 58736.31    GRSq 0.948825    RSq 0.9605624\n\n$model_info$was_tuned\n[1] \"not_tuned\"\n\n\n$model_calibration\n$model_calibration$plot\n\n$model_calibration$calibration_tbl\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;       &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;workflow&gt; EARTH       Test  &lt;tibble [12 × 4]&gt;\n\n$model_calibration$model_accuracy\n# A tibble: 1 × 9\n  .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 EARTH       Test   44.6  8.38 0.924  8.99  60.0 0.970\n\n\nattr(,\".tune\")\n[1] FALSE\nattr(,\".grid_size\")\n[1] 20\nattr(,\".cv_assess\")\n[1] 12\nattr(,\".cv_skip\")\n[1] 3\nattr(,\".cv_slice_limit\")\n[1] 6\nattr(,\".best_metric\")\n[1] \"rmse\"\nattr(,\".bootstrap_final\")\n[1] FALSE\nattr(,\".mode\")\n[1] \"regression\"\nattr(,\".parsnip_engine\")\n[1] \"earth\"\nattr(,\".function_family\")\n[1] \"boilerplate\""
  },
  {
    "objectID": "posts/2024-06-17/index.html#vectorized-function",
    "href": "posts/2024-06-17/index.html#vectorized-function",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Vectorized Function",
    "text": "Vectorized Function\n\nts_growth_rate_vec\n\n# Calculate the growth rate of a time series without any transformations.\nts_growth_rate_vec(c(100, 110, 120, 130))\n\n[1]        NA 10.000000  9.090909  8.333333\nattr(,\"name\")\n[1] \"c(100, 110, 120, 130)\"\n\n# Calculate the growth rate with scaling and a power transformation.\nts_growth_rate_vec(c(100, 110, 120, 130), .scale = 10, .power = 2)\n\n[1]       NA 2.100000 1.900826 1.736111\nattr(,\"name\")\n[1] \"c(100, 110, 120, 130)\"\n\n# Calculate the log differences of a time series with lags.\nts_growth_rate_vec(c(100, 110, 120, 130), .log_diff = TRUE, .lags = -1)\n\n[1] -9.531018 -8.701138 -8.004271        NA\nattr(,\"name\")\n[1] \"c(100, 110, 120, 130)\"\n\n# Plot\nplot.ts(AirPassengers)\n\n\n\n\n\n\n\nplot.ts(ts_growth_rate_vec(AirPassengers))"
  },
  {
    "objectID": "posts/2024-06-17/index.html#helper-functions",
    "href": "posts/2024-06-17/index.html#helper-functions",
    "title": "Introducing healthyR.ts: A Comprehensive R Package for Time Series Analysis",
    "section": "Helper Functions",
    "text": "Helper Functions\n\nts_auto_stationarize\n\n# Example 1: Using the AirPassengers dataset\nauto_stationarize(AirPassengers)\n\nThe time series is already stationary via ts_adf_test().\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n# Example 2: Using the BJsales dataset\nauto_stationarize(BJsales)\n\nThe time series is not stationary. Attempting to make it stationary...\n\n\nLogrithmic Transformation Failed.\nData requires more single differencing than its frequency, trying double\ndifferencing\nDouble Differencing of order 1 made the time series stationary\n\n\n$stationary_ts\nTime Series:\nStart = 3 \nEnd = 150 \nFrequency = 1 \n  [1]  0.5 -0.4  0.6  1.1 -2.8  3.0 -1.1  0.6 -0.5 -0.5  0.1  2.0 -0.6  0.8  1.2\n [16] -3.4 -0.7 -0.3  1.7  3.0 -3.2  0.9  2.2 -2.5 -0.4  2.6 -4.3  2.0 -3.1  2.7\n [31] -2.1  0.1  2.1 -0.2 -2.2  0.6  1.0 -2.6  3.0  0.3  0.2 -0.8  1.0  0.0  3.2\n [46] -2.2 -4.7  1.2  0.8 -0.6 -0.4  0.6  1.0 -1.6 -0.1  3.4 -0.9 -1.7 -0.5  0.8\n [61]  2.4 -1.9  0.6 -2.2  2.6 -0.1 -2.7  1.7 -0.3  1.9 -2.7  1.1 -0.6  0.9  0.0\n [76]  1.8 -0.5 -0.4 -1.2  2.6 -1.8  1.7 -0.9  0.6 -0.4  3.0 -2.8  3.1 -2.3 -1.1\n [91]  2.1 -0.3 -1.7 -0.8 -0.4  1.1 -1.5  0.3  1.4 -2.0  1.3 -0.3  0.4 -3.5  1.1\n[106]  2.6  0.4 -1.3  2.0 -1.6  0.6 -0.1 -1.4  1.6  1.6 -3.4  1.7 -2.2  2.1 -2.0\n[121] -0.2  0.2  0.7 -1.4  1.8 -0.1 -0.7  0.4  0.4  1.0 -2.4  1.0 -0.4  0.8 -1.0\n[136]  1.4 -1.2  1.1 -0.9  0.5  1.9 -0.6  0.3 -1.4 -0.9 -0.5  1.4  0.1\n\n$ndiffs\n[1] 1\n\n$adf_stats\n$adf_stats$test_stat\n[1] -6.562008\n\n$adf_stats$p_value\n[1] 0.01\n\n\n$trans_type\n[1] \"double_diff\"\n\n$ret\n[1] TRUE"
  },
  {
    "objectID": "posts/2024-06-18/index.html",
    "href": "posts/2024-06-18/index.html",
    "title": "How to Add Leading Zeros to Numbers in R",
    "section": "",
    "text": "Hello! Today, we’re going to discuss a common yet essential task in data manipulation: adding leading zeros to numbers. This might come in handy when dealing with IDs, ZIP codes, or any situation where a fixed-width numeric format is needed. We’ll be exploring this using base R, keeping things simple and straightforward."
  },
  {
    "objectID": "posts/2024-06-18/index.html#step-1-converting-numbers-to-strings",
    "href": "posts/2024-06-18/index.html#step-1-converting-numbers-to-strings",
    "title": "How to Add Leading Zeros to Numbers in R",
    "section": "Step 1: Converting Numbers to Strings",
    "text": "Step 1: Converting Numbers to Strings\nFirst, we need to convert our numbers to character strings. This is because leading zeros don’t hold any significance in numeric form but are essential in string form.\n\nnumber &lt;- 123\nstr_number &lt;- as.character(number)\nprint(str_number)\n\n[1] \"123\""
  },
  {
    "objectID": "posts/2024-06-18/index.html#step-2-adding-leading-zeros",
    "href": "posts/2024-06-18/index.html#step-2-adding-leading-zeros",
    "title": "How to Add Leading Zeros to Numbers in R",
    "section": "Step 2: Adding Leading Zeros",
    "text": "Step 2: Adding Leading Zeros\nWe can use the sprintf() function in base R to add leading zeros. The sprintf() function is powerful and versatile for string formatting.\n\nnumber &lt;- 123\nformatted_number &lt;- sprintf(\"%05d\", number)\nprint(formatted_number)\n\n[1] \"00123\"\n\n\nHere’s what’s happening:\n\n\"%05d\" is the format specifier.\n%d tells sprintf() that we’re dealing with an integer.\n05 indicates that the output should be 5 characters wide, with leading zeros added if necessary."
  },
  {
    "objectID": "posts/2024-06-18/index.html#step-3-applying-to-a-vector",
    "href": "posts/2024-06-18/index.html#step-3-applying-to-a-vector",
    "title": "How to Add Leading Zeros to Numbers in R",
    "section": "Step 3: Applying to a Vector",
    "text": "Step 3: Applying to a Vector\nOften, you’ll be working with a vector of numbers. Let’s see how to apply this to each element in a vector.\n\nnumbers &lt;- c(1, 23, 456)\nformatted_numbers &lt;- sprintf(\"%05d\", numbers)\nprint(formatted_numbers)\n\n[1] \"00001\" \"00023\" \"00456\""
  },
  {
    "objectID": "posts/2024-06-18/index.html#step-4-dealing-with-non-numeric-input",
    "href": "posts/2024-06-18/index.html#step-4-dealing-with-non-numeric-input",
    "title": "How to Add Leading Zeros to Numbers in R",
    "section": "Step 4: Dealing with Non-Numeric Input",
    "text": "Step 4: Dealing with Non-Numeric Input\nIt’s important to handle non-numeric input gracefully. You can use a combination of ifelse() and is.na() to manage this.\n\nmixed_input &lt;- c(12, \"abc\", 345)\nformatted_mixed_input &lt;- ifelse(\n  is.na(as.numeric(mixed_input)), \n  mixed_input, \n  sprintf(\"%05d\", as.numeric(mixed_input))\n  )\n\nWarning in ifelse(is.na(as.numeric(mixed_input)), mixed_input, sprintf(\"%05d\",\n: NAs introduced by coercion\n\n\nWarning in sprintf(\"%05d\", as.numeric(mixed_input)): NAs introduced by coercion\n\nprint(formatted_mixed_input)\n\n[1] \"00012\" \"abc\"   \"00345\""
  },
  {
    "objectID": "posts/2024-06-19/index.html",
    "href": "posts/2024-06-19/index.html",
    "title": "Extracting Data from Another Workbook Using VBA and Executing It from R",
    "section": "",
    "text": "When working with Excel files, you may need to extract data from one workbook and use it in another. This can be done manually by copying and pasting the data, but it can be time-consuming and error-prone, especially when dealing with large datasets. One way to automate this process is by using Visual Basic for Applications (VBA) to extract the data from one workbook and execute the VBA code from R.\nIn this blog post, I’ll walk you through the process of extracting data from another workbook using VBA and how to execute this from R. We’ll use the data in Sheet1 from an example workbook."
  },
  {
    "objectID": "posts/2024-06-19/index.html#step-1-setting-up-the-vba-code",
    "href": "posts/2024-06-19/index.html#step-1-setting-up-the-vba-code",
    "title": "Extracting Data from Another Workbook Using VBA and Executing It from R",
    "section": "Step 1: Setting Up the VBA Code",
    "text": "Step 1: Setting Up the VBA Code\nFirst, we need to write a VBA script that will open another workbook, extract data from Sheet1, and return this data. Here’s a simple VBA code to accomplish this:\n\nOpen the VBA editor by pressing Alt + F11.\nInsert a new module by right-clicking on any existing module or the workbook name, then selecting Insert &gt; Module.\nCopy and paste the following VBA code into the module:\n\nSub ExtractData()\n    Dim sourceWorkbook As Workbook\n    Dim targetWorkbook As Workbook\n    Dim sourceSheet As Worksheet\n    Dim targetSheet As Worksheet\n    Dim sourceRange As Range\n    Dim targetRange As Range\n\n    ' Define the path to the source workbook\n    Dim sourceFilePath As String\n    sourceFilePath = \"C:\\Users\\ssanders\\Documents\\GitHub\\steveondata\\posts\\2024-06-19\\random_data.xlsx\" ' Change this to your actual file path\n\n    ' Open the source workbook\n    Set sourceWorkbook = Workbooks.Open(sourceFilePath)\n    Set sourceSheet = sourceWorkbook.Sheets(\"Sheet1\")\n    Set sourceRange = sourceSheet.Range(\"A1:B30\") ' Adjust the range as needed\n\n    ' Open the target workbook\n    Set targetWorkbook = ThisWorkbook\n    Set targetSheet = targetWorkbook.Sheets(\"Sheet1\")\n    Set targetRange = targetSheet.Range(\"A1:B30\") ' Adjust the range as needed\n\n    ' Clear the target range before pasting\n    targetRange.Clear\n\n    ' Copy the data from source to target\n    sourceRange.Copy Destination:=targetRange\n\n    ' Close the source workbook without saving\n    sourceWorkbook.Close SaveChanges:=False\n\n    ' Save and close the target workbook\n    targetWorkbook.Save\n    targetWorkbook.Close SaveChanges:=True\n    \n    ' Quit Excel\n    Application.Quit\nEnd Sub\nThis script opens another workbook, copies the data from Sheet1, and pastes it into the current workbook’s Sheet1. Modify the sourceFilePath to the location of your source workbook and adjust the ranges as necessary. The data was already in a workbook and thus we knew the dimensions of the data."
  },
  {
    "objectID": "posts/2024-06-19/index.html#step-2-executing-the-vba-code-from-r",
    "href": "posts/2024-06-19/index.html#step-2-executing-the-vba-code-from-r",
    "title": "Extracting Data from Another Workbook Using VBA and Executing It from R",
    "section": "Step 2: Executing the VBA Code from R",
    "text": "Step 2: Executing the VBA Code from R\nNow that we have the VBA code ready, let’s write some R code to execute this VBA macro. We’ll use the RDCOMClient package to interact with Excel from R.\n\nInstall the RDCOMClient package if you haven’t already:\n\n\ninstall.packages(\"RDCOMClient\", repos = \"http://www.omegahat.net/R\")\n\n\nLoad the package and write the R code to run the VBA macro:\n\n\nlibrary(RDCOMClient)\n\n# Path to your Excel workbook containing the VBA macro\nexcelFilePath &lt;- \"C:/Users/ssanders/Documents/GitHub/steveondata/posts/2024-06-19/get_data_from_another_workbook.xlsm\"\n\n# Create a COM object to interact with Excel\nexcelApp &lt;- COMCreate(\"Excel.Application\")\n\n# Open the workbook\nworkbook &lt;- excelApp$Workbooks()$Open(excelFilePath)\n\n# Make Excel visible (optional)\nexcelApp[[\"Visible\"]] &lt;- FALSE\n\n# Run the VBA macro\nexcelApp$Run(\"ExtractData\")\n\nNULL\n\n# Close the workbook without saving\nworkbook$Close(FALSE)\n\n[1] TRUE\n\n# Quit Excel\nexcelApp$Quit()\n\nNULL\n\n# Release COM object\nrm(excelApp, workbook)\n\nThis R script creates a COM object to interact with Excel, opens the workbook containing our VBA macro, runs the macro, and then quits Excel. Make sure to modify the excelFilePath to point to your actual workbook.\nNow let’s see if it actually worked:\n\nlibrary(readxl)\n\nf_path &lt;- \"C:/Users/ssanders/Documents/GitHub/steveondata/posts/2024-06-19/random_data.xlsx\"\nread_excel(f_path, sheet = \"Sheet1\", col_names = FALSE) \n\nNew names:\n• `` -&gt; `...1`\n\n\n# A tibble: 30 × 1\n     ...1\n    &lt;dbl&gt;\n 1 -0.371\n 2 -1.00 \n 3  0.226\n 4 -0.323\n 5 -0.142\n 6  1.19 \n 7 -0.827\n 8  0.715\n 9 -0.105\n10 -1.06 \n# ℹ 20 more rows"
  },
  {
    "objectID": "posts/2024-06-20/index.html",
    "href": "posts/2024-06-20/index.html",
    "title": "Practical Examples with healthyR.ts",
    "section": "",
    "text": "Introduction\nToday I am going to go over some quick yet practical examples of ways that you can use the healthyR.ts package. This package is designed to help you analyze time series data in a more efficient and effective manner.\nLet’s just jump right into it!\n\n\nLoad the libraries\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(plotly)\nlibrary(timetk)\nlibrary(modeltime)\n\n\n\nLoad the data\nWe are going to use the timeseries data called BJSales.lead that comes with Base R. We will do this to showcase a couple of things like turning a ts object into a tibble and plotting the data.\n\n# Load the data, which has no time series information other than it is\n# a time series object and 150 points in length, so we will go ahead and\n# create a date column for it and name it date_col.\ndf &lt;- BJsales.lead |&gt;\n  ts_to_tbl() |&gt;\n  mutate(date_col = seq.Date(from = as.Date(\"1991-01-01\"), \n                              by = \"month\", \n                              length.out = 150)) |&gt;\n  select(date_col, everything())\n\n# Print the first few rows of the data\nhead(df)\n\n# A tibble: 6 × 2\n  date_col   value\n  &lt;date&gt;     &lt;dbl&gt;\n1 1991-01-01 10.0 \n2 1991-02-01 10.1 \n3 1991-03-01 10.3 \n4 1991-04-01  9.75\n5 1991-05-01 10.3 \n6 1991-06-01 10.1 \n\n\nSo far, we have loaded the data and created a date column for it. Now, let’s plot the data. We are going to use the ts_vva_plot function to do this.\n\n# Plot the data\nplt_data &lt;- ts_vva_plot(df, date_col, value)\n\nhead(plt_data[[\"data\"]][[\"augmented_data_tbl\"]])\n\n# A tibble: 6 × 3\n  date_col   name           value\n  &lt;date&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1 1991-01-01 Value        10.0   \n2 1991-01-01 Velocity     NA     \n3 1991-01-01 Acceleration NA     \n4 1991-02-01 Value        10.1   \n5 1991-02-01 Velocity      0.0600\n6 1991-02-01 Acceleration NA     \n\nplt_data[[\"plots\"]][[\"interactive_plot\"]]\n\n\n\n\n\nNow we have created the augmented data that gets the first order difference of the time series velocity and then the second order difference which gets us the acceleration. The function then creates a ggplot2 plot and a plotly plot of the data. Let’s move on to see the growth rate of this data.\n\n# Plot the growth rate of the data\ndf_growth_augment_tbl &lt;- ts_growth_rate_augment(\n  df,\n  value\n)\n\nhead(df_growth_augment_tbl)\n\n# A tibble: 6 × 3\n  date_col   value growth_rate_value\n  &lt;date&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1 1991-01-01 10.0             NA    \n2 1991-02-01 10.1              0.599\n3 1991-03-01 10.3              2.48 \n4 1991-04-01  9.75            -5.52 \n5 1991-05-01 10.3              5.95 \n6 1991-06-01 10.1             -1.94 \n\n\nLet’s now view the data:\n\nplt &lt;- df_growth_augment_tbl |&gt;\n  pivot_longer(cols = -date_col) |&gt;\n  ggplot(aes(x = date_col, y = value, color = name)) +\n  facet_wrap(~ name, ncol = 1, scales = \"free\") +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"Growth Rate of Time Series Data\",\n    color = \"Variable\"\n  )\n\nprint(plt)\n\n\n\n\n\n\n\nggplotly(plt)\n\n\n\n\n\n\n\nStationary?\nIs the data stationary? Meaning does the joint probability of the distribution change when shifted in time? Let’s find out.\n\nts_adf_test(df[[\"value\"]])\n\n$test_stat\n[1] -1.723664\n\n$p_value\n[1] 0.6915227\n\n\nThe p-value from this test is 0.692. This means that we can accept the null hypothesis that the data is non-stationary. We can, however, make the data stationary by using a built in function in this package.\n\nauto_stationary_df &lt;- auto_stationarize(df[[\"value\"]])\n\nThe time series is not stationary. Attempting to make it stationary...\n\nstationary_vec &lt;- auto_stationary_df[[\"stationary_ts\"]]\nndiffs &lt;- auto_stationary_df[[\"ndiffs\"]]\ntrans_type &lt;- auto_stationary_df[[\"trans_type\"]]\ntest_stat &lt;- auto_stationary_df[[\"adf_stats\"]][[\"test_stat\"]]\np_value &lt;- auto_stationary_df[[\"adf_stats\"]][[\"p_value\"]]\n\nThe data is now stationary after 1 differencing. The transformation type used was diff. The test statistic was -4.839 and the p-value was 0.01.\nLet’s now add the stationary data to the df_growth_augment_tbl and plot it. First in order to do this we are going to have to pad the data since it is shorter than the original data. We will simply add an NA to the vector then attach.\n\nstationary_vec &lt;- c(rep(NA, ndiffs), stationary_vec)\ndf_growth_augment_tbl &lt;- df_growth_augment_tbl |&gt;\n  mutate(stationary = stationary_vec)\n\ndf_growth_augment_tbl |&gt;\n  pivot_longer(cols = -date_col) |&gt;\n  ggplot(aes(x = date_col, y = value, color = name)) +\n  facet_wrap(~ name, ncol = 1, scales = \"free\") +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"Growth Rate/Value and Stationary Data of Time Series\",\n    color = \"Variable\"\n  )\n\n\n\n\n\n\n\n\nIt’s close to the growth rate as it is the first order difference of the data.\nNow, lets see if there is any lags that are present in the data.\n\noutput &lt;- ts_lag_correlation(df_growth_augment_tbl,\n                .date_col = date_col,\n                .value_col = value,\n                .lags = c(1,2,3,4,6,12,24))\n\noutput[[\"plots\"]][[\"plotly_lag_plot\"]]\n\n\n\n\noutput[[\"plots\"]][[\"plotly_heatmap\"]]\n\n\n\n\n\nWe can tell from the data, and from the automatic stationarization that the data is highly correlated at lag 1. So lags 2, 3, …, etc are not necessary. We can see the linear correlation fall apart the further the lags\nLet’s go ahead and model it and see what happens.\n\n\nModel the data\n\nsplits &lt;- time_series_split(\n  df_growth_augment_tbl, \n  date_col, \n  assess= 12, \n  skip = 3, \n  cumulative = TRUE\n  )\n\nts_aa &lt;- ts_auto_arima(\n  df_growth_augment_tbl,\n  .num_cores = 2,\n  .date_col = date_col,\n  .value_col = value,\n  .rsamp_obj = splits,\n  .formula = value ~ .,\n  .grid_size = 10,\n  .cv_slice_limit = 12,\n  .tune = FALSE\n)\n\nfrequency = 12 observations per 1 year\n\nts_aa[[\"recipe_info\"]]\n\n$recipe_call\nrecipe(.data = df_growth_augment_tbl, .date_col = date_col, .value_col = value, \n    .formula = value ~ ., .rsamp_obj = splits, .tune = FALSE, \n    .grid_size = 10, .num_cores = 2, .cv_slice_limit = 12)\n\n$recipe_syntax\n[1] \"ts_arima_recipe &lt;-\"                                                                                                                                                                                              \n[2] \"\\n  recipe(.data = df_growth_augment_tbl, .date_col = date_col, .value_col = value, \\n    .formula = value ~ ., .rsamp_obj = splits, .tune = FALSE, .grid_size = 10, \\n    .num_cores = 2, .cv_slice_limit = 12)\"\n\n$rec_obj\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\nts_aa[[\"model_info\"]]\n\n$model_spec\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nARIMA Regression Model Specification (regression)\n\nComputational engine: auto_arima \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nSeries: outcome \nRegression with ARIMA(0,1,2) errors \n\nCoefficients:\n         ma1      ma2   drift  growth_rate_value  stationary\n      0.4626  -0.4013  0.0233            -0.0021      0.5168\ns.e.  0.0794   0.0849  0.0131             0.0098      0.0831\n\nsigma^2 = 0.02124:  log likelihood = 70.34\nAIC=-128.69   AICc=-128.04   BIC=-111.21\n\n$was_tuned\n[1] \"not_tuned\"\n\nts_aa[[\"model_calibration\"]][[\"plot\"]]\n\n\n\n\nts_aa[[\"model_calibration\"]]\n\n$plot\n\n$calibration_tbl\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc                       .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                             &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;workflow&gt; REGRESSION WITH ARIMA(0,1,2) ERR… Test  &lt;tibble [12 × 4]&gt;\n\n$model_accuracy\n# A tibble: 1 × 9\n  .model_id .model_desc                .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 REGRESSION WITH ARIMA(0,1… Test  0.179  1.32 0.832  1.33 0.229 0.307\n\n\n\n\nConclusion\nThis in short is a very simple practical example of how to use the healthyR.ts package. This post was not meant to be a comprehensive guide to time series analysis, but rather a simple example of how to use the package. The package is still in development and will be updated with more features and functions in the future.\nHappy Coding!"
  },
  {
    "objectID": "posts/2024-06-23/index.html",
    "href": "posts/2024-06-23/index.html",
    "title": "Writing Excel Spreadsheets to Disk with R and Python",
    "section": "",
    "text": "When working with data, exporting your results to an Excel file can be very handy. Today, I’ll show you how to write the iris dataset to an Excel file using R and Python. We will explore three R packages: writexl, openxlsx, and xlsx, and the openpyxl library in Python. Let’s dive in!\n\n\nFirst, let’s start with R. We’ll use the well-known iris dataset and write it to a temporary file using three different packages.\n\n\nThe writexl package is straightforward and easy to use for writing data frames to Excel files.\n# Install and load the writexl package\ninstall.packages(\"writexl\")\nlibrary(writexl)\n\n# Write iris dataset to a temporary file\nwritexl::write_xlsx(iris, tempfile())\nThe write_xlsx function does exactly what it says: it writes your data frame to an Excel file. The tempfile() function creates a temporary file, which is useful for quick testing without cluttering your directory.\n\n\n\nThe openxlsx package provides more flexibility and additional features compared to writexl.\n# Install and load the openxlsx package\ninstall.packages(\"openxlsx\")\nlibrary(openxlsx)\n\n# Write the iris dataset to a temporary file\nopenxlsx::write.xlsx(iris, tempfile())\nWith openxlsx, you can directly write the data frame to an Excel file using the write.xlsx function, making the process simple and efficient.\n\n\n\nThe xlsx package is another option that can be useful, though it requires Java.\n# Install and load the xlsx package\ninstall.packages(\"xlsx\")\nlibrary(xlsx)\n\n# Write the iris dataset to a temporary file\nxlsx::write.xlsx(iris, paste0(tempfile(), \".xlsx\"))\nwrite.xlsx from the xlsx package works similarly to the previous functions but requires the .xlsx extension to be explicitly added to the temporary file name.\n\n\n\n\nNow, let’s see how to achieve the same with Python using the openpyxl library.\n# Install openpyxl if you haven't already\n!pip install openpyxl\n\nimport openpyxl\n\n# Load an existing workbook\nworkbook = openpyxl.load_workbook(\"example.xlsx\")\n\n# Add a new seet\nworkbook.create_sheet(title = \"Sheet1\")\n\nsheet_name = \"Sheet1\"\n\nsheet = workbook[sheet_name]\n\nsheet[\"A1\"] = \"Hello, World!\"\n\nworkbook.save(\"example.xlsx\")\nHere is a concise breakdown of what this script does:\n\nInstalls the openpyxl library (if necessary).\nImports the openpyxl library.\nLoads an existing Excel workbook named example.xlsx.\nCreates a new sheet titled “Sheet1” in the workbook.\nAssigns the value “Hello, World!” to cell A1 of the new sheet.\nSaves the changes back to “example.xlsx”."
  },
  {
    "objectID": "posts/2024-06-23/index.html#writing-excel-files-in-r",
    "href": "posts/2024-06-23/index.html#writing-excel-files-in-r",
    "title": "Writing Excel Spreadsheets to Disk with R and Python",
    "section": "",
    "text": "First, let’s start with R. We’ll use the well-known iris dataset and write it to a temporary file using three different packages.\n\n\nThe writexl package is straightforward and easy to use for writing data frames to Excel files.\n# Install and load the writexl package\ninstall.packages(\"writexl\")\nlibrary(writexl)\n\n# Write iris dataset to a temporary file\nwritexl::write_xlsx(iris, tempfile())\nThe write_xlsx function does exactly what it says: it writes your data frame to an Excel file. The tempfile() function creates a temporary file, which is useful for quick testing without cluttering your directory.\n\n\n\nThe openxlsx package provides more flexibility and additional features compared to writexl.\n# Install and load the openxlsx package\ninstall.packages(\"openxlsx\")\nlibrary(openxlsx)\n\n# Write the iris dataset to a temporary file\nopenxlsx::write.xlsx(iris, tempfile())\nWith openxlsx, you can directly write the data frame to an Excel file using the write.xlsx function, making the process simple and efficient.\n\n\n\nThe xlsx package is another option that can be useful, though it requires Java.\n# Install and load the xlsx package\ninstall.packages(\"xlsx\")\nlibrary(xlsx)\n\n# Write the iris dataset to a temporary file\nxlsx::write.xlsx(iris, paste0(tempfile(), \".xlsx\"))\nwrite.xlsx from the xlsx package works similarly to the previous functions but requires the .xlsx extension to be explicitly added to the temporary file name."
  },
  {
    "objectID": "posts/2024-06-23/index.html#writing-excel-files-in-python",
    "href": "posts/2024-06-23/index.html#writing-excel-files-in-python",
    "title": "Writing Excel Spreadsheets to Disk with R and Python",
    "section": "",
    "text": "Now, let’s see how to achieve the same with Python using the openpyxl library.\n# Install openpyxl if you haven't already\n!pip install openpyxl\n\nimport openpyxl\n\n# Load an existing workbook\nworkbook = openpyxl.load_workbook(\"example.xlsx\")\n\n# Add a new seet\nworkbook.create_sheet(title = \"Sheet1\")\n\nsheet_name = \"Sheet1\"\n\nsheet = workbook[sheet_name]\n\nsheet[\"A1\"] = \"Hello, World!\"\n\nworkbook.save(\"example.xlsx\")\nHere is a concise breakdown of what this script does:\n\nInstalls the openpyxl library (if necessary).\nImports the openpyxl library.\nLoads an existing Excel workbook named example.xlsx.\nCreates a new sheet titled “Sheet1” in the workbook.\nAssigns the value “Hello, World!” to cell A1 of the new sheet.\nSaves the changes back to “example.xlsx”."
  },
  {
    "objectID": "posts/2024-06-24/index.html",
    "href": "posts/2024-06-24/index.html",
    "title": "An Introduction to healthyR.ai",
    "section": "",
    "text": "This post will introduction to the healthyR.ai package. The healthyR.ai package is a collection of functions that I have developed to help me analyze and visualize data. The package is designed to be easy to use and to provide a wide range of functionality for data analysis. The package is also meant to help and provide some easy boilerplate funcationality for machine learning.\nIt might be best to view this post in light mode to see the tables better."
  },
  {
    "objectID": "posts/2024-06-24/index.html#the-goal",
    "href": "posts/2024-06-24/index.html#the-goal",
    "title": "An Introduction to healthyR.ai",
    "section": "The Goal",
    "text": "The Goal\n\nThe ultimate goal really is to make it easier to do data analysis and machine learning in R. The package is designed to be easy to use and to provide a wide range of functionality for data analysis. The package is also meant to help and provide some easy boilerplate functionality for machine learning. This package is in its early stages and will be updated frequently.\nIt also keeps with the same framework of all of the healthyverse packages in that it is meant for the user to be able to use the package without having to know a lot of R. Many rural hospitals do not have the resources to perform this sort of work, so I am working hard to build these types of things out for them for free.\nLet’s go through some examples.\n\nlibrary(healthyR.ai)\nlibrary(tidyverse)\nlibrary(DT)\n\nNow let’s get a list of all the functions that are exposed in the package.\n\n# Functions and their arguments for healthyR\n\npat &lt;- c(\"%&gt;%\",\":=\",\"as_label\",\"as_name\",\"enquo\",\"enquos\",\"expr\",\n         \"sym\",\"syms\",\"required_pkgs.step_hai_fourier\",\n         \"required_pkgs.step_hai_fourier_discrete\",\n         \"required_pkgs.step_hai_hyperbolic\",\n         \"required_pkgs.step_hai_scale_zero_one\",\n         \"required_pkgs.step_hai_scal_zscore\",\n         \"required_pkgs.step_hai_winsorized_move\",\n         \"required_pkgs.step_hai_winsorized_truncate\")\n\ntibble(fns = ls.str(\"package:healthyR.ai\")) |&gt;\n  filter(!fns %in% pat) |&gt;\n  mutate(params = purrr::map(fns, formalArgs)) |&gt; \n  group_by(fns) |&gt; \n  mutate(func_with_params = toString(params)) |&gt;\n  mutate(\n    func_with_params = ifelse(\n      str_detect(\n        func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |&gt;\n  select(fns, func_with_params) |&gt;\n  mutate(fns = as.factor(fns)) |&gt;\n  datatable(\n    #class = 'cell-boarder-stripe',\n    colnames = c(\"Function\", \"Full Call\"),\n    options = list(\n      autowidth = TRUE,\n      pageLength = 10\n    )\n  )"
  },
  {
    "objectID": "posts/2024-06-24/index.html#syntax",
    "href": "posts/2024-06-24/index.html#syntax",
    "title": "An Introduction to healthyR.ai",
    "section": "Syntax",
    "text": "Syntax\npca_your_recipe(.recipe_object, .data, .threshold = 0.75, .top_n = 5)"
  },
  {
    "objectID": "posts/2024-06-24/index.html#arguments",
    "href": "posts/2024-06-24/index.html#arguments",
    "title": "An Introduction to healthyR.ai",
    "section": "Arguments",
    "text": "Arguments\n\n.recipe_object -\n.data - The full data set that is used in the original recipe object passed into .recipe_object in order to obtain the baked data of the transform.\n.threshold - A number between 0 and 1. A fraction of the total variance that should be covered by the components.\n.top_n - How many variables loadings should be returned per PC"
  },
  {
    "objectID": "posts/2024-06-24/index.html#value",
    "href": "posts/2024-06-24/index.html#value",
    "title": "An Introduction to healthyR.ai",
    "section": "Value",
    "text": "Value\nA list object with several components."
  },
  {
    "objectID": "posts/2024-06-24/index.html#details",
    "href": "posts/2024-06-24/index.html#details",
    "title": "An Introduction to healthyR.ai",
    "section": "Details",
    "text": "Details\nThis is a simple wrapper around some recipes functions to perform a PCA on a given recipe. This function will output a list and return it invisible. All of the components of the analysis will be returned in a list as their own object that can be selected individually. A scree plot is also included. The items that get returned are:\n\npca_transform - This is the pca recipe.\nvariable_loadings\nvariable_variance\npca_estimates\npca_juiced_estimates\npca_baked_data\npca_variance_df\npca_rotattion_df\npca_variance_scree_plt\npca_loadings_plt\npca_loadings_plotly\npca_top_n_loadings_plt\npca_top_n_plotly"
  },
  {
    "objectID": "posts/2024-06-24/index.html#example",
    "href": "posts/2024-06-24/index.html#example",
    "title": "An Introduction to healthyR.ai",
    "section": "Example",
    "text": "Example\n\nlibrary(rsample)\nlibrary(recipes)\n\nsplits &lt;- initial_split(mtcars, prop = 0.8)\n\nrec_obj &lt;- recipe(mpg ~ ., data = training(splits)) |&gt;\n  step_normalize(all_predictors())\n\npca_output &lt;- pca_your_recipe(\n  .recipe_object = rec_obj, \n  .data = mtcars, \n  .threshold = 0.75, \n  .top_n = 5\n  )\n\nNow let’s check the output:\n\npca_output\n\n$pca_transform\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_predictors()\n\n\n• Centering for: recipes::all_numeric()\n\n\n• Scaling for: recipes::all_numeric()\n\n\n• Sparse, unbalanced variable filter on: recipes::all_numeric()\n\n\n• PCA extraction with: recipes::all_numeric_predictors()\n\n\n\n$variable_loadings\n# A tibble: 100 × 4\n   terms  value component id       \n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 cyl   -0.415 PC1       pca_Nf8S6\n 2 disp  -0.406 PC1       pca_Nf8S6\n 3 hp    -0.372 PC1       pca_Nf8S6\n 4 drat   0.282 PC1       pca_Nf8S6\n 5 wt    -0.364 PC1       pca_Nf8S6\n 6 qsec   0.266 PC1       pca_Nf8S6\n 7 vs     0.340 PC1       pca_Nf8S6\n 8 am     0.201 PC1       pca_Nf8S6\n 9 gear   0.166 PC1       pca_Nf8S6\n10 carb  -0.243 PC1       pca_Nf8S6\n# ℹ 90 more rows\n\n$variable_variance\n# A tibble: 40 × 4\n   terms     value component id       \n   &lt;chr&gt;     &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance 5.47           1 pca_Nf8S6\n 2 variance 2.86           2 pca_Nf8S6\n 3 variance 0.610          3 pca_Nf8S6\n 4 variance 0.349          4 pca_Nf8S6\n 5 variance 0.220          5 pca_Nf8S6\n 6 variance 0.184          6 pca_Nf8S6\n 7 variance 0.144          7 pca_Nf8S6\n 8 variance 0.0816         8 pca_Nf8S6\n 9 variance 0.0614         9 pca_Nf8S6\n10 variance 0.0203        10 pca_Nf8S6\n# ℹ 30 more rows\n\n$pca_estimates\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\n── Training information \n\n\nTraining data contained 25 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: cyl, disp, hp, drat, wt, qsec, ... | Trained\n\n\n• Centering for: cyl, disp, hp, drat, wt, qsec, vs, am, gear, ... | Trained\n\n\n• Scaling for: cyl, disp, hp, drat, wt, qsec, vs, am, gear, ... | Trained\n\n\n• Sparse, unbalanced variable filter removed: &lt;none&gt; | Trained\n\n\n• PCA extraction with: cyl, disp, hp, drat, wt, qsec, vs, am, ... | Trained\n\n\n\n$pca_juiced_estimates\n# A tibble: 25 × 3\n        mpg     PC1       PC2\n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 -1.04    -3.10    3.66    \n 2 -0.00551  0.167   1.42    \n 3 -1.16    -3.14   -0.181   \n 4  0.0633   0.0181 -2.36    \n 5  0.0805   1.78   -1.90    \n 6 -0.315    0.285  -0.266   \n 7 -0.797   -2.46   -1.05    \n 8  1.96     2.95    0.000351\n 9  0.0633   2.09    0.390   \n10  0.580    1.62   -1.32    \n# ℹ 15 more rows\n\n$pca_baked_data\n# A tibble: 32 × 3\n        mpg     PC1     PC2\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.00551  0.201   1.59  \n 2 -0.00551  0.167   1.42  \n 3  0.304    2.41    0.0722\n 4  0.0633   0.0181 -2.36  \n 5 -0.401   -2.33   -1.08  \n 6 -0.505   -0.0175 -2.76  \n 7 -1.16    -3.14   -0.181 \n 8  0.580    1.62   -1.32  \n 9  0.304    2.01   -1.66  \n10 -0.315    0.285  -0.266 \n# ℹ 22 more rows\n\n$pca_variance_df\n# A tibble: 10 × 6\n   PC    var_explained var_pct_txt cum_var_pct cum_var_pct_txt ou_threshold\n   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;fct&gt;       \n 1 PC1         0.547   54.70%            0.547 54.70%          Under       \n 2 PC2         0.286   28.59%            0.833 83.30%          Over        \n 3 PC3         0.0610  6.10%             0.894 89.40%          Over        \n 4 PC4         0.0349  3.49%             0.929 92.89%          Over        \n 5 PC5         0.0220  2.20%             0.951 95.09%          Over        \n 6 PC6         0.0184  1.84%             0.969 96.93%          Over        \n 7 PC7         0.0144  1.44%             0.984 98.37%          Over        \n 8 PC8         0.00816 0.82%             0.992 99.18%          Over        \n 9 PC9         0.00614 0.61%             0.998 99.80%          Over        \n10 PC10        0.00203 0.20%             1     100.00%         Over        \n\n$pca_rotation_df\n# A tibble: 10 × 10\n      PC1     PC2     PC3     PC4    PC5     PC6      PC7      PC8     PC9\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.415  0.0149 -0.102   0.0507 -0.128  0.107  -0.00265 -0.186    0.811 \n 2 -0.406 -0.0765 -0.143   0.161  -0.291 -0.365   0.247   -0.00403  0.0366\n 3 -0.372  0.222   0.143  -0.0658 -0.493 -0.0860 -0.0196   0.585   -0.244 \n 4  0.282  0.313   0.0575  0.889  -0.148  0.0177  0.0252   0.00765  0.0446\n 5 -0.364 -0.186   0.375   0.202   0.196 -0.485  -0.194   -0.433   -0.263 \n 6  0.266 -0.407   0.341   0.0549  0.169 -0.379  -0.120    0.538    0.406 \n 7  0.340 -0.217   0.404  -0.172  -0.710  0.0654 -0.0479  -0.359    0.0534\n 8  0.201  0.465  -0.282  -0.201  -0.105 -0.468  -0.604   -0.0821   0.124 \n 9  0.166  0.491   0.289  -0.246   0.131 -0.336   0.649   -0.0950   0.139 \n10 -0.243  0.376   0.602  -0.0403  0.187  0.364  -0.312    0.0414   0.0886\n# ℹ 1 more variable: PC10 &lt;dbl&gt;\n\n$pca_variance_scree_plt\n\n\n\n\n\n\n\n\n\n\n$pca_loadings_plt\n\n\n\n\n\n\n\n\n\n\n$pca_loadings_plotly\n\n$pca_top_n_loadings_plt\n\n\n\n\n\n\n\n\n\n\n$pca_top_n_plotly"
  },
  {
    "objectID": "posts/2024-06-24/index.html#example---pca-a-recipe",
    "href": "posts/2024-06-24/index.html#example---pca-a-recipe",
    "title": "An Introduction to healthyR.ai",
    "section": "Example - PCA a recipe",
    "text": "Example - PCA a recipe\n\nSyntax\npca_your_recipe(.recipe_object, .data, .threshold = 0.75, .top_n = 5)\n\n\nArguments\n\n.recipe_object -\n.data - The full data set that is used in the original recipe object passed into .recipe_object in order to obtain the baked data of the transform.\n.threshold - A number between 0 and 1. A fraction of the total variance that should be covered by the components.\n.top_n - How many variables loadings should be returned per PC\n\n\n\nValue\nA list object with several components.\n\n\nDetails\nThis is a simple wrapper around some recipes functions to perform a PCA on a given recipe. This function will output a list and return it invisible. All of the components of the analysis will be returned in a list as their own object that can be selected individually. A scree plot is also included. The items that get returned are:\n\npca_transform - This is the pca recipe.\nvariable_loadings\nvariable_variance\npca_estimates\npca_juiced_estimates\npca_baked_data\npca_variance_df\npca_rotattion_df\npca_variance_scree_plt\npca_loadings_plt\npca_loadings_plotly\npca_top_n_loadings_plt\npca_top_n_plotly\n\n\n\nWorking Example\n\nlibrary(rsample)\nlibrary(recipes)\n\nsplits &lt;- initial_split(mtcars, prop = 0.8)\n\nrec_obj &lt;- recipe(mpg ~ ., data = training(splits)) |&gt;\n  step_normalize(all_predictors())\n\npca_output &lt;- pca_your_recipe(\n  .recipe_object = rec_obj, \n  .data = mtcars, \n  .threshold = 0.75, \n  .top_n = 5\n  )\n\nNow let’s check the output:\n\npca_output\n\n$pca_transform\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_predictors()\n\n\n• Centering for: recipes::all_numeric()\n\n\n• Scaling for: recipes::all_numeric()\n\n\n• Sparse, unbalanced variable filter on: recipes::all_numeric()\n\n\n• PCA extraction with: recipes::all_numeric_predictors()\n\n\n\n$variable_loadings\n# A tibble: 100 × 4\n   terms  value component id       \n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 cyl   -0.394 PC1       pca_RSbN6\n 2 disp  -0.389 PC1       pca_RSbN6\n 3 hp    -0.356 PC1       pca_RSbN6\n 4 drat   0.321 PC1       pca_RSbN6\n 5 wt    -0.358 PC1       pca_RSbN6\n 6 qsec   0.248 PC1       pca_RSbN6\n 7 vs     0.319 PC1       pca_RSbN6\n 8 am     0.248 PC1       pca_RSbN6\n 9 gear   0.238 PC1       pca_RSbN6\n10 carb  -0.232 PC1       pca_RSbN6\n# ℹ 90 more rows\n\n$variable_variance\n# A tibble: 40 × 4\n   terms     value component id       \n   &lt;chr&gt;     &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance 6.09           1 pca_RSbN6\n 2 variance 2.42           2 pca_RSbN6\n 3 variance 0.619          3 pca_RSbN6\n 4 variance 0.231          4 pca_RSbN6\n 5 variance 0.215          5 pca_RSbN6\n 6 variance 0.171          6 pca_RSbN6\n 7 variance 0.112          7 pca_RSbN6\n 8 variance 0.0848         8 pca_RSbN6\n 9 variance 0.0409         9 pca_RSbN6\n10 variance 0.0219        10 pca_RSbN6\n# ℹ 30 more rows\n\n$pca_estimates\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\n── Training information \n\n\nTraining data contained 25 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: cyl, disp, hp, drat, wt, qsec, ... | Trained\n\n\n• Centering for: cyl, disp, hp, drat, wt, qsec, vs, am, gear, ... | Trained\n\n\n• Scaling for: cyl, disp, hp, drat, wt, qsec, vs, am, gear, ... | Trained\n\n\n• Sparse, unbalanced variable filter removed: &lt;none&gt; | Trained\n\n\n• PCA extraction with: cyl, disp, hp, drat, wt, qsec, vs, am, ... | Trained\n\n\n\n$pca_juiced_estimates\n# A tibble: 25 × 3\n       mpg    PC1      PC2\n     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 -1.67   -3.54  -0.529  \n 2  0.0945  0.633  2.03   \n 3  0.394   2.31  -1.60   \n 4  0.178   1.88  -1.88   \n 5  1.99    3.29   0.00164\n 6  1.66    3.79   0.988  \n 7 -0.670  -2.14  -0.503  \n 8 -0.953  -3.45  -0.248  \n 9  2.24    3.59   0.0209 \n10  0.161   2.47   0.534  \n# ℹ 15 more rows\n\n$pca_baked_data\n# A tibble: 32 × 3\n       mpg    PC1    PC2\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.0945  0.633  2.03 \n 2  0.0945  0.613  1.87 \n 3  0.394   2.76   0.137\n 4  0.161   0.228 -2.17 \n 5 -0.288  -2.01  -0.623\n 6 -0.388   0.191 -2.55 \n 7 -1.02   -2.82   0.438\n 8  0.660   1.91  -1.13 \n 9  0.394   2.31  -1.60 \n10 -0.205   0.622  0.125\n# ℹ 22 more rows\n\n$pca_variance_df\n# A tibble: 10 × 6\n   PC    var_explained var_pct_txt cum_var_pct cum_var_pct_txt ou_threshold\n   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;fct&gt;       \n 1 PC1         0.609   60.86%            0.609 60.86%          Under       \n 2 PC2         0.242   24.19%            0.850 85.05%          Over        \n 3 PC3         0.0619  6.19%             0.912 91.24%          Over        \n 4 PC4         0.0231  2.31%             0.935 93.55%          Over        \n 5 PC5         0.0215  2.15%             0.957 95.70%          Over        \n 6 PC6         0.0171  1.71%             0.974 97.41%          Over        \n 7 PC7         0.0112  1.12%             0.985 98.52%          Over        \n 8 PC8         0.00848 0.85%             0.994 99.37%          Over        \n 9 PC9         0.00409 0.41%             0.998 99.78%          Over        \n10 PC10        0.00219 0.22%             1     100.00%         Over        \n\n$pca_rotation_df\n# A tibble: 10 × 10\n      PC1     PC2     PC3     PC4      PC5     PC6     PC7     PC8     PC9\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.394  0.0328 -0.146   0.0804 -0.162    0.0655 -0.223  -0.0200 -0.728 \n 2 -0.389 -0.0623  0.0134 -0.0763  0.200   -0.383   0.447   0.0126 -0.369 \n 3 -0.356  0.213   0.240   0.342  -0.195   -0.145   0.270   0.614   0.282 \n 4  0.321  0.295   0.0556  0.336   0.768   -0.125  -0.0333  0.127  -0.204 \n 5 -0.358 -0.124   0.391  -0.367   0.305   -0.347  -0.105  -0.338   0.262 \n 6  0.248 -0.432   0.425  -0.317   0.00567 -0.0377 -0.272   0.568  -0.259 \n 7  0.319 -0.255   0.421   0.505  -0.328   -0.313   0.156  -0.373  -0.163 \n 8  0.248  0.443  -0.217  -0.241  -0.294   -0.693  -0.253   0.0794 -0.0246\n 9  0.238  0.449   0.346  -0.431  -0.125    0.278   0.509  -0.0781 -0.222 \n10 -0.232  0.445   0.490   0.151  -0.0541   0.189  -0.494  -0.133  -0.0201\n# ℹ 1 more variable: PC10 &lt;dbl&gt;\n\n$pca_variance_scree_plt\n\n\n\n\n\n\n\n\n\n\n$pca_loadings_plt\n\n\n\n\n\n\n\n\n\n\n$pca_loadings_plotly\n\n$pca_top_n_loadings_plt\n\n\n\n\n\n\n\n\n\n\n$pca_top_n_plotly\n\n\nPretty easy as you can see."
  },
  {
    "objectID": "posts/2024-06-24/index.html#example---histogram-facet-plot",
    "href": "posts/2024-06-24/index.html#example---histogram-facet-plot",
    "title": "An Introduction to healthyR.ai",
    "section": "Example - Histogram Facet Plot",
    "text": "Example - Histogram Facet Plot\n\nSyntax\nhai_histogram_facet_plot(\n  .data,\n  .bins = 10,\n  .scale_data = FALSE,\n  .ncol = 5,\n  .fct_reorder = FALSE,\n  .fct_rev = FALSE,\n  .fill = \"steelblue\",\n  .color = \"white\",\n  .scale = \"free\",\n  .interactive = FALSE\n)\n\n\nArguments\n\n.data - The data you want to pass to the function.\n.bins - The number of bins for the histograms.\n.scale_data - This is a boolean set to FALSE. TRUE will use hai_scale_zero_one_vec() to [0, 1] scale the data.\n.ncol - The number of columns for the facet_warp argument.\n.fct_reorder - Should the factor column be reordered? TRUE/FALSE, default of FALSE\n.fct_rev - Should the factor column be reversed? TRUE/FALSE, default of FALSE\n.fill - Default is steelblue\n.color - Default is ‘white’\n.scale - Default is ‘free’\n.interactive - Default is FALSE, TRUE will produce a plotly plot.\n\n\n\nWorking Example\n\nhai_histogram_facet_plot(mtcars, .interactive = FALSE)\n\n\n\n\n\n\n\nhai_histogram_facet_plot(mtcars, .interactive = FALSE, .scale_data = TRUE)"
  },
  {
    "objectID": "posts/2024-06-24/index.html#example---boilerplacte-funcationality",
    "href": "posts/2024-06-24/index.html#example---boilerplacte-funcationality",
    "title": "An Introduction to healthyR.ai",
    "section": "Example - Boilerplacte Funcationality",
    "text": "Example - Boilerplacte Funcationality\nNow we are going to go over some simple boilerplate funcationality. I call it boilerplate because you don’t have to change anything if you dont want to. For the boilerplate function there is a corresponding data preprocessor that will get the data into the shape it needs to be in for the algorithm. Let’s take a look.\n\nWorking Example\nFirst lets look at the data, then we will look at it after the preprocessor.\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\nrec_obj &lt;- hai_earth_data_prepper(iris, Species ~ .)\n\nrec_obj\n\nNow to run it through the boilerplate:\n\nauto_earth &lt;- hai_auto_earth(\n  .data = iris,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\"\n)\n\nNow let’s inspect the output:\n\nnames(auto_earth)\n\n[1] \"recipe_info\" \"model_info\"  \"tuned_info\" \n\n\n\n\nRecipe Information\n\nauto_earth[[\"recipe_info\"]]\n\n\n\nModel Information\n\nauto_earth[[\"model_info\"]]\n\n$model_spec\nMARS Model Specification (classification)\n\nMain Arguments:\n  num_terms = tune::tune()\n  prod_degree = tune::tune()\n  prune_method = none\n\nComputational engine: earth \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mars()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMARS Model Specification (classification)\n\nMain Arguments:\n  num_terms = tune::tune()\n  prod_degree = tune::tune()\n  prune_method = none\n\nComputational engine: earth \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mars()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nGLM (family binomial, link logit):\n           nulldev  df       dev  df   devratio     AIC iters converged\nsetosa     144.779 111   52.6908 110     0.6360   56.69    22         1\nversicolor 137.505 111  125.5536 110     0.0869  129.60     4         1\nvirginica  144.779 111   15.1575 110     0.8950   19.16     9         1\n\nEarth selected 2 of 15 terms, and 1 of 4 predictors (pmethod=\"none\") (nprune=2)\nTermination condition: Reached nk 21\nImportance: Petal.Length-unused, Sepal.Length-unused, Sepal.Width-unused, ...\nNumber of terms at each degree of interaction: 1 1 (additive model)\n\nEarth\n                  GCV       RSS       GRSq        RSq\nsetosa     0.15145196 16.066078 0.34455933 0.36796602\nversicolor 0.20252995 21.484449 0.05906052 0.09266277\nvirginica  0.04535734  4.811523 0.80370644 0.81071635\nAll        0.36072282 38.265605 0.46747354 0.48649080\n\n$was_tuned\n[1] \"tuned\"\n\n\n\n\nTuned Information\n\nauto_earth[[\"tuned_info\"]]\n\n$tuning_grid\n# A tibble: 7 × 2\n  num_terms prod_degree\n      &lt;int&gt;       &lt;int&gt;\n1         3           2\n2         4           1\n3         5           2\n4         3           1\n5         4           2\n6         2           2\n7         2           1\n\n$cv_obj\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   &lt;list&gt;          &lt;chr&gt;     \n 1 &lt;split [84/28]&gt; Resample01\n 2 &lt;split [84/28]&gt; Resample02\n 3 &lt;split [84/28]&gt; Resample03\n 4 &lt;split [84/28]&gt; Resample04\n 5 &lt;split [84/28]&gt; Resample05\n 6 &lt;split [84/28]&gt; Resample06\n 7 &lt;split [84/28]&gt; Resample07\n 8 &lt;split [84/28]&gt; Resample08\n 9 &lt;split [84/28]&gt; Resample09\n10 &lt;split [84/28]&gt; Resample10\n# ℹ 15 more rows\n\n$tuned_results\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics          .notes          \n   &lt;list&gt;          &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [84/28]&gt; Resample01 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 2 &lt;split [84/28]&gt; Resample02 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 3 &lt;split [84/28]&gt; Resample03 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 4 &lt;split [84/28]&gt; Resample04 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 5 &lt;split [84/28]&gt; Resample05 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 6 &lt;split [84/28]&gt; Resample06 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 7 &lt;split [84/28]&gt; Resample07 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 8 &lt;split [84/28]&gt; Resample08 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 9 &lt;split [84/28]&gt; Resample09 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n10 &lt;split [84/28]&gt; Resample10 &lt;tibble [77 × 6]&gt; &lt;tibble [5 × 3]&gt;\n# ℹ 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x5: While computing multiclass `precision()`, some levels had no pred...\n  - Warning(s) x6: While computing multiclass `precision()`, some levels had no pred...\n  - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...\n  - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...\n  - Warning(s) x2: While computing multiclass `precision()`, some levels had no pred...\n  - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...\n  - Warning(s) x5: While computing multiclass `precision()`, some levels had no pred...\n  - Warning(s) x4: While computing multiclass `precision()`, some levels had no pred...\n  - Warning(s) x48: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie...\n  - Warning(s) x2: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie...\n  - Warning(s) x49: glm.fit: fitted probabilities numerically 0 or 1 occurred, glm.fi...\n  - Warning(s) x1: glm.fit: fitted probabilities numerically 0 or 1 occurred, glm.fi...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n$grid_size\n[1] 10\n\n$best_metric\n[1] \"f_meas\"\n\n$best_result_set\n# A tibble: 1 × 8\n  num_terms prod_degree .metric .estimator  mean     n std_err .config          \n      &lt;int&gt;       &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1         2           1 f_meas  macro      0.124    25  0.0154 Preprocessor1_Mo…\n\n$tuning_grid_plot\n\n\n\n\n\n\n\n\n\n\n$plotly_grid_plot"
  },
  {
    "objectID": "posts/2024-06-24/index.html#classification",
    "href": "posts/2024-06-24/index.html#classification",
    "title": "An Introduction to healthyR.ai",
    "section": "Classification",
    "text": "Classification\n\nhai_default_classification_metric_set()\n\nA metric set, consisting of:\n- `sensitivity()`, a class metric  | direction: maximize\n- `specificity()`, a class metric  | direction: maximize\n- `recall()`, a class metric       | direction: maximize\n- `precision()`, a class metric    | direction: maximize\n- `mcc()`, a class metric          | direction: maximize\n- `accuracy()`, a class metric     | direction: maximize\n- `f_meas()`, a class metric       | direction: maximize\n- `kap()`, a class metric          | direction: maximize\n- `ppv()`, a class metric          | direction: maximize\n- `npv()`, a class metric          | direction: maximize\n- `bal_accuracy()`, a class metric | direction: maximize"
  },
  {
    "objectID": "posts/2024-06-24/index.html#regression",
    "href": "posts/2024-06-24/index.html#regression",
    "title": "An Introduction to healthyR.ai",
    "section": "Regression",
    "text": "Regression\n\nhai_default_regression_metric_set()\n\nA metric set, consisting of:\n- `mae()`, a numeric metric   | direction: minimize\n- `mape()`, a numeric metric  | direction: minimize\n- `mase()`, a numeric metric  | direction: minimize\n- `smape()`, a numeric metric | direction: minimize\n- `rmse()`, a numeric metric  | direction: minimize\n- `rsq()`, a numeric metric   | direction: maximize\n\n\nHere is a list of the items currently on it as of writing this article:\n\nPlotting Functions - Functions for plotting.\nClustering Functions - Functions for clustering and analysis.\nBoiler Plate Functions - Functions for automatic recipes, workflows, and tuned models.\nDimensionality Reduction - Functions for dimension reduction.\nData Wrangling - Functions for data wrangling.\nData Preprocessors - Functions for data preprocessing.\nRecipe Steps - Functions to add recipe steps.\nTable Functions - Functions that return tibbles.\nVectorized Functions - Vector functions.\nAugmenting Functions - Functions for data augmentation.\nMiscellaneous Functions - Miscellaneous utility functions.\nMetric Sets - Metric sets for evaluation.\n\nFor more detailed information, you can visit the healthyR.ai function reference page."
  },
  {
    "objectID": "posts/2024-06-25/index.html",
    "href": "posts/2024-06-25/index.html",
    "title": "How to Extract Strings Between Specific Characters in R",
    "section": "",
    "text": "Hello, R enthusiasts! Today, we’re jumping into a common text processing task: extracting strings between specific characters. This is a great skill for data cleaning and manipulation, especially when working with raw text data. I’m going to show you how to achieve this using base R, the stringr package, and the stringi package. Let’s go!\n\n\nBase R provides several ways to extract substrings, including sub and gregexpr. Here, we’ll use sub and gsub for some examples.\n\n\nSuppose you have a string and you want to extract the text between two characters, say [ and ].\n\n# Sample string\ntext &lt;- \"Extract this [text] from the string.\"\n\n# Using sub to extract text between square brackets\nresult &lt;- sub(\".*\\\\[(.*?)\\\\].*\", \"\\\\1\", text)\n\n# Print the result\nprint(result)\n\n[1] \"text\"\n\n\n\n\n\nNow, let’s extract text between parentheses ( and ).\n\n# Example string\ntext2 &lt;- \"This is a sample (extract this part) string.\"\n\n# Extract string between parentheses using base R\nextracted_base &lt;- gsub(\".*\\\\((.*)\\\\).*\", \"\\\\1\", text2)\nprint(extracted_base)\n\n[1] \"extract this part\"\n\n\nIn these examples, sub and gsub use regular expressions to find the text between the specified characters and replace the entire string with the extracted part. The pattern .*\\\\[(.*?)\\\\].* and .*\\\\((.*)\\\\).* break down as follows: - .* matches any character (except for line terminators) zero or more times. - \\\\[ matches the literal [ and \\\\( matches the literal (. - (.*?) and (.*) are non-greedy matches for any character (.) zero or more times. - \\\\] matches the literal ] and \\\\) matches the literal ). - \\\\1 in the replacement string refers to the first capture group, i.e., the text between [ ] and ( ).\n\n\n\n\nThe stringr package, part of the tidyverse, makes string manipulation more straightforward with consistent functions.\n\n\n\n# Load the stringr package\nlibrary(stringr)\n\n# Using str_extract to extract text between square brackets\nresult_str_extract &lt;- str_extract(text, \"(?&lt;=\\\\[).*?(?=\\\\])\")\n\n# Print the result\nprint(result_str_extract)\n\n[1] \"text\"\n\n\n\n\n\n\n# Example using stringr\nextracted_str &lt;- str_extract(text2, \"\\\\(.*?\\\\)\")\nextracted_str &lt;- str_sub(extracted_str, 2, -2)\nprint(extracted_str)\n\n[1] \"extract this part\"\n\n\nThe str_extract function extracts the first substring matching a regex pattern. Here, (?&lt;=\\\\[).*?(?=\\\\]) and \\\\(.*?\\\\) use lookbehind (?&lt;=\\\\[) and lookahead (?=\\\\]) assertions to match text between [ and ], and simple matching for text between ( and ). str_sub is then used to remove the enclosing parentheses.\n\n\n\n\nThe stringi package provides robust and efficient tools for string manipulation.\n\n\n\n# Load the stringi package\nlibrary(stringi)\n\n# Using stri_extract to extract text between square brackets\nresult_stri_extract &lt;- stri_extract(text, regex = \"(?&lt;=\\\\[).*?(?=\\\\])\")\n\n# Print the result\nprint(result_stri_extract)\n\n[1] \"text\"\n\n\n\n\n\n\n# Example using stringi\nextracted_stri &lt;- stringi::stri_extract_first_regex(text2, \"\\\\(.*?\\\\)\")\nextracted_stri &lt;- stringi::stri_sub(extracted_stri, 2, -2)\nprint(extracted_stri)\n\n[1] \"extract this part\"\n\n\nThe stri_extract function from stringi works similarly to str_extract, utilizing regex patterns for text extraction. It’s highly optimized for performance, especially with large datasets. stri_sub is used to remove the enclosing parentheses.\n\n\n\n\nExperimenting with these functions and patterns on your own datasets will help you understand their nuances. Here are a few additional exercises to solidify your understanding:\n\nExtract text between parentheses ( and ).\nExtract text between the first and last occurrences of a specific character in a string.\nExtract all occurrences of text between two characters in a string.\n\nFeel free to use the examples provided as a template for your own tasks.\nHappy coding!\n\n\n\nFor more complex scenarios, you might need to combine different methods. Here’s a quick example of how you can handle multiple extractions.\n\n# Sample string with multiple patterns\ntext_multiple &lt;- \"Here is [text1] and here is (text2).\"\n\n# Using gregexpr and regmatches to extract all matches\nmatches &lt;- regmatches(\n  text_multiple, \n  gregexpr(\"(?&lt;=\\\\[).*?(?=\\\\])|(?&lt;=\\\\().*?(?=\\\\))\", \n           text_multiple, \n           perl = TRUE)\n  )\n\n# Print the matches\nprint(unlist(matches))\n\n[1] \"text1\" \"text2\"\n\n\nThis example uses gregexpr to find all matches and regmatches to extract them.\n\nUntil next time, keep exploring and enjoying the power of R!"
  },
  {
    "objectID": "posts/2024-06-25/index.html#extracting-strings-using-base-r",
    "href": "posts/2024-06-25/index.html#extracting-strings-using-base-r",
    "title": "How to Extract Strings Between Specific Characters in R",
    "section": "",
    "text": "Base R provides several ways to extract substrings, including sub and gregexpr. Here, we’ll use sub and gsub for some examples.\n\n\nSuppose you have a string and you want to extract the text between two characters, say [ and ].\n\n# Sample string\ntext &lt;- \"Extract this [text] from the string.\"\n\n# Using sub to extract text between square brackets\nresult &lt;- sub(\".*\\\\[(.*?)\\\\].*\", \"\\\\1\", text)\n\n# Print the result\nprint(result)\n\n[1] \"text\"\n\n\n\n\n\nNow, let’s extract text between parentheses ( and ).\n\n# Example string\ntext2 &lt;- \"This is a sample (extract this part) string.\"\n\n# Extract string between parentheses using base R\nextracted_base &lt;- gsub(\".*\\\\((.*)\\\\).*\", \"\\\\1\", text2)\nprint(extracted_base)\n\n[1] \"extract this part\"\n\n\nIn these examples, sub and gsub use regular expressions to find the text between the specified characters and replace the entire string with the extracted part. The pattern .*\\\\[(.*?)\\\\].* and .*\\\\((.*)\\\\).* break down as follows: - .* matches any character (except for line terminators) zero or more times. - \\\\[ matches the literal [ and \\\\( matches the literal (. - (.*?) and (.*) are non-greedy matches for any character (.) zero or more times. - \\\\] matches the literal ] and \\\\) matches the literal ). - \\\\1 in the replacement string refers to the first capture group, i.e., the text between [ ] and ( )."
  },
  {
    "objectID": "posts/2024-06-25/index.html#extracting-strings-using-stringr",
    "href": "posts/2024-06-25/index.html#extracting-strings-using-stringr",
    "title": "How to Extract Strings Between Specific Characters in R",
    "section": "",
    "text": "The stringr package, part of the tidyverse, makes string manipulation more straightforward with consistent functions.\n\n\n\n# Load the stringr package\nlibrary(stringr)\n\n# Using str_extract to extract text between square brackets\nresult_str_extract &lt;- str_extract(text, \"(?&lt;=\\\\[).*?(?=\\\\])\")\n\n# Print the result\nprint(result_str_extract)\n\n[1] \"text\"\n\n\n\n\n\n\n# Example using stringr\nextracted_str &lt;- str_extract(text2, \"\\\\(.*?\\\\)\")\nextracted_str &lt;- str_sub(extracted_str, 2, -2)\nprint(extracted_str)\n\n[1] \"extract this part\"\n\n\nThe str_extract function extracts the first substring matching a regex pattern. Here, (?&lt;=\\\\[).*?(?=\\\\]) and \\\\(.*?\\\\) use lookbehind (?&lt;=\\\\[) and lookahead (?=\\\\]) assertions to match text between [ and ], and simple matching for text between ( and ). str_sub is then used to remove the enclosing parentheses."
  },
  {
    "objectID": "posts/2024-06-25/index.html#extracting-strings-using-stringi",
    "href": "posts/2024-06-25/index.html#extracting-strings-using-stringi",
    "title": "How to Extract Strings Between Specific Characters in R",
    "section": "",
    "text": "The stringi package provides robust and efficient tools for string manipulation.\n\n\n\n# Load the stringi package\nlibrary(stringi)\n\n# Using stri_extract to extract text between square brackets\nresult_stri_extract &lt;- stri_extract(text, regex = \"(?&lt;=\\\\[).*?(?=\\\\])\")\n\n# Print the result\nprint(result_stri_extract)\n\n[1] \"text\"\n\n\n\n\n\n\n# Example using stringi\nextracted_stri &lt;- stringi::stri_extract_first_regex(text2, \"\\\\(.*?\\\\)\")\nextracted_stri &lt;- stringi::stri_sub(extracted_stri, 2, -2)\nprint(extracted_stri)\n\n[1] \"extract this part\"\n\n\nThe stri_extract function from stringi works similarly to str_extract, utilizing regex patterns for text extraction. It’s highly optimized for performance, especially with large datasets. stri_sub is used to remove the enclosing parentheses."
  },
  {
    "objectID": "posts/2024-06-25/index.html#encouragement-to-try",
    "href": "posts/2024-06-25/index.html#encouragement-to-try",
    "title": "How to Extract Strings Between Specific Characters in R",
    "section": "",
    "text": "Experimenting with these functions and patterns on your own datasets will help you understand their nuances. Here are a few additional exercises to solidify your understanding: 1. Extract text between parentheses ( and ). 2. Extract text between the first and last occurrences of a specific character in a string. 3. Extract all occurrences of text between two characters in a string.\nFeel free to use the examples provided as a template for your own tasks. Happy coding!\n\n\n\nFor more complex scenarios, you might need to combine different methods. Here’s a quick example of how you can handle multiple extractions.\n\n# Sample string with multiple patterns\ntext_multiple &lt;- \"Here is [text1] and here is [text2].\"\n\n# Using gregexpr and regmatches to extract all matches\nmatches &lt;- regmatches(text_multiple, gregexpr(\"(?&lt;=\\\\[).*?(?=\\\\])\", text_multiple, perl = TRUE))\n\n# Print the matches\nprint(unlist(matches))\n\n[1] \"text1\" \"text2\"\n\n\nThis example uses gregexpr to find all matches and regmatches to extract them.\n\nLet me know if you have any questions or run into any issues. I’m here to help you get the most out of your R programming experience. Until next time, keep exploring and enjoying the power of R!"
  },
  {
    "objectID": "posts/2024-06-25/index.html#your-turn",
    "href": "posts/2024-06-25/index.html#your-turn",
    "title": "How to Extract Strings Between Specific Characters in R",
    "section": "",
    "text": "Experimenting with these functions and patterns on your own datasets will help you understand their nuances. Here are a few additional exercises to solidify your understanding:\n\nExtract text between parentheses ( and ).\nExtract text between the first and last occurrences of a specific character in a string.\nExtract all occurrences of text between two characters in a string.\n\nFeel free to use the examples provided as a template for your own tasks.\nHappy coding!\n\n\n\nFor more complex scenarios, you might need to combine different methods. Here’s a quick example of how you can handle multiple extractions.\n\n# Sample string with multiple patterns\ntext_multiple &lt;- \"Here is [text1] and here is (text2).\"\n\n# Using gregexpr and regmatches to extract all matches\nmatches &lt;- regmatches(\n  text_multiple, \n  gregexpr(\"(?&lt;=\\\\[).*?(?=\\\\])|(?&lt;=\\\\().*?(?=\\\\))\", \n           text_multiple, \n           perl = TRUE)\n  )\n\n# Print the matches\nprint(unlist(matches))\n\n[1] \"text1\" \"text2\"\n\n\nThis example uses gregexpr to find all matches and regmatches to extract them.\n\nUntil next time, keep exploring and enjoying the power of R!"
  },
  {
    "objectID": "posts/2024-06-26/index.html",
    "href": "posts/2024-06-26/index.html",
    "title": "How to Run a Macro When a Cell Value Changes in VBA",
    "section": "",
    "text": "Hello, everyone! Today, we’ll be diving into a practical example of how to run a macro when a cell value changes in VBA. This is particularly useful when you need to trigger certain actions based on user input or dynamic data changes in your Excel sheets. Let’s get started!"
  },
  {
    "objectID": "posts/2024-06-26/index.html#step-by-step-guide",
    "href": "posts/2024-06-26/index.html#step-by-step-guide",
    "title": "How to Run a Macro When a Cell Value Changes in VBA",
    "section": "Step-by-Step Guide",
    "text": "Step-by-Step Guide\n\nOpen the VBA Editor:\n\nPress ALT + F11 to open the VBA editor.\n\nInsert the Code:\n\nIn the VBA editor, find the sheet where you want to apply the change event. For example, Sheet1.\nDouble-click on Sheet1 to open its code window.\nInsert the following code:\n\n\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    ' Check if the changed cell is A1\n    If Not Intersect(Target, Me.Range(\"A1\")) Is Nothing Then\n        ' Run your macro here\n        Call MyMacro\n    End If\nEnd Sub"
  },
  {
    "objectID": "posts/2024-06-26/index.html#explanation",
    "href": "posts/2024-06-26/index.html#explanation",
    "title": "How to Run a Macro When a Cell Value Changes in VBA",
    "section": "Explanation",
    "text": "Explanation\n\nWorksheet_Change Event: This event gets triggered whenever any cell value in Sheet1 changes.\nIntersect Function: We use the Intersect function to check if the changed cell (Target) overlaps with cell A1 (Me.Range(\"A1\")). If there is an intersection (i.e., the changed cell is A1), the condition returns True.\nCall MyMacro: When the condition is True, we call another macro named MyMacro. This is where you define what actions you want to perform when cell A1 changes."
  },
  {
    "objectID": "posts/2024-06-26/index.html#defining-the-macro",
    "href": "posts/2024-06-26/index.html#defining-the-macro",
    "title": "How to Run a Macro When a Cell Value Changes in VBA",
    "section": "Defining the Macro",
    "text": "Defining the Macro\nNext, let’s define the MyMacro that gets called when cell A1 changes. For simplicity, we’ll make it display a message box.\nSub MyMacro()\n    MsgBox \"Cell A1 has changed!\"\nEnd Sub"
  },
  {
    "objectID": "posts/2024-06-26/index.html#putting-it-all-together",
    "href": "posts/2024-06-26/index.html#putting-it-all-together",
    "title": "How to Run a Macro When a Cell Value Changes in VBA",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nHere’s the complete code for Sheet1:\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    ' Check if the changed cell is A1\n    If Not Intersect(Target, Me.Range(\"A1\")) Is Nothing Then\n        ' Run your macro here\n        Call MyMacro\n    End If\nEnd Sub\n\nSub MyMacro()\n    MsgBox \"Cell A1 has changed!\"\nEnd Sub"
  },
  {
    "objectID": "posts/2024-06-26/index.html#testing-the-macro",
    "href": "posts/2024-06-26/index.html#testing-the-macro",
    "title": "How to Run a Macro When a Cell Value Changes in VBA",
    "section": "Testing the Macro",
    "text": "Testing the Macro\nTo test the macro:\n\nClose the VBA editor and go back to Excel.\nChange the value in cell A1.\nYou should see a message box saying, “Cell A1 has changed!”"
  },
  {
    "objectID": "posts/2024-06-27/index.html",
    "href": "posts/2024-06-27/index.html",
    "title": "Exploring Random Walks and Brownian Motions with healthyR.ts",
    "section": "",
    "text": "In the world of time series analysis, Random Walks, Brownian Motion, and Geometric Brownian Motion are fundamental concepts used in various fields, including finance, physics, and biology. Today, we’ll explore these concepts using functions from the healthyR.ts package.\n\n\nA Random Walk is a path that consists of a series of random steps. It’s a simple but powerful concept used to model seemingly unpredictable paths, such as stock prices or animal movements.\nLet’s generate and plot some Random Walks using the ts_random_walk() function from healthyR.ts.\n\n\nts_random_walk(\n  .mean = 0,\n  .sd = 0.1,\n  .num_walks = 100,\n  .periods = 100,\n  .initial_value = 1000\n)\n\n.mean: The desired mean of the random walks.\n.sd: The standard deviation of the random walks.\n.num_walks: The number of random walks you want to generate.\n.periods: The length of the random walk(s) you want to generate.\n.initial_value: The initial value where the random walks should start.\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nrandom_walk_data &lt;- ts_random_walk(\n  .mean = 0, \n  .sd = 0.1, \n  .num_walks = 10, \n  .periods = 100, \n  .initial_value = 1000\n  )\nhead(random_walk_data)\n\n# A tibble: 6 × 4\n    run     x       y cum_y\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1     1  0.0725 1072.\n2     1     2  0.182  1267.\n3     1     3  0.110  1407.\n4     1     4  0.0275 1445.\n5     1     5 -0.0546 1367.\n6     1     6  0.0712 1464.\n\nrandom_walk_plot &lt;- random_walk_data |&gt;\n  ggplot(\n    mapping = aes(\n      x = x,\n      y = cum_y,\n      color = factor(run),\n      group = factor(run)\n    )\n  ) +\n  geom_line(alpha = 0.8) +\n  ts_random_walk_ggplot_layers(random_walk_data)\nprint(random_walk_plot)\n\n\n\n\n\n\n\n\nThis code generates 10 random walks over 100 periods, starting from an initial value of 1000. The resulting plot visualizes the paths of these random walks, each represented by a different color.\n\n\n\n\nBrownian Motion, also known as Wiener Process, is a continuous-time stochastic process that is often used to model random movements in physics and finance.\n\n\nts_brownian_motion(\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = 1,\n  .initial_value = 0,\n  .return_tibble = TRUE\n)\n\n.time: Total time of the simulation.\n.num_sims: Total number of simulations.\n.delta_time: Time step size.\n.initial_value: Initial value of the simulation.\n.return_tibble: Return a tibble (TRUE) or a matrix (FALSE).\n\n\n\n\n\nbrownian_data &lt;- ts_brownian_motion(\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = 1,\n  .initial_value = 0,\n  .return_tibble = TRUE\n)\nhead(brownian_data)\n\n# A tibble: 6 × 3\n  sim_number       t     y\n  &lt;fct&gt;        &lt;int&gt; &lt;dbl&gt;\n1 sim_number 1     1     0\n2 sim_number 2     1     0\n3 sim_number 3     1     0\n4 sim_number 4     1     0\n5 sim_number 5     1     0\n6 sim_number 6     1     0\n\nbrownian_plot &lt;- ts_brownian_motion_plot(\n  .data = brownian_data,\n  .date_col = t,\n  .value_col = y,\n  .interactive = TRUE\n)\nbrownian_plot\n\n\n\n\n\nThis code simulates 10 paths of Brownian Motion over 100 time units, starting from an initial value of 0. The ts_brownian_motion_plot() function creates a static plot of these simulations.\n\n\n\n\nGeometric Brownian Motion (GBM) is a variation of Brownian Motion where the logarithm of the variable follows a Brownian Motion. It is commonly used to model stock prices in the Black-Scholes option pricing model.\n\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\n.num_sims: Total number of simulations.\n.time: Total time of the simulation.\n.mean: Expected return.\n.sigma: Volatility.\n.initial_value: Initial value of the simulation.\n.delta_time: Time step size.\n.return_tibble: Return a tibble (TRUE) or a matrix (FALSE).\n\n\n\n\n\ngbm_data &lt;- ts_geometric_brownian_motion(\n  .num_sims = 10, \n  .time = 25, \n  .mean = 0.05, \n  .sigma = 0.2, \n  .initial_value = 100\n  )\nhead(gbm_data)\n\n# A tibble: 6 × 3\n  sim_number       t     y\n  &lt;fct&gt;        &lt;int&gt; &lt;dbl&gt;\n1 sim_number 1     1   100\n2 sim_number 2     1   100\n3 sim_number 3     1   100\n4 sim_number 4     1   100\n5 sim_number 5     1   100\n6 sim_number 6     1   100\n\ngbm_plot &lt;- ts_brownian_motion_plot(\n  .data = gbm_data,\n  .date_col = t,\n  .value_col = y,\n  .interactive = TRUE\n)\ngbm_plot\n\n\n\n\n\nThis code simulates 10 paths of Geometric Brownian Motion over 25 time units with an expected return of 0.05 and volatility of 0.2. The ts_brownian_motion_plot() function again helps in visualizing the simulations."
  },
  {
    "objectID": "posts/2024-06-27/index.html#random-walks",
    "href": "posts/2024-06-27/index.html#random-walks",
    "title": "Exploring Random Walks and Brownian Motions with healthyR.ts",
    "section": "",
    "text": "A Random Walk is a path that consists of a series of random steps. It’s a simple but powerful concept used to model seemingly unpredictable paths, such as stock prices or animal movements.\nLet’s generate and plot some Random Walks using the ts_random_walk() function from healthyR.ts.\n\n\nts_random_walk(\n  .mean = 0,\n  .sd = 0.1,\n  .num_walks = 100,\n  .periods = 100,\n  .initial_value = 1000\n)\n\n.mean: The desired mean of the random walks.\n.sd: The standard deviation of the random walks.\n.num_walks: The number of random walks you want to generate.\n.periods: The length of the random walk(s) you want to generate.\n.initial_value: The initial value where the random walks should start.\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nrandom_walk_data &lt;- ts_random_walk(\n  .mean = 0, \n  .sd = 0.1, \n  .num_walks = 10, \n  .periods = 100, \n  .initial_value = 1000\n  )\nhead(random_walk_data)\n\n# A tibble: 6 × 4\n    run     x       y cum_y\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1     1  0.0725 1072.\n2     1     2  0.182  1267.\n3     1     3  0.110  1407.\n4     1     4  0.0275 1445.\n5     1     5 -0.0546 1367.\n6     1     6  0.0712 1464.\n\nrandom_walk_plot &lt;- random_walk_data |&gt;\n  ggplot(\n    mapping = aes(\n      x = x,\n      y = cum_y,\n      color = factor(run),\n      group = factor(run)\n    )\n  ) +\n  geom_line(alpha = 0.8) +\n  ts_random_walk_ggplot_layers(random_walk_data)\nprint(random_walk_plot)\n\n\n\n\n\n\n\n\nThis code generates 10 random walks over 100 periods, starting from an initial value of 1000. The resulting plot visualizes the paths of these random walks, each represented by a different color."
  },
  {
    "objectID": "posts/2024-06-27/index.html#brownian-motion",
    "href": "posts/2024-06-27/index.html#brownian-motion",
    "title": "Exploring Random Walks and Brownian Motions with healthyR.ts",
    "section": "",
    "text": "Brownian Motion, also known as Wiener Process, is a continuous-time stochastic process that is often used to model random movements in physics and finance.\n\n\nts_brownian_motion(\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = 1,\n  .initial_value = 0,\n  .return_tibble = TRUE\n)\n\n.time: Total time of the simulation.\n.num_sims: Total number of simulations.\n.delta_time: Time step size.\n.initial_value: Initial value of the simulation.\n.return_tibble: Return a tibble (TRUE) or a matrix (FALSE).\n\n\n\n\n\nbrownian_data &lt;- ts_brownian_motion(\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = 1,\n  .initial_value = 0,\n  .return_tibble = TRUE\n)\nhead(brownian_data)\n\n# A tibble: 6 × 3\n  sim_number       t     y\n  &lt;fct&gt;        &lt;int&gt; &lt;dbl&gt;\n1 sim_number 1     1     0\n2 sim_number 2     1     0\n3 sim_number 3     1     0\n4 sim_number 4     1     0\n5 sim_number 5     1     0\n6 sim_number 6     1     0\n\nbrownian_plot &lt;- ts_brownian_motion_plot(\n  .data = brownian_data,\n  .date_col = t,\n  .value_col = y,\n  .interactive = TRUE\n)\nbrownian_plot\n\n\n\n\n\nThis code simulates 10 paths of Brownian Motion over 100 time units, starting from an initial value of 0. The ts_brownian_motion_plot() function creates a static plot of these simulations."
  },
  {
    "objectID": "posts/2024-06-27/index.html#geometric-brownian-motion",
    "href": "posts/2024-06-27/index.html#geometric-brownian-motion",
    "title": "Exploring Random Walks and Brownian Motions with healthyR.ts",
    "section": "",
    "text": "Geometric Brownian Motion (GBM) is a variation of Brownian Motion where the logarithm of the variable follows a Brownian Motion. It is commonly used to model stock prices in the Black-Scholes option pricing model.\n\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\n.num_sims: Total number of simulations.\n.time: Total time of the simulation.\n.mean: Expected return.\n.sigma: Volatility.\n.initial_value: Initial value of the simulation.\n.delta_time: Time step size.\n.return_tibble: Return a tibble (TRUE) or a matrix (FALSE).\n\n\n\n\n\ngbm_data &lt;- ts_geometric_brownian_motion(\n  .num_sims = 10, \n  .time = 25, \n  .mean = 0.05, \n  .sigma = 0.2, \n  .initial_value = 100\n  )\nhead(gbm_data)\n\n# A tibble: 6 × 3\n  sim_number       t     y\n  &lt;fct&gt;        &lt;int&gt; &lt;dbl&gt;\n1 sim_number 1     1   100\n2 sim_number 2     1   100\n3 sim_number 3     1   100\n4 sim_number 4     1   100\n5 sim_number 5     1   100\n6 sim_number 6     1   100\n\ngbm_plot &lt;- ts_brownian_motion_plot(\n  .data = gbm_data,\n  .date_col = t,\n  .value_col = y,\n  .interactive = TRUE\n)\ngbm_plot\n\n\n\n\n\nThis code simulates 10 paths of Geometric Brownian Motion over 25 time units with an expected return of 0.05 and volatility of 0.2. The ts_brownian_motion_plot() function again helps in visualizing the simulations."
  },
  {
    "objectID": "posts/2024-06-28/index.html",
    "href": "posts/2024-06-28/index.html",
    "title": "How to Execute VBA Code in Excel via R using RDCOMClient",
    "section": "",
    "text": "Hey everyone,\nToday, I want to share a neat way to bridge the gap between R and Excel using VBA. Specifically, we’ll look at how to run VBA code in Excel directly from R. This can be incredibly useful if you’re looking to automate repetitive tasks or leverage the power of VBA while working within the R environment.\nWe’ll use the RDCOMClient library, which allows R to control COM (Component Object Model) objects, such as an Excel application. If you’ve ever found yourself toggling between R and Excel, this method will streamline your workflow significantly.\n\n\nWe’ll write a VBA macro that populates cells A1:A10 with random numbers and then run this macro from R.\n\n\n\n\n\nFirst, you’ll need to install the RDCOMClient package. It’s not available on CRAN, so you have to install it from the omegahat repository.\ninstall.packages(\"RDCOMClient\", repos = \"http://www.omegahat.net/R\")\nlibrary(RDCOMClient)\n\n\n\nOpen Excel and press ALT + F11 to open the VBA editor. Insert a new module and add the following VBA code:\nSub FillRandomNumbers()\n    Dim i As Integer\n    For i = 1 To 10\n        Cells(i, 1).Value = Rnd()\n    Next i\nEnd Sub\nThis macro fills cells A1 to A10 with random numbers.\n\n\n\nNow, let’s write the R code to open Excel, run the macro, and then close Excel.\n\n# Load the RDCOMClient library\nlibrary(RDCOMClient)\n\n# Create a new instance of Excel application\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# Make Excel visible (optional)\nexcel_app[[\"Visible\"]] &lt;- TRUE\n\n# Add a new workbook\nwb_path &lt;- \"C:\\\\Users\\\\steve\\\\Documents\\\\GitHub\\\\steveondata\\\\posts\\\\2024-06-28\\\\vba_rand_from_r.xlsm\"\nworkbook &lt;- excel_app[[\"Workbooks\"]]$Open(wb_path)\n\n# Reference the first sheet\nsheet &lt;- workbook$Worksheets(1)\n\n# Run the macro\nexcel_app$Run(\"FillRandomNumbers\")\n\nNULL\n\n# Save the workbook (optional)\nworkbook$SaveAs(\"C:\\\\Users\\\\steve\\\\Documents\\\\GitHub\\\\steveondata\\\\posts\\\\2024-06-28\\\\random_numbers.xlsm\")\n\n[1] TRUE\n\n# Close Excel\nexcel_app$Quit()\n\nNULL\n\n# Release the COM object\nrm(excel_app)\nrm(sheet)\nrm(workbook)\ngc()\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  666819 35.7    1477710   79  1122664 60.0\nVcells 1220653  9.4    8388608   64  1770896 13.6\n\n\n\n\n\n\nInitialize Excel Application: COMCreate(\"Excel.Application\") starts a new instance of Excel.\nMake Excel Visible: This step is optional but useful for debugging.\nAdd Workbook and Reference Worksheet: We create a new workbook and reference the first sheet.\nRun the Macro: excel_app$Run(\"FillRandomNumbers\") executes the macro.\nSave Workbook: Optionally save the workbook with the generated random numbers.\nClose and Clean Up: Close Excel and clean up the COM object to free up resources."
  },
  {
    "objectID": "posts/2024-06-28/index.html#what-well-do",
    "href": "posts/2024-06-28/index.html#what-well-do",
    "title": "How to Execute VBA Code in Excel via R using RDCOMClient",
    "section": "",
    "text": "We’ll write a VBA macro that populates cells A1:A10 with random numbers and then run this macro from R."
  },
  {
    "objectID": "posts/2024-06-28/index.html#step-by-step-guide",
    "href": "posts/2024-06-28/index.html#step-by-step-guide",
    "title": "How to Execute VBA Code in Excel via R using RDCOMClient",
    "section": "",
    "text": "First, you’ll need to install the RDCOMClient package. It’s not available on CRAN, so you have to install it from the omegahat repository.\ninstall.packages(\"RDCOMClient\", repos = \"http://www.omegahat.net/R\")\nlibrary(RDCOMClient)\n\n\n\nOpen Excel and press ALT + F11 to open the VBA editor. Insert a new module and add the following VBA code:\nSub FillRandomNumbers()\n    Dim i As Integer\n    For i = 1 To 10\n        Cells(i, 1).Value = Rnd()\n    Next i\nEnd Sub\nThis macro fills cells A1 to A10 with random numbers.\n\n\n\nNow, let’s write the R code to open Excel, run the macro, and then close Excel.\n\n# Load the RDCOMClient library\nlibrary(RDCOMClient)\n\n# Create a new instance of Excel application\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# Make Excel visible (optional)\nexcel_app[[\"Visible\"]] &lt;- TRUE\n\n# Add a new workbook\nwb_path &lt;- \"C:\\\\Users\\\\steve\\\\Documents\\\\GitHub\\\\steveondata\\\\posts\\\\2024-06-28\\\\vba_rand_from_r.xlsm\"\nworkbook &lt;- excel_app[[\"Workbooks\"]]$Open(wb_path)\n\n# Reference the first sheet\nsheet &lt;- workbook$Worksheets(1)\n\n# Run the macro\nexcel_app$Run(\"FillRandomNumbers\")\n\nNULL\n\n# Save the workbook (optional)\nworkbook$SaveAs(\"C:\\\\Users\\\\steve\\\\Documents\\\\GitHub\\\\steveondata\\\\posts\\\\2024-06-28\\\\random_numbers.xlsm\")\n\n[1] TRUE\n\n# Close Excel\nexcel_app$Quit()\n\nNULL\n\n# Release the COM object\nrm(excel_app)\nrm(sheet)\nrm(workbook)\ngc()\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  666819 35.7    1477710   79  1122664 60.0\nVcells 1220653  9.4    8388608   64  1770896 13.6\n\n\n\n\n\n\nInitialize Excel Application: COMCreate(\"Excel.Application\") starts a new instance of Excel.\nMake Excel Visible: This step is optional but useful for debugging.\nAdd Workbook and Reference Worksheet: We create a new workbook and reference the first sheet.\nRun the Macro: excel_app$Run(\"FillRandomNumbers\") executes the macro.\nSave Workbook: Optionally save the workbook with the generated random numbers.\nClose and Clean Up: Close Excel and clean up the COM object to free up resources."
  },
  {
    "objectID": "posts/2024-07-01/index.html",
    "href": "posts/2024-07-01/index.html",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "Hello, fellow data enthusiasts! Today, I’m excited to share insights into the { healthyR.data } package, an essential tool in the healthyverse that will streamline your data exploration and testing processes. Whether you’re a seasoned data scientist or just starting out in data analytics, this package is designed to be valuable for everyone.\n\n\nThe { healthyR.data } package serves two primary purposes: providing a robust administrative data set for testing functions in the { healthyR } package and facilitating the download of important data from the Centers for Medicare and Medicaid Services (CMS), a division of the Department of Health and Human Services (HHS). This package is your resource for comprehensive data that can enhance your analytical capabilities and simplify your testing procedures.\n\n\n\n\n\nOne of the main features of { healthyR.data } is its extensive administrative data set. This data set is carefully curated to include a variety of scenarios and variables commonly found in healthcare data analysis. This makes it an excellent tool for testing the functions of the { healthyR } package, ensuring your analytical methods are reliable and effective.\n\n\n\nIn addition to its built-in data set, { healthyR.data } allows you to download data directly from CMS. This feature is especially useful for healthcare analysts who need up-to-date and detailed data for their analyses. With { healthyR.data }, you can easily access a wealth of information to drive insightful analysis.\n\n\n\n\nTo start using { healthyR.data }, you can install it from CRAN with the following command:\ninstall.packages(\"healthyR.data\")\nOnce installed, load the package with:\nlibrary(healthyR.data)\n\n\n\nLoad the libraries:\n\nlibrary(healthyR.data)\nlibrary(tidyverse)\nlibrary(DT)\n\n\n# Functions and their arguments for healthyR\n\npat &lt;- c(\"%&gt;%\",\":=\",\"as_label\",\"as_name\",\"enquo\",\"enquos\",\"expr\",\n         \"sym\",\"syms\",\"healthyR_data\")\n\ntibble(fns = ls.str(\"package:healthyR.data\")) |&gt;\n  filter(!fns %in% pat) |&gt;\n  mutate(params = purrr::map(fns, formalArgs)) |&gt; \n  group_by(fns) |&gt; \n  mutate(func_with_params = toString(params)) |&gt;\n  mutate(\n    func_with_params = ifelse(\n      str_detect(\n        func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |&gt;\n  select(fns, func_with_params) |&gt;\n  mutate(fns = as.factor(fns)) |&gt;\n  datatable(\n    #class = 'cell-boarder-stripe',\n    colnames = c(\"Function\", \"Full Call\"),\n    options = list(\n      autowidth = TRUE,\n      pageLength = 10\n    )\n  )\n\n\n\n\n\n\n\n\nThe administrative data set included in { healthyR.data } is ready for your analytical projects. Here’s a quick example:\n# Load the healthyR.data package\nlibrary(healthyR.data)\n\n# Explore the dataset\ndata(\"healthyR_data\")\nhead(healthyR_data)\nThis will give you a look at the data and its structure, providing a strong foundation for your analysis.\n\n\n\nAccessing CMS data is simple with { healthyR.data }. The package includes functions that allow you to download various datasets directly from the CMS website. Here’s how:\n# Download CMS data\ncms_data &lt;- get_cms_meta_data()\n\n# View the downloaded data\nhead(cms_data)\nThis function fetches the latest data from CMS, ensuring your analyses are based on current information.\n\n\n\nThe { healthyR.data } package is continually updated, with new features and improvements added regularly. The latest version, 1.1.0, includes several enhancements that make the package even more powerful and user-friendly. For a detailed overview of the latest updates, check out the NEWS section.\n\n\n\nIn summary, { healthyR.data } is a versatile package that provides essential tools for healthcare data analysis. Whether you’re testing functions from the { healthyR } package or downloading the latest CMS data, { healthyR.data } has you covered. I encourage you to download the package and explore how it can enhance your analytical projects.\nHappy coding, and may your data always be insightful!\n\nFor more information and detailed documentation, visit the reference page. Stay tuned for more updates and tips on how to get the most out of the healthyverse packages!"
  },
  {
    "objectID": "posts/2024-07-01/index.html#what-is-healthyr.data",
    "href": "posts/2024-07-01/index.html#what-is-healthyr.data",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "The { healthyR.data } package serves two primary purposes: providing a robust administrative data set for testing functions in the { healthyR } package and facilitating the download of important data from the Centers for Medicare and Medicaid Services (CMS), a division of the Department of Health and Human Services (HHS). This package is your resource for comprehensive data that can enhance your analytical capabilities and simplify your testing procedures."
  },
  {
    "objectID": "posts/2024-07-01/index.html#key-features",
    "href": "posts/2024-07-01/index.html#key-features",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "One of the main features of { healthyR.data } is its extensive administrative data set. This data set is carefully curated to include a variety of scenarios and variables commonly found in healthcare data analysis. This makes it an excellent tool for testing the functions of the { healthyR } package, ensuring your analytical methods are reliable and effective.\n\n\n\nIn addition to its built-in data set, { healthyR.data } allows you to download data directly from CMS. This feature is especially useful for healthcare analysts who need up-to-date and detailed data for their analyses. With { healthyR.data }, you can easily access a wealth of information to drive insightful analysis."
  },
  {
    "objectID": "posts/2024-07-01/index.html#getting-started",
    "href": "posts/2024-07-01/index.html#getting-started",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "To start using { healthyR.data }, you can install it from CRAN with the following command:\ninstall.packages(\"healthyR.data\")\nOnce installed, load the package with:\nlibrary(healthyR.data)"
  },
  {
    "objectID": "posts/2024-07-01/index.html#functions",
    "href": "posts/2024-07-01/index.html#functions",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "Load the libraries:\n\nlibrary(healthyR.data)\nlibrary(tidyverse)\nlibrary(DT)\n\n\n# Functions and their arguments for healthyR\n\npat &lt;- c(\"%&gt;%\",\":=\",\"as_label\",\"as_name\",\"enquo\",\"enquos\",\"expr\",\n         \"sym\",\"syms\",\"healthyR_data\")\n\ntibble(fns = ls.str(\"package:healthyR.data\")) |&gt;\n  filter(!fns %in% pat) |&gt;\n  mutate(params = purrr::map(fns, formalArgs)) |&gt; \n  group_by(fns) |&gt; \n  mutate(func_with_params = toString(params)) |&gt;\n  mutate(\n    func_with_params = ifelse(\n      str_detect(\n        func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |&gt;\n  select(fns, func_with_params) |&gt;\n  mutate(fns = as.factor(fns)) |&gt;\n  datatable(\n    #class = 'cell-boarder-stripe',\n    colnames = c(\"Function\", \"Full Call\"),\n    options = list(\n      autowidth = TRUE,\n      pageLength = 10\n    )\n  )"
  },
  {
    "objectID": "posts/2024-07-01/index.html#using-the-administrative-data-set",
    "href": "posts/2024-07-01/index.html#using-the-administrative-data-set",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "The administrative data set included in { healthyR.data } is ready for your analytical projects. Here’s a quick example:\n# Load the healthyR.data package\nlibrary(healthyR.data)\n\n# Explore the dataset\ndata(\"healthyR_data\")\nhead(healthyR_data)\nThis will give you a look at the data and its structure, providing a strong foundation for your analysis."
  },
  {
    "objectID": "posts/2024-07-01/index.html#downloading-cms-data",
    "href": "posts/2024-07-01/index.html#downloading-cms-data",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "Accessing CMS data is simple with { healthyR.data }. The package includes functions that allow you to download various datasets directly from the CMS website. Here’s how:\n# Download CMS data\ncms_data &lt;- get_cms_meta_data()\n\n# View the downloaded data\nhead(cms_data)\nThis function fetches the latest data from CMS, ensuring your analyses are based on current information."
  },
  {
    "objectID": "posts/2024-07-01/index.html#latest-updates-and-features",
    "href": "posts/2024-07-01/index.html#latest-updates-and-features",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "The { healthyR.data } package is continually updated, with new features and improvements added regularly. The latest version, 1.1.0, includes several enhancements that make the package even more powerful and user-friendly. For a detailed overview of the latest updates, check out the NEWS section."
  },
  {
    "objectID": "posts/2024-07-01/index.html#conclusion",
    "href": "posts/2024-07-01/index.html#conclusion",
    "title": "Unlocking the Power of Administrative Data with healthyR.data",
    "section": "",
    "text": "In summary, { healthyR.data } is a versatile package that provides essential tools for healthcare data analysis. Whether you’re testing functions from the { healthyR } package or downloading the latest CMS data, { healthyR.data } has you covered. I encourage you to download the package and explore how it can enhance your analytical projects.\nHappy coding, and may your data always be insightful!\n\nFor more information and detailed documentation, visit the reference page. Stay tuned for more updates and tips on how to get the most out of the healthyverse packages!"
  },
  {
    "objectID": "posts/2024-07-02/index.html",
    "href": "posts/2024-07-02/index.html",
    "title": "How to Extract String After a Specific Character in R",
    "section": "",
    "text": "Welcome back, R Programmers! Today, we’ll explore a common task: extracting a substring after a specific character in R. Whether you’re cleaning data or transforming strings, this skill is quite handy. We’ll look at three approaches: using base R, stringr, and stringi. Let’s dive in!"
  },
  {
    "objectID": "posts/2024-07-02/index.html#using-base-r",
    "href": "posts/2024-07-02/index.html#using-base-r",
    "title": "How to Extract String After a Specific Character in R",
    "section": "Using Base R",
    "text": "Using Base R\nBase R provides several functions to manipulate strings. Here, we’ll use sub and strsplit to extract a substring after a specific character.\n\nExample 1: Using sub\nThe sub function allows us to replace parts of a string based on a pattern. Here’s how to extract the part after a specific character, say a hyphen (-).\n\n# Example string\nstring &lt;- \"data-science\"\n\n# Extract substring after the hyphen\nresult &lt;- sub(\".*-\", \"\", string)\nprint(result)  # Output: \"science\"\n\n[1] \"science\"\n\n\nExplanation:\n\n.*- is a regular expression where .* matches any character (except for line terminators) zero or more times, and - matches the hyphen.\n\"\" is the replacement, effectively removing everything up to and including the hyphen.\n\n\n\nExample 2: Using strsplit\nThe strsplit function splits a string into substrings based on a delimiter.\n\n# Example string\nstring &lt;- \"hello-world\"\n\n# Split the string at the hyphen\nparts &lt;- strsplit(string, \"-\")[[1]]\n\n# Extract the part after the hyphen\nresult &lt;- parts[2]\nprint(result)  # Output: \"world\"\n\n[1] \"world\"\n\n\nExplanation:\n\nstrsplit(string, \"-\") splits the string into parts at the hyphen, returning a list.\n[[1]] extracts the first element of the list.\n[2] extracts the second part of the split string."
  },
  {
    "objectID": "posts/2024-07-02/index.html#using-stringr",
    "href": "posts/2024-07-02/index.html#using-stringr",
    "title": "How to Extract String After a Specific Character in R",
    "section": "Using stringr",
    "text": "Using stringr\nThe stringr package, part of the tidyverse, provides consistent and easy-to-use string functions.\n\nExample 1: Using str_extract\nThe str_extract function extracts matching patterns from a string.\n\nlibrary(stringr)\n\n# Example string\nstring &lt;- \"apple-pie\"\n\n# Extract substring after the hyphen\nresult &lt;- str_extract(string, \"(?&lt;=-).*\")\nprint(result)  # Output: \"pie\"\n\n[1] \"pie\"\n\n\nExplanation:\n\n(?&lt;=-) is a look behind assertion, ensuring the match occurs after a hyphen.\n.* matches any character zero or more times.\n\n\n\nExample 2: Using str_split\nSimilar to strsplit in base R, str_split splits a string based on a pattern.\n\n# Example string\nstring &lt;- \"open-source\"\n\n# Split the string at the hyphen\nparts &lt;- str_split(string, \"-\")[[1]]\n\n# Extract the part after the hyphen\nresult &lt;- parts[2]\nprint(result)  # Output: \"source\"\n\n[1] \"source\"\n\n\nExplanation:\n\nstr_split(string, \"-\") splits the string into parts at the hyphen, returning a list.\n[[1]] extracts the first element of the list.\n[2] extracts the second part of the split string."
  },
  {
    "objectID": "posts/2024-07-02/index.html#using-stringi",
    "href": "posts/2024-07-02/index.html#using-stringi",
    "title": "How to Extract String After a Specific Character in R",
    "section": "Using stringi",
    "text": "Using stringi\nThe stringi package is another powerful tool for string manipulation, providing high-performance functions.\n\nExample 1: Using stri_extract\nThe stri_extract function extracts substrings based on patterns.\n\nlibrary(stringi)\n\n# Example string\nstring &lt;- \"front-end\"\n\n# Extract substring after the hyphen\nresult &lt;- stri_extract(string, regex = \"(?&lt;=-).*\")\nprint(result)  # Output: \"end\"\n\n[1] \"end\"\n\n\nExplanation:\n\nregex = \"(?&lt;=-).*\" uses a regular expression where (?&lt;=-) is a lookbehind assertion ensuring the match occurs after a hyphen, and .* matches any character zero or more times.\n\n\n\nExample 2: Using stri_split\nSimilar to strsplit and str_split, stri_split splits a string based on a pattern.\n\n# Example string\nstring &lt;- \"full-stack\"\n\n# Split the string at the hyphen\nparts &lt;- stri_split(string, regex = \"-\")[[1]]\n\n# Extract the part after the hyphen\nresult &lt;- parts[2]\nprint(result)  # Output: \"stack\"\n\n[1] \"stack\"\n\n\nExplanation:\n\nstri_split(string, regex = \"-\") splits the string into parts at the hyphen, returning a list.\n[[1]] extracts the first element of the list.\n[2] extracts the second part of the split string."
  },
  {
    "objectID": "posts/2024-07-03/index.html",
    "href": "posts/2024-07-03/index.html",
    "title": "Mastering Zoom Functionality in Excel with VBA",
    "section": "",
    "text": "When working with large datasets in Excel, the ability to zoom in and out quickly can significantly enhance your productivity. If you often find yourself adjusting the zoom level manually, why not automate it with VBA? In this blog post, we’ll explore how to use the zoom functionality in VBA to control the zoom level of your worksheets efficiently."
  },
  {
    "objectID": "posts/2024-07-03/index.html#basic-zoom-in-and-zoom-out",
    "href": "posts/2024-07-03/index.html#basic-zoom-in-and-zoom-out",
    "title": "Mastering Zoom Functionality in Excel with VBA",
    "section": "Basic Zoom In and Zoom Out",
    "text": "Basic Zoom In and Zoom Out\nLet’s start with simple macros to zoom in and zoom out.\nZoom In:\nSub ZoomIn()\n    Dim currentZoom As Integer\n    currentZoom = ActiveWindow.Zoom\n    If currentZoom &lt; 400 Then\n        ActiveWindow.Zoom = currentZoom + 10\n    End If\nEnd Sub\nZoom Out:\nSub ZoomOut()\n    Dim currentZoom As Integer\n    currentZoom = ActiveWindow.Zoom\n    If currentZoom &gt; 10 Then\n        ActiveWindow.Zoom = currentZoom - 10\n    End If\nEnd Sub\nIn these macros, ZoomIn increases the current zoom level by 10%, while ZoomOut decreases it by 10%. The code ensures that the zoom level stays within the permissible range of 10% to 400%."
  },
  {
    "objectID": "posts/2024-07-03/index.html#setting-a-specific-zoom-level",
    "href": "posts/2024-07-03/index.html#setting-a-specific-zoom-level",
    "title": "Mastering Zoom Functionality in Excel with VBA",
    "section": "Setting a Specific Zoom Level",
    "text": "Setting a Specific Zoom Level\nSometimes, you might need to set the zoom level to a specific percentage. You can do this easily with the following macro:\nSub SetZoomLevel(zoomLevel As Integer)\n    If zoomLevel &gt;= 10 And zoomLevel &lt;= 400 Then\n        ActiveWindow.Zoom = zoomLevel\n    Else\n        MsgBox \"Please enter a zoom level between 10 and 400.\"\n    End If\nEnd Sub\nYou can call this macro with any desired zoom level. For example:\nSub ZoomToSpecificLevel()\n    Call SetZoomLevel(150) ' Sets the zoom level to 150%\nEnd Sub"
  },
  {
    "objectID": "posts/2024-07-03/index.html#resetting-the-zoom-level",
    "href": "posts/2024-07-03/index.html#resetting-the-zoom-level",
    "title": "Mastering Zoom Functionality in Excel with VBA",
    "section": "Resetting the Zoom Level",
    "text": "Resetting the Zoom Level\nIf you need to reset the zoom level to its default setting (usually 100%), you can use the following macro:\nSub ResetZoom()\n    ActiveWindow.Zoom = 100\nEnd Sub"
  },
  {
    "objectID": "posts/2024-07-03/index.html#applying-zoom-to-a-specific-worksheet",
    "href": "posts/2024-07-03/index.html#applying-zoom-to-a-specific-worksheet",
    "title": "Mastering Zoom Functionality in Excel with VBA",
    "section": "Applying Zoom to a Specific Worksheet",
    "text": "Applying Zoom to a Specific Worksheet\nThe above examples modify the zoom level of the currently active window. If you want to set the zoom level for a specific worksheet, you can activate that sheet first and then set the zoom level:\nSub ZoomSpecificSheet(sheetName As String, zoomLevel As Integer)\n    Worksheets(sheetName).Activate\n    If zoomLevel &gt;= 10 And zoomLevel &lt;= 400 Then\n        ActiveWindow.Zoom = zoomLevel\n    Else\n        MsgBox \"Please enter a zoom level between 10 and 400.\"\n    End If\nEnd Sub"
  },
  {
    "objectID": "posts/2024-07-03/index.html#using-zoom-with-user-forms",
    "href": "posts/2024-07-03/index.html#using-zoom-with-user-forms",
    "title": "Mastering Zoom Functionality in Excel with VBA",
    "section": "Using Zoom with User Forms",
    "text": "Using Zoom with User Forms\nZoom functionality isn’t limited to worksheets. You can also control the zoom level of user forms in VBA. This is especially useful if your user form contains detailed information or numerous controls.\nSub ZoomUserForm(zoomLevel As Double)\n    With UserForm1\n        .Zoom = zoomLevel\n    End With\nEnd Sub\nCall this macro with a zoom level between 10 and 400 to adjust the user form’s zoom."
  },
  {
    "objectID": "posts/2024-07-06/index.html",
    "href": "posts/2024-07-06/index.html",
    "title": "Automate Your R Scripts with taskscheduleR",
    "section": "",
    "text": "Today, let’s dive into a nifty R package called taskscheduleR that can automate running your R scripts. Whether you need to execute a task every hour or just once a day, taskscheduleR has you covered. This package leverages the Windows Task Scheduler, making it a breeze to schedule and automate repetitive tasks directly from R. Let’s walk through a couple of examples from my new book, “Extending Excel with Python and R”."
  },
  {
    "objectID": "posts/2024-07-06/index.html#example-1-hourly-script-execution",
    "href": "posts/2024-07-06/index.html#example-1-hourly-script-execution",
    "title": "Automate Your R Scripts with taskscheduleR",
    "section": "Example 1: Hourly Script Execution",
    "text": "Example 1: Hourly Script Execution\nFirst, let’s set up a task that runs a script every hour. Here’s the code:\nlibrary(taskscheduleR)\n\n# Create a task scheduler job that runs the script every hour\ntaskscheduler_create(\n  taskname = \"Hello World Hourly\",\n  rscript = \"hello_world.R\",\n  schedule = \"0 * * * *\"\n)\nIn this snippet, we use the taskscheduler_create() function to create a new task. Let’s break down the arguments:\n\ntaskname: A unique name for the task, in this case, “Hello World Hourly”.\nrscript: The path to the R script you want to run, here it’s “hello_world.R”.\nschedule: This is the cron expression for scheduling. 0 * * * * means the script will run at the start of every hour."
  },
  {
    "objectID": "posts/2024-07-06/index.html#example-2-daily-script-execution-at-a-specific-time",
    "href": "posts/2024-07-06/index.html#example-2-daily-script-execution-at-a-specific-time",
    "title": "Automate Your R Scripts with taskscheduleR",
    "section": "Example 2: Daily Script Execution at a Specific Time",
    "text": "Example 2: Daily Script Execution at a Specific Time\nNow, let’s set up a task that runs the script once a day at 10:00 AM. Here’s how you can do it:\n# Create a task scheduler job that runs the script once a day at 10:00 AM\ntaskscheduler_create(\n  taskname = \"Hello World Daily\",\n  rscript = \"hello_world.R\",\n  schedule = \"0 10 * * *\"\n)\nIn this example, the schedule argument 0 10 * * * ensures the script runs daily at 10:00 AM."
  },
  {
    "objectID": "posts/2024-07-07/index.html",
    "href": "posts/2024-07-07/index.html",
    "title": "Exploring Linear Models with R and Exporting to Excel",
    "section": "",
    "text": "Today, we’re going to walk through an example of fitting a linear model in R, summarizing the results, and exporting the findings to an Excel file. This workflow is useful for documenting and sharing your statistical analysis.\nLet’s break down the code step by step."
  },
  {
    "objectID": "posts/2024-07-07/index.html#step-1-loading-the-necessary-libraries",
    "href": "posts/2024-07-07/index.html#step-1-loading-the-necessary-libraries",
    "title": "Exploring Linear Models with R and Exporting to Excel",
    "section": "Step 1: Loading the Necessary Libraries",
    "text": "Step 1: Loading the Necessary Libraries\nFirst, we need to load the openxlsx library, which helps us create and manipulate Excel files. If you don’t have it installed, you can get it using install.packages(\"openxlsx\").\n\nlibrary(openxlsx)\n\nThis line of code loads the openxlsx library into R so we can use its functions later."
  },
  {
    "objectID": "posts/2024-07-07/index.html#step-2-fitting-the-linear-model",
    "href": "posts/2024-07-07/index.html#step-2-fitting-the-linear-model",
    "title": "Exploring Linear Models with R and Exporting to Excel",
    "section": "Step 2: Fitting the Linear Model",
    "text": "Step 2: Fitting the Linear Model\nNext, we fit a linear model using the built-in mtcars dataset. We model mpg (miles per gallon) based on all other available variables in the dataset.\n\nmodel &lt;- lm(mpg ~ ., data = mtcars)\n\nHere, lm stands for linear model. The mpg ~ . part means we want to predict mpg using all other variables in the mtcars dataset."
  },
  {
    "objectID": "posts/2024-07-07/index.html#step-3-summarizing-the-model",
    "href": "posts/2024-07-07/index.html#step-3-summarizing-the-model",
    "title": "Exploring Linear Models with R and Exporting to Excel",
    "section": "Step 3: Summarizing the Model",
    "text": "Step 3: Summarizing the Model\nWe obtain a summary of our linear model, which includes details like coefficients, R-squared values, and the F-statistic.\n\nmodel_summary &lt;- summary(model)\n\nThis code generates a summary of the linear model we just created, giving us important statistics about the model’s performance."
  },
  {
    "objectID": "posts/2024-07-07/index.html#step-4-extracting-key-components",
    "href": "posts/2024-07-07/index.html#step-4-extracting-key-components",
    "title": "Exploring Linear Models with R and Exporting to Excel",
    "section": "Step 4: Extracting Key Components",
    "text": "Step 4: Extracting Key Components\nWe extract essential parts of the summary for easy access and to organize them in our Excel file.\n\ncoefficients &lt;- model_summary$coefficients\nr_squared &lt;- model_summary$r.squared\nadj_r_squared &lt;- model_summary$adj.r.squared\nf_statistic &lt;- model_summary$fstatistic\np_value &lt;- pf(\n  f_statistic[1], \n  f_statistic[2], \n  f_statistic[3], \n  lower.tail = FALSE\n  )\nmodel_formula &lt;- paste0(\n  model_summary[[\"terms\"]][[2]], \" \", \n  model_summary[[\"terms\"]][[1]], \" \",\n  model_summary[[\"terms\"]])[[3]]\n\n\ncoefficients: The estimated coefficients of the model.\nr_squared: How well the model explains the variability of the data.\nadj_r_squared: Adjusted version of R-squared for the number of predictors.\nf_statistic: Overall significance of the model.\np_value: Probability value indicating the significance of the F-statistic.\nmodel_formula: The formula used to fit the model."
  },
  {
    "objectID": "posts/2024-07-07/index.html#step-5-creating-and-populating-the-workbook",
    "href": "posts/2024-07-07/index.html#step-5-creating-and-populating-the-workbook",
    "title": "Exploring Linear Models with R and Exporting to Excel",
    "section": "Step 5: Creating and Populating the Workbook",
    "text": "Step 5: Creating and Populating the Workbook\nNow, we create a new Excel workbook and add a worksheet to it. We then write our extracted model summary components to this worksheet.\n\nwb &lt;- createWorkbook()\naddWorksheet(wb, \"Model Summary\")\n\nwriteData(wb, \"Model Summary\", \"Coefficients\", \n          startRow = 1, startCol = 1)\nwriteData(wb, \"Model Summary\", coefficients, startRow = 2, \n          startCol = 1, rowNames = TRUE)\n\nwriteData(wb, \"Model Summary\", \"R-Squared\", \n          startRow = 2 + nrow(coefficients) + 2, startCol = 1)\nwriteData(wb, \"Model Summary\", r_squared, \n          startRow = 2 + nrow(coefficients) + 2, startCol = 2)\n\nwriteData(wb, \"Model Summary\", \"Adjusted R-Squared\", \n          startRow = 2 + nrow(coefficients) + 3, startCol = 1)\nwriteData(wb, \"Model Summary\", adj_r_squared, \n          startRow = 2 + nrow(coefficients) + 3, startCol = 2)\n\nwriteData(wb, \"Model Summary\", \"F-Statistic\", \n          startRow = 2 + nrow(coefficients) + 4, startCol = 1)\nwriteData(wb, \"Model Summary\", f_statistic[1], \n          startRow = 2 + nrow(coefficients) + 4, startCol = 2)\n\nwriteData(wb, \"Model Summary\", \"p-Value\", \n          startRow = 2 + nrow(coefficients) + 5, startCol = 1)\nwriteData(wb, \"Model Summary\", p_value, \n          startRow = 2 + nrow(coefficients) + 5, startCol = 2)\n\nwriteData(wb, \"Model Summary\", \"Model Formula\", \n          startRow = 2 + nrow(coefficients) + 6, startCol = 1)\nwriteData(wb, \"Model Summary\", model_formula, \n          startRow = 2 + nrow(coefficients) + 6, startCol = 2)\n\n\ncreateWorkbook(): Creates a new Excel workbook.\naddWorksheet(wb, \"Model Summary\"): Adds a new sheet named “Model Summary” to the workbook.\nwriteData: Writes data to the specified location in the sheet. Here, we write various parts of the model summary in different rows and columns."
  },
  {
    "objectID": "posts/2024-07-07/index.html#step-6-saving-the-workbook",
    "href": "posts/2024-07-07/index.html#step-6-saving-the-workbook",
    "title": "Exploring Linear Models with R and Exporting to Excel",
    "section": "Step 6: Saving the Workbook",
    "text": "Step 6: Saving the Workbook\nFinally, we save our workbook to a file named lm_model_summary.xlsx.\n\nsaveWorkbook(\n  wb, \n  file = paste0(getwd(),\"/lm_model_summary.xlsx\"), \n  overwrite = TRUE\n  )\n\nThis line saves the workbook to your working directory with the specified file name.\nHere is a screenshot:\n\n\n\nExcel Screenshot"
  },
  {
    "objectID": "posts/2024-07-08/index.html",
    "href": "posts/2024-07-08/index.html",
    "title": "Unleashing the Power of TidyDensity: Simplifying Distribution Analysis in R",
    "section": "",
    "text": "If you’re a data scientist or statistician who often deals with probability distributions, you know the importance of seamlessly integrating these functions into your workflow. That’s where the TidyDensity package comes into play. Designed to make producing r, d, p, and q data easy and compatible with the tidyverse, TidyDensity is a must-have tool in your R arsenal. In this post, we’ll explore the features and benefits of TidyDensity and show you why you should give it a try."
  },
  {
    "objectID": "posts/2024-07-08/index.html#seamless-integration-with-tidyverse",
    "href": "posts/2024-07-08/index.html#seamless-integration-with-tidyverse",
    "title": "Unleashing the Power of TidyDensity: Simplifying Distribution Analysis in R",
    "section": "Seamless Integration with Tidyverse",
    "text": "Seamless Integration with Tidyverse\nTidyDensity ensures that all its output is in a tidy format, which means you can use the familiar suite of tidyverse tools to manipulate, visualize, and analyze your data. This compatibility streamlines your workflow and reduces the amount of data wrangling required."
  },
  {
    "objectID": "posts/2024-07-08/index.html#comprehensive-distribution-functions",
    "href": "posts/2024-07-08/index.html#comprehensive-distribution-functions",
    "title": "Unleashing the Power of TidyDensity: Simplifying Distribution Analysis in R",
    "section": "Comprehensive Distribution Functions",
    "text": "Comprehensive Distribution Functions\nWhether you’re dealing with normal, binomial, Poisson, or other distributions, TidyDensity has you covered. It includes functions for a wide range of distributions, each with options to generate random samples, calculate density, cumulative probabilities, and quantiles. This comprehensive coverage means you can rely on TidyDensity for almost any distribution-related task."
  },
  {
    "objectID": "posts/2024-07-08/index.html#easy-to-use-functions",
    "href": "posts/2024-07-08/index.html#easy-to-use-functions",
    "title": "Unleashing the Power of TidyDensity: Simplifying Distribution Analysis in R",
    "section": "Easy-to-Use Functions",
    "text": "Easy-to-Use Functions\nTidyDensity’s functions are designed with simplicity in mind. For example, to generate random samples from a normal distribution, you can use:\n\nlibrary(TidyDensity)\n\n# Generate random samples from a normal distribution\nnormal_samples &lt;- tidy_normal(.n = 100, .mean = 0, .sd = 1, .num_sims = 5)\n\n# View the first few rows\nhead(normal_samples)\n\n# A tibble: 6 × 7\n  sim_number     x       y    dx       dy      p       q\n  &lt;fct&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 1              1 -1.50   -3.15 0.000182 0.0664 -1.50  \n2 1              2  0.370  -3.08 0.000325 0.644   0.370 \n3 1              3  0.558  -3.01 0.000561 0.712   0.558 \n4 1              4 -1.28   -2.95 0.000938 0.101  -1.28  \n5 1              5  0.0298 -2.88 0.00153  0.512   0.0298\n6 1              6  0.189  -2.82 0.00241  0.575   0.189 \n\nsummary(normal_samples)\n\n sim_number       x                y                  dx         \n 1:100      Min.   :  1.00   Min.   :-2.45677   Min.   :-3.5658  \n 2:100      1st Qu.: 25.75   1st Qu.:-0.68839   1st Qu.:-1.5753  \n 3:100      Median : 50.50   Median :-0.02975   Median : 0.1216  \n 4:100      Mean   : 50.50   Mean   :-0.02445   Mean   : 0.1223  \n 5:100      3rd Qu.: 75.25   3rd Qu.: 0.66779   3rd Qu.: 1.8087  \n            Max.   :100.00   Max.   : 3.10887   Max.   : 4.3583  \n       dy                  p                 q           \n Min.   :0.0001153   Min.   :0.00701   Min.   :-2.45677  \n 1st Qu.:0.0198717   1st Qu.:0.24560   1st Qu.:-0.68839  \n Median :0.1003394   Median :0.48813   Median :-0.02975  \n Mean   :0.1468798   Mean   :0.49049   Mean   :-0.02445  \n 3rd Qu.:0.2658815   3rd Qu.:0.74787   3rd Qu.: 0.66779  \n Max.   :0.4688206   Max.   :0.99906   Max.   : 3.10887  \n\n\nThis code generates a tidy data frame with 100 random samples from a normal distribution with a mean of 0 and standard deviation of 1. You can then use dplyr and ggplot2 to manipulate and visualize this data effortlessly."
  },
  {
    "objectID": "posts/2024-07-08/index.html#practical-example",
    "href": "posts/2024-07-08/index.html#practical-example",
    "title": "Unleashing the Power of TidyDensity: Simplifying Distribution Analysis in R",
    "section": "Practical Example",
    "text": "Practical Example\nLet’s walk through a practical example to demonstrate how TidyDensity can be used in a typical data analysis workflow. Suppose you’re interested in analyzing the distribution of a sample dataset and visualizing its density.\n\n# Load required libraries\nlibrary(TidyDensity)\nlibrary(ggplot2)\n\n# Generate random samples from a normal distribution\nset.seed(123)\nnormal_samples &lt;- tidy_normal(.n = 1000, .mean = 5, .sd = 2)\n\n# Plot the density of the samples\ntidy_autoplot(normal_samples)\n\n\n\n\n\n\n\n\nIn this example, we generate 1,000 random samples from a normal distribution with a mean of 5 and a standard deviation of 2. We then use ggplot2 to create a density plot, providing a clear visual representation of the distribution."
  },
  {
    "objectID": "posts/2024-07-09/index.html",
    "href": "posts/2024-07-09/index.html",
    "title": "Extracting Strings Before a Space in R",
    "section": "",
    "text": "Hello, R users! Today, we’ll dive into a common text manipulation task: extracting strings before a space. This is a handy trick for dealing with names, addresses, or any text data where you need to isolate the first part of a string.\nWe’ll explore three approaches: using base R, stringr, and stringi. Each method offers its unique advantages, so you can choose the one that fits your style best."
  },
  {
    "objectID": "posts/2024-07-09/index.html#base-r-approach",
    "href": "posts/2024-07-09/index.html#base-r-approach",
    "title": "Extracting Strings Before a Space in R",
    "section": "Base R Approach",
    "text": "Base R Approach\nLet’s start with base R. The sub function is a versatile tool for pattern matching and replacement. To extract the string before a space, we can use a regular expression.\n\n# Sample data\ntext &lt;- c(\"John Doe\", \"Jane Smith\", \"Alice Johnson\")\n\n# Extract strings before the first space\nfirst_part_base &lt;- sub(\" .*\", \"\", text)\n\n# Display the result\nprint(first_part_base)\n\n[1] \"John\"  \"Jane\"  \"Alice\"\n\n\nIn this example, the sub function replaces the space and everything after it with an empty string, effectively extracting the first part of each string."
  },
  {
    "objectID": "posts/2024-07-09/index.html#using-stringr",
    "href": "posts/2024-07-09/index.html#using-stringr",
    "title": "Extracting Strings Before a Space in R",
    "section": "Using stringr",
    "text": "Using stringr\nNext, let’s see how stringr simplifies this task. The stringr package, part of the tidyverse, provides a consistent and easy-to-use interface for string manipulation.\n\n# Load stringr package\nlibrary(stringr)\n\n# Sample data\ntext &lt;- c(\"John Doe\", \"Jane Smith\", \"Alice Johnson\")\n\n# Extract strings before the first space\nfirst_part_stringr &lt;- str_extract(text, \"^[^ ]+\")\n\n# Display the result\nprint(first_part_stringr)\n\n[1] \"John\"  \"Jane\"  \"Alice\"\n\n\nHere, str_extract is used with a regular expression to match and extract the part of the string before the first space. The ^[^ ]+ pattern matches the beginning of the string (^) followed by one or more characters that are not a space ([^ ]+)."
  },
  {
    "objectID": "posts/2024-07-09/index.html#using-stringi",
    "href": "posts/2024-07-09/index.html#using-stringi",
    "title": "Extracting Strings Before a Space in R",
    "section": "Using stringi",
    "text": "Using stringi\nFinally, let’s use stringi, a powerful package for advanced string operations. stringi functions are optimized for performance, making it a great choice for handling large datasets.\n\n# Load stringi package\nlibrary(stringi)\n\n# Sample data\ntext &lt;- c(\"John Doe\", \"Jane Smith\", \"Alice Johnson\")\n\n# Extract strings before the first space\nfirst_part_stringi &lt;- stri_extract_first_regex(text, \"^[^ ]+\")\n\n# Display the result\nprint(first_part_stringi)\n\n[1] \"John\"  \"Jane\"  \"Alice\"\n\n\nWith stringi, stri_extract_first_regex performs similarly to str_extract from stringr, using the same regular expression pattern."
  },
  {
    "objectID": "posts/2024-07-10/index.html",
    "href": "posts/2024-07-10/index.html",
    "title": "Using the FileDateTime Function in VBA from R",
    "section": "",
    "text": "Welcome back to our series where we explore the synergy between R and VBA! Today, we’re diving into the FileDateTime function in VBA and how you can leverage it within R. This function is incredibly useful for anyone dealing with files, as it allows you to get the date and time when a file was last modified."
  },
  {
    "objectID": "posts/2024-07-10/index.html#basic-usage-of-filedatetime-in-vba",
    "href": "posts/2024-07-10/index.html#basic-usage-of-filedatetime-in-vba",
    "title": "Using the FileDateTime Function in VBA from R",
    "section": "Basic Usage of FileDateTime in VBA",
    "text": "Basic Usage of FileDateTime in VBA\nLet’s start with a simple example of how to use FileDateTime in VBA. Suppose you have a file located at C:\\example\\myfile.txt. Here’s how you can get its last modified date and time:\nSub GetFileDateTime()\n    Dim filePath As String\n    Dim fileModifiedDate As String\n\n    filePath = \"C:\\example\\myfile.txt\"\n    fileModifiedDate = FileDateTime(filePath)\n\n    MsgBox \"The file was last modified on: \" & fileModifiedDate\nEnd Sub\nIn this script: - filePath stores the path to the file. - fileModifiedDate gets the last modified date and time using FileDateTime. - MsgBox displays the result in a message box."
  },
  {
    "objectID": "posts/2024-07-10/index.html#executing-vba-from-r",
    "href": "posts/2024-07-10/index.html#executing-vba-from-r",
    "title": "Using the FileDateTime Function in VBA from R",
    "section": "Executing VBA from R",
    "text": "Executing VBA from R\nTo execute VBA code from R, you can use the RDCOMClient package, which allows R to interact with COM objects like Excel. Below is a step-by-step guide on how to achieve this:\n\nInstall and Load the RDCOMClient Package\nFirst, ensure you have the RDCOMClient package installed. If not, you can install it from CRAN:\n\ninstall.packages(\"RDCOMClient\")\nThen, load the package:\nlibrary(RDCOMClient)\n\nCreate a VBA Macro in Excel\nOpen Excel and press ALT + F11 to open the VBA editor. Create a new module and paste the GetFileDateTime function code. Save the Excel workbook with a .xlsm extension to enable macros.\nRun the VBA Macro from R\nNow, let’s write an R script to open the Excel workbook and run the macro:\n\n\nlibrary(RDCOMClient)\n\n# Define the path to your Excel workbook\nexcelFilePath &lt;- \"C:/Users/steve/Documents/GitHub/steveondata/posts/2024-07-10/file_date_time.xlsm\"\n\n# Create an Excel application object\nexcelApp &lt;- COMCreate(\"Excel.Application\")\n\n# Open the workbook\nworkbook &lt;- excelApp$Workbooks()$Open(excelFilePath)\n\n# Make Excel visible (optional)\nexcelApp[[\"Visible\"]] &lt;- FALSE\n\n# Run the macro\nexcelApp$Run(\"GetFileDateTime\")\n\nNULL\n\n# Close the workbook without saving changes\nworkbook$Close(FALSE)\n\n[1] TRUE\n\n# Quit the Excel application\nexcelApp$Quit()\n\nNULL\n\n\nIn this R script:\n\nexcelFilePath specifies the path to your Excel workbook.\nexcelApp creates an Excel application object.\nworkbook opens the specified workbook.\nexcelApp$Run(\"GetFileDateTime\") runs the VBA macro.\nworkbook$Close(FALSE) closes the workbook without saving changes.\nexcelApp$Quit() quits the Excel application.\n\nHere is the message box:\n\n\n\nVBA FileDateTime"
  },
  {
    "objectID": "posts/2024-07-11/index.html",
    "href": "posts/2024-07-11/index.html",
    "title": "Exploring Random Walks with TidyDensity in R",
    "section": "",
    "text": "Welcome back, data enthusiasts! Today, we’re diving into the fascinating world of random walks using the TidyDensity R package. If you’re working with time series data, financial modeling, or stochastic processes, understanding random walks is essential. And with TidyDensity, implementing and visualizing these walks has never been easier."
  },
  {
    "objectID": "posts/2024-07-11/index.html#arguments-breakdown",
    "href": "posts/2024-07-11/index.html#arguments-breakdown",
    "title": "Exploring Random Walks with TidyDensity in R",
    "section": "Arguments Breakdown",
    "text": "Arguments Breakdown\n\n.data: The dataset from a tidy_ distribution function. This forms the basis of your random walk.\n.initial_value: The starting value of the random walk. The default is 0, but you can set it to any numeric value.\n.sample: A boolean indicating whether to sample the y values from the tidy_ distribution. Defaults to FALSE.\n.replace: If both .sample and .replace are TRUE, sampling is done with replacement. Defaults to FALSE.\n.value_type: Determines how the walk is computed. Options are:\n\"cum_prod\": Computes the cumulative product of y.\n\"cum_sum\": Computes the cumulative sum of y."
  },
  {
    "objectID": "posts/2024-07-11/index.html#example-1-simple-random-walk-with-cumulative-sum",
    "href": "posts/2024-07-11/index.html#example-1-simple-random-walk-with-cumulative-sum",
    "title": "Exploring Random Walks with TidyDensity in R",
    "section": "Example 1: Simple Random Walk with Cumulative Sum",
    "text": "Example 1: Simple Random Walk with Cumulative Sum\nFirst, let’s create a simple random walk using a normal distribution and compute the cumulative sum.\n\nlibrary(TidyDensity)\n\nset.seed(123)\ntidy_normal(.num_sims = 25, .n = 100) |&gt;\n  tidy_random_walk(.value_type = \"cum_sum\") |&gt;\n  tidy_random_walk_autoplot()\n\n\n\n\n\n\n\n\nIn this example, we generate 25 simulations of 100 points each from a normal distribution. The tidy_random_walk() function then computes the cumulative sum of these points, simulating a simple random walk. The tidy_random_walk_autoplot() function is used to visualize the random walk."
  },
  {
    "objectID": "posts/2024-07-11/index.html#example-2-random-walk-with-sampling",
    "href": "posts/2024-07-11/index.html#example-2-random-walk-with-sampling",
    "title": "Exploring Random Walks with TidyDensity in R",
    "section": "Example 2: Random Walk with Sampling",
    "text": "Example 2: Random Walk with Sampling\nNext, we’ll explore a random walk where values are sampled.\n\nset.seed(123)\ntidy_normal(.num_sims = 25, .n = 100) |&gt;\n  tidy_random_walk(.value_type = \"cum_sum\", .sample = TRUE) |&gt;\n  tidy_random_walk_autoplot()\n\n\n\n\n\n\n\n\nHere, setting .sample to TRUE ensures that each step in the random walk is taken by randomly sampling from the original dataset. This can introduce additional variability and randomness to the walk."
  },
  {
    "objectID": "posts/2024-07-11/index.html#example-3-random-walk-with-sampling-and-replacement",
    "href": "posts/2024-07-11/index.html#example-3-random-walk-with-sampling-and-replacement",
    "title": "Exploring Random Walks with TidyDensity in R",
    "section": "Example 3: Random Walk with Sampling and Replacement",
    "text": "Example 3: Random Walk with Sampling and Replacement\nFinally, let’s create a random walk with sampling and replacement.\n\nset.seed(123)\ntidy_normal(.num_sims = 25, .n = 100) |&gt;\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) |&gt;\n  tidy_random_walk_autoplot()\n\n\n\n\n\n\n\n\nIn this example, setting both .sample and .replace to TRUE ensures that values are sampled with replacement. This can be useful in bootstrapping scenarios or when simulating more complex stochastic processes."
  },
  {
    "objectID": "posts/2024-07-11/index.html#bonus-section-comparing-different-random-walk-sampling-methods",
    "href": "posts/2024-07-11/index.html#bonus-section-comparing-different-random-walk-sampling-methods",
    "title": "Exploring Random Walks with TidyDensity in R",
    "section": "Bonus Section: Comparing Different Random Walk Sampling Methods",
    "text": "Bonus Section: Comparing Different Random Walk Sampling Methods\nTo wrap up, let’s combine multiple random walks and visualize them using ggplot2. This bonus section will show you how different sampling methods impact the random walks.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\ndf &lt;- rbind(\n  tidy_normal(.num_sims = 25, .n = 100) |&gt;\n    tidy_random_walk(.value_type = \"cum_sum\") |&gt;\n    mutate(type = \"No_Sample\"),\n  tidy_normal(.num_sims = 25, .n = 100) |&gt;\n    tidy_random_walk(.value_type = \"cum_sum\", .sample = TRUE) |&gt;\n    mutate(type = \"Sample_No_Replace\"),\n  tidy_normal(.num_sims = 25, .n = 100) |&gt;\n    tidy_random_walk(.value_type = \"cum_sum\", .sample = TRUE, .replace = TRUE) |&gt;\n    mutate(type = \"Sample_Replace\")\n) |&gt;\n  select(sim_number, x, random_walk_value, type) |&gt;\n  mutate(\n    low_ci = -1.96 * sqrt(x),\n    hi_ci = 1.96 * sqrt(x)\n  )\n\natb &lt;- attributes(df)\n\ndf |&gt;\n  ggplot(aes(\n    x = x, \n    y = random_walk_value, \n    group = sim_number, \n    color = factor(type))\n  ) +\n  geom_line(aes(alpha = 0.382)) +\n  geom_line(aes(y = low_ci, group = sim_number), \n            linetype = \"dashed\", size = 0.6, color = \"black\") +\n  geom_line(aes(y = hi_ci, group = sim_number), \n            linetype = \"dashed\", size = 0.6, color = \"black\") +\n  theme_minimal() +\n  theme(legend.position=\"none\") +\n  facet_wrap(~type) +\n  labs(\n    x = \"Time\",\n    y = \"Random Walk Value\",\n    title = \"Random Walk with Different Sampling Methods\",\n    subtitle = paste0(\"Simulations: \", atb$all$.num_sims, \n                      \" | Steps: \", atb$all$.n,\n                      \" | Distribution: \", atb$all$dist_with_params\n                      )\n  )"
  },
  {
    "objectID": "posts/2024-07-11/index.html#code-explanation",
    "href": "posts/2024-07-11/index.html#code-explanation",
    "title": "Exploring Random Walks with TidyDensity in R",
    "section": "Code Explanation",
    "text": "Code Explanation\n\nGenerating Data: We generate three sets of random walks using different sampling methods:\n\n\nNo sampling.\nSampling without replacement.\nSampling with replacement.\n\nEach set consists of 25 simulations of 100 steps.\n\nCombining Data: The results are combined into a single data frame, with a new column type to indicate the sampling method used.\nCalculating Confidence Intervals: We calculate the 95% confidence intervals for each step.\nPlotting: Using ggplot2, we plot the random walks, coloring by sampling method and adding dashed lines to indicate the confidence intervals. We also facet the plot by type to separate the different sampling methods visually."
  },
  {
    "objectID": "posts/2024-07-15/index.html",
    "href": "posts/2024-07-15/index.html",
    "title": "tidyAML: Automated Machine Learning with tidymodels",
    "section": "",
    "text": "Introduction\nWelcome to {tidyAML} which is an R package that makes it easy to use the tidymodels ecosystem to perform automated machine learning (AutoML). This package provides a simple and intuitive interface that allows users to quickly generate machine learning models without worrying about the underlying details. It also includes a safety mechanism that ensures that the package will fail gracefully if any required extension packages are not installed on the user’s machine. With {tidyAML}, users can easily build high-quality machine learning models in just a few lines of code. Whether you are a beginner or an experienced machine learning practitioner, {tidyAML} has something to offer.\nSome ideas are that we should be able to generate regression models on the fly without having to actually go through the process of building the specification, especially if it is a non-tuning model, meaning we are not planing on tuning hyper-parameters like penalty and cost.\nThe idea is not to re-write the excellent work the tidymodels team has done (because it’s not possible) but rather to try and make an enhanced easy to use set of functions that do what they say and can generate many models and predictions at once.\nThis is similar to the great h2o package, but, {tidyAML} does not require java to be setup properly like h2o because {tidyAML} is built on tidymodels.\n\n\nInstallation\nYou can install {tidyAML} like so:\ninstall.packages(\"tidyAML\")\nOr the development version from GitHub\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/tidyAML\")\nPart of the reason to use {tidyAML} is so that you can generate many models of your data set. One way of modeling a data set is using regression for some numeric output. There is a convienent function in tidyAML that will generate a set of non-tuning models for fast regression. Let’s take a look below.\nFirst let’s load the library\n\nlibrary(tidyAML)\n\nNow lets see the function in action.\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;        &lt;list&gt;    \n 1         1 lm              regression    linear_reg   &lt;spec[+]&gt; \n 2         2 brulee          regression    linear_reg   &lt;spec[+]&gt; \n 3         3 gee             regression    linear_reg   &lt;spec[+]&gt; \n 4         4 glm             regression    linear_reg   &lt;spec[+]&gt; \n 5         5 glmer           regression    linear_reg   &lt;spec[+]&gt; \n 6         6 glmnet          regression    linear_reg   &lt;spec[+]&gt; \n 7         7 gls             regression    linear_reg   &lt;spec[+]&gt; \n 8         8 lme             regression    linear_reg   &lt;spec[+]&gt; \n 9         9 lmer            regression    linear_reg   &lt;spec[+]&gt; \n10        10 stan            regression    linear_reg   &lt;spec[+]&gt; \n11        11 stan_glmer      regression    linear_reg   &lt;spec[+]&gt; \n\n\n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;        &lt;list&gt;    \n1         1 lm              regression    linear_reg   &lt;spec[+]&gt; \n2         2 glm             regression    linear_reg   &lt;spec[+]&gt; \n3         3 glm             regression    poisson_reg  &lt;spec[+]&gt; \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\",\"gee\"), \n                                 .parsnip_fns = \"linear_reg\")\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;        &lt;list&gt;    \n1         1 lm              regression    linear_reg   &lt;spec[+]&gt; \n2         2 gee             regression    linear_reg   &lt;spec[+]&gt; \n3         3 glm             regression    linear_reg   &lt;spec[+]&gt; \n\n\nAs shown we can easily select the models we want either by choosing the supported parsnip function like linear_reg() or by choose the desired engine, you can also use them both in conjunction with each other!\nThis function also does add a class to the output. Let’s see it.\n\nclass(fast_regression_parsnip_spec_tbl())\n\n[1] \"tidyaml_mod_spec_tbl\" \"fst_reg_spec_tbl\"     \"tidyaml_base_tbl\"    \n[4] \"tbl_df\"               \"tbl\"                  \"data.frame\"          \n\n\nWe see that there are two added classes, first fst_reg_spec_tbl because this creates a set of non-tuning regression models and then tidyaml_mod_spec_tbl because this is a model specification tibble built with {tidyAML}\nNow, what if you want to create a non-tuning model spec without using the fast_regression_parsnip_spec_tbl() function. Well, you can. The function is called create_model_spec().\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      \"linear_reg\",\n      \"linear_reg\",\n      \"linear_reg\",\n      \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;        &lt;list&gt;     \n1 lm              regression    linear_reg   &lt;spec[+]&gt;  \n2 glm             regression    linear_reg   &lt;spec[+]&gt;  \n3 glmnet          regression    linear_reg   &lt;spec[+]&gt;  \n4 cubist          regression    cubist_rules &lt;spec[+]&gt;  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      \"linear_reg\",\n      \"linear_reg\",\n      \"linear_reg\",\n      \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\n\n\n! parsnip could not locate an implementation for `cubist_rules` regression\n  model specifications using the `cubist` engine.\n\n\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nThe first example shows the output as a tibble, the second example shows the output as a list of model specs. The last one for cubist rules also shows how it will gracefully fail if the package is not loaded.\n\nHappy Coding!"
  },
  {
    "objectID": "posts/2024-07-16/index.html",
    "href": "posts/2024-07-16/index.html",
    "title": "How to Extract Substring Starting from the End of a String in R",
    "section": "",
    "text": "Hey useR’s! Today, we’re going to discuss a neat trick: extracting substrings starting from the end of a string. We’ll cover how to achieve this using base R, stringr, and stringi. By the end of this post, you’ll have several tools in your R toolbox for string manipulation. Let’s get started!"
  },
  {
    "objectID": "posts/2024-07-16/index.html#using-base-r",
    "href": "posts/2024-07-16/index.html#using-base-r",
    "title": "How to Extract Substring Starting from the End of a String in R",
    "section": "Using Base R",
    "text": "Using Base R\nFirst up, let’s use base R functions to extract substrings from the end of a string. The substr function is your friend here.\nHere’s a simple example:\n\n# Define a string\nmy_string &lt;- \"Hello, world!\"\n\n# Extract the last 6 characters\nsubstring_from_end &lt;- substr(my_string, nchar(my_string) - 5, nchar(my_string))\n\n# Print the result\nprint(substring_from_end)\n\n[1] \"world!\"\n\n\nExplanation:\n\nnchar(my_string) returns the total number of characters in my_string.\nnchar(my_string) - 5 calculates the starting position of the substring, counting from the end.\nsubstr(my_string, start, stop) extracts the substring from the start position to the stop position."
  },
  {
    "objectID": "posts/2024-07-16/index.html#using-stringr",
    "href": "posts/2024-07-16/index.html#using-stringr",
    "title": "How to Extract Substring Starting from the End of a String in R",
    "section": "Using stringr",
    "text": "Using stringr\nThe stringr package makes string manipulation more straightforward and readable. We’ll use the str_sub function for this task.\nFirst, install and load the stringr package if you haven’t already:\n\n#install.packages(\"stringr\")\nlibrary(stringr)\n\nNow, let’s extract a substring from the end:\n\n# Define a string\nmy_string &lt;- \"Hello, world!\"\n\n# Extract the last 6 characters using stringr\nsubstring_from_end &lt;- str_sub(my_string, -6, -1)\n\n# Print the result\nprint(substring_from_end)\n\n[1] \"world!\"\n\n\nExplanation:\n\nstr_sub(my_string, start, end) extracts the substring from the start to the end position.\nNegative indices in str_sub count from the end of the string. So -6 refers to the sixth character from the end, and -1 refers to the last character."
  },
  {
    "objectID": "posts/2024-07-16/index.html#using-stringi",
    "href": "posts/2024-07-16/index.html#using-stringi",
    "title": "How to Extract Substring Starting from the End of a String in R",
    "section": "Using stringi",
    "text": "Using stringi\nThe stringi package is another powerful tool for string manipulation. We’ll use the stri_sub function here.\nFirst, install and load the stringi package:\n\n#install.packages(\"stringi\")\nlibrary(stringi)\n\nLet’s extract our substring:\n\n# Define a string\nmy_string &lt;- \"Hello, world!\"\n\n# Extract the last 6 characters using stringi\nsubstring_from_end &lt;- stri_sub(my_string, from = -6, to = -1)\n\n# Print the result\nprint(substring_from_end)\n\n[1] \"world!\"\n\n\nExplanation:\n\nstri_sub(my_string, from, to) works similarly to str_sub, using from and to parameters to define the start and end positions.\nNegative values count from the end of the string."
  },
  {
    "objectID": "posts/2024-07-17/index.html",
    "href": "posts/2024-07-17/index.html",
    "title": "VBA: Saving and Closing a Workbook",
    "section": "",
    "text": "In this tutorial, you’ll learn how to save and close an Excel workbook using VBA (Visual Basic for Applications) and then doing it from R. We’ll create a simple VBA script that saves and closes a workbook, and then we’ll call this script from R using the RDCOMClient package."
  },
  {
    "objectID": "posts/2024-07-17/index.html#vba-script",
    "href": "posts/2024-07-17/index.html#vba-script",
    "title": "VBA: Saving and Closing a Workbook",
    "section": "VBA Script",
    "text": "VBA Script\nFirst, let’s create a simple VBA script that saves and closes a workbook. Here’s the VBA code:\nSub SaveAndCloseWorkbook()\n    Dim wb As Workbook\n    Set wb = ThisWorkbook\n    wb.Save\n    wb.Close\nEnd Sub"
  },
  {
    "objectID": "posts/2024-07-17/index.html#explanation",
    "href": "posts/2024-07-17/index.html#explanation",
    "title": "VBA: Saving and Closing a Workbook",
    "section": "Explanation:",
    "text": "Explanation:\n\nSub SaveAndCloseWorkbook(): This line starts the subroutine named SaveAndCloseWorkbook.\nDim wb As Workbook: This declares a variable wb as a Workbook object.\nSet wb = ThisWorkbook: This sets wb to refer to the workbook where the VBA code is running.\nwb.Save: This saves the workbook.\nwb.Close: This closes the workbook."
  },
  {
    "objectID": "posts/2024-07-17/index.html#calling-vba-from-r",
    "href": "posts/2024-07-17/index.html#calling-vba-from-r",
    "title": "VBA: Saving and Closing a Workbook",
    "section": "Calling VBA from R",
    "text": "Calling VBA from R\nNow, let’s see how you can call this VBA script from R using the RDCOMClient package. This package allows R to interact with COM objects, such as Excel.\n\nStep-by-Step R Code\n\nInstall RDCOMClient: If you haven’t installed it yet, you can do so from the R console.\ninstall.packages(\"RDCOMClient\", repos = \"http://www.omegahat.net/R\")\nWrite the R Code: Here’s the R script to run the VBA code.\n\nlibrary(RDCOMClient)\n\n# Create a new Excel application\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# Make the Excel application visible\nexcel_app[[\"Visible\"]] &lt;- TRUE\n\n# Open an existing workbook or create a new one\nworkbook_path &lt;- \"C:/path/to/your/workbook.xlsx\"\nwb &lt;- excel_app$Workbooks()$Open(workbook_path)\n\n# Run the VBA macro\nexcel_app$Run(\"SaveAndCloseWorkbook\")\n\n# Quit the Excel application\nexcel_app$Quit()\n\n# Release the COM object\nrm(excel_app)\ngc()"
  },
  {
    "objectID": "posts/2024-07-17/index.html#explanation-1",
    "href": "posts/2024-07-17/index.html#explanation-1",
    "title": "VBA: Saving and Closing a Workbook",
    "section": "Explanation:",
    "text": "Explanation:\n\nlibrary(RDCOMClient): Loads the RDCOMClient library to interact with COM objects.\n*excel_app &lt;- COMCreate(“Excel.Application”)**: Creates a new Excel application instance.\nexcel_app[[“Visible”]] &lt;- TRUE: Makes the Excel application visible (optional).\nworkbook_path: Path to your Excel workbook.\nwb &lt;- excel_app\\(Workbooks()\\)Open(workbook_path): Opens the workbook.\nexcel_app$Run(“SaveAndCloseWorkbook”): Runs the VBA macro SaveAndCloseWorkbook.\nexcel_app$Quit(): Quits the Excel application.\nrm(excel_app) and gc(): Releases the COM object and performs garbage collection to free up memory."
  },
  {
    "objectID": "posts/2024-07-18/index.html",
    "href": "posts/2024-07-18/index.html",
    "title": "Simplify Regression Modeling with tidyAML’s fast_regression()",
    "section": "",
    "text": "If you’ve ever faced the daunting task of setting up multiple regression models in R, you’ll appreciate the convenience and efficiency that tidyAML brings to the table. Today, we’re diving into one of its standout functions: fast_regression(). This function is designed to streamline the regression modeling process, allowing you to quickly create and evaluate a variety of model specifications with minimal code."
  },
  {
    "objectID": "posts/2024-07-18/index.html#syntax",
    "href": "posts/2024-07-18/index.html#syntax",
    "title": "Simplify Regression Modeling with tidyAML’s fast_regression()",
    "section": "Syntax",
    "text": "Syntax\nHere’s a look at the function’s syntax:\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL,\n  .drop_na = TRUE\n)"
  },
  {
    "objectID": "posts/2024-07-18/index.html#arguments",
    "href": "posts/2024-07-18/index.html#arguments",
    "title": "Simplify Regression Modeling with tidyAML’s fast_regression()",
    "section": "Arguments",
    "text": "Arguments\n\n.data: The data frame to be used in the regression problem.\n.rec_obj: A recipe object from the recipes package that defines the pre-processing steps.\n.parsnip_fns: Specifies which parsnip functions to use. The default \"all\" will create all possible regression model specifications.\n.parsnip_eng: Specifies which parsnip engines to use. The default \"all\" will create all possible regression model specifications.\n.split_type: Defines the type of data split, with \"initial_split\" as the default. Other split types supported by rsample can also be used.\n.split_args: Additional arguments for the split type. When set to NULL, default parameters for the chosen split type are used.\n.drop_na: Determines whether to drop NA values from the data. Default is TRUE."
  },
  {
    "objectID": "posts/2024-07-22/index.html",
    "href": "posts/2024-07-22/index.html",
    "title": "How to Concatenate Strings in R",
    "section": "",
    "text": "Hello, R users! Today, we’re going to talk about a fundamental yet essential aspect of data manipulation: concatenating strings. String concatenation is the process of joining two or more strings together. It doesn’t matter if you’re working with text data, creating labels, or generating dynamic outputs, knowing how to concatenate strings efficiently is a must. We’ll explore how to do this using base R, the stringr package, and the stringi package. Let’s get started!"
  },
  {
    "objectID": "posts/2024-07-22/index.html#concatenating-strings-in-base-r",
    "href": "posts/2024-07-22/index.html#concatenating-strings-in-base-r",
    "title": "How to Concatenate Strings in R",
    "section": "Concatenating Strings in Base R",
    "text": "Concatenating Strings in Base R\nBase R provides a straightforward way to concatenate strings using the paste() and paste0() functions. Here’s how you can use them:\n\nUsing paste()\nThe paste() function combines strings and adds a separator (default is a space).\n\n# Example\nstring1 &lt;- \"Hello\"\nstring2 &lt;- \"World\"\nresult &lt;- paste(string1, string2)\nprint(result)  # Output: \"Hello World\"\n\n[1] \"Hello World\"\n\n\nIn this example, paste(string1, string2) joins “Hello” and “World” with a space in between.\n\n\nUsing paste0()\nThe paste0() function is similar to paste(), but it doesn’t add a separator by default.\n\n# Example\nresult_no_space &lt;- paste0(string1, string2)\nprint(result_no_space)  # Output: \"HelloWorld\"\n\n[1] \"HelloWorld\"\n\n\nHere, paste0(string1, string2) joins “Hello” and “World” without any spaces.\n\n\nCustom Separator\nYou can also specify a custom separator with paste().\n\n# Example\nresult_custom_sep &lt;- paste(string1, string2, sep = \", \")\nprint(result_custom_sep)  # Output: \"Hello, World\"\n\n[1] \"Hello, World\"\n\n\nBy setting sep = \", \", we add a comma and a space between the strings."
  },
  {
    "objectID": "posts/2024-07-22/index.html#concatenating-strings-with-stringr",
    "href": "posts/2024-07-22/index.html#concatenating-strings-with-stringr",
    "title": "How to Concatenate Strings in R",
    "section": "Concatenating Strings with stringr",
    "text": "Concatenating Strings with stringr\nThe stringr package offers a more consistent and user-friendly way to handle strings in R. For concatenation, we use the str_c() function.\n\nUsing str_c()\nThe str_c() function from stringr is similar to paste0() but provides more control over the process.\n\n# Load stringr package\nlibrary(stringr)\n\n# Example\nresult_str_c &lt;- str_c(string1, string2)\nprint(result_str_c)  # Output: \"HelloWorld\"\n\n[1] \"HelloWorld\"\n\n\nThis example is equivalent to paste0().\n\n\nCustom Separator\nTo add a separator, use the sep argument in str_c().\n\n# Example with separator\nresult_str_c_sep &lt;- str_c(string1, string2, sep = \" \")\nprint(result_str_c_sep)  # Output: \"Hello World\"\n\n[1] \"Hello World\"\n\n\nHere, sep = \" \" adds a space between the strings."
  },
  {
    "objectID": "posts/2024-07-22/index.html#concatenating-strings-with-stringi",
    "href": "posts/2024-07-22/index.html#concatenating-strings-with-stringi",
    "title": "How to Concatenate Strings in R",
    "section": "Concatenating Strings with stringi",
    "text": "Concatenating Strings with stringi\nThe stringi package is another powerful tool for string manipulation in R. For concatenation, we use the stri_c() function.\n\nUsing stri_c()\nThe stri_c() function works similarly to paste0() and str_c().\n\n# Load stringi package\nlibrary(stringi)\n\n# Example\nresult_stri_c &lt;- stri_c(string1, string2)\nprint(result_stri_c)  # Output: \"HelloWorld\"\n\n[1] \"HelloWorld\"\n\n\nThis joins “Hello” and “World” without spaces.\n\n\nCustom Separator\nTo include a separator, use the sep argument in stri_c().\n\n# Example with separator\nresult_stri_c_sep &lt;- stri_c(string1, string2, sep = \" \")\nprint(result_stri_c_sep)  # Output: \"Hello World\"\n\n[1] \"Hello World\"\n\n\nThe sep argument adds a space between the strings."
  },
  {
    "objectID": "posts/2024-07-23/index.html",
    "href": "posts/2024-07-23/index.html",
    "title": "Checking if a String Contains Multiple Substrings in R",
    "section": "",
    "text": "Hello, fellow R programmers! Today, we’re looking at a practical topic that often comes up when dealing with text data: how to check if a string contains multiple substrings. We’ll cover how to do this in base R, as well as using the stringr and stringi packages. Each approach has its own advantages, so let’s explore them together."
  },
  {
    "objectID": "posts/2024-07-23/index.html#base-r-approach",
    "href": "posts/2024-07-23/index.html#base-r-approach",
    "title": "Checking if a String Contains Multiple Substrings in R",
    "section": "Base R Approach",
    "text": "Base R Approach\nFirst, let’s start with base R. Suppose we have a string and we want to check if it contains both “apple” and “banana”. Here’s how you can do it:\n\n# Our main string\nmain_string &lt;- \"I have an apple and a banana.\"\n\n# Substrings to check\nsubstrings &lt;- c(\"apple\", \"banana\")\n\n# Check if all substrings are in the main string\ncontains_all &lt;- all(sapply(substrings, function(x) grepl(x, main_string)))\n\n# Output the result\ncontains_all\n\n[1] TRUE\n\nsapply(substrings, grepl, x = main_string)\n\n apple banana \n  TRUE   TRUE \n\n\n\nExplanation\n\nmain_string: This is the string we are checking.\nsubstrings: A vector containing the substrings we are looking for.\nsapply(substrings, function(x) grepl(x, main_string)): We use sapply to apply grepl (which checks if a pattern is found in a string) to each substring. This returns a logical vector indicating if each substring is present.\nall(): This function checks if all values in the logical vector are TRUE.\n\nBy combining these functions, we can efficiently check if all the substrings are present in our main string."
  },
  {
    "objectID": "posts/2024-07-23/index.html#using-stringr",
    "href": "posts/2024-07-23/index.html#using-stringr",
    "title": "Checking if a String Contains Multiple Substrings in R",
    "section": "Using stringr",
    "text": "Using stringr\nThe stringr package provides a set of functions designed to make string manipulation easier and more intuitive. Here’s how we can use it to achieve the same goal:\n\n# Load the stringr package\nlibrary(stringr)\n\n# Our main string\nmain_string &lt;- \"I have an apple and a banana.\"\n\n# Substrings to check\nsubstrings &lt;- c(\"apple\", \"banana\")\n\n# Check if all substrings are in the main string\ncontains_all &lt;- all(str_detect(main_string, substrings))\n\n# Output the result\ncontains_all\n\n[1] TRUE\n\nstr_detect(main_string, substrings)\n\n[1] TRUE TRUE\n\n\n\nExplanation\n\nlibrary(stringr): Loads the stringr package.\nstr_detect(main_string, substrings): The str_detect function checks if each pattern in substrings is found in main_string. It returns a logical vector.\nall(): As before, all checks if all values in the logical vector are TRUE.\n\nThe stringr package simplifies the syntax and makes the code more readable."
  },
  {
    "objectID": "posts/2024-07-23/index.html#using-stringi",
    "href": "posts/2024-07-23/index.html#using-stringi",
    "title": "Checking if a String Contains Multiple Substrings in R",
    "section": "Using stringi",
    "text": "Using stringi\nThe stringi package is another powerful tool for string manipulation. It offers a highly efficient way to handle strings. Here’s how we can use stringi to check for multiple substrings:\n\n# Load the stringi package\nlibrary(stringi)\n\n# Our main string\nmain_string &lt;- \"I have an apple and a banana.\"\n\n# Substrings to check\nsubstrings &lt;- c(\"apple\", \"banana\")\n\n# Check if all substrings are in the main string\ncontains_all &lt;- all(stri_detect_fixed(main_string, substrings))\n\n# Output the result\ncontains_all\n\n[1] TRUE\n\nstri_detect_fixed(main_string, substrings)\n\n[1] TRUE TRUE\n\n\n\nExplanation\n\nlibrary(stringi): Loads the stringi package.\nstri_detect_fixed(main_string, substrings): The stri_detect_fixed function checks if each fixed pattern in substrings is found in main_string. This function is optimized for fixed patterns and is very fast.\nall(): Again, we use all to check if all values in the logical vector are TRUE.\n\nstringi provides highly optimized functions that can be very useful for handling large datasets or performance-critical applications."
  },
  {
    "objectID": "posts/2024-07-24/index.html",
    "href": "posts/2024-07-24/index.html",
    "title": "Getting the Workbook Name in VBA and Calling It from R",
    "section": "",
    "text": "When working with Excel, it’s often useful to know the name of the workbook you’re working in, especially if you’re managing multiple files. Today, we’ll look at how to retrieve the workbook name using VBA (Visual Basic for Applications) and then call this VBA code from R. This post will walk you through the steps with clear examples and explanations. Let’s get to it!"
  },
  {
    "objectID": "posts/2024-07-24/index.html#getting-the-workbook-name-using-vba",
    "href": "posts/2024-07-24/index.html#getting-the-workbook-name-using-vba",
    "title": "Getting the Workbook Name in VBA and Calling It from R",
    "section": "Getting the Workbook Name Using VBA",
    "text": "Getting the Workbook Name Using VBA\nFirst, we’ll start with a simple VBA script to get the workbook name. VBA is a powerful tool integrated into Microsoft Office applications, allowing you to automate tasks and interact with various elements in your documents.\nHere’s a basic example of VBA code that retrieves the name of the active workbook:\nSub GetWorkbookName()\n    Dim wbName As String\n    wbName = ThisWorkbook.Name\n    MsgBox \"The name of the active workbook is: \" & wbName\nEnd Sub\nExplanation:\n\nSub GetWorkbookName(): This line defines a new subroutine named GetWorkbookName. A subroutine in VBA is a block of code that performs a specific task.\nDim wbName As String: This line declares a variable wbName that will hold the workbook’s name as a string.\nwbName = ThisWorkbook.Name: Here, we’re assigning the name of the active workbook (the one where this VBA code is being run) to the wbName variable.\nMsgBox “The name of the active workbook is:” & wbName: Finally, we use a message box to display the workbook name."
  },
  {
    "objectID": "posts/2024-07-24/index.html#calling-vba-code-from-r",
    "href": "posts/2024-07-24/index.html#calling-vba-code-from-r",
    "title": "Getting the Workbook Name in VBA and Calling It from R",
    "section": "Calling VBA Code from R",
    "text": "Calling VBA Code from R\nNow that we have our VBA macro, the next step is to call it from R. This is particularly useful if you’re integrating Excel operations into your R workflows.\nWe’ll use the RDCOMClient package in R, which allows us to interact with COM (Component Object Model) objects, such as Excel. If you haven’t installed this package, you can do so with:\ninstall.packages(\"RDCOMClient\")\nHere’s a simple R script to call our VBA subroutine:\nlibrary(RDCOMClient)\n\n# Create an instance of the Excel application\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# File Path\nf_path &lt;- \"C:/path_to/workbook_name.xlsm\"\n\n# Open the workbook (replace 'f_path' with the actual path)\nworkbook &lt;- excel_app$Workbooks()$Open(f_path)\n\n# Run the VBA macro\nexcel_app$Run(\"GetWorkbookName\")\n\n# Close the workbook without saving changes\nworkbook$Close(FALSE)\n\n# Quit Excel\nexcel_app$Quit()\n\n# Release the object\nrm(excel_app)\nExplanation:\n\nlibrary(RDCOMClient): This line loads the RDCOMClient package.\nCOMCreate(“Excel.Application”): We create an instance of the Excel application.\nworkbook &lt;- excel_app\\(Workbooks()\\)Open(“f_path”): This line opens the specified workbook. Replace \"f_path\" with the path to your actual Excel file.\nexcel_app$Run(“GetWorkbookName”): Here, we call the VBA subroutine GetWorkbookName to display the workbook’s name.\nworkbook$Close(FALSE): We close the workbook without saving any changes.\nexcel_app$Quit(): This closes the Excel application.\nrm(excel_app): Finally, we release the Excel application object to free up resources.\n\nHere is a picture of the message:\n\n\n\nVBA Workbook Name"
  },
  {
    "objectID": "posts/2024-07-25/index.html",
    "href": "posts/2024-07-25/index.html",
    "title": "Mastering Wildcard Searches in R with grep()",
    "section": "",
    "text": "In R, finding patterns in text is a common task, and one of the most powerful functions to do this is grep(). This function is used to search for patterns in strings, allowing you to locate elements that match a specific pattern. Today, we’ll explore how to use wildcard characters with grep() to enhance your string searching capabilities. Let’s dive in!\n\n\nAt its core, grep() is a function that searches for matches to a pattern (regular expression) within a vector of strings. It returns the indices of the elements that contain the pattern. Here’s a basic syntax:\ngrep(pattern, x, ignore.case = FALSE, value = FALSE)\n\npattern: A character string containing a regular expression.\nx: A character vector where the search is performed.\nignore.case: If TRUE, the search will be case-insensitive.\nvalue: If TRUE, grep() returns the matching elements instead of their indices.\n\n\n\nWildcard characters are incredibly useful in searching for patterns that may not be exactly known. In regular expressions, which grep() uses, wildcards are represented in specific ways:\n\n^: Asserts the start of a string.\n$: Asserts the end of a string.\n.: Matches any single character.\n.*: Matches any number of any characters (including none).\n\nLet’s look at some practical examples to see these in action!"
  },
  {
    "objectID": "posts/2024-07-25/index.html#understanding-grep",
    "href": "posts/2024-07-25/index.html#understanding-grep",
    "title": "Mastering Wildcard Searches in R with grep()",
    "section": "",
    "text": "At its core, grep() is a function that searches for matches to a pattern (regular expression) within a vector of strings. It returns the indices of the elements that contain the pattern. Here’s a basic syntax:\ngrep(pattern, x, ignore.case = FALSE, value = FALSE)\n\npattern: A character string containing a regular expression.\nx: A character vector where the search is performed.\nignore.case: If TRUE, the search will be case-insensitive.\nvalue: If TRUE, grep() returns the matching elements instead of their indices.\n\n\n\nWildcard characters are incredibly useful in searching for patterns that may not be exactly known. In regular expressions, which grep() uses, wildcards are represented in specific ways:\n\n^: Asserts the start of a string.\n$: Asserts the end of a string.\n.: Matches any single character.\n.*: Matches any number of any characters (including none).\n\nLet’s look at some practical examples to see these in action!"
  },
  {
    "objectID": "posts/2024-07-25/index.html#strings-that-start-with-a-pattern",
    "href": "posts/2024-07-25/index.html#strings-that-start-with-a-pattern",
    "title": "Mastering Wildcard Searches in R with grep()",
    "section": "Strings that Start with a Pattern",
    "text": "Strings that Start with a Pattern\nTo find strings that start with a specific pattern, use ^ at the beginning of your pattern. For instance, if you’re looking for words starting with “data”:\n\nwords &lt;- c(\"data\", \"dataframe\", \"database\", \"analytics\", \"visualization\")\ngrep(\"^data\", words)\n\n[1] 1 2 3\n\n\nThis code will return the indices of “data”, “dataframe”, and “database” because they all start with “data”. If you set value = TRUE, it will return the matching elements:\n\ngrep(\"^data\", words, value = TRUE)\n\n[1] \"data\"      \"dataframe\" \"database\""
  },
  {
    "objectID": "posts/2024-07-25/index.html#strings-that-end-with-a-pattern",
    "href": "posts/2024-07-25/index.html#strings-that-end-with-a-pattern",
    "title": "Mastering Wildcard Searches in R with grep()",
    "section": "Strings that End with a Pattern",
    "text": "Strings that End with a Pattern\nTo find strings ending with a certain pattern, use $ at the end of your pattern. For example, to find words ending with “base”:\n\ngrep(\"base$\", words, value = TRUE)\n\n[1] \"database\""
  },
  {
    "objectID": "posts/2024-07-25/index.html#strings-that-contain-a-pattern",
    "href": "posts/2024-07-25/index.html#strings-that-contain-a-pattern",
    "title": "Mastering Wildcard Searches in R with grep()",
    "section": "Strings that Contain a Pattern",
    "text": "Strings that Contain a Pattern\nTo find strings containing a pattern anywhere within them, use the pattern directly. For example, to find words containing “viz”:\n\nwords &lt;- c(\"data\", \"visualization\", \"database\", \"analyze\", \"predict\")\ngrep(\"vis\", words, value = TRUE)\n\n[1] \"visualization\""
  },
  {
    "objectID": "posts/2024-07-25/index.html#combining-patterns-with-.",
    "href": "posts/2024-07-25/index.html#combining-patterns-with-.",
    "title": "Mastering Wildcard Searches in R with grep()",
    "section": "Combining Patterns with .*",
    "text": "Combining Patterns with .*\nThe combination of .* can be used to match any number of characters, making it useful for finding patterns within strings. For instance, to find words containing “a” followed by “z”:\n\ngrep(\"a.*z\", words, value = TRUE)\n\n[1] \"visualization\" \"analyze\""
  },
  {
    "objectID": "posts/2024-07-26/index.html",
    "href": "posts/2024-07-26/index.html",
    "title": "Creating Summary Tables in R with tidyquant and dplyr",
    "section": "",
    "text": "Creating summary tables is a key part of data analysis, allowing you to see trends and patterns in your data. In this post, we’ll explore how to create these tables using tidyquant and dplyr in R. These packages make it easy to manipulate and summarize your data."
  },
  {
    "objectID": "posts/2024-07-26/index.html#using-tidyquant-for-summary-tables",
    "href": "posts/2024-07-26/index.html#using-tidyquant-for-summary-tables",
    "title": "Creating Summary Tables in R with tidyquant and dplyr",
    "section": "Using tidyquant for Summary Tables",
    "text": "Using tidyquant for Summary Tables\ntidyquant is a versatile package that extends the tidyverse for financial and time series analysis. It simplifies working with data by integrating tidy principles.\n\nExample: Calculating Average Price by Month\nHere’s an example of how to calculate the average price by month using tidyquant:\n\n# Load necessary libraries\nlibrary(tidyquant)\nlibrary(dplyr)\n\n# Sample data: Daily stock prices\ndata &lt;- tibble(\n  date = seq(as.Date('2023-01-01'), as.Date('2023-06-30'), by = 'day'),\n  price = runif(181, 100, 200)\n)\n\n# Create a summary table with average closing price by month\nsummary_table &lt;- data |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  pivot_table(\n    .rows = month, \n    .values = ~ mean(price, na.rm = TRUE)\n  ) |&gt;\n  setNames(c(\"date\", \"avg_price\"))\n\nprint(summary_table)\n\n# A tibble: 6 × 2\n  date       avg_price\n  &lt;date&gt;         &lt;dbl&gt;\n1 2023-01-01      149.\n2 2023-02-01      162.\n3 2023-03-01      151.\n4 2023-04-01      151.\n5 2023-05-01      145.\n6 2023-06-01      149.\n\n\nIn this example:\n\ntidyquant and tibble are loaded to handle data manipulation.\nWe create a sample dataset with daily stock prices.\nThe mutate function adds a new column month, which extracts the month from each date.\npivot_table calculates the average price for each month.\nFinally, we rename the columns for clarity."
  },
  {
    "objectID": "posts/2024-07-26/index.html#using-dplyr-for-summary-tables",
    "href": "posts/2024-07-26/index.html#using-dplyr-for-summary-tables",
    "title": "Creating Summary Tables in R with tidyquant and dplyr",
    "section": "Using dplyr for Summary Tables",
    "text": "Using dplyr for Summary Tables\ndplyr is a core tidyverse package known for its powerful data manipulation functions. It helps streamline the process of filtering, summarizing, and mutating data.\n\nExample: Calculating Average Closing Price by Month\nHere’s a similar example using dplyr:\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Sample data: Daily stock prices\ndata &lt;- tibble(\n  date = seq(as.Date('2023-01-01'), as.Date('2023-06-30'), by = 'day'),\n  price = runif(181, 100, 200)\n)\n\n# Create a summary table with average closing price by month\nsummary_table &lt;- data %&gt;%\n  mutate(month = floor_date(date, \"month\")) %&gt;%\n  group_by(month) %&gt;%\n  summarise(avg_close = mean(price))\n\nprint(summary_table)\n\n# A tibble: 6 × 2\n  month      avg_close\n  &lt;date&gt;         &lt;dbl&gt;\n1 2023-01-01      149.\n2 2023-02-01      140.\n3 2023-03-01      147.\n4 2023-04-01      146.\n5 2023-05-01      147.\n6 2023-06-01      151.\n\n\nIn this dplyr example:\n\nWe load dplyr and lubridate for data manipulation and date handling.\nThe dataset creation process is the same.\nThe mutate function is used to add a month column.\nWe group the data by month using group_by and then calculate the average closing price for each group using summarise."
  },
  {
    "objectID": "posts/2024-07-29/index.html",
    "href": "posts/2024-07-29/index.html",
    "title": "Stratified Sampling in R: A Practical Guide with Base R and dplyr",
    "section": "",
    "text": "Stratified sampling is a technique used to ensure that different subgroups (strata) within a population are represented in a sample. This method is particularly useful when certain strata are underrepresented in a simple random sample. In this post, we’ll explore how to perform stratified sampling in R using both base R and the dplyr package. We’ll walk through examples and explain the code, so you can try these techniques on your own data."
  },
  {
    "objectID": "posts/2024-07-29/index.html#stratified-sampling-with-base-r",
    "href": "posts/2024-07-29/index.html#stratified-sampling-with-base-r",
    "title": "Stratified Sampling in R: A Practical Guide with Base R and dplyr",
    "section": "Stratified Sampling with Base R",
    "text": "Stratified Sampling with Base R\nLet’s start with an example using base R. Suppose we have a dataset with information about individuals, including their gender and income. We want to sample a specific number of individuals from each gender group.\nHere’s how we can do it:\n\n# Sample data\nset.seed(123) # For reproducibility\ndata &lt;- data.frame(\n  ID = 1:100,\n  Gender = sample(c(\"Male\", \"Female\"), 100, replace = TRUE),\n  Income = rnorm(100, mean = 50000, sd = 10000)\n)\n\n# View the first few rows of the data\nhead(data)\n\n  ID Gender   Income\n1  1   Male 52533.19\n2  2   Male 49714.53\n3  3   Male 49571.30\n4  4 Female 63686.02\n5  5   Male 47742.29\n6  6 Female 65164.71\n\n\nIn this dataset, we have a column for Gender and another for Income. Let’s say we want to sample 10 males and 10 females.\n\n# Stratified sampling function\nstratified_sample &lt;- function(data, strat_column, size_per_stratum) {\n  strata &lt;- unique(data[[strat_column]])\n  sampled_data &lt;- do.call(rbind, lapply(strata, function(stratum) {\n    subset_data &lt;- data[data[[strat_column]] == stratum, ]\n    subset_data[sample(nrow(subset_data), size_per_stratum), ]\n  }))\n  return(sampled_data)\n}\n\n# Perform stratified sampling\nsampled_data &lt;- stratified_sample(data, \"Gender\", 10)\n\n# View the sampled data\ntable(sampled_data$Gender)\n\n\nFemale   Male \n    10     10 \n\nhead(sampled_data)\n\n     ID Gender   Income\n45   45   Male 63606.52\n69   69   Male 41502.96\n83   83   Male 50412.33\n29   29   Male 51813.03\n49   49   Male 47643.00\n100 100   Male 37129.70\n\n\nIn this example:\n\nWe first create a function stratified_sample that takes the data, the column to stratify by, and the number of samples per stratum.\nThe function identifies unique strata, then samples the specified number of rows from each stratum.\nThe result is a combined dataset with samples from each group."
  },
  {
    "objectID": "posts/2024-07-29/index.html#stratified-sampling-with-dplyr",
    "href": "posts/2024-07-29/index.html#stratified-sampling-with-dplyr",
    "title": "Stratified Sampling in R: A Practical Guide with Base R and dplyr",
    "section": "Stratified Sampling with dplyr",
    "text": "Stratified Sampling with dplyr\n\nUsing sample_n\nThe dplyr package makes data manipulation straightforward and efficient. Here’s how to do stratified sampling using dplyr:\n\nlibrary(dplyr)\n\n# Stratified sampling with sample_n()\nsampled_data_n &lt;- data %&gt;%\n  group_by(Gender) %&gt;%\n  sample_n(10)\n\n# View the sampled data\nsampled_data_n %&gt;% count(Gender)\n\n# A tibble: 2 × 2\n# Groups:   Gender [2]\n  Gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Female    10\n2 Male      10\n\nhead(sampled_data_n)\n\n# A tibble: 6 × 3\n# Groups:   Gender [1]\n     ID Gender Income\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1    81 Female 64446.\n2     6 Female 65165.\n3     8 Female 55846.\n4    22 Female 26908.\n5    98 Female 56879.\n6    11 Female 53796.\n\n\nIn this approach:\n\nWe use group_by() to group the data by the Gender column.\nsample_n() is used to take 10 samples from each group.\ncount() helps us verify the number of samples from each group.\n\n\n\nUsing sample_frac() for Proportional Sampling\nIf you want to sample a proportion of each stratum, you can use the sample_frac() function. For example, if you want to sample 20% of each gender group:\n\n# Stratified sampling with sample_frac()\nsampled_data_frac &lt;- data %&gt;%\n  group_by(Gender) %&gt;%\n  sample_frac(0.2)\n\n# View the sampled data\nsampled_data_frac %&gt;% count(Gender)\n\n# A tibble: 2 × 2\n# Groups:   Gender [2]\n  Gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Female     9\n2 Male      11\n\nhead(sampled_data_frac)\n\n# A tibble: 6 × 3\n# Groups:   Gender [1]\n     ID Gender Income\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1    71 Female 51176.\n2    92 Female 47378.\n3    13 Female 46668.\n4    48 Female 65326.\n5    42 Female 55484.\n6    76 Female 43481.\n\n\nIn this example:\n\nsample_frac() is used to take 20% of the rows from each group.\nThis is useful when you want the sample size to be proportional to the size of each stratum."
  },
  {
    "objectID": "posts/2024-07-30/index.html",
    "href": "posts/2024-07-30/index.html",
    "title": "Mastering String Conversion to Lowercase in R",
    "section": "",
    "text": "In data analysis and manipulation, handling text data is a common task. One of the essential operations you might need to perform is converting strings to lowercase. In R, this is easily done using the tolower() function. Let’s explore how to convert your text data into lowercase, along with practical examples and a real-world use case."
  },
  {
    "objectID": "posts/2024-07-30/index.html#example-1-converting-a-single-string",
    "href": "posts/2024-07-30/index.html#example-1-converting-a-single-string",
    "title": "Mastering String Conversion to Lowercase in R",
    "section": "Example 1: Converting a Single String",
    "text": "Example 1: Converting a Single String\n\ntext &lt;- \"Hello World!\"\nlower_text &lt;- tolower(text)\nprint(lower_text)\n\n[1] \"hello world!\""
  },
  {
    "objectID": "posts/2024-07-30/index.html#example-2-converting-a-vector-of-strings",
    "href": "posts/2024-07-30/index.html#example-2-converting-a-vector-of-strings",
    "title": "Mastering String Conversion to Lowercase in R",
    "section": "Example 2: Converting a Vector of Strings",
    "text": "Example 2: Converting a Vector of Strings\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Cherry\")\nlower_fruits &lt;- tolower(fruits)\nprint(lower_fruits)\n\n[1] \"apple\"  \"banana\" \"cherry\""
  },
  {
    "objectID": "posts/2024-07-30/index.html#example-3-handling-mixed-case-strings",
    "href": "posts/2024-07-30/index.html#example-3-handling-mixed-case-strings",
    "title": "Mastering String Conversion to Lowercase in R",
    "section": "Example 3: Handling Mixed Case Strings",
    "text": "Example 3: Handling Mixed Case Strings\n\nmixed_case &lt;- \"ThiS Is A MiXeD CaSe StrIng.\"\nlower_case &lt;- tolower(mixed_case)\nprint(lower_case)\n\n[1] \"this is a mixed case string.\""
  },
  {
    "objectID": "posts/2024-07-30/index.html#practical-use-checking-users-favorite-color",
    "href": "posts/2024-07-30/index.html#practical-use-checking-users-favorite-color",
    "title": "Mastering String Conversion to Lowercase in R",
    "section": "Practical Use: Checking User’s Favorite Color",
    "text": "Practical Use: Checking User’s Favorite Color\nA practical application of converting strings to lowercase is in user input validation. Let’s consider a simple function that checks a user’s favorite color and responds accordingly. By converting the input to lowercase, we can ensure that the function handles different cases uniformly.\nHere’s the function:\n\n# Function to check user's favorite color\ncheck_favorite_color &lt;- function(color) {\n  color &lt;- tolower(color)  # Convert input to lowercase\n  if (color == \"blue\") {\n    return(\"Blue is my favorite color!\")\n  } else if (color == \"red\") {\n    return(\"Red is not a good choice!\")\n  } else {\n    return(\"That's a nice color too!\")\n  }\n}\n\n# Test the function\nprint(check_favorite_color(\"BLUE\"))  # Works with uppercase\n\n[1] \"Blue is my favorite color!\"\n\nprint(check_favorite_color(\"Red\"))   # Works with mixed case\n\n[1] \"Red is not a good choice!\"\n\nprint(check_favorite_color(\"green\")) # Works with lowercase\n\n[1] \"That's a nice color too!\"\n\n\nIn this function, we use tolower() to ensure that the input is in lowercase, making it easier to compare against predefined color choices. This approach helps handle inputs consistently, regardless of how the user types them.\n\nUnderstanding the Code\nThe tolower() function converts uppercase characters to lowercase in a given string or vector of strings. It only affects alphabetic characters, leaving other characters unchanged. This makes it an essential tool for standardizing text data."
  },
  {
    "objectID": "posts/2024-07-31/index.html",
    "href": "posts/2024-07-31/index.html",
    "title": "How to List All Open Workbooks Using VBA and Call It from R",
    "section": "",
    "text": "Hello, fellow R useRs! Today, we’re going to discuss a fascinating topic that bridges the gap between VBA (Visual Basic for Applications) and R. We’ll explore how to get a list of all open workbooks in Excel using VBA and then call this VBA code from R. This can be particularly useful if you’re working with multiple Excel files and need to manage them efficiently from R."
  },
  {
    "objectID": "posts/2024-07-31/index.html#step-1-writing-the-vba-code",
    "href": "posts/2024-07-31/index.html#step-1-writing-the-vba-code",
    "title": "How to List All Open Workbooks Using VBA and Call It from R",
    "section": "Step 1: Writing the VBA Code",
    "text": "Step 1: Writing the VBA Code\nFirst, let’s write a simple VBA macro to list all open workbooks. Open Excel, press Alt + F11 to open the VBA editor, and insert a new module. Here’s the VBA code:\nSub ListAllOpenWorkbooks()\n    Dim wb As Workbook\n    Dim wbNames As String\n    wbNames = \"Open Workbooks:\" & vbCrLf\n    \n    For Each wb In Application.Workbooks\n        wbNames = wbNames & wb.Name & vbCrLf\n    Next wb\n    \n    MsgBox wbNames\nEnd Sub\n\nExplanation:\n\nSub ListAllOpenWorkbooks(): This starts our macro.\nDim wb As Workbook: Declares a variable wb to represent each workbook.\nDim wbNames As String: Declares a string variable to store the names of open workbooks.\nFor Each wb In Application.Workbooks: Loops through each open workbook.\nwbNames = wbNames & wb.Name & vbCrLf: Appends the name of each workbook to the wbNames string.\nMsgBox wbNames: Displays the names of all open workbooks in a message box."
  },
  {
    "objectID": "posts/2024-07-31/index.html#step-2-saving-the-vba-macro",
    "href": "posts/2024-07-31/index.html#step-2-saving-the-vba-macro",
    "title": "How to List All Open Workbooks Using VBA and Call It from R",
    "section": "Step 2: Saving the VBA Macro",
    "text": "Step 2: Saving the VBA Macro\nSave your VBA macro by clicking File &gt; Save. Make sure to save your Excel file as a macro-enabled workbook (.xlsm)."
  },
  {
    "objectID": "posts/2024-07-31/index.html#step-3-calling-the-vba-macro-from-r",
    "href": "posts/2024-07-31/index.html#step-3-calling-the-vba-macro-from-r",
    "title": "How to List All Open Workbooks Using VBA and Call It from R",
    "section": "Step 3: Calling the VBA Macro from R",
    "text": "Step 3: Calling the VBA Macro from R\nNow, let’s move to R. We’ll use the RDCOMClient package to interact with Excel and call our VBA macro. If you haven’t installed this package yet, you can do so using:\ninstall.packages(\"RDCOMClient\", repos = \"http://www.omegahat.net/R\")\nHere’s the R code to call our VBA macro:\nlibrary(RDCOMClient)\n\n# Create a COM object to interact with Excel\nexcel_app &lt;- COMCreate(\"Excel.Application\")\n\n# Make Excel visible (optional)\nexcel_app[[\"Visible\"]] &lt;- TRUE\n\n# Open the workbook containing the VBA macro\nworkbook &lt;- excel_app[[\"Workbooks\"]]$Open(\"C:\\\\path\\\\to\\\\your\\\\workbook.xlsm\")\n\n# Run the VBA macro\nexcel_app$Run(\"ListAllOpenWorkbooks\")\n\n# Close the workbook without saving\nworkbook$Close(FALSE)\n\n# Quit Excel\nexcel_app$Quit()\n\nExplanation:\n\nlibrary(RDCOMClient): Loads the RDCOMClient package.\nCOMCreate(“Excel.Application”): Creates a COM object to interact with Excel.\nexcel_app[[“Visible”]] &lt;- TRUE: Makes Excel visible (optional).\nexcel_app[[“Workbooks”]]$Open(“C:.xlsm”): Opens the workbook containing the VBA macro. Replace “C:\\path\\to\\your\\workbook.xlsm” with the actual path to your workbook.\nexcel_app$Run(“ListAllOpenWorkbooks”): Runs the VBA macro.\nworkbook$Close(FALSE): Closes the workbook without saving changes.\nexcel_app$Quit(): Quits Excel.\n\nHere are some sample outputs for me:\n\n\n\nMany Open Workbooks\n\n\n\n\n\nTwo Open Workbook"
  },
  {
    "objectID": "posts/2024-08-01/index.html",
    "href": "posts/2024-08-01/index.html",
    "title": "Automate Your Blog Workflow with a Custom R Function: Creating QMD Files",
    "section": "",
    "text": "As a blogger who uses R for content creation, I’ve found it incredibly useful to automate some of the repetitive tasks. One such task is creating Quarto Markdown (QMD) files for new blog posts. To simplify this, I’ve added a custom R function that not only creates the necessary file structure. Let’s take a look at this function and how you can integrate it into your own workflow."
  },
  {
    "objectID": "posts/2024-08-01/index.html#how-it-works",
    "href": "posts/2024-08-01/index.html#how-it-works",
    "title": "Automate Your Blog Workflow with a Custom R Function: Creating QMD Files",
    "section": "How It Works",
    "text": "How It Works\n\nSetting the Base Path: The function starts by defining the base path where blog posts will be stored, appending \"/posts\" to the current working directory. This centralizes all posts in one location.\nCreating Directories: It then converts the date to a string and uses it to create a directory path. If this directory doesn’t exist, the function creates it. This helps in organizing posts by date.\nFile Path Definition: The function then defines the full path for the QMD file, defaulting the filename to “index.qmd” if none is provided.\nContent Creation: The main content for the QMD file is generated next. This includes a YAML front matter section with metadata like title, author, date, and categories. The function also adds a script for Giscus, which handles the comments section.\nFile Writing: Finally, the function writes the generated content to the specified file path and informs you that the file has been created."
  },
  {
    "objectID": "posts/2024-08-01/index.html#automating-with-.rprofile",
    "href": "posts/2024-08-01/index.html#automating-with-.rprofile",
    "title": "Automate Your Blog Workflow with a Custom R Function: Creating QMD Files",
    "section": "Automating with .Rprofile",
    "text": "Automating with .Rprofile\nTo make this function available every time you start your project, you can use the .Rprofile file. This file is sourced whenever you start a new R session, making it perfect for setting up your environment.\nHere’s the relevant .Rprofile setup:\nsource(paste0(getwd(),\"/create_qmd_file.R\"))\nBy sourcing the create_qmd_file.R script, the function is loaded automatically, so you don’t have to manually source it each time."
  }
]